[{"nb_reuse": 6, "tools": ["BEDTools"], "nb_own": 3, "list_own": ["clairecoleman1", "nf-core", "oisinmccaffrey"], "nb_wf": 3, "list_wf": ["clipseq.nextflow", "clipseq", "clipseq1"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "CharlotteAnne", "drpatelh", "clairecoleman1", "oisinmccaffrey"], "nb_contrib": 8, "codes": [" process paraclu_motif_dreme {\n            tag \"$name\"\n            label 'process_low'\n            publishDir \"${params.outdir}/paraclu_motif\", mode: params.publish_dir_mode\n\n            when:\n            'paraclu' in callers\n\n            input:\n            tuple val(name), path(peaks) from ch_peaks_paraclu\n            path(fasta) from ch_fasta_dreme_paraclu.collect()\n            path(fai) from ch_fai_paraclu_motif.collect()\n\n            output:\n            tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_paraclu\n\n            script:\n            motif_sample = params.motif_sample\n            \"\"\"\n            pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n            bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n            shuf -n $motif_sample > resized_peaks.bed\n\n            bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n\n            dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n            \"\"\"\n        }", " process pureclip_motif_dreme {\n            tag \"$name\"\n            label 'process_low'\n            publishDir \"${params.outdir}/pureclip_motif\", mode: params.publish_dir_mode\n\n            input:\n            tuple val(name), path(peaks) from ch_peaks_pureclip\n            path(fasta) from ch_fasta_dreme_pureclip.collect()\n            path(fai) from ch_fai_pureclip_motif.collect()\n\n            output:\n            tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_pureclip\n\n            script:\n            motif_sample = params.motif_sample\n            \"\"\"\n            pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n            bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n            shuf -n $motif_sample > resized_peaks.bed\n\n            bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n\n            dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n            \"\"\"\n        }", " process piranha_motif_dreme {\n            tag \"$name\"\n            label 'process_low'\n            publishDir \"${params.outdir}/piranha_motif\", mode: params.publish_dir_mode\n\n            input:\n            tuple val(name), path(peaks) from ch_peaks_piranha\n            path(fasta) from ch_fasta_dreme_piranha.collect()\n            path(fai) from ch_fai_piranha_motif.collect()\n\n            output:\n            tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_piranha\n\n            script:\n            motif_sample = params.motif_sample\n            \"\"\"\n            pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n            bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n            shuf -n $motif_sample > resized_peaks.bed\n\n            bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n\n            dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n            \"\"\"\n        }", "\nprocess piranha_motif_dreme {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/piranha_motif\", mode: 'copy'\n\n        input:\n        tuple val(name), path(peaks) from ch_peaks_piranha\n        path(fasta) from ch_fasta_dreme_piranha.collect()\n        path(fai) from ch_fai_piranha_motif.collect()\n\n        output:\n         tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_piranha\n\n        script:\n        motif_sample = params.motif_sample\n        \"\"\"\n        pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n        bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n        shuf -n $motif_sample > resized_peaks.bed\n        bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n        dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n        \"\"\"\n }", "\nprocess piranha_motif_dreme {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/piranha_motif\", mode: 'copy'\n\n        input:\n        tuple val(name), path(peaks) from ch_peaks_piranha\n        path(fasta) from ch_fasta_dreme_piranha.collect()\n        path(fai) from ch_fai_piranha_motif.collect()\n\n        output:\n         tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_piranha\n\n        script:\n        motif_sample = params.motif_sample\n        \"\"\"\n        pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n        bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n        shuf -n $motif_sample > resized_peaks.bed\n        bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n        dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n        \"\"\"\n }", " process icount_motif_dreme {\n            tag \"$name\"\n            label 'process_low'\n            publishDir \"${params.outdir}/icount_motif\", mode: params.publish_dir_mode\n\n            input:\n            tuple val(name), path(peaks) from ch_peaks_icount\n            path(fasta) from ch_fasta_dreme_icount.collect()\n            path(fai) from ch_fai_icount_motif.collect()\n\n            output:\n            tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_icount\n\n            script:\n            motif_sample = params.motif_sample\n            \"\"\"\n            pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n            bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n            shuf -n $motif_sample > resized_peaks.bed\n\n            bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n\n            dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n            \"\"\"\n        }"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/paraclu_motif_dreme", "nf-core/clipseq/nf-core__clipseq/pureclip_motif_dreme", "nf-core/clipseq/nf-core__clipseq/piranha_motif_dreme", "clairecoleman1/clipseq1/clairecoleman1__clipseq1/piranha_motif_dreme", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/piranha_motif_dreme", "nf-core/clipseq/nf-core__clipseq/icount_motif_dreme"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess fastqc {\n    label 'mc_small'\n    tag \"${libraryid}_L${lane}\"\n    publishDir \"${params.outdir}/fastqc/input_fastq\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n\n    input:\n    tuple samplename, libraryid, lane, colour, seqtype, organism, strandedness, udg, file(r1), file(r2) from ch_convertbam_for_fastqc\n\n    output:\n    path \"*_fastqc.{zip,html}\" into ch_prefastqc_for_multiqc\n\n    when: \n    !params.skip_fastqc\n\n    script:\n    if ( seqtype == 'PE' ) {\n    \"\"\"\n    fastqc -t ${task.cpus} -q $r1 $r2\n    rename 's/_fastqc\\\\.zip\\$/_raw_fastqc.zip/' *_fastqc.zip\n    rename 's/_fastqc\\\\.html\\$/_raw_fastqc.html/' *_fastqc.html\n    \"\"\"\n    } else {\n    \"\"\"\n    fastqc -q $r1\n    rename 's/_fastqc\\\\.zip\\$/_raw_fastqc.zip/' *_fastqc.zip\n    rename 's/_fastqc\\\\.html\\$/_raw_fastqc.html/' *_fastqc.html\n    \"\"\"\n    }\n}"], "list_proc": ["nf-core/eager/nf-core__eager/fastqc"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["blima"], "nb_own": 2, "list_own": ["sguizard", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "isoseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "Alexey-ebi", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "peterwharrison", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process LIMA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::lima=2.2.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/lima:2.2.0--h9ee0642_0' :\n        'quay.io/biocontainers/lima:2.2.0--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(ccs)\n    path primers\n\n    output:\n    tuple val(meta), path(\"*.counts\") , emit: counts\n    tuple val(meta), path(\"*.report\") , emit: report\n    tuple val(meta), path(\"*.summary\"), emit: summary\n    path \"versions.yml\"               , emit: versions\n\n    tuple val(meta), path(\"*.bam\")              , optional: true, emit: bam\n    tuple val(meta), path(\"*.bam.pbi\")          , optional: true, emit: pbi\n    tuple val(meta), path(\"*.{fa, fasta}\")      , optional: true, emit: fasta\n    tuple val(meta), path(\"*.{fa.gz, fasta.gz}\"), optional: true, emit: fastagz\n    tuple val(meta), path(\"*.fastq\")            , optional: true, emit: fastq\n    tuple val(meta), path(\"*.fastq.gz\")         , optional: true, emit: fastqgz\n    tuple val(meta), path(\"*.xml\")              , optional: true, emit: xml\n    tuple val(meta), path(\"*.json\")             , optional: true, emit: json\n    tuple val(meta), path(\"*.clips\")            , optional: true, emit: clips\n    tuple val(meta), path(\"*.guess\")            , optional: true, emit: guess\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if( \"$ccs\" == \"${prefix}.bam\" )      error \"Input and output names are the same, set prefix in module configuration\"\n    if( \"$ccs\" == \"${prefix}.fasta\" )    error \"Input and output names are the same, set prefix in module configuration\"\n    if( \"$ccs\" == \"${prefix}.fasta.gz\" ) error \"Input and output names are the same, set prefix in module configuration\"\n    if( \"$ccs\" == \"${prefix}.fastq\" )    error \"Input and output names are the same, set prefix in module configuration\"\n    if( \"$ccs\" == \"${prefix}.fastq.gz\" ) error \"Input and output names are the same, set prefix in module configuration\"\n\n    \"\"\"\n    OUT_EXT=\"\"\n\n    if [[ $ccs =~ bam\\$ ]]; then\n        OUT_EXT=\"bam\"\n    elif [[ $ccs =~ fasta\\$ ]]; then\n        OUT_EXT=\"fasta\"\n    elif [[ $ccs =~ fasta.gz\\$ ]]; then\n        OUT_EXT=\"fasta.gz\"\n    elif [[ $ccs =~ fastq\\$ ]]; then\n        OUT_EXT=\"fastq\"\n    elif [[ $ccs =~ fastq.gz\\$ ]]; then\n        OUT_EXT=\"fastq.gz\"\n    fi\n\n    lima \\\\\n        $ccs \\\\\n        $primers \\\\\n        $prefix.\\$OUT_EXT \\\\\n        -j $task.cpus \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        lima: \\$( lima --version | sed 's/lima //g' | sed 's/ (.\\\\+//g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess LIMA {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::lima=2.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/lima:2.2.0--h9ee0642_0\"\n    } else {\n        container \"quay.io/biocontainers/lima:2.2.0--h9ee0642_0\"\n    }\n\n    input:\n    tuple val(meta), path(ccs)\n    path primers\n\n    output:\n    tuple val(meta), path(\"*.clips\")  , emit: clips\n    tuple val(meta), path(\"*.counts\") , emit: counts\n    tuple val(meta), path(\"*.guess\")  , emit: guess\n    tuple val(meta), path(\"*.report\") , emit: report\n    tuple val(meta), path(\"*.summary\"), emit: summary\n    path \"versions.yml\"               , emit: versions\n\n    tuple val(meta), path(\"*.bam\")              , optional: true, emit: bam\n    tuple val(meta), path(\"*.bam.pbi\")          , optional: true, emit: pbi\n    tuple val(meta), path(\"*.{fa, fasta}\")      , optional: true, emit: fasta\n    tuple val(meta), path(\"*.{fa.gz, fasta.gz}\"), optional: true, emit: fastagz\n    tuple val(meta), path(\"*.fastq\")            , optional: true, emit: fastq\n    tuple val(meta), path(\"*.fastq.gz\")         , optional: true, emit: fastqgz\n    tuple val(meta), path(\"*.xml\")              , optional: true, emit: xml\n    tuple val(meta), path(\"*.json\")             , optional: true, emit: json\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    OUT_EXT=\"\"\n\n    if [[ $ccs =~ bam\\$ ]]; then\n        OUT_EXT=\"bam\"\n    elif [[ $ccs =~ fasta\\$ ]]; then\n        OUT_EXT=\"fasta\"\n    elif [[ $ccs =~ fasta.gz\\$ ]]; then\n        OUT_EXT=\"fasta.gz\"\n    elif [[ $ccs =~ fastq\\$ ]]; then\n        OUT_EXT=\"fastq\"\n    elif [[ $ccs =~ fastq.gz\\$ ]]; then\n        OUT_EXT=\"fastq.gz\"\n    fi\n\n    echo \\$OUT_EXT\n    lima \\\\\n        $ccs \\\\\n        $primers \\\\\n        $prefix.\\$OUT_EXT \\\\\n        -j $task.cpus \\\\\n        $options.args\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( lima --version | sed 's/lima //g' | sed 's/ (.\\\\+//g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/LIMA", "sguizard/isoseq/sguizard__isoseq/LIMA"], "list_wf_names": ["sguizard/isoseq", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Salmon"], "nb_own": 2, "list_own": ["bhagesh-codebeast", "nf-core"], "nb_wf": 2, "list_wf": ["dualrnaseq", "nextflowdualrnaseq"], "list_contrib": ["lbarquist", "nf-core-bot", "apeltzer", "reganhayward", "bhagesh-codebeast", "bozmik"], "nb_contrib": 6, "codes": ["\tprocess salmon_quantification {\n\t    storeDir \"${params.outdir}/salmon\"\n\t    tag \"${sample}\"\n\n            label 'process_high'\n\n\t    input:\n\t    file(index) from salmon_index.collect()\n\t    set val(sample), file(reads) from trimming_results_to_salmon\n\t    val(libtype) from libtype_salmon\n\n\t    output:\n\t    set val(sample_name), file(\"${sample_name}\") into split_table\n\t    set val(sample_name), file(\"${sample_name}\") into split_table_uniq_ambig\n\t    file(\"${sample_name}\") into salmon_files_to_combine\n\t    file(\"${sample_name}\") into multiqc_salmon_quant\n\t    set val(sample_name), file(\"${sample_name}\") into collect_processed_read_counts\n\n\t    script:\n\t    UnmappedNames = params.writeUnmappedNames ? \"--writeUnmappedNames\" : ''\n\t    softclip = params.softclipOverhangs ? \"--softclipOverhangs\" : ''\n\t    incompatPrior = params.incompatPrior\n\t    dumpEq = params.dumpEq ? \"--dumpEq\" : ''\n\t    salmon_sa_params_mapping = params.salmon_sa_params_mapping \n\t    if (params.single_end){\n\t    \tsample_name = sample.replaceFirst(/.fastq.gz|.fq.gz|.fastq|.fq/, \"\")\n\t\twriteMappings = params.writeMappings ? \"--writeMappings=$sample_name/mapping.sam\" : ''\n\t\t\"\"\"\n\t\tsalmon quant -p ${task.cpus} -i $index -l $libtype -r $reads $softclip --incompatPrior $incompatPrior $UnmappedNames --validateMappings $dumpEq $writeMappings -o $sample_name $salmon_sa_params_mapping\n\t\t\"\"\"\n\t    } else{\n\t\tsample_name = sample.replaceFirst(/.fastq.gz|.fq.gz|.fastq|.fq/, \"\")\n\t\twriteMappings = params.writeMappings ? \"--writeMappings=$sample_name/mapping.sam\" : ''\n\t\t\"\"\"\n\t\tsalmon quant -p ${task.cpus} -i $index -l $libtype -1 ${reads[0]} -2 ${reads[1]} $softclip --incompatPrior $incompatPrior $UnmappedNames --validateMappings $dumpEq $writeMappings -o $sample_name $salmon_sa_params_mapping\n \t\t\"\"\"\n\t    }\n\t}", "\tprocess salmon_quantification {\n\t    storeDir \"${params.outdir}/salmon\"\n\t    tag \"${sample}\"\n\n            label 'process_high'\n\n\t    input:\n\t    file(index) from salmon_index.collect()\n\t    set val(sample), file(reads) from trimming_results_to_salmon\n\t    val(libtype) from libtype_salmon\n\n\t    output:\n\t    set val(sample_name), file(\"${sample_name}\") into split_table\n\t    set val(sample_name), file(\"${sample_name}\") into split_table_uniq_ambig\n\t    file(\"${sample_name}\") into salmon_files_to_combine\n\t    file(\"${sample_name}\") into multiqc_salmon_quant\n\t    set val(sample_name), file(\"${sample_name}\") into collect_processed_read_counts\n\n\t    script:\n\t    UnmappedNames = params.writeUnmappedNames ? \"--writeUnmappedNames\" : ''\n\t    softclip = params.softclipOverhangs ? \"--softclipOverhangs\" : ''\n\t    incompatPrior = params.incompatPrior\n\t    dumpEq = params.dumpEq ? \"--dumpEq\" : ''\n\t    salmon_sa_params_mapping = params.salmon_sa_params_mapping \n\t    if (params.single_end){\n\t    \tsample_name = sample.replaceFirst(/.fastq.gz|.fq.gz|.fastq|.fq/, \"\")\n\t\twriteMappings = params.writeMappings ? \"--writeMappings=$sample_name/mapping.sam\" : ''\n\t\t\"\"\"\n\t\tsalmon quant -p ${task.cpus} -i $index -l $libtype -r $reads $softclip --incompatPrior $incompatPrior $UnmappedNames --validateMappings $dumpEq $writeMappings -o $sample_name $salmon_sa_params_mapping\n\t\t\"\"\"\n\t    } else{\n\t\tsample_name = sample.replaceFirst(/.fastq.gz|.fq.gz|.fastq|.fq/, \"\")\n\t\twriteMappings = params.writeMappings ? \"--writeMappings=$sample_name/mapping.sam\" : ''\n\t\t\"\"\"\n\t\tsalmon quant -p ${task.cpus} -i $index -l $libtype -1 ${reads[0]} -2 ${reads[1]} $softclip --incompatPrior $incompatPrior $UnmappedNames --validateMappings $dumpEq $writeMappings -o $sample_name $salmon_sa_params_mapping\n \t\t\"\"\"\n\t    }\n\t}"], "list_proc": ["nf-core/dualrnaseq/nf-core__dualrnaseq/salmon_quantification", "bhagesh-codebeast/nextflowdualrnaseq/bhagesh-codebeast__nextflowdualrnaseq/salmon_quantification"], "list_wf_names": ["nf-core/dualrnaseq", "bhagesh-codebeast/nextflowdualrnaseq"]}, {"nb_reuse": 8, "tools": ["gffread"], "nb_own": 5, "list_own": ["raygozag", "nf-core", "mahesh-panchal", "harleenduggal", "jianhong"], "nb_wf": 7, "list_wf": ["rnavar", "RNASEQ", "nf-core-hicar", "test_nfcore_workflow_chain", "modules", "nfcore-rnaseq", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "jordwil", "FelixKrueger", "kmurat1", "chuan-wang", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "aanil", "silviamorins", "d4straub", "SPPearce", "Midnighter", "rannick", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "mvanins", "nebfield", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 148, "codes": ["process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/raygozag__rnaseq/GFFREAD", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/GFFREAD", "nf-core/modules/nf-core__modules/GFFREAD", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/GFFREAD", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/GFFREAD", "nf-core/rnaseq/nf-core__rnaseq/GFFREAD", "nf-core/rnavar/nf-core__rnavar/GFFREAD", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/GFFREAD"], "list_wf_names": ["jianhong/nf-core-hicar", "raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/rnavar"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess variantAnnotateGATK{     \n    tag \"$name\"\n    publishDir \"${params.outdir}/GATK_AnnotatedVariants\", mode: 'copy'\n\n    input:\n    set file(phased_vcf), file(phased_vcf_ind) from combined_variants_gatk\n    file(phased_vcf_snpeff) from combined_variants_gatk_snpeff\n\n    output:\n    file \"*.{vcd,idx}\"\n\n    script:\n    \"\"\"\n    gatk -T VariantAnnotator \\\\\n        -R $params.gfasta \\\\\n        -A SnpEff \\\\\n        --variant $phased_vcf \\\\\n        --snpEffFile ${name}_combined_phased_variants.snpeff \\\\\n        --out ${name}_combined_phased_annotated_variants.vcf\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/variantAnnotateGATK"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process fastp {\n        label 'process_low'\n        tag \"$name\"\n        publishDir \"${params.outdir}/fastp\", mode: params.publish_dir_mode,\n          saveAs: {filename ->\n                      if (filename.indexOf(\".fastq.gz\") == -1) \"logs/$filename\"\n                      else if (reads[1] == null) \"single_end/$filename\"\n                      else if (reads[1] != null) \"paired_end/$filename\"\n                      else null\n                  }\n\n        input:\n        set val(name), file(reads) from ch_read_files_trimming_to_trim\n\n        output:\n        set val(name), file(\"*trimmed.fastq.gz\") into ch_reads_all_trimmed\n        file \"*fastp.json\" into ch_fastp_results\n        file \"*fastp.html\" into ch_fastp_html\n\n        script:\n                                          \n        if (reads[1] == null) {\n            \"\"\"\n            fastp \\\\\n                --in1 ${reads} \\\\\n                --out1 ${name}_R1_trimmed.fastq.gz \\\\\n                --json ${name}_fastp.json \\\\\n                --html ${name}_fastp.html\n            \"\"\"\n        } else if (reads[1] != null ){\n                                                      \n            \"\"\"\n            fastp \\\\\n                --in1 ${reads[0]} \\\\\n                --in2 ${reads[1]} \\\\\n                --out1 ${name}_R1_trimmed.fastq.gz \\\\\n                --out2 ${name}_R2_trimmed.fastq.gz \\\\\n                --json ${name}_fastp.json \\\\\n                --html ${name}_fastp.html\n            \"\"\"\n        } else {\n          \"\"\"\n          echo name ${name}\n          echo reads: ${reads}\n          echo \"Number of reads is not equal to 1 or 2 --> don't know how to trim non-paired-end and non-single-end reads\"\n          \"\"\"\n        }\n    }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/fastp"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["seqcluster"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess collapse {\n    label 'process_medium'\n    tag \"$reads\"\n\n    input:\n    file reads from trimmed_reads_collapse\n\n    output:\n    file 'final/*.fastq' into collapsed_fasta\n\n    script:\n    prefix = reads.toString() - '_trimmed.fq.gz'\n    \"\"\"\n    seqcluster collapse -f $reads -m 1 --min_size 15 -o collapsed\n    mkdir final\n    mv collapsed/${prefix}_trimmed_trimmed.fastq final/${prefix}.fastq\n    \"\"\"\n}"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/collapse"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 15, "tools": ["GATK"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 15, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "sarek-mirror", "germline_somatic", "custom_sarek", "pgp-chronek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["FriederikeHanssen", "alneberg", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'process_low'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSample, file(\"${idSample}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idPatient}_${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSample, file(\"${idSample}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idPatient}_${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'process_medium'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSample, file(\"${idSample}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSample, file(\"${idSample}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idPatient}_${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idPatient}_${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag {idPatient + \"_\" + idSampleTumor}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        file(\"${idSampleTumor}_pileupsummaries.table.tsv\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n    script:\n        allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table.tsv\n    \"\"\"\n}"], "list_proc": ["cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/MergePileupSummaries", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/MergePileupSummaries", "cgpu/sarek-mirror/cgpu__sarek-mirror/MergePileupSummaries", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/MergePileupSummaries", "rmoran7/custom_sarek/rmoran7__custom_sarek/MergePileupSummaries", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergePileupSummaries", "nf-core/sarek/nf-core__sarek/MergePileupSummaries", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/MergePileupSummaries", "rmoran7/dx_sarek/rmoran7__dx_sarek/MergePileupSummaries", "cgpu/haplosarek/cgpu__haplosarek/MergePileupSummaries", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MergePileupSummaries", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MergePileupSummaries", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/MergePileupSummaries", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/MergePileupSummaries", "cgpu/pgp-chronek/cgpu__pgp-chronek/MergePileupSummaries"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-mirror-cache"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": ["\nprocess cutadapt {\n    tag \"$name\"\n    label 'process_high'\n    publishDir \"${params.outdir}/cutadapt\", mode: params.publish_dir_mode\n\n    input:\n    tuple val(name), path(reads) from ch_umi_moved\n\n    output:\n    tuple val(name), path(\"${name}.trimmed.fastq.gz\") into ch_trimmed\n    path \"*.log\" into ch_cutadapt_mqc\n\n    script:\n    \"\"\"\n    ln -s $reads ${name}.fastq.gz\n    cutadapt -j $task.cpus -a ${params.adapter} -m 12 -o ${name}.trimmed.fastq.gz ${name}.fastq.gz > ${name}_cutadapt.log\n    \"\"\"\n}"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/cutadapt"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 2, "tools": ["PLINK"], "nb_own": 2, "list_own": ["PGScatalog", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "pgsc_calc"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "smlmbrt", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process PLINK_VCF {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::plink=1.90b6.21\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/plink:1.90b6.21--h779adbc_1' :\n        'quay.io/biocontainers/plink:1.90b6.21--h779adbc_1' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed, optional: true\n    tuple val(meta), path(\"*.bim\"), emit: bim, optional: true\n    tuple val(meta), path(\"*.fam\"), emit: fam, optional: true\n\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    \"\"\"\n    plink \\\\\n        --vcf ${vcf} \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --out ${prefix}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        plink: \\$(echo \\$(plink --version 2>&1) | sed 's/^PLINK v//' | sed 's/..-bit.*//' )\n    END_VERSIONS\n    \"\"\"\n}", "process PLINK_VCF {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::plink=1.90b6.21\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/plink:1.90b6.21--h779adbc_1' :\n        'quay.io/biocontainers/plink:1.90b6.21--h779adbc_1' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed, optional: true\n    tuple val(meta), path(\"*.bim\"), emit: bim, optional: true\n    tuple val(meta), path(\"*.fam\"), emit: fam, optional: true\n\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    \"\"\"\n    plink \\\\\n        --vcf ${vcf} \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --out ${prefix}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        plink: \\$(echo \\$(plink --version 2>&1) | sed 's/^PLINK v//' | sed 's/..-bit.*//' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PLINK_VCF", "PGScatalog/pgsc_calc/PGScatalog__pgsc_calc/PLINK_VCF"], "list_wf_names": ["PGScatalog/pgsc_calc", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MAFFT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::mafft=7.490\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mafft:7.490--h779adbc_0':\n        'quay.io/biocontainers/mafft:7.490--h779adbc_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.fas\"), emit: fas\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mafft \\\\\n        --thread ${task.cpus} \\\\\n        ${args} \\\\\n        ${fasta} \\\\\n        > ${prefix}.fas\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mafft: \\$(mafft --version 2>&1 | sed 's/^v//' | sed 's/ (.*)//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MAFFT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess post_ar_fastq_trimming {\n  label 'mc_small'\n  tag \"${libraryid}\"\n  publishDir \"${params.outdir}/post_ar_fastq_trimmed\", mode: params.publish_dir_mode\n\n  when: params.run_post_ar_trimming\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(r1), path(r2) from ch_adapterremoval_for_post_ar_trimming\n\n  output:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*_R1_postartrimmed.fq.gz\") into ch_post_ar_trimming_for_lanemerge_r1\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*_R2_postartrimmed.fq.gz\") optional true into ch_post_ar_trimming_for_lanemerge_r2\n\n  script:\n  if ( seqtype == 'SE' | (seqtype == 'PE' && !params.skip_collapse) ) {\n  \"\"\"\n  fastp --in1 ${r1} --trim_front1 ${params.post_ar_trim_front} --trim_tail1 ${params.post_ar_trim_tail} -A -G -Q -L -w ${task.cpus} --out1 \"${libraryid}\"_L\"${lane}\"_R1_postartrimmed.fq.gz\n  \"\"\"\n  } else if ( seqtype == 'PE' && params.skip_collapse ) {\n  \"\"\"\n  fastp --in1 ${r1} --in2 ${r2}  --trim_front1 ${params.post_ar_trim_front} --trim_tail1 ${params.post_ar_trim_tail} --trim_front2 ${params.post_ar_trim_front2} --trim_tail2 ${params.post_ar_trim_tail2} -A -G -Q -L -w ${task.cpus} --out1 \"${libraryid}\"_L\"${lane}\"_R1_postartrimmed.fq.gz --out2 \"${libraryid}\"_L\"${lane}\"_R2_postartrimmed.fq.gz\n  \"\"\"\n  }\n\n}"], "list_proc": ["nf-core/eager/nf-core__eager/post_ar_fastq_trimming"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["SummaryAUC", "WormGender"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process ASCAT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ascat=3.0.0 bioconda::cancerit-allelecount-4.3.0\": null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-c278c7398beb73294d78639a864352abef2931ce:dfe5aaa885de434adb2b490b68972c5840c6d761-0':\n        'quay.io/biocontainers/mulled-v2-c278c7398beb73294d78639a864352abef2931ce:dfe5aaa885de434adb2b490b68972c5840c6d761-0' }\"\n\n    input:\n    tuple val(meta), path(input_normal), path(index_normal), path(input_tumor), path(index_tumor)\n    path(allele_files)\n    path(loci_files)\n\n    output:\n    tuple val(meta), path(\"*png\"),               emit: png\n    tuple val(meta), path(\"*cnvs.txt\"),          emit: cnvs\n    tuple val(meta), path(\"*purityploidy.txt\"),  emit: purityploidy\n    tuple val(meta), path(\"*segments.txt\"),      emit: segments\n    path \"versions.yml\",                         emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args           = task.ext.args        ?: ''\n    def prefix         = task.ext.prefix      ?: \"${meta.id}\"\n    def gender         = args.gender          ?  \"$args.gender\" :        \"NULL\"\n    def genomeVersion  = args.genomeVersion   ?  \"$args.genomeVersion\" : \"NULL\"\n    def purity         = args.purity          ?  \"$args.purity\" :        \"NULL\"\n    def ploidy         = args.ploidy          ?  \"$args.ploidy\" :        \"NULL\"\n    def gc_files       = args.gc_files        ?  \"$args.gc_files\" :      \"NULL\"\n\n    def minCounts_arg                    = args.minCounts                     ?  \",minCounts = $args.minCounts\" : \"\"\n    def chrom_names_arg                  = args.chrom_names                   ?  \",chrom_names = $args.chrom_names\" : \"\"\n    def min_base_qual_arg                = args.min_base_qual                 ?  \",min_base_qual = $args.min_base_qual\" : \"\"\n    def min_map_qual_arg                 = args.min_map_qual                  ?  \",min_map_qual = $args.min_map_qual\" : \"\"\n    def ref_fasta_arg                    = args.ref_fasta                     ?  \",ref.fasta = '$args.ref_fasta'\" : \"\"\n    def skip_allele_counting_tumour_arg  = args.skip_allele_counting_tumour   ?  \",skip_allele_counting_tumour = $args.skip_allele_counting_tumour\" : \"\"\n    def skip_allele_counting_normal_arg  = args.skip_allele_counting_normal   ?  \",skip_allele_counting_normal = $args.skip_allele_counting_normal\" : \"\"\n\n\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(RColorBrewer)\n    library(ASCAT)\n    options(bitmapType='cairo')\n\n\n    #prepare from BAM files\n    ascat.prepareHTS(\n        tumourseqfile = \"$input_tumor\",\n        normalseqfile = \"$input_normal\",\n        tumourname = \"Tumour\",\n        normalname = \"Normal\",\n        allelecounter_exe = \"alleleCounter\",\n        alleles.prefix = \"$allele_files\",\n        loci.prefix = \"$loci_files\",\n        gender = \"$gender\",\n        genomeVersion = \"$genomeVersion\",\n        nthreads = $task.cpus\n        $minCounts_arg\n        $chrom_names_arg\n        $min_base_qual_arg\n        $min_map_qual_arg\n        $ref_fasta_arg\n        $skip_allele_counting_tumour_arg\n        $skip_allele_counting_normal_arg\n    )\n\n\n    #Load the data\n    ascat.bc = ascat.loadData(\n        Tumor_LogR_file = \"Tumour_tumourLogR.txt\",\n        Tumor_BAF_file = \"Tumour_normalBAF.txt\",\n        Germline_LogR_file = \"Tumour_normalLogR.txt\",\n        Germline_BAF_file = \"Tumour_normalBAF.txt\",\n        genomeVersion = \"$genomeVersion\",\n        gender = \"$gender\"\n    )\n\n    #optional GC wave correction\n    if(!is.null($gc_files)){\n        ascat.bc = ascat.GCcorrect(ascat.bc, $gc_files)\n    }\n\n    #Plot the raw data\n    ascat.plotRawData(ascat.bc)\n\n    #Segment the data\n    ascat.bc = ascat.aspcf(ascat.bc)\n\n    #Plot the segmented data\n    ascat.plotSegmentedData(ascat.bc)\n\n    #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers\n    #If psi and rho are manually set:\n    if (!is.null($purity) && !is.null($ploidy)){\n        ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=$purity, psi_manual=$ploidy)\n    } else if(!is.null($purity) && is.null($ploidy)){\n        ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=$purity)\n    } else if(!is.null($ploidy) && is.null($purity)){\n        ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=$ploidy)\n    } else {\n        ascat.output <- ascat.runAscat(ascat.bc, gamma=1)\n    }\n\n    #Write out segmented regions (including regions with one copy of each allele)\n    write.table(ascat.output[[\"segments\"]], file=paste0(\"$prefix\", \".segments.txt\"), sep=\"\\t\", quote=F, row.names=F)\n\n    #Write out CNVs in bed format\n    cnvs=ascat.output[[\"segments\"]][2:6]\n    write.table(cnvs, file=paste0(\"$prefix\",\".cnvs.txt\"), sep=\"\\t\", quote=F, row.names=F, col.names=T)\n\n    #Write out purity and ploidy info\n    summary <- tryCatch({\n            matrix(c(ascat.output[[\"aberrantcellfraction\"]], ascat.output[[\"ploidy\"]]), ncol=2, byrow=TRUE)}, error = function(err) {\n                # error handler picks up where error was generated\n                print(paste(\"Could not find optimal solution:  \",err))\n                return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))\n        }\n    )\n    colnames(summary) <- c(\"AberrantCellFraction\",\"Ploidy\")\n    write.table(summary, file=paste0(\"$prefix\",\".purityploidy.txt\"), sep=\"\\t\", quote=F, row.names=F, col.names=T)\n\n    #version export. Have to hardcode process name and software name because\n    #won't run inside an R-block\n    version_file_path=\"versions.yml\"\n    f <- file(version_file_path,\"w\")\n    writeLines(\"ASCAT:\", f)\n    writeLines(\" ascat: 3.0.0\",f)\n    close(f)\n    \"\"\"\n\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    echo stub > ${prefix}.cnvs.txt\n    echo stub > ${prefix}.purityploidy.txt\n    echo stub > ${prefix}.segments.txt\n    echo stub > Tumour.ASCATprofile.png\n    echo stub > Tumour.ASPCF.png\n    echo stub > Tumour.germline.png\n    echo stub > Tumour.rawprofile.png\n    echo stub > Tumour.sunrise.png\n    echo stub > Tumour.tumour.png\n\n    echo 'ASCAT:' > versions.yml\n    echo ' ascat: 3.0.0' >> versions.yml\n    \"\"\"\n\n\n}"], "list_proc": ["nf-core/modules/nf-core__modules/ASCAT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_ANCOM_ASV {\n    tag \"${table.baseName}\"\n    label 'process_medium'\n    label 'single_cpu'\n    label 'process_long'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(table)\n\n    output:\n    path(\"ancom/*\")     , emit: ancom\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime composition add-pseudocount \\\n        --i-table ${table} \\\n        --o-composition-table comp-${table}\n    qiime composition ancom \\\n        --i-table comp-${table} \\\n        --m-metadata-file ${metadata} \\\n        --m-metadata-column ${table.baseName} \\\n        --o-visualization comp-${table.baseName}.qzv\n    qiime tools export --input-path comp-${table.baseName}.qzv \\\n        --output-path ancom/Category-${table.baseName}-ASV\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_ANCOM_ASV {\n    tag \"${table.baseName}\"\n    label 'process_medium'\n    label 'single_cpu'\n    label 'process_long'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(table)\n\n    output:\n    path(\"ancom/*\")     , emit: ancom\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime composition add-pseudocount \\\n        --i-table ${table} \\\n        --o-composition-table comp-${table}\n    qiime composition ancom \\\n        --i-table comp-${table} \\\n        --m-metadata-file ${metadata} \\\n        --m-metadata-column ${table.baseName} \\\n        --o-visualization comp-${table.baseName}.qzv\n    qiime tools export --input-path comp-${table.baseName}.qzv \\\n        --output-path ancom/Category-${table.baseName}-ASV\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_ANCOM_ASV", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_ANCOM_ASV"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["Salmon"], "nb_own": 2, "list_own": ["jiangfuqing", "nf-core"], "nb_wf": 2, "list_wf": ["scrnaseq", "scrna-seq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 7, "codes": [" process run_alevin {\n    tag \"$name\"\n    publishDir \"${params.outdir}/alevin\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from read_files_alevin\n    file index from salmon_index_alevin.collect()\n    file txp2gene from txp2gene_alevin.collect()\n\n\n    output:\n    file \"${name}_alevin_results\" into alevin_results\n\n    script:\n    \"\"\"\n    salmon alevin -l ISR -1 ${reads[0]} -2 ${reads[1]} --chromium -i $index -o ${name}_alevin_results -p 5 --tgMap $txp2gene --dumpFeatures\n    \"\"\"\n  }", "\nprocess alevin {\n    tag \"$name\"\n    label 'high_memory'\n    publishDir \"${params.outdir}/alevin/alevin\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from read_files_alevin\n    file index from salmon_index_alevin.collect()\n    file txp2gene from txp2gene.collect()\n\n    output:\n    file \"${name}_alevin_results\" into alevin_results, alevin_logs\n\n    when:\n    params.aligner == \"alevin\"\n\n    script:\n    read1 = reads[0]\n    read2 = reads[1]\n    \"\"\"\n    salmon alevin -l ISR -1 ${read1} -2 ${read2} \\\n      --chromium -i $index -o ${name}_alevin_results -p ${task.cpus} --tgMap $txp2gene --dumpFeatures \u2013-dumpMtx\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/scrna-seq/jiangfuqing__scrna-seq/run_alevin", "nf-core/scrnaseq/nf-core__scrnaseq/alevin"], "list_wf_names": ["nf-core/scrnaseq", "jiangfuqing/scrna-seq"]}, {"nb_reuse": 2, "tools": ["MultiQC"], "nb_own": 2, "list_own": ["bhagesh-codebeast", "nf-core"], "nb_wf": 2, "list_wf": ["dualrnaseq", "nextflowdualrnaseq"], "list_contrib": ["lbarquist", "nf-core-bot", "apeltzer", "reganhayward", "bhagesh-codebeast", "bozmik"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n    \n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_results.last().collect().ifEmpty([]) \n    file ('fastqc_after_trimming/*') from ch_fastqc_trimmed_results.last().collect().ifEmpty([])\n    file ('salmon/*') from multiqc_salmon_quant.collect().ifEmpty([])\n    file ('salmon_alignment_mode/*') from multiqc_salmon_alignment_quant.collect().ifEmpty([])\n    file ('STAR/*') from multiqc_star_alignment.collect().ifEmpty([])\n    file ('STAR_for_salmon/*') from multiqc_star_for_salmon_alignment.collect().ifEmpty([])\n    file ('uniquely_mapped/*') from multiqc_htseq_unique.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -d --export -f $rtitle $rfilename $custom_config_file . \n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n    \n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_results.last().collect().ifEmpty([]) \n    file ('fastqc_after_trimming/*') from ch_fastqc_trimmed_results.last().collect().ifEmpty([])\n    file ('salmon/*') from multiqc_salmon_quant.collect().ifEmpty([])\n    file ('salmon_alignment_mode/*') from multiqc_salmon_alignment_quant.collect().ifEmpty([])\n    file ('STAR/*') from multiqc_star_alignment.collect().ifEmpty([])\n    file ('STAR_for_salmon/*') from multiqc_star_for_salmon_alignment.collect().ifEmpty([])\n    file ('uniquely_mapped/*') from multiqc_htseq_unique.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -d --export -f $rtitle $rfilename $custom_config_file . \n    \"\"\"\n}"], "list_proc": ["nf-core/dualrnaseq/nf-core__dualrnaseq/multiqc", "bhagesh-codebeast/nextflowdualrnaseq/bhagesh-codebeast__nextflowdualrnaseq/multiqc"], "list_wf_names": ["nf-core/dualrnaseq", "bhagesh-codebeast/nextflowdualrnaseq"]}, {"nb_reuse": 9, "tools": ["bedGraphToBigWig"], "nb_own": 5, "list_own": ["raygozag", "nf-core", "mahesh-panchal", "harleenduggal", "jianhong"], "nb_wf": 8, "list_wf": ["RNASEQ", "nf-core-hicar", "modules", "test_nfcore_workflow_chain", "nfcore-rnaseq", "cutandrun", "rnaseq", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "dladd", "kmurat1", "chuan-wang", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "cjfields", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "charlotte-west", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 151, "codes": ["\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    } else {\n        container \"quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    }\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo $VERSION)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/raygozag__rnaseq/UCSC_BEDGRAPHTOBIGWIG", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/UCSC_BEDGRAPHTOBIGWIG", "nf-core/modules/nf-core__modules/UCSC_BEDGRAPHTOBIGWIG", "nf-core/ssds/nf-core__ssds/UCSC_BEDGRAPHTOBIGWIG", "nf-core/cutandrun/nf-core__cutandrun/UCSC_BEDGRAPHTOBIGWIG", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/UCSC_BEDGRAPHTOBIGWIG", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/UCSC_BEDGRAPHTOBIGWIG", "nf-core/rnaseq/nf-core__rnaseq/UCSC_BEDGRAPHTOBIGWIG", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/UCSC_BEDGRAPHTOBIGWIG"], "list_wf_names": ["jianhong/nf-core-hicar", "raygozag/rnaseq", "nf-core/ssds", "harleenduggal/RNASEQ", "nf-core/cutandrun", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["FastQC", "MultiQC", "QualiMap", "Cutadapt", "SAMtools", "BWA"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo \"$params.version\" &> v_nfcore_exoseq.txt\n    echo \"$workflow.nextflow.version\" &> v_nextflow.txt\n    fastqc --version &> v_fastqc.txt\n    cutadapt --version &> v_cutadapt.txt\n    trim_galore --version &> v_trim_galore.txt\n    samtools --version &> v_samtools.txt\n    bwa &> v_bwa.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt\n    gatk-launch BaseRecalibrator --version &> v_gatk.txt\n    multiqc --version &> v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/get_software_versions"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 2, "tools": ["gffread"], "nb_own": 2, "list_own": ["nf-core", "robinfchan"], "nb_wf": 2, "list_wf": ["citeseq-nf", "scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "robinfchan", "olgabot"], "nb_contrib": 8, "codes": ["\nprocess extract_transcriptome {\n    if (params.custom_container) container \"${params.custom_container}\"\n    \n    tag \"${genome_fasta}\"\n    label 'midi_memory'\n    publishDir \"${params.outdir}/reference_data/extract_transcriptome\", mode: 'copy'\n\n    input:\n    file genome_fasta from genome_fasta_extract_transcriptome\n    file gtf from gtf_extract_transcriptome_trimmed\n\n    output:\n    file \"${genome_fasta}.transcriptome.fa\" into transcriptome_fasta_alevin_extracted\n\n    when:\n    !params.transcriptome_fasta && !params.skip_rna \n    \n    script:\n                                                        \n    \"\"\"\n    gffread -F $gtf -w \"${genome_fasta}.transcriptome.fa\" -g $genome_fasta\n    \"\"\"\n}", "\nprocess extract_transcriptome {\n    tag \"${genome_fasta}\"\n    publishDir \"${params.outdir}/reference_data/extract_transcriptome\", mode: 'copy'\n\n    input:\n    file genome_fasta from genome_fasta_extract_transcriptome\n    file gtf from gtf_extract_transcriptome\n\n    output:\n    file \"${genome_fasta}.transcriptome.fa\" into (transcriptome_fasta_alevin_extracted, transcriptome_fasta_kallisto_extracted)\n\n    when: !params.transcript_fasta && (params.aligner == 'alevin' || params.aligner == 'kallisto')\n    script:\n                                                        \n    \"\"\"\n    gffread -F $gtf -w \"${genome_fasta}.transcriptome.fa\" -g $genome_fasta\n    \"\"\"\n}"], "list_proc": ["robinfchan/citeseq-nf/robinfchan__citeseq-nf/extract_transcriptome", "nf-core/scrnaseq/nf-core__scrnaseq/extract_transcriptome"], "list_wf_names": ["robinfchan/citeseq-nf", "nf-core/scrnaseq"]}, {"nb_reuse": 1, "tools": ["SortMeRna"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process sortmerna {\n        label 'mid_memory_long'\n        label 'mid_cpu'\n        tag \"$name\"\n        publishDir \"${params.outdir}/SortMeRNA\", mode: \"${params.publish_dir_mode}\",\n            saveAs: {filename ->\n                if (filename.indexOf(\"_rRNA_report.txt\") > 0) \"logs/$filename\"\n                else if (params.save_non_rrna_reads) \"reads/$filename\"\n                else null\n            }\n\n        input:\n        set val(name), file(reads) from ch_reads_for_ribosomal_removal\n        val(db_name) from sortmerna_db_name.collect()\n        file(db_fasta) from sortmerna_db_fasta.collect()\n        file(db) from sortmerna_db.collect()\n\n        output:\n        set val(name), file(\"*.fq.gz\") into ch_reads_to_translate\n        file \"*_rRNA_report.txt\" into sortmerna_logs\n\n\n        script:\n                                                                                        \n        def Refs = ''\n        for (i=0; i<db_fasta.size(); i++) { Refs+= \":${db_fasta[i]},${db_name[i]}\" }\n        Refs = Refs.substring(1)\n\n                                          \n        if (reads[1] == null) {\n            \"\"\"\n            gzip -d --force < ${reads} > all-reads.fastq\n            sortmerna --ref ${Refs} \\\n                --reads all-reads.fastq \\\n                --num_alignments 1 \\\n                -a ${task.cpus} \\\n                --fastx \\\n                --aligned rRNA-reads \\\n                --other non-rRNA-reads \\\n                --log -v\n            gzip --force < non-rRNA-reads.fastq > ${name}.fq.gz\n            mv rRNA-reads.log ${name}_rRNA_report.txt\n            \"\"\"\n        } else {\n            \"\"\"\n            gzip -d --force < ${reads[0]} > reads-fw.fq\n            gzip -d --force < ${reads[1]} > reads-rv.fq\n            merge-paired-reads.sh reads-fw.fq reads-rv.fq all-reads.fastq\n            sortmerna --ref ${Refs} \\\n                --reads all-reads.fastq \\\n                --num_alignments 1 \\\n                -a ${task.cpus} \\\n                --fastx --paired_in \\\n                --aligned rRNA-reads \\\n                --other non-rRNA-reads \\\n                --log -v\n            unmerge-paired-reads.sh non-rRNA-reads.fastq non-rRNA-reads-fw.fq non-rRNA-reads-rv.fq\n            gzip < non-rRNA-reads-fw.fq > ${name}-fw.fq.gz\n            gzip < non-rRNA-reads-rv.fq > ${name}-rv.fq.gz\n            mv rRNA-reads.log ${name}_rRNA_report.txt\n            \"\"\"\n        }\n    }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/sortmerna"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess mirna_post_alignment {\n    label 'process_medium'\n    tag \"$input\"\n    publishDir \"${params.outdir}/bowtie\", mode: params.publish_dir_mode, saveAs: wrap_mature_and_hairpin\n\n    input:\n    file input from miRBase_mature_bam.mix(miRBase_hairpin_bam)\n\n    output:\n    file \"${input.baseName}.stats\" into miRBase_counts\n    file \"*.{flagstat,idxstats,stats}\" into ch_sort_bam_flagstat_mqc\n    file \"${input.baseName}.sorted.bam\" into miRBase_bam\n    file \"${input.baseName}.sorted.bam.bai\" into miRBase_bai\n\n    script:\n    \"\"\"\n    samtools sort ${input.baseName}.bam -o ${input.baseName}.sorted.bam\n    samtools index ${input.baseName}.sorted.bam\n    samtools idxstats ${input.baseName}.sorted.bam > ${input.baseName}.stats\n    samtools flagstat ${input.baseName}.sorted.bam > ${input.baseName}.sorted.bam.flagstat\n    samtools stats ${input.baseName}.sorted.bam > ${input.baseName}.sorted.bam.stats\n    \"\"\"\n}", " process genome_post_alignment  {\n        label 'process_low'\n        tag \"$input\"\n        publishDir \"${params.outdir}/bowtie_ref\", mode: 'copy'\n\n        input:\n        file input from bowtie_bam\n\n        output:\n        file \"*.{flagstat,idxstats,stats}\" into ch_genome_bam_flagstat_mqc\n\n        script:\n        \"\"\"\n        samtools sort ${input.baseName}.bam -o ${input.baseName}.sorted.bam\n        samtools index ${input.baseName}.sorted.bam\n        samtools idxstats ${input.baseName}.sorted.bam > ${input.baseName}.stats\n        samtools flagstat ${input.baseName}.sorted.bam > ${input.baseName}.sorted.bam.flagstat\n        samtools stats ${input.baseName}.sorted.bam > ${input.baseName}.sorted.bam.stats\n        \"\"\"\n    }"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/mirna_post_alignment", "nf-core/smrnaseq/nf-core__smrnaseq/genome_post_alignment"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 11, "tools": ["SAMtools"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 10, "list_wf": ["saw.sarek", "sarek_ubec", "PGP-UK-sarek", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_max'\n    label 'memory_max'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamMergeBamRecal\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bamRecal\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bamRecalQC\n        set idPatient, idSample into bamRecalTSV\n        file(\"${idSample}.recal.bam\") into (bamGenomeChronicler, bamGenomeChroniclerToPrint)\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_4'\n     disk '120 GB'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_max'\n    label 'memory_max'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamMergeBamRecal\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bamRecal\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bamRecalQC\n        set idPatient, idSample into bamRecalTSV\n        file(\"${idSample}.recal.bam\") into (bamGenomeChronicler, bamGenomeChroniclerToPrint)\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeRecalibratedReadGroupsForSample {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(idPatient), val(idSample), file(bam)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\")\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.bam\")\n        tuple val(idPatient), val(idSample)\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}"], "list_proc": ["Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MergeBamRecal", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/MergeBamRecal", "nf-core/sarek/nf-core__sarek/MergeBamRecal", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/MergeBamRecal", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergeBamRecal", "rmoran7/custom_sarek/rmoran7__custom_sarek/MergeBamRecal", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/MergeBamRecal", "rmoran7/dx_sarek/rmoran7__dx_sarek/MergeBamRecal", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/MergeBamRecal", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergeRecalibratedReadGroupsForSample", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MergeBamRecal"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 8, "tools": ["GATK"], "nb_own": 7, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 8, "list_wf": ["saw.sarek", "sarek_ubec", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 23, "codes": ["\nprocess FilterMutect2Calls {\n    label 'process_low'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSample, file(\"Mutect2_filtered_${idSample}.vcf.gz\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSamplePair, file(\"Mutect2_filtered_${idSamplePair}.vcf.gz\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSamplePair}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSamplePair, file(\"Mutect2_filtered_${idSamplePair}.vcf.gz\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSamplePair}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'process_medium'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSample, file(\"Mutect2_filtered_${idSample}.vcf.gz\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSample, file(\"Mutect2_filtered_${idSample}.vcf.gz\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSamplePair, file(\"Mutect2_filtered_${idSamplePair}.vcf.gz\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSamplePair}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSamplePair, file(\"Mutect2_filtered_${idSamplePair}.vcf.gz\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSamplePair}.vcf.gz\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSample, file(\"Mutect2_filtered_${idSample}.vcf.gz\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSample}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSample}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/rmoran7__custom_sarek/FilterMutect2Calls", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/FilterMutect2Calls", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/FilterMutect2Calls", "rmoran7/dx_sarek/rmoran7__dx_sarek/FilterMutect2Calls", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/FilterMutect2Calls", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/FilterMutect2Calls", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/FilterMutect2Calls", "nf-core/sarek/nf-core__sarek/FilterMutect2Calls"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), val(reads), file(longreads), file('reference')\n\n    output:\n    tuple val(meta), val(reads), file(longreads), file('reference'), path(\"*.paf\"), emit: paf\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        reference \\\\\n        $longreads \\\\\n        > ${prefix}.paf\n\n    echo \\$(minimap2 --version 2>&1) > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/MINIMAP2_ALIGN"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 3, "tools": ["Bowtie"], "nb_own": 3, "list_own": ["clairecoleman1", "nf-core", "oisinmccaffrey"], "nb_wf": 3, "list_wf": ["clipseq.nextflow", "clipseq", "clipseq1"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "CharlotteAnne", "drpatelh", "clairecoleman1", "oisinmccaffrey"], "nb_contrib": 8, "codes": [" process generate_premap_index {\n        tag \"$smrna_fasta\"\n        label 'process_low'\n\n        input:\n        path(smrna_fasta) from ch_smrna_fasta\n\n        output:\n        path(\"${smrna_fasta.simpleName}.*.bt2\") into ch_bt2_index\n\n        script:\n        \"\"\"\n        bowtie2-build --threads $task.cpus $smrna_fasta ${smrna_fasta.simpleName}\n        \"\"\"\n    }", "\nprocess generate_premap_index{\n\n\ttag \"$smrna_fasta\"\n\n    \tinput:\n    \tpath(smrna_fasta) from ch_smrna_fasta\n\n    \toutput:\n\n    \tpath(\"${smrna_fasta.simpleName}.*.bt2\") into ch_bt2_index\n\n    \tscript:\n    \t\"\"\"\n    \tbowtie2-build --threads $task.cpus $smrna_fasta ${smrna_fasta.simpleName}\n    \t\"\"\"\n}", "\nprocess generate_premap_index{\n\n\ttag \"$smrna_fasta\"\n\n    \tinput:\n    \tpath(smrna_fasta) from ch_smrna_fasta\n\n    \toutput:\n\n    \tpath(\"${smrna_fasta.simpleName}.*.bt2\") into ch_bt2_index\n\n    \tscript:\n    \t\"\"\"\n    \tbowtie2-build --threads $task.cpus $smrna_fasta ${smrna_fasta.simpleName}\n    \t\"\"\"\n}"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/generate_premap_index", "clairecoleman1/clipseq1/clairecoleman1__clipseq1/generate_premap_index", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/generate_premap_index"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["BEDTools", "TRUmiCount"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": [" process icount_peak_call {\n        tag \"$name\"\n        label 'process_low'\n        publishDir \"${params.outdir}/icount\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(name), path(xlinks) from ch_xlinks_icount\n        path(segment) from ch_segment.collect()\n\n        output:\n        tuple val(name), path(\"${name}.${half_window}nt.sigxl.bed.gz\") into ch_sigxls_icount\n        tuple val(name), path(\"${name}.${half_window}nt_${merge_window}nt.peaks.bed.gz\") into ch_peaks_icount\n        path \"*.peaks.bed.gz\" into ch_icount_qc\n\n        script:\n        half_window = params.half_window\n        merge_window = params.merge_window\n        \"\"\"\n        mkdir tmp\n        export ICOUNT_TMP_ROOT=\\$PWD/tmp\n\n        iCount peaks $segment $xlinks ${name}.${half_window}nt.sigxl.bed.gz --half_window ${half_window} --fdr 0.05\n\n        pigz -d -c ${name}.${half_window}nt.sigxl.bed.gz | \\\\\n        bedtools sort | \\\\\n        bedtools merge -s -d ${merge_window} -c 4,5,6 -o distinct,sum,distinct | \\\\\n        pigz > ${name}.${half_window}nt_${merge_window}nt.peaks.bed.gz\n        \"\"\"\n    }"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/icount_peak_call"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["Bismark"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BISMARK_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bismark=0.23.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bismark:0.23.0--0' :\n        'quay.io/biocontainers/bismark:0.23.0--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*bam\")       , emit: bam\n    tuple val(meta), path(\"*report.txt\"), emit: report\n    tuple val(meta), path(\"*fq.gz\")     , optional:true, emit: unmapped\n    path \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def fastq      = meta.single_end ? reads : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    bismark \\\\\n        $fastq \\\\\n        $args \\\\\n        --genome $index \\\\\n        --bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bismark: \\$(echo \\$(bismark -v 2>&1) | sed 's/^.*Bismark Version: v//; s/Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BISMARK_ALIGN"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_DIVERSITY_CORE {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    path(tree)\n    path(stats)\n\n    output:\n    path(\"diversity_core/*_pcoa_results.qza\")   , emit: pcoa\n    path(\"diversity_core/*_vector.qza\")         , emit: vector\n    path(\"diversity_core/*_distance_matrix.qza\"), emit: distance\n    path \"versions.yml\"                         , emit: versions\n    path(\"*rarefaction.txt\")                    , emit: depth\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    mindepth=\\$(count_table_minmax_reads.py $stats minimum 2>&1)\n    if [ \\\"\\$mindepth\\\" -gt \\\"10000\\\" ]; then echo \\$mindepth >\\\"Use the sampling depth of \\$mindepth for rarefaction.txt\\\" ; fi\n    if [ \\\"\\$mindepth\\\" -lt \\\"10000\\\" -a \\\"\\$mindepth\\\" -gt \\\"5000\\\" ]; then echo \\$mindepth >\\\"WARNING The sampling depth of \\$mindepth is quite small for rarefaction.txt\\\" ; fi\n    if [ \\\"\\$mindepth\\\" -lt \\\"5000\\\" -a \\\"\\$mindepth\\\" -gt \\\"1000\\\" ]; then echo \\$mindepth >\\\"WARNING The sampling depth of \\$mindepth is very small for rarefaction.txt\\\" ; fi\n    if [ \\\"\\$mindepth\\\" -lt \\\"1000\\\" ]; then echo \\$mindepth >\\\"WARNING The sampling depth of \\$mindepth seems too small for rarefaction.txt\\\" ; fi\n\n    qiime diversity core-metrics-phylogenetic \\\n        --m-metadata-file ${metadata} \\\n        --i-phylogeny ${tree} \\\n        --i-table ${table} \\\n        --p-sampling-depth \\$mindepth \\\n        --output-dir diversity_core \\\n        --p-n-jobs-or-threads ${task.cpus} \\\n        --verbose\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_DIVERSITY_CORE {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    path(tree)\n    path(stats)\n\n    output:\n    path(\"diversity_core/*_pcoa_results.qza\")   , emit: pcoa\n    path(\"diversity_core/*_vector.qza\")         , emit: vector\n    path(\"diversity_core/*_distance_matrix.qza\"), emit: distance\n    path \"versions.yml\"                         , emit: versions\n    path(\"*rarefaction.txt\")                    , emit: depth\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    mindepth=\\$(count_table_minmax_reads.py $stats minimum 2>&1)\n    if [ \\\"\\$mindepth\\\" -gt \\\"10000\\\" ]; then echo \\$mindepth >\\\"Use the sampling depth of \\$mindepth for rarefaction.txt\\\" ; fi\n    if [ \\\"\\$mindepth\\\" -lt \\\"10000\\\" -a \\\"\\$mindepth\\\" -gt \\\"5000\\\" ]; then echo \\$mindepth >\\\"WARNING The sampling depth of \\$mindepth is quite small for rarefaction.txt\\\" ; fi\n    if [ \\\"\\$mindepth\\\" -lt \\\"5000\\\" -a \\\"\\$mindepth\\\" -gt \\\"1000\\\" ]; then echo \\$mindepth >\\\"WARNING The sampling depth of \\$mindepth is very small for rarefaction.txt\\\" ; fi\n    if [ \\\"\\$mindepth\\\" -lt \\\"1000\\\" ]; then echo \\$mindepth >\\\"WARNING The sampling depth of \\$mindepth seems too small for rarefaction.txt\\\" ; fi\n\n    qiime diversity core-metrics-phylogenetic \\\n        --m-metadata-file ${metadata} \\\n        --i-phylogeny ${tree} \\\n        --i-table ${table} \\\n        --p-sampling-depth \\$mindepth \\\n        --output-dir diversity_core \\\n        --p-n-jobs-or-threads ${task.cpus} \\\n        --verbose\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_DIVERSITY_CORE", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_DIVERSITY_CORE"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["BEDTools"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process ASCIIGENOME {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::asciigenome=1.16.0 bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-093691b47d719890dc19ac0c13c4528e9776897f:27211b8c38006480d69eb1be3ef09a7bf0a49d76-0' :\n        'quay.io/biocontainers/mulled-v2-093691b47d719890dc19ac0c13c4528e9776897f:27211b8c38006480d69eb1be3ef09a7bf0a49d76-0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(vcf)\n    path fasta\n    path sizes\n    path gff\n    path bed\n    val window\n    val track_height\n\n    output:\n    tuple val(meta), path(\"*pdf\"), emit: pdf\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def gff_track = gff ? \"$gff\" : ''\n    def bed_track = bed ? \"$bed\" : ''\n    def paired_end = meta.single_end ? '' : '&& readsAsPairs -on'\n    \"\"\"\n    zcat $vcf \\\\\n        | grep -v '#' \\\\\n        | awk -v FS='\\t' -v OFS='\\t' '{print \\$1, (\\$2-1), (\\$2)}' \\\\\n        > variants.bed\n\n    bedtools \\\\\n        slop \\\\\n        -i variants.bed \\\\\n        -g $sizes \\\\\n        -b $window \\\\\n        > variants.slop.bed\n\n    ASCIIGenome \\\\\n        -ni \\\\\n        -x \"trackHeight 0 bam#1 && trackHeight $track_height bam@2 $paired_end && filterVariantReads && save ${prefix}.%r.pdf\" \\\\\n        --batchFile variants.slop.bed \\\\\n        --fasta $fasta \\\\\n        $bam \\\\\n        $vcf \\\\\n        $bed_track \\\\\n        $gff_track \\\\\n        > /dev/null\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        asciigenome: \\$(echo \\$(ASCIIGenome -ni --version 2>&1) | sed -e \"s/ASCIIGenome //g\")\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process ASCIIGENOME {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::asciigenome=1.16.0 bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-093691b47d719890dc19ac0c13c4528e9776897f:27211b8c38006480d69eb1be3ef09a7bf0a49d76-0' :\n        'quay.io/biocontainers/mulled-v2-093691b47d719890dc19ac0c13c4528e9776897f:27211b8c38006480d69eb1be3ef09a7bf0a49d76-0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(vcf)\n    path fasta\n    path sizes\n    path gff\n    path bed\n    val window\n    val track_height\n\n    output:\n    tuple val(meta), path(\"*pdf\"), emit: pdf\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def gff_track = gff ? \"$gff\" : ''\n    def bed_track = bed ? \"$bed\" : ''\n    def paired_end = meta.single_end ? '' : '&& readsAsPairs -on'\n    \"\"\"\n    zcat $vcf \\\\\n        | grep -v '#' \\\\\n        | awk -v FS='\\t' -v OFS='\\t' '{print \\$1, (\\$2-1), (\\$2)}' \\\\\n        > variants.bed\n\n    bedtools \\\\\n        slop \\\\\n        -i variants.bed \\\\\n        -g $sizes \\\\\n        -b $window \\\\\n        > variants.slop.bed\n\n    ASCIIGenome \\\\\n        -ni \\\\\n        -x \"trackHeight 0 bam#1 && trackHeight $track_height bam@2 $paired_end && filterVariantReads && save ${prefix}.%r.pdf\" \\\\\n        --batchFile variants.slop.bed \\\\\n        --fasta $fasta \\\\\n        $bam \\\\\n        $vcf \\\\\n        $bed_track \\\\\n        $gff_track \\\\\n        > /dev/null\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        asciigenome: \\$(echo \\$(ASCIIGenome -ni --version 2>&1) | sed -e \"s/ASCIIGenome //g\")\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/ASCIIGENOME", "nf-core/viralrecon/nf-core__viralrecon/ASCIIGENOME"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 11, "tools": ["SAMtools"], "nb_own": 5, "list_own": ["raygozag", "nf-core", "mahesh-panchal", "harleenduggal", "jianhong"], "nb_wf": 9, "list_wf": ["RNASEQ", "nf-core-hicar", "viralrecon", "test_nfcore_workflow_chain", "nanoseq", "modules", "nfcore-rnaseq", "rnaseq", "cutandrun"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "ktrns", "jordwil", "FelixKrueger", "dladd", "kmurat1", "chuan-wang", "yuxuth", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "cying111", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "aanil", "silviamorins", "d4straub", "SPPearce", "Midnighter", "rannick", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "mvanins", "nebfield", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "saramonzon", "pcantalupo", "cjfields", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "charlotte-west", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "svarona", "Erkison", "bjohnnyd", "grst", "lwratten", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "csawye01", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 161, "codes": ["process CHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0' :\n        'quay.io/biocontainers/samtools:1.12--hd5e65b6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai | sort -k 1,1 > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        custom: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        custom: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        custom: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GET_CHROM_SIZES {\n    tag \"$fasta\"\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'     , emit: sizes\n    path '*.fai'       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GET_CHROM_SIZES {\n    tag \"$fasta\"\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'     , emit: sizes\n    path '*.fai'       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess GET_CHROM_SIZES {\n                                    \n                                        \n                                                                                    \n\n    conda     (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple path(fasta), val(name)\n\n    output:\n    tuple path('*.sizes'), val(name) , emit: sizes\n                                                    \n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        custom: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        custom: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.14--hb421002_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GET_CHROM_SIZES {\n    tag \"$fasta\"\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'     , emit: sizes\n    path '*.fai'       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/nf-core-hicar/jianhong__nf-core-hicar/CHROMSIZES", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/CUSTOM_GETCHROMSIZES", "nf-core/rnaseq/nf-core__rnaseq/CUSTOM_GETCHROMSIZES", "nf-core/viralrecon/nf-core__viralrecon/CUSTOM_GETCHROMSIZES", "raygozag/rnaseq/raygozag__rnaseq/GET_CHROM_SIZES", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/GET_CHROM_SIZES", "nf-core/nanoseq/nf-core__nanoseq/GET_CHROM_SIZES", "nf-core/modules/nf-core__modules/CUSTOM_GETCHROMSIZES", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/CUSTOM_GETCHROMSIZES", "nf-core/cutandrun/nf-core__cutandrun/CUSTOM_GETCHROMSIZES", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/GET_CHROM_SIZES"], "list_wf_names": ["jianhong/nf-core-hicar", "raygozag/rnaseq", "harleenduggal/RNASEQ", "nf-core/cutandrun", "harleenduggal/nfcore-rnaseq", "nf-core/nanoseq", "nf-core/modules", "nf-core/viralrecon", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess qualiMap {\n    tag \"${name}\"\n    publishDir \"${params.outdir}/Qualimap\", mode: 'copy'\n\n    input:\n    set val(name), file(realign_bam), file(realign_bam_ind) from bam_metrics\n\n    output:\n    file \"${name}\" into qualimap_results\n    file '.command.log' into qualimap_stdout\n\n    script:\n    gcref = ''\n    gff = ''\n    if(params.genome == 'GRCh37') gcref = '-gd HUMAN'\n    if(params.genome == 'GRCm38') gcref = '-gd MOUSE'\n    if(params.exome) gff =\"-gff ${params.target_bed}\"\n    \"\"\"\n    qualimap bamqc $gcref \\\\\n    -bam $realign_bam \\\\\n    -outdir ${name} \\\\\n    --skip-duplicated \\\\\n    --collect-overlap-pairs \\\\\n    -nt ${task.cpus} \\\\\n    $gff \\\\\n    --java-mem-size=${task.memory.toGiga()}G \\\\\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/qualiMap"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 7, "codes": ["\nprocess star {\n    label 'high_memory'\n\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/STAR\", mode: 'copy'\n\n    input:\n    set val(samplename), file(reads) from read_files_star\n    file index from star_index.collect()\n    file gtf from gtf_star.collect()\n    file whitelist from barcode_whitelist_star.mix(barcode_whitelist_star_unzip).collect()\n\n    output:\n    set file(\"*Log.final.out\"), file ('*.bam') into star_aligned\n    file \"*.out\" into alignment_logs\n    file \"*SJ.out.tab\"\n    file \"*Log.out\" into star_log\n    file \"${prefix}Aligned.sortedByCoord.out.bam.bai\" into bam_index_rseqc, bam_index_genebody\n\n    when: params.aligner == 'star'\n\n    script:\n    prefix = reads[0].toString() - ~/(_R1)?(_trimmed)?(_val_1)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    def star_mem = task.memory ?: params.star_memory ?: false\n    def avail_mem = star_mem ? \"--limitBAMsortRAM ${star_mem.toBytes() - 100000000}\" : ''\n\n    seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center'\" : ''\n    cdna_read = reads[0]\n    barcode_read = reads[1]\n    \"\"\"\n    STAR --genomeDir $index \\\\\n          --sjdbGTFfile $gtf \\\\\n          --readFilesIn $barcode_read $cdna_read  \\\\\n          --runThreadN ${task.cpus} \\\\\n          --twopassMode Basic \\\\\n          --outWigType bedGraph \\\\\n          --outSAMtype BAM SortedByCoordinate $avail_mem \\\\\n          --readFilesCommand zcat \\\\\n          --runDirPerm All_RWX \\\\\n          --outFileNamePrefix $prefix $seq_center \\\\\n          --soloType Droplet \\\\\n          --soloCBwhitelist $whitelist\n\n    samtools index ${prefix}Aligned.sortedByCoord.out.bam\n    \"\"\"\n}"], "list_proc": ["nf-core/scrnaseq/nf-core__scrnaseq/star"], "list_wf_names": ["nf-core/scrnaseq"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process sourmash_compute_sketch_fastx_nucleotide {\n      tag \"${sig_id}\"\n      label \"low_memory\"\n      publishDir \"${params.outdir}/sketches_nucleotide/${sketch_id}\", mode: \"${params.publish_dir_mode}\",\n          saveAs: {filename ->\n              if (filename.indexOf(\".csv\") > 0) \"description/$filename\"\n              else if (filename.indexOf(\".sig\") > 0) \"sigs/$filename\"\n              else null\n          }\n\n      input:\n      val track_abundance\n      val sketch_value_parsed\n      val sketch_style_parsed\n      set val(sample_id), file(reads) from ch_reads_to_sketch\n\n      output:\n      file(csv) into ch_sourmash_sig_describe_nucleotides\n      set val(sample_id), val(sketch_id), val(\"dna\"), val(params.ksizes), file(sig) into sourmash_sketches_all_nucleotide\n\n      script:\n                                                                            \n                                                 \n      sketch_id = make_sketch_id(\n        \"dna\", \n        params.ksizes, \n        sketch_value_parsed[0], \n        track_abundance, \n        sketch_style_parsed[0]\n      )\n      sketch_value_flag = make_sketch_value_flag(sketch_style_parsed[0], sketch_value_parsed[0])\n      track_abundance_flag = track_abundance ? '--track-abundance' : ''\n      sig_id = \"${sample_id}__${sketch_id}\"\n      sig = \"${sig_id}.sig\"\n      csv = \"${sig_id}.csv\"\n      \"\"\"\n        sourmash compute \\\\\n          ${sketch_value_flag} \\\\\n          --ksizes ${params.ksizes} \\\\\n          --dna \\\\\n          $track_abundance_flag \\\\\n          --output ${sig} \\\\\n          --name '${sample_id}' \\\\\n          $reads\n        sourmash sig describe --csv ${csv} ${sig}\n      \"\"\"\n    }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/sourmash_compute_sketch_fastx_nucleotide"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": [" process bowtie_ref {\n        label 'process_high'\n        tag \"$reads\"\n        publishDir \"${params.outdir}/bowtie_ref\", mode: 'copy'\n\n        input:\n        file reads from trimmed_reads_bowtie_ref\n        file indices from bt_indices.collect()\n\n        output:\n        file '*.genome.bam' into bowtie_bam, bowtie_bam_for_unmapped\n\n        script:\n        index_base = indices[0].toString() - ~/.rev.\\d.ebwt?/ - ~/.\\d.ebwt?/\n        prefix = reads.toString() - ~/(.R1)?(_R1)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        seq_center = params.seq_center ? \"--sam-RG ID:${prefix} --sam-RG 'CN:${params.seq_center}'\" : ''\n        \"\"\"\n        bowtie \\\\\n            $index_base \\\\\n            -q <(zcat $reads) \\\\\n            -p ${task.cpus} \\\\\n            -t \\\\\n            -k 50 \\\\\n            --best \\\\\n            --strata \\\\\n            -e 99999 \\\\\n            --chunkmbs 2048 \\\\\n            -S $seq_center \\\\\n            | samtools view -bS - > ${prefix}.genome.bam\n        \"\"\"\n    }"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/bowtie_ref"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 9, "tools": ["FastQC"], "nb_own": 8, "list_own": ["vladsaveliev", "nf-core", "melnel000", "sagc-bioinformatics", "UCL-BLIC", "maxibor", "DLBPointon", "GMS6804-master"], "nb_wf": 8, "list_wf": ["madman", "Sarek_v2.3.FIX1", "eager", "Sarek_CBIO", "Sarek", "modules", "annotation-pipeline-nextflow", "cawdor"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "alneberg", "ewels", "jimmybgammyknee", "arontommi", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ashleethomson", "pallolason", "szilvajuhos", "ZandraFagernas", "nf-core-bot", "Sebastian-D", "aidaanva", "TCLamnidis", "pditommaso", "IdoBar", "olgabot", "marcelm", "malinlarsson", "DLBPointon", "charles-plessy", "ashildv", "a-lud", "nathanhaigh", "J35P312", "jongtaek-kim", "vladsaveliev", "sc13-bioinf", "apeltzer", "waffle-iron", "maxibor", "jtk622", "scarlhoff"], "nb_contrib": 39, "codes": ["\nprocess fastqc_2 {\n    tag \"FASTQC2 on ${f1} AND ${f2}\"\n    cpus 2\n    publishDir \"${params.outdir}/fastqc_MERGED_logs/\", mode: \"copy\"\n\n    input:\n    path(f1)\n    path(f2)\n\n    output:\n    tuple file(\"*fastqc.zip\"), file(\"*fastqc.html\"), emit: fastqc_2\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} -q ${f1} ${f2}\n    \"\"\"\n}", "\nprocess RunFastQC {\n  tag {idPatient + \"-\" + idLane}\n\n  publishDir \"${params.outDir}/Reports/FastQC/${idLane}\", mode: params.publishDirMode\n\n  input:\n  set idPatient, status, idSample, idLane, file(inputFile1), file(inputFile2) from inputFilesforFastQC\n\n  output:\n  file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  script:\n  def inputs = Utils.isFq(inputFile1) ? \"${inputFile1} ${inputFile2}\" : \"${inputFile1}\"\n  \"\"\"\n  fastqc -t 2 -q ${inputs}\n  \"\"\"\n}", "process fastqc {\n    tag \"$name\"\n\n    label 'process_low'\n    label 'process_mandatory'\n    \n    input:\n        tuple val(name), path(reads)\n    output:\n        path '*_fastqc.{zip,html}'\n    script:\n        \"\"\"\n        fastqc -t ${task.cpus} -q $reads\n        \"\"\"\n}", "process fastqc {\n\n    tag { \"FastQC - ${sample_id}\" } \n    publishDir \"${outdir}/${sampleProject}/QC-results/fastqc\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    tuple val(sample_id), file(reads)\n    val outdir\n    val opt_args\n    val sampleProject\n\n    output:\n    tuple val(sample_id), path(\"*.{zip,html}\"), emit: fastqc_output\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    fastqc ${usr_args} -t ${task.cpus} -q ${reads[0]}\n    \"\"\"\n}", "\nprocess fastqc_after_clipping {\n    label 'mc_small'\n    tag \"${libraryid}_L${lane}\"\n    publishDir \"${params.outdir}/fastqc/after_clipping\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n\n    when: !params.skip_adapterremoval && !params.skip_fastqc\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(r1), file(r2) from ch_inlinebarcoderemoval_for_fastqc_after_clipping\n\n    output:\n    path(\"*_fastqc.{zip,html}\") into ch_fastqc_after_clipping\n\n    script:\n    if ( params.skip_collapse && seqtype == 'PE' ) {\n    \"\"\"\n    fastqc -t ${task.cpus} -q ${r1} ${r2}\n    \"\"\"\n    } else {\n    \"\"\"\n    fastqc -t ${task.cpus} -q ${r1}\n    \"\"\"\n    }\n\n}", "\nprocess RunFastQC {\n  tag {idPatient + \"-\" + idRun}\n\n  publishDir \"${params.outDir}/Reports/FastQC/${idRun}\", mode: params.publishDirMode\n\n  input:\n    set idPatient, status, idSample, idRun, file(inputFile1), file(inputFile2) from inputFilesforFastQC\n\n  output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  when: step == 'mapping' && !params.noReports\n\n  script:\n  inputFiles = SarekUtils.hasExtension(inputFile1,\"fastq.gz\") ? \"${inputFile1} ${inputFile2}\" : \"${inputFile1}\"\n  \"\"\"\n  fastqc -t 2 -q ${inputFiles}\n  \"\"\"\n}", "\nprocess RunFastQC {\n  tag {idPatient + \"-\" + idRun}\n\n  publishDir \"${directoryMap.fastQC}/${idRun}\", mode: 'link'\n\n  input:\n    set idPatient, status, idSample, idRun, file(fastqFile1), file(fastqFile2) from fastqFilesforFastQC\n\n  output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  when: step == 'mapping' && !params.noReports\n\n  script:\n  \"\"\"\n  fastqc -t 2 -q ${fastqFile1} ${fastqFile2}\n  \"\"\"\n}", "\nprocess RunFastQC {\n  tag {idPatient + \"-\" + idRun}\n\n  publishDir \"${directoryMap.fastQC}/${idRun}\", mode: 'link'\n\n  input:\n    set idPatient, status, idSample, idRun, file(fastqFile1), file(fastqFile2) from fastqFilesforFastQC\n\n  output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  when: step == 'mapping' && !params.noReports\n\n  script:\n  \"\"\"\n  fastqc -t 2 -q ${fastqFile1} ${fastqFile2}\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $pair_id 1 and 2\"\n    cpus 2\n    publishDir \"${params.outdir}/fastqc_${pair_id}_logs/\", mode: 'copy'\n\n    input:\n    tuple val(pair_id), path(reads)\n\n    output:\n    val(pair_id), emit: pair_ids\n    tuple file(\"${pair_id}_1*.zip\"), file(\"${pair_id}_2*.zip\"), file(\"${pair_id}_1*.html\"), file(\"${pair_id}_2*.html\"), emit: fastqc_1\n\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} -q ${reads}\n    \"\"\"  \n}"], "list_proc": ["DLBPointon/annotation-pipeline-nextflow/DLBPointon__annotation-pipeline-nextflow/fastqc_2", "vladsaveliev/cawdor/vladsaveliev__cawdor/RunFastQC", "maxibor/madman/maxibor__madman/fastqc", "sagc-bioinformatics/modules/sagc-bioinformatics__modules/fastqc", "nf-core/eager/nf-core__eager/fastqc_after_clipping", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/RunFastQC", "melnel000/Sarek_CBIO/melnel000__Sarek_CBIO/RunFastQC", "GMS6804-master/Sarek/GMS6804-master__Sarek/RunFastQC", "DLBPointon/annotation-pipeline-nextflow/DLBPointon__annotation-pipeline-nextflow/fastqc"], "list_wf_names": ["vladsaveliev/cawdor", "DLBPointon/annotation-pipeline-nextflow", "sagc-bioinformatics/modules", "GMS6804-master/Sarek", "melnel000/Sarek_CBIO", "maxibor/madman", "UCL-BLIC/Sarek_v2.3.FIX1", "nf-core/eager"]}, {"nb_reuse": 15, "tools": ["SAMtools"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 15, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStat\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStat\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamRecalSamToolsStats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_samtools_stats\n\n    output:\n        file (\"${bam}.samtools.stats.out\") into samtoolsStatsReport\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}"], "list_proc": ["UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/SamtoolsStats", "cgpu/haplosarek/cgpu__haplosarek/SamtoolsStats", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/SamtoolsStats", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/SamtoolsStats", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/SamtoolsStats", "nf-core/sarek/nf-core__sarek/SamtoolsStats", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/SamtoolsStats", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/SamtoolsStats", "cgpu/pgp-chronek/cgpu__pgp-chronek/SamtoolsStats", "rmoran7/custom_sarek/rmoran7__custom_sarek/SamtoolsStats", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/SamtoolsStats", "cgpu/sarek-mirror/cgpu__sarek-mirror/SamtoolsStats", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/SamtoolsStats", "rmoran7/dx_sarek/rmoran7__dx_sarek/SamtoolsStats", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/SamtoolsStats"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror-cache", "cgpu/sarek-mirror", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bactmap"], "list_contrib": ["alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh"], "nb_contrib": 8, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path 'multiqc_config.yaml'\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path ('fastp/*')\n    path ('samtools/*')\n    path ('variants/*')\n\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bactmap/nf-core__bactmap/MULTIQC"], "list_wf_names": ["nf-core/bactmap"]}, {"nb_reuse": 2, "tools": ["BamTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BAMTOOLS_SPLIT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bamtools=2.5.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bamtools:2.5.2--hd03093a_0' :\n        'quay.io/biocontainers/bamtools:2.5.2--hd03093a_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = bam.collect{\"-in $it\"}.join(' ')\n    \"\"\"\n    bamtools \\\\\n        merge \\\\\n        $input_list \\\\\n        | bamtools \\\\\n            split \\\\\n            -stub $prefix \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bamtools: \\$( bamtools --version | grep -e 'bamtools' | sed 's/^.*bamtools //' )\n    END_VERSIONS\n    \"\"\"\n}", "process BAMTOOLS_CONVERT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bamtools=2.5.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bamtools:2.5.1--h9a82719_9' :\n        'quay.io/biocontainers/bamtools:2.5.1--h9a82719_9' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.{bed,fasta,fastq,json,pileup,sam,yaml}\"), emit: data\n    path \"versions.yml\"                                              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def test = args ==~ /-format (bed|fasta|fastq|json|pileup|sam|yaml)/\n    if ( test == false ) error \"-format option must be provided in args. Possible values: bed fasta fastq json pileup sam yaml\"\n    m = args =~ /-format ([a-z]+)/\n    ext = m[0][1]\n\n\n    \"\"\"\n    bamtools \\\\\n        convert \\\\\n        $args \\\\\n        -in $bam \\\\\n        -out ${prefix}.${ext}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bamtools: \\$( bamtools --version | grep -e 'bamtools' | sed 's/^.*bamtools //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BAMTOOLS_SPLIT", "nf-core/modules/nf-core__modules/BAMTOOLS_CONVERT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["crisprvar"], "list_contrib": ["Eletham", "olgabot"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file (fastqc:'fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('trimgalore/*') from trimgalore_results.collect()\n    file ('crispresso/*') from crispresso_logs.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . -m flash\n    \"\"\"\n}"], "list_proc": ["nf-core/crisprvar/nf-core__crisprvar/multiqc"], "list_wf_names": ["nf-core/crisprvar"]}, {"nb_reuse": 1, "tools": ["BaMM"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BAMUTIL_TRIMBAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bamutil=1.0.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bamutil:1.0.15--h2e03b76_1' :\n        'quay.io/biocontainers/bamutil:1.0.15--h2e03b76_1' }\"\n\n    input:\n    tuple val(meta), path(bam), val(trim_left), val(trim_right)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bam \\\\\n        trimBam \\\\\n        $bam \\\\\n        ${prefix}.bam \\\\\n        $args \\\\\n        -L $trim_left \\\\\n        -R $trim_right\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bamutil: \\$( echo \\$( bam trimBam 2>&1 ) | sed 's/^Version: //;s/;.*//' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BAMUTIL_TRIMBAM"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_INTAX {\n    tag \"${tax}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(tax)                      \n\n    output:\n    path(\"taxonomy.qza\") , emit: qza\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    \"\"\"\n    parse_dada2_taxonomy.r $tax\n\n    qiime tools import \\\n        --type 'FeatureData[Taxonomy]' \\\n        --input-format HeaderlessTSVTaxonomyFormat \\\n        --input-path tax.tsv \\\n        --output-path taxonomy.qza\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_INTAX {\n    tag \"${tax}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(tax)                      \n\n    output:\n    path(\"taxonomy.qza\") , emit: qza\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    \"\"\"\n    parse_dada2_taxonomy.r $tax\n\n    qiime tools import \\\n        --type 'FeatureData[Taxonomy]' \\\n        --input-format HeaderlessTSVTaxonomyFormat \\\n        --input-path tax.tsv \\\n        --output-path taxonomy.qza\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_INTAX", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_INTAX"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["MinION", "ARTIC"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["test_nfcore_workflow_chain", "modules", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process ARTIC_MINION {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::artic=1.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/artic:1.2.1--py_0' :\n        'quay.io/biocontainers/artic:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(fastq)\n    path  fast5_dir\n    path  sequencing_summary\n    path  (\"primer-schemes/${scheme}/V${scheme_version}/${scheme}.reference.fasta\")\n    path  (\"primer-schemes/${scheme}/V${scheme_version}/${scheme}.scheme.bed\")\n    path  medaka_model_file\n    val   medaka_model_string\n    val   scheme\n    val   scheme_version\n\n    output:\n    tuple val(meta), path(\"${prefix}.*\")                              , emit: results\n    tuple val(meta), path(\"${prefix}.sorted.bam\")                     , emit: bam\n    tuple val(meta), path(\"${prefix}.sorted.bam.bai\")                 , emit: bai\n    tuple val(meta), path(\"${prefix}.trimmed.rg.sorted.bam\")          , emit: bam_trimmed\n    tuple val(meta), path(\"${prefix}.trimmed.rg.sorted.bam.bai\")      , emit: bai_trimmed\n    tuple val(meta), path(\"${prefix}.primertrimmed.rg.sorted.bam\")    , emit: bam_primertrimmed\n    tuple val(meta), path(\"${prefix}.primertrimmed.rg.sorted.bam.bai\"), emit: bai_primertrimmed\n    tuple val(meta), path(\"${prefix}.consensus.fasta\")                , emit: fasta\n    tuple val(meta), path(\"${prefix}.pass.vcf.gz\")                    , emit: vcf\n    tuple val(meta), path(\"${prefix}.pass.vcf.gz.tbi\")                , emit: tbi\n    tuple val(meta), path(\"*.json\"), optional:true                    , emit: json\n    path  \"versions.yml\"                                              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def version  = scheme_version.toString().toLowerCase().replaceAll('v','')\n    def fast5    = fast5_dir ? \"--fast5-directory $fast5_dir\"             : \"\"\n    def summary  = sequencing_summary ? \"--sequencing-summary $sequencing_summary\" : \"\"\n    def model    = \"\"\n    if (args.tokenize().contains('--medaka')) {\n        fast5   = \"\"\n        summary = \"\"\n        model   = medaka_model_file ? \"--medaka-model ./$medaka_model_file\" : \"--medaka-model $medaka_model_string\"\n    }\n    def hd5_plugin_path = task.ext.hd5_plugin_path ? \"export HDF5_PLUGIN_PATH=\" + task.ext.hd5_plugin_path : \"export HDF5_PLUGIN_PATH=/usr/local/lib/python3.6/site-packages/ont_fast5_api/vbz_plugin\"\n    \"\"\"\n    $hd5_plugin_path\n\n    artic \\\\\n        minion \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --read-file $fastq \\\\\n        --scheme-directory ./primer-schemes \\\\\n        --scheme-version $version \\\\\n        $model \\\\\n        $fast5 \\\\\n        $summary \\\\\n        $scheme \\\\\n        $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        artic: \\$(artic --version 2>&1 | sed 's/^.*artic //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process ARTIC_MINION {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::artic=1.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/artic:1.2.1--py_0' :\n        'quay.io/biocontainers/artic:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(fastq)\n    path  fast5_dir\n    path  sequencing_summary\n    path  (\"primer-schemes/${scheme}/V${scheme_version}/${scheme}.reference.fasta\")\n    path  (\"primer-schemes/${scheme}/V${scheme_version}/${scheme}.scheme.bed\")\n    path  medaka_model_file\n    val   medaka_model_string\n    val   scheme\n    val   scheme_version\n\n    output:\n    tuple val(meta), path(\"${prefix}.*\")                              , emit: results\n    tuple val(meta), path(\"${prefix}.sorted.bam\")                     , emit: bam\n    tuple val(meta), path(\"${prefix}.sorted.bam.bai\")                 , emit: bai\n    tuple val(meta), path(\"${prefix}.trimmed.rg.sorted.bam\")          , emit: bam_trimmed\n    tuple val(meta), path(\"${prefix}.trimmed.rg.sorted.bam.bai\")      , emit: bai_trimmed\n    tuple val(meta), path(\"${prefix}.primertrimmed.rg.sorted.bam\")    , emit: bam_primertrimmed\n    tuple val(meta), path(\"${prefix}.primertrimmed.rg.sorted.bam.bai\"), emit: bai_primertrimmed\n    tuple val(meta), path(\"${prefix}.consensus.fasta\")                , emit: fasta\n    tuple val(meta), path(\"${prefix}.pass.vcf.gz\")                    , emit: vcf\n    tuple val(meta), path(\"${prefix}.pass.vcf.gz.tbi\")                , emit: tbi\n    tuple val(meta), path(\"*.json\"), optional:true                    , emit: json\n    path  \"versions.yml\"                                              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def version  = scheme_version.toString().toLowerCase().replaceAll('v','')\n    def fast5    = fast5_dir ? \"--fast5-directory $fast5_dir\"             : \"\"\n    def summary  = sequencing_summary ? \"--sequencing-summary $sequencing_summary\" : \"\"\n    def model    = \"\"\n    if (args.tokenize().contains('--medaka')) {\n        fast5   = \"\"\n        summary = \"\"\n        model   = medaka_model_file ? \"--medaka-model ./$medaka_model_file\" : \"--medaka-model $medaka_model_string\"\n    }\n    def hd5_plugin_path = task.ext.hd5_plugin_path ? \"export HDF5_PLUGIN_PATH=\" + task.ext.hd5_plugin_path : \"export HDF5_PLUGIN_PATH=/usr/local/lib/python3.6/site-packages/ont_fast5_api/vbz_plugin\"\n    \"\"\"\n    $hd5_plugin_path\n\n    artic \\\\\n        minion \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --read-file $fastq \\\\\n        --scheme-directory ./primer-schemes \\\\\n        --scheme-version $version \\\\\n        $model \\\\\n        $fast5 \\\\\n        $summary \\\\\n        $scheme \\\\\n        $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        artic: \\$(artic --version 2>&1 | sed 's/^.*artic //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process ARTIC_MINION {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::artic=1.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/artic:1.2.1--py_0' :\n        'quay.io/biocontainers/artic:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(fastq)\n    path  fast5_dir\n    path  sequencing_summary\n    path  (\"primer-schemes/${scheme}/V${scheme_version}/${scheme}.reference.fasta\")\n    path  (\"primer-schemes/${scheme}/V${scheme_version}/${scheme}.scheme.bed\")\n    path  medaka_model_file\n    val   medaka_model_string\n    val   scheme\n    val   scheme_version\n\n    output:\n    tuple val(meta), path(\"${prefix}.*\")                              , emit: results\n    tuple val(meta), path(\"${prefix}.sorted.bam\")                     , emit: bam\n    tuple val(meta), path(\"${prefix}.sorted.bam.bai\")                 , emit: bai\n    tuple val(meta), path(\"${prefix}.trimmed.rg.sorted.bam\")          , emit: bam_trimmed\n    tuple val(meta), path(\"${prefix}.trimmed.rg.sorted.bam.bai\")      , emit: bai_trimmed\n    tuple val(meta), path(\"${prefix}.primertrimmed.rg.sorted.bam\")    , emit: bam_primertrimmed\n    tuple val(meta), path(\"${prefix}.primertrimmed.rg.sorted.bam.bai\"), emit: bai_primertrimmed\n    tuple val(meta), path(\"${prefix}.consensus.fasta\")                , emit: fasta\n    tuple val(meta), path(\"${prefix}.pass.vcf.gz\")                    , emit: vcf\n    tuple val(meta), path(\"${prefix}.pass.vcf.gz.tbi\")                , emit: tbi\n    tuple val(meta), path(\"*.json\"), optional:true                    , emit: json\n    path  \"versions.yml\"                                              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def version  = scheme_version.toString().toLowerCase().replaceAll('v','')\n    def fast5    = fast5_dir ? \"--fast5-directory $fast5_dir\"             : \"\"\n    def summary  = sequencing_summary ? \"--sequencing-summary $sequencing_summary\" : \"\"\n    def model    = \"\"\n    if (args.tokenize().contains('--medaka')) {\n        fast5   = \"\"\n        summary = \"\"\n        model   = medaka_model_file ? \"--medaka-model ./$medaka_model_file\" : \"--medaka-model $medaka_model_string\"\n    }\n    def hd5_plugin_path = task.ext.hd5_plugin_path ? \"export HDF5_PLUGIN_PATH=\" + task.ext.hd5_plugin_path : \"export HDF5_PLUGIN_PATH=/usr/local/lib/python3.6/site-packages/ont_fast5_api/vbz_plugin\"\n    \"\"\"\n    $hd5_plugin_path\n\n    artic \\\\\n        minion \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --read-file $fastq \\\\\n        --scheme-directory ./primer-schemes \\\\\n        --scheme-version $version \\\\\n        $model \\\\\n        $fast5 \\\\\n        $summary \\\\\n        $scheme \\\\\n        $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        artic: \\$(artic --version 2>&1 | sed 's/^.*artic //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/ARTIC_MINION", "nf-core/viralrecon/nf-core__viralrecon/ARTIC_MINION", "nf-core/modules/nf-core__modules/ARTIC_MINION"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SnpSift"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["epitopeprediction"], "list_contrib": ["nf-core-bot", "maxulysse", "KevinMenden", "apeltzer", "ggabernet", "lkuchenb", "skrakau", "christopher-mohr", "marissaDubbelaar", "jonasscheid"], "nb_contrib": 10, "codes": ["process SNPSIFT_SPLIT {\n    label 'process_low'\n\n    conda (params.enable_conda ? \"conda-forge::snpsift:4.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpsift:4.2--hdfd78af_5' :\n        'quay.io/biocontainers/snpsift:4.2--hdfd78af_5' }\"\n\n    input:\n    tuple val(meta), path(input_file)\n\n    output:\n    tuple val(meta), path(\"*.vcf\"), emit: splitted\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    SnpSift split ${input_file}\n\n    cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            snpsift: \\$(echo \\$(snpsift -version 2>&1 | sed -n 3p | cut -d\\$' ' -f3))\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/epitopeprediction/nf-core__epitopeprediction/SNPSIFT_SPLIT"], "list_wf_names": ["nf-core/epitopeprediction"]}, {"nb_reuse": 31, "tools": ["FastQC"], "nb_own": 19, "list_own": ["porchard", "SannaAb", "antunderwood", "lifebit-ai", "SoutheyLab", "jiangweiyao", "MDegener", "JingQiChong", "awilson0", "mmcogle", "lucacozzuto", "vibbits", "nf-core", "marcodelapierre", "PhilPalmer", "gavinf97", "UnseenBio", "biocorecrg", "nextflow-io"], "nb_wf": 26, "list_wf": ["nextflow-jnj", "RefNAAP_nf", "nextflow-aws-batch-issue-1024", "RNAseq-pipeline", "nextflow-workflows", "CoursesCRG_Containers_Nextflow_May_2021", "elixir-workshop-21", "elixir_NF", "RNAseq-NextFlow", "shotgun-qc-nf", "RNA_Seq_Pipe", "PHIND_course_nextflow_Feb_2022", "Nextflow_AntiSMASH", "bugseq-pipeline", "GEMmakerCam", "illumina-nf", "SIB_course_nextflow_Nov_2021", "nascent", "qc-dsl2", "qc-nf", "lbf-hack-tutorial", "chipseq-nextflow", "nf_CRACpipeline", "C4LWG-2018", "ELIXIR_containers_nextflow", "paramsReadernf2"], "list_contrib": ["porchard", "SannaAb", "cwytko", "schorlton", "antunderwood", "bentsherman", "jiangweiyao", "MDegener", "JingQiChong", "ignaciot", "biggstd", "pditommaso", "saioruganti", "sarahbonnin", "lucacozzuto", "JohnHadish", "tmuylder", "burkej1", "marcodelapierre", "jvpon", "PhilPalmer", "gavinf97", "jasteen", "heathervant", "toniher", "JoseEspinosa", "apeltzer", "spficklin", "Midnighter", "dianamarek"], "nb_contrib": 30, "codes": ["\nprocess fastqc {\n    \n                            \n                                                          \n    memory '3 GB' \n\n\n    input:\n    set val(name), file(fastq) from fastq_files1 \n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files\n    file \"*_fastqc.{zip,html}\" into qc_files1\n\n    \"\"\"\n    fastqc ${fastq}\n    \"\"\"\n}", "\nprocess fastqc {\n    publishDir fastqcOutputFolder  \t\t\t                                             \n    tag \"$read\"  \t\t\t\t\t\t\t                                                                   \n    label 'onecpu' \n\n    input:\n    file(read) from reads_for_fastqc  \t\t                                                                     \n\n    output:\t\t\t\t\t\t\t\t\t\t                                                                                                 \n   \tfile(\"*_fastqc.*\") into raw_fastqc_files\n\n    script:\t\t\t\t\t\t\t\t\t                                                                                     \n    \"\"\"\n        fastqc ${read} \n    \"\"\"\n}", "\nprocess fastqc {\n    publishDir(params.OUTPUT, mode: 'copy')\n    tag { \"${reads}\" }\n    container params.CONTAINER\n\n    input:\n    path(reads)\n\n    output:\n    path(\"*_fastqc*\") \n    \n\tscript:\n    \"\"\"\n\t\tfastqc ${reads}\n    \"\"\"\n}", "\nprocess fastQC {\n    publishDir fastqcOutputFolder  \t\t\t                                             \n    tag { \"${reads}\" }  \t\t\t\t\t\t\t                                                                   \n    label 'big_mem' \n\n    input:\n    path reads   \t\t\t\t\t\t\t                                                                     \n\n    output:\t\t\t\t\t\t\t\t\t                                                                              \n   \tpath \"*_fastqc.*\"\n\n    script:\t\t\t\t\t\t\t\t\t                                                                                     \n    \"\"\"\n        fastqc ${reads} \n    \"\"\"\n}", "\nprocess qc_pre_trimming {\n  tag { pair_id }\n  \n  publishDir \"${output_dir}/fastqc/pre_trimming\",\n    mode: 'copy',\n    pattern: \"*.html\"\n\n  input:\n  set pair_id, file(file_pair) from raw_fastqs_for_qc\n\n  output:\n  file('*.html')\n\n  \"\"\"\n  fastqc ${file_pair[0]} ${file_pair[1]}\n  \"\"\"\n}", "\nprocess qc_post_merge_interl {\n  tag \"${dir}/${name}\"\n  publishDir \"${dir}/${params.outprefix}${name}\", mode: 'copy'\n\n  input:\n  tuple val(dir), val(name), path(processed_fastq_gz)\n\n  output:\n  tuple val(dir), val(name), path{ params.interleave ? 'interleaved_fastqc.html' : 'merged_fastqc.html' }, path{ params.interleave ? 'interleaved_fastqc.zip' : 'merged_fastqc.zip' }\n\n  script:\n  \"\"\"\n  fastqc $processed_fastq_gz\n  \"\"\"\n}", "\nprocess fastQC {\n    publishDir fastqcOutputFolder  \t\t\t                                             \n    tag { reads }  \t\t\t\t\t\t\t                                                                   \n    label 'twocpus' \n\n    input:\n    path reads   \t\t\t\t\t\t\t                                                                     \n\n    output:\t\t\t\t\t\t\t\t\t                                                                              \n   \tpath \"*_fastqc.*\"\n\n    script:\t\t\t\t\t\t\t\t\t                                                                                     \n    \"\"\"\n        fastqc ${reads} \n    \"\"\"\n}", "\nprocess fastqc {\n    publishDir(params.OUTPUT, mode: 'copy')\n    tag { \"${reads}\" }\n    container params.CONTAINER\n\n    input:\n    path(reads)\n\n    output:\n    path(\"*_fastqc*\")\n\n    script:\n    \"\"\"\n        fastqc ${reads}\n    \"\"\"\n}", "\nprocess fastQC {\n    publishDir fastqcOutputFolder  \t\t\t                                             \n    tag { \"${reads}\" }  \t\t\t\t\t\t\t                                                                   \n    label 'big_mem' \n\n    input:\n    path reads   \t\t\t\t\t\t\t                                                                     \n\n    output:\t\t\t\t\t\t\t\t\t                                                                              \n   \tpath \"*_fastqc.*\"\n\n    script:\t\t\t\t\t\t\t\t\t                                                                                     \n    \"\"\"\n        fastqc ${reads} \n    \"\"\"\n}", "\nprocess fastqc_1 {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: \"*_fastqc.*\"\n  tag { sample_id }\n  label \"fastqc\"\n\n  input:\n    set val(sample_id), file(pass_files) from COMBINED_SAMPLES_FOR_FASTQC_1\n\n  output:\n    set file(\"${sample_id}_?_fastqc.html\") , file(\"${sample_id}_?_fastqc.zip\") optional true into FASTQC_1_OUTPUT\n    set val(sample_id), val(1) into CLEAN_MERGED_FASTQ_FASTQC_SIGNAL\n\n  script:\n  \"\"\"\n  fastqc $pass_files\n  \"\"\"\n}", "\nprocess fastqc_2 {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: \"*_fastqc.*\"\n  tag { sample_id }\n  label \"fastqc\"\n\n  input:\n    set val(sample_id), file(pass_files) from TRIMMED_SAMPLES_FOR_FASTQC\n\n  output:\n    set file(\"${sample_id}_??_trim_fastqc.html\"), file(\"${sample_id}_??_trim_fastqc.zip\") optional true into FASTQC_2_OUTPUT\n    set val(sample_id), val(1) into CLEAN_TRIMMED_FASTQ_FASTQC_SIGNAL\n\n  script:\n  \"\"\"\n  fastqc $pass_files\n  \"\"\"\n}", "\nprocess fastqc {\n    publishDir(params.OUTPUT, mode: 'copy')\n    tag { \"${reads}\" }\n    container params.CONTAINER\n\n    input:\n    path(reads)\n\n    output:\n    path(\"*_fastqc*\")\n\n    script:\n    \"\"\"\n        fastqc ${reads}\n    \"\"\"\n}", "\nprocess fastqc {\n    \n                                                  \n    publishDir(params.OUTPUT, mode: 'copy') \n\n                                                                         \n    container params.CONTAINER\n\n                                                   \n    tag \"${reads}\" \n    \n    input:\n    path(reads)\n\n    output:\n    path(\"*_fastqc*\")\n\n    script:\n    \"\"\"\n        fastqc ${reads}\n    \"\"\"\n}", "\nprocess fastqc {\n\n    cpus 1\n    memory 1.GB\n    errorStrategy 'ignore'\n    publishDir \"${params.publish_dir}\", mode: \"copy\", overwrite: true, enabled: params.publish_dir\n                                                          \n\n                                                                                                                                                      \n    input:\n    tuple val(state), file(fastq) \n\n    output:\n    file \"*_fastqc.{zip,html}\" \n    \"\"\"\n    fastqc ${fastq}\n    \"\"\"\n}", "\nprocess fastQC {\n    publishDir fastqcOutputFolder  \t\t\t\n    tag { \"${reads}\" }  \t\t\t\t\t\t\t\n\n    input:\n    path reads   \t\t\t\t\t\t\t\n\n    output:\t\t\t\t\t\t\t\t\t\n   \tpath \"*_fastqc.*\"\n\n    script:\t\t\t\t\t\t\t\t\t\n    \"\"\"\n        fastqc ${reads} \n    \"\"\"\n}", "\nprocess fastqc {\n  publishDir \"${params.outdir}\", mode:'link'\n\n  input:\n  path fastq\n\n  output:\n  path \"${fastq.getSimpleName()}_fastqc.zip\", emit: data\n  path \"${fastq.getSimpleName()}_fastqc.html\", emit: report\n\n  \"\"\"\n  fastqc ${fastq}\n  \"\"\"\n}", "\nprocess fastqc {\n\n    publishDir \"${params.results}/fastqc\", mode: 'rellink', overwrite: true\n    container 'library://porchard/default/general:20220107'\n    maxForks 6\n    tag \"${library} ${readgroup}\"\n\n    input:\n    tuple val(library), val(readgroup), path(fastq)\n\n    output:\n    tuple path(outfile_1), path(outfile_2)\n\n    script:\n    outfile_1 = fastq.getName().replaceAll('.fastq.gz', '_fastqc.html')\n    outfile_2 = fastq.getName().replaceAll('.fastq.gz', '_fastqc.zip')\n\n    \"\"\"\n    fastqc $fastq\n    \"\"\"\n\n}", "\nprocess fastqc {\n  publishDir \"$params.outdir/quality-control-$sample/\", mode: 'copy', overwrite: true\n  label 'low'\n  container 'quay.io/biocontainers/fastqc:0.11.9--0'\n  \n  input:\n  tuple val(sample), path(reads)\n\n  output:\n  path(\"*_fastqc.{zip,html}\"), emit: fastqc_out\n\n  script:\n                                                      \n  \"\"\"\n  fastqc ${reads}\n  \"\"\"\n}", "\nprocess fastqc_raw_reads {\n\n    input:\n    path read from reads_ch \n    \n    script:\n    \"\"\"\n    fastqc ${read}\n    \"\"\"\n}", "\nprocess fastQC {\n    publishDir path: './fastqc_results', mode: 'copy'\n    input:\n        set baseName, file(fastq) from forFastqc\n    output:\n        set baseName, file(\"${baseName}*.zip\"), file(\"${baseName}*.html\") into fastQCOutput\n\n    executor    globalExecutor\n    stageInMode globalStageInMode\n    cpus        1\n    module      'fastqc'\n    memory      globalMemoryS\n    time        globalTimeM\n    queue       globalQueueL\n\n    \"\"\"\n    fastqc $fastq\n    \"\"\"\n}", "\nprocess fastqc {\nconda 'bioconda::fastqc=0.11.8'\n                                              \ncpus 1\nmemory '2 GB'\n\ninput:\nset file(fastq),val(path),val(case_control), val(passed) from passed_fastqs\n\noutput:\nfile '*_fastqc.zip' into fastqc\n\n\"\"\"\nfastqc $fastq\n\"\"\"\n\n}", " process fastqc {\n          label \"fastqc\"\n\t\t  conda \"$envDir\"\n\n          publishDir = [path: \"$outDir/fastqc\", mode: 'copy']\n          tag \"reads: $sample_id\"\n  \n          input:\n          set val(sample_id), file(reads) from read_files_1\n  \n          output:\n          file(\"${sample_id}*fastqc.html\") into fastqc_html\n          file(\"${sample_id}*fastqc.zip\") into fastqc_zip\n  \n          script:\n          def single = reads instanceof Path\n          \n          if(single)\n              \"\"\"\n              fastqc ${reads}\n              \"\"\"\n          else\n              \"\"\"\n              fastqc ${reads[0]} ${reads[1]}\n              \"\"\"\n      }", "\nprocess fastqc {\n\n    tag \"$name\"\n    publishDir \"results\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc $reads\n    \"\"\"\n}", "\nprocess FastQC {\n    container \"allin.sif\"\n\n    publishDir \"${params.accession}_results/qc/fastqc/untrimmed/\", mode:'copy'\n\n    input:\n        tuple val(key), file(reads) from reads1_ch\n\n    output:\n        file(\"*.zip\") into fastqc_ch\n\n    script:\n        \"\"\"\n        fastqc $reads\n        \"\"\" \n}", "\nprocess FastQC_trim {\n        container 'allin.sif'\n\n        publishDir \"${params.accession}_results/qc/fastqc/trimmed/\", mode:'copy'\n\n        input:\n                tuple val(key), file(reads2) from trimmed_reads_ch\n\n        output:\n                file(\"*.zip\") into trimmed_fastqc_ch\n\n        script:\n        \"\"\"\n        fastqc $reads2\n        \"\"\"\n}", "\nprocess run_fastqc {\n        publishDir params.outdir, mode: 'copy', overwrite: true\n                                \n\n        clusterOptions='-pe mpi 1'\n        executor 'sge'\n        queue 'bfxcore.q@node3-bfx.medair.lcl,bfxcore.q@node2-bfx.medair.lcl'\n\n        input:\n\tfile a from fastqin\n\t       \n\toutput:\n\tfile \"${a.baseName}_fastqc.*\" into fastqcout\n\n        script:\n        \"\"\"\n\t/apps/bio/apps/fastqc/0.11.2/fastqc ${a}\n        \"\"\"\n}", "\nprocess fastqc {\n  tag \"$name\"\n  \n  input:\n  set val(name), file(reads)\n\n  output:\n  file \"*_fastqc.{zip,html}\"\n\n  script:\n  \"\"\"\n  fastqc $reads\n  \"\"\"\n}", "\nprocess fastqc {\n  tag \"$name\"\n  publishDir params.outdir, mode: 'copy'\n\n  input:\n  set val(name), file(reads) from raw_reads_fastqc\n\n  output:\n  file \"*_fastqc.{zip,html}\" into fastqc_results\n\n  script:\n  \"\"\"\n  fastqc $reads\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/qc/fastqc/\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(prefix), file(reads) from fastq_reads_qc.mix(fastq_reads_qc_sra)\n\n    output:\n    file \"*.{zip,html,txt}\" into fastqc_results\n\n    script:\n    prefix = reads.baseName\n    \"\"\"\n    fastqc $reads\n    \"\"\"\n}", "\nprocess runFastQC {\n  publishDir \"${params.output_dir}/demultiplexed_fastqc\", mode: \"copy\"\n  tag \"${demultiplexed_reads}\"\n  \n  input:\n  path demultiplexed_reads\n\n  output:\n  path \"*.*\"\n\n  script:\n  \"\"\"\n  fastqc ${demultiplexed_reads}\n  \"\"\"\n}", "\nprocess fastqc {\n    \n                                                  \n    publishDir(params.OUTPUT, mode: 'copy') \n\n                                                                         \n    container params.CONTAINER\n\n                                                   \n    tag \"${reads}\" \n    \n    input:\n    path(reads)\n\n    output:\n    path(\"*_fastqc*\")\n\n    script:\n    \"\"\"\n        fastqc ${reads}\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/RefNAAP_nf/jiangweiyao__RefNAAP_nf/fastqc", "biocorecrg/C4LWG-2018/biocorecrg__C4LWG-2018/fastqc", "biocorecrg/CoursesCRG_Containers_Nextflow_May_2021/biocorecrg__CoursesCRG_Containers_Nextflow_May_2021/fastqc", "biocorecrg/CoursesCRG_Containers_Nextflow_May_2021/biocorecrg__CoursesCRG_Containers_Nextflow_May_2021/fastQC", "antunderwood/nextflow-aws-batch-issue-1024/antunderwood__nextflow-aws-batch-issue-1024/qc_pre_trimming", "marcodelapierre/illumina-nf/marcodelapierre__illumina-nf/qc_post_merge_interl", "biocorecrg/ELIXIR_containers_nextflow/biocorecrg__ELIXIR_containers_nextflow/fastQC", "biocorecrg/PHIND_course_nextflow_Feb_2022/biocorecrg__PHIND_course_nextflow_Feb_2022/fastqc", "biocorecrg/PHIND_course_nextflow_Feb_2022/biocorecrg__PHIND_course_nextflow_Feb_2022/fastQC", "mmcogle/GEMmakerCam/mmcogle__GEMmakerCam/fastqc_1", "mmcogle/GEMmakerCam/mmcogle__GEMmakerCam/fastqc_2", "biocorecrg/SIB_course_nextflow_Nov_2021/biocorecrg__SIB_course_nextflow_Nov_2021/fastqc", "nextflow-io/elixir-workshop-21/nextflow-io__elixir-workshop-21/fastqc", "jiangweiyao/paramsReadernf2/jiangweiyao__paramsReadernf2/fastqc", "biocorecrg/SIB_course_nextflow_Nov_2021/biocorecrg__SIB_course_nextflow_Nov_2021/fastQC", "UnseenBio/shotgun-qc-nf/UnseenBio__shotgun-qc-nf/fastqc", "porchard/RNAseq-NextFlow/porchard__RNAseq-NextFlow/fastqc", "vibbits/chipseq-nextflow/vibbits__chipseq-nextflow/fastqc", "vibbits/nextflow-jnj/vibbits__nextflow-jnj/fastqc_raw_reads", "SoutheyLab/nextflow-workflows/SoutheyLab__nextflow-workflows/fastQC", "awilson0/bugseq-pipeline/awilson0__bugseq-pipeline/fastqc", "MDegener/RNAseq-pipeline/MDegener__RNAseq-pipeline/fastqc", "PhilPalmer/lbf-hack-tutorial/PhilPalmer__lbf-hack-tutorial/fastqc", "gavinf97/Nextflow_AntiSMASH/gavinf97__Nextflow_AntiSMASH/FastQC", "gavinf97/Nextflow_AntiSMASH/gavinf97__Nextflow_AntiSMASH/FastQC_trim", "SannaAb/RNA_Seq_Pipe/SannaAb__RNA_Seq_Pipe/run_fastqc", "lifebit-ai/qc-dsl2/lifebit-ai__qc-dsl2/fastqc", "lifebit-ai/qc-nf/lifebit-ai__qc-nf/fastqc", "nf-core/nascent/nf-core__nascent/fastqc", "JingQiChong/nf_CRACpipeline/JingQiChong__nf_CRACpipeline/runFastQC", "lucacozzuto/elixir_NF/lucacozzuto__elixir_NF/fastqc"], "list_wf_names": ["antunderwood/nextflow-aws-batch-issue-1024", "biocorecrg/SIB_course_nextflow_Nov_2021", "SoutheyLab/nextflow-workflows", "gavinf97/Nextflow_AntiSMASH", "jiangweiyao/paramsReadernf2", "nextflow-io/elixir-workshop-21", "marcodelapierre/illumina-nf", "awilson0/bugseq-pipeline", "PhilPalmer/lbf-hack-tutorial", "biocorecrg/ELIXIR_containers_nextflow", "lifebit-ai/qc-nf", "vibbits/chipseq-nextflow", "porchard/RNAseq-NextFlow", "nf-core/nascent", "vibbits/nextflow-jnj", "MDegener/RNAseq-pipeline", "JingQiChong/nf_CRACpipeline", "jiangweiyao/RefNAAP_nf", "biocorecrg/C4LWG-2018", "mmcogle/GEMmakerCam", "UnseenBio/shotgun-qc-nf", "lucacozzuto/elixir_NF", "lifebit-ai/qc-dsl2", "SannaAb/RNA_Seq_Pipe", "biocorecrg/PHIND_course_nextflow_Feb_2022", "biocorecrg/CoursesCRG_Containers_Nextflow_May_2021"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 2, "list_own": ["chelauk", "nf-core"], "nb_wf": 2, "list_wf": ["nf-core-mutectplatypus", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "chelauk", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process GATK4_CALCULATECONTAMINATION {\n    tag \"${patient}_${sample}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(patient), val(sample), path(tumour_table), path(normal_table)\n\n    output:\n    tuple val(patient), val(sample), path('*.contamination.table'), emit: contamination\n    tuple val(patient), val(sample), path('*.segmentation.table') , emit: segmentation, optional:true\n    path \"versions.yml\"                           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${patient}_${sample}\"\n    def matched_command = normal_table ? \" -matched ${normal_table} \" : ''\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CalculateContamination] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CalculateContamination \\\\\n        --input $tumour_table \\\\\n        $matched_command \\\\\n        -segments ${prefix}.segmentation.table \\\\\n        --output ${prefix}.contamination.table \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n    stub:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${patient}_${sample}\"\n    def matched_command = normal_table ? \" -matched ${normal_table} \" : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CalculateContamination] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    echo -e \"gatk --java-options \"-Xmx${avail_mem}g\" CalculateContamination \\\\\n        --input $tumour_table \\\\\n        $matched_command \\\\\n        -segments ${prefix}.segmentation.table \\\\\n        --output ${prefix}.contamination.table \\\\\n        --tmp-dir . \\\\\n        $args\"\n\n    touch ${prefix}.contamination.table\n    touch ${prefix}.segmentation.table\n    touch versions.yml \n    \"\"\"\n}", "process GATK4_CALCULATECONTAMINATION {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(pileup), path(matched)\n\n    output:\n    tuple val(meta), path('*.contamination.table'), emit: contamination\n    tuple val(meta), path('*.segmentation.table') , emit: segmentation, optional:true\n    path \"versions.yml\"                           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def matched_command = matched ? \"--matched-normal $matched\" : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CalculateContamination] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CalculateContamination \\\\\n        --input $pileup \\\\\n        --output ${prefix}.contamination.table \\\\\n        $matched_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["chelauk/nf-core-mutectplatypus/chelauk__nf-core-mutectplatypus/GATK4_CALCULATECONTAMINATION", "nf-core/modules/nf-core__modules/GATK4_CALCULATECONTAMINATION"], "list_wf_names": ["chelauk/nf-core-mutectplatypus", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["BUStools"], "nb_own": 2, "list_own": ["nf-core", "redst4r"], "nb_wf": 2, "list_wf": ["nf-10x-kallisto", "scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "redst4r", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 8, "codes": [" process bustools_correct_sort{\n                  \n     publishDir \"${params.outdir}/kallisto/sort_bus\", mode: 'copy'\n\n     input:\n     file bus from kallisto_bus_to_sort\n     file whitelist from barcode_whitelist_kallisto\n\n     output:\n     file bus into (kallisto_corrected_sort_to_count, kallisto_corrected_sort_to_metrics)\n\n\n     script:\n     if(params.bustools_correct) {\n       correct = \"bustools correct -w $whitelist -o ${bus}/output.corrected.bus ${bus}/output.bus\"\n       sort_file = \"${bus}/output.corrected.bus\"\n                                                                    \n                                   \n       cleanup = \"rm ${bus}/output.corrected.bus && rm ${bus}/output.bus\"\n     } else {\n       correct = \"\"\n       sort_file = \"${bus}/output.bus\"\n       cleanup = \"rm ${bus}/output.bus\"\n\n     }\n     \"\"\"\n     $correct\n     mkdir -p tmp\n     bustools sort -T tmp/ -t ${params.cpus} -m ${params.mem} -o ${bus}/output.corrected.sort.bus $sort_file\n     $cleanup\n     \"\"\"\n }", "\nprocess bustools_correct_sort{\n    tag \"$bus\"\n    label 'mid_memory'\n    publishDir \"${params.outdir}/kallisto/sort_bus\", mode: 'copy'\n\n    input:\n    file bus from kallisto_bus_to_sort\n    file whitelist from barcode_whitelist_kallisto.mix(barcode_whitelist_kallisto_unzip).collect()\n\n    output:\n    file bus into (kallisto_corrected_sort_to_count, kallisto_corrected_sort_to_metrics)\n\n    when: !params.skip_bustools\n\n    script:\n    if(params.bustools_correct) {\n      correct = \"bustools correct -w $whitelist -o ${bus}/output.corrected.bus ${bus}/output.bus\"\n      sort_file = \"${bus}/output.corrected.bus\"\n    } else {\n      correct = \"\"\n      sort_file = \"${bus}/output.bus\"\n    }\n    \"\"\"\n    $correct    \n    mkdir -p tmp\n    bustools sort -T tmp/ -t ${task.cpus} -m ${task.memory.toGiga()}G -o ${bus}/output.corrected.sort.bus $sort_file\n    \"\"\"\n}"], "list_proc": ["redst4r/nf-10x-kallisto/redst4r__nf-10x-kallisto/bustools_correct_sort", "nf-core/scrnaseq/nf-core__scrnaseq/bustools_correct_sort"], "list_wf_names": ["redst4r/nf-10x-kallisto", "nf-core/scrnaseq"]}, {"nb_reuse": 1, "tools": ["FastQC", "Skewer"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": ["\nprocess trim_and_combine {\n    tag { \"Preprocessing of \" + reads.size()/2 + \"  read pairs for \" + sample_id }\n                                                                     \n\n    input:\n        set sample_id, file(reads) from fastq_ch\n    output:\n        set sample_id, file(\"${sample_id}_R1-trimmed.fastq.gz\"), file(\"${sample_id}_R2-trimmed.fastq.gz\") into trim_and_combine_ch\n    script:\n        \"\"\"\n        # loop over readunits in pairs per sample\n        pairno=0\n        echo ${reads.join(\" \")} | xargs -n2 | while read fq1 fq2; do\n            let pairno=pairno+1\n            # note: don't make reads smaller than assembler kmer length\n            skewer --quiet -t ${task.cpus} -m pe -q 3 -n -l 31 -z -o pair\\${pairno}-skewer-out \\$fq1 \\$fq2;\n            cat *-trimmed-pair1.fastq.gz >> ${sample_id}_R1-trimmed.fastq.gz;\n            cat *-trimmed-pair2.fastq.gz >> ${sample_id}_R2-trimmed.fastq.gz;\n            rm *-trimmed-pair[12].fastq.gz;\n        done\n        fastqc -t {task.cpus} ${sample_id}_R1-trimmed.fastq.gz ${sample_id}_R2-trimmed.fastq.gz;\n        \"\"\"\n}"], "list_proc": ["nf-core/vipr/nf-core__vipr/trim_and_combine"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 19, "tools": ["GATK"], "nb_own": 12, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "UCL-BLIC", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 19, "list_wf": ["sarek_ubec", "layer_lab_chco", "layer_lab_vc", "germline_somatic", "dx_sarek", "haplosarek", "sarek-mirror-cache", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "sarek-mirror", "pgp-chronek", "test_nextflow_sarek", "sarek-genomechronicler", "saw.sarek", "custom_sarek", "sarek", "GenomeChronicler-Sarek-nf", "layer_lab_caw", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        each file(dbsnp) from ch_dbsnp\n        each file(dbsnpIndex) from ch_dbsnpIndex\n        each file(dict) from ch_dict\n        each file(fasta) from ch_fasta\n        each file(fastaFai) from ch_fastaFai\n\n    output:\n                                 \n        set val(\"HaplotypeCaller${gvcf_tag}\"), idPatient, idSample, file(outputVcf), file(\"${outputVcf}.idx\") into vcfHaplotypeCallerVEP\n        set val(\"HaplotypeCaller${gvcf_tag}\"), idPatient, idSample, file(outputVcf) into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(outputVcf) into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    name = \"${intervalBed.baseName}_${idSample}\"\n    output_suffix = params.noGVCF ? '.vcf' : '.g.vcf'\n    outputVcf = name + output_suffix\n    gvcf_arg = params.noGVCF ? '' : '-ERC GVCF'\n    gvcf_tag = params.noGVCF ? '' : 'GVCF'\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${outputVcf} \\\n        ${gvcf_arg}\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    memory '16 GB'\n    cpus '1'\n\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        each file(dbsnp) from ch_dbsnp\n        each file(dbsnpIndex) from ch_dbsnpIndex\n        each file(dict) from ch_dict\n        each file(fasta) from ch_fasta\n        each file(fastaFai) from ch_fastaFai\n\n    output:\n                                 \n        set val(\"HaplotypeCaller${gvcf_tag}\"), idPatient, idSample, file(outputVcf), file(\"${outputVcf}.idx\") into vcfHaplotypeCallerVEP\n        set val(\"HaplotypeCaller${gvcf_tag}\"), idPatient, idSample, file(outputVcf) into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(outputVcf) into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    name = \"${intervalBed.baseName}_${idSample}\"\n    output_suffix = params.noGVCF ? '.vcf' : '.g.vcf'\n    outputVcf = name + output_suffix\n    gvcf_arg = params.noGVCF ? '' : '-ERC GVCF'\n    gvcf_tag = params.noGVCF ? '' : 'GVCF'\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${outputVcf} \\\n        ${gvcf_arg}\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess RunHaplotypecaller {\n  tag {idSample + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSample, file(bam), file(bai), file(intervalBed), recalTable from bamsForHC                                              \n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.dbsnp,\n      referenceMap.dbsnpIndex\n    ])\n\n  output:\n    set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into hcGenomicVCF\n    set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into vcfsToGenotype\n\n  when: 'haplotypecaller' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n    HaplotypeCaller \\\n    -R ${genomeFile} \\\n    -I ${bam} \\\n    -L ${intervalBed} \\\n    -D ${dbsnp} \\\n    -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n    -ERC GVCF\n  \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'process_medium'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}"], "list_proc": ["cgpu/haplosarek/cgpu__haplosarek/HaplotypeCaller", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/HaplotypeCaller", "cgpu/sarek-mirror/cgpu__sarek-mirror/HaplotypeCaller", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/HaplotypeCaller", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/HaplotypeCaller", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/HaplotypeCaller", "cgpu/pgp-chronek/cgpu__pgp-chronek/HaplotypeCaller", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/HaplotypeCaller", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/HaplotypeCaller", "nf-core/sarek/nf-core__sarek/HaplotypeCaller", "rmoran7/custom_sarek/rmoran7__custom_sarek/HaplotypeCaller", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/HaplotypeCaller", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/HaplotypeCaller", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/HaplotypeCaller", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/HaplotypeCaller", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/RunHaplotypecaller", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/HaplotypeCaller", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/HaplotypeCaller", "rmoran7/dx_sarek/rmoran7__dx_sarek/HaplotypeCaller"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "ryanlayerlab/layer_lab_chco", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "cgpu/sarek-mirror", "sickle-in-africa/saw.sarek", "cgpu/haplosarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "javaidm/layer_lab_vc", "cgpu/pgp-chronek", "UCL-BLIC/Sarek_v2.3.FIX1", "ryanlayerlab/layer_lab_caw", "rmoran7/dx_sarek", "cgpu/sarek-mirror-cache"]}, {"nb_reuse": 1, "tools": ["RapidNJ"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bactmap"], "list_contrib": ["alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh"], "nb_contrib": 8, "codes": ["\nprocess RAPIDNJ {\n    label 'process_medium'\n    memory { 1.GB * task.attempt }\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::rapidnj=2.3.2 conda-forge::biopython=1.78\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-805c6e0f138f952f9c61cdd57c632a1a263ea990:3c52e4c8da6b3e4d69b9ca83fa4d366168898179-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-805c6e0f138f952f9c61cdd57c632a1a263ea990:3c52e4c8da6b3e4d69b9ca83fa4d366168898179-0\"\n    }\n\n    input:\n    path alignment\n\n    output:\n    path \"*.sth\"        , emit: stockholm_alignment\n    path \"*.tre\"        , emit: phylogeny\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    python \\\\\n        -c 'from Bio import SeqIO; SeqIO.convert(\"$alignment\", \"fasta\", \"alignment.sth\", \"stockholm\")'\n\n    rapidnj \\\\\n        alignment.sth \\\\\n        $options.args \\\\\n        -i sth \\\\\n        -c $task.cpus \\\\\n        -x rapidnj_phylogeny.tre\n\n    # Doesn't appear to be a way of getting the version number\n    echo 2.3.2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bactmap/nf-core__bactmap/RAPIDNJ"], "list_wf_names": ["nf-core/bactmap"]}, {"nb_reuse": 1, "tools": ["ULTRA"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process ULTRA_PIPELINE {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::ultra_bioinformatics=0.0.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ultra_bioinformatics:0.0.4.1--pyh5e36f6f_0' :\n        'quay.io/biocontainers/ultra_bioinformatics:0.0.4.1--pyh5e36f6f_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path genome\n    path gtf\n\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    uLTRA \\\\\n        pipeline \\\\\n        --t $task.cpus \\\\\n        --prefix $prefix \\\\\n        $args \\\\\n        $genome \\\\\n        $gtf \\\\\n        $reads \\\\\n        ./\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ultra: \\$( uLTRA --version|sed 's/uLTRA //g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/ULTRA_PIPELINE"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 3, "tools": ["SAMtools", "STAR"], "nb_own": 3, "list_own": ["nf-core", "clairecoleman1", "oisinmccaffrey"], "nb_wf": 3, "list_wf": ["clipseq.nextflow", "clipseq1", "clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "oisinmccaffrey", "drpatelh", "clairecoleman1", "CharlotteAnne"], "nb_contrib": 8, "codes": ["\nprocess align {\n    tag \"$name\"\n    publishDir \"${params.outdir}/mapped\", mode: 'copy'\n\n    input:\n    tuple val(name), path(reads) from ch_unmapped\n    path(index) from ch_star_index.collect()\n\n    output:\n    tuple val(name), path(\"${name}.Aligned.sortedByCoord.out.bam\"), path(\"${name}.Aligned.sortedByCoord.out.bam.bai\") into ch_aligned, ch_aligned_preseq\n    path \"*.Log.final.out\" into ch_align_mqc, ch_align_qc\n\n    script:\n    clip_args = \"--outFilterMultimapNmax 1 \\\n                --outFilterMultimapScoreRange 1 \\\n                --outSAMattributes All \\\n                --alignSJoverhangMin 8 \\\n                --alignSJDBoverhangMin 1 \\\n                --outFilterType BySJout \\\n                --alignIntronMin 20 \\\n                --alignIntronMax 1000000 \\\n                --outFilterScoreMin 10  \\\n                --alignEndsType Extend5pOfRead1 \\\n                --twopassMode Basic \\\n                --outSAMtype BAM Unsorted\"\n    \"\"\"\n    STAR \\\\\n        --runThreadN $task.cpus \\\\\n        --runMode alignReads \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads --readFilesCommand gunzip -c \\\\\n        --outFileNamePrefix ${name}. $clip_args\n    samtools sort -@ $task.cpus -o ${name}.Aligned.sortedByCoord.out.bam ${name}.Aligned.out.bam\n    samtools index -@ $task.cpus ${name}.Aligned.sortedByCoord.out.bam\n    \"\"\"\n}", "\nprocess align {\n    tag \"$name\"\n    publishDir \"${params.outdir}/mapped\", mode: 'copy'\n\n    input:\n    tuple val(name), path(reads) from ch_unmapped\n    path(index) from ch_star_index.collect()\n\n    output:\n    tuple val(name), path(\"${name}.Aligned.sortedByCoord.out.bam\"), path(\"${name}.Aligned.sortedByCoord.out.bam.bai\") into ch_aligned, ch_aligned_preseq\n    path \"*.Log.final.out\" into ch_align_mqc, ch_align_qc\n\n    script:\n    clip_args = \"--outFilterMultimapNmax 1 \\\n                --outFilterMultimapScoreRange 1 \\\n                --outSAMattributes All \\\n                --alignSJoverhangMin 8 \\\n                --alignSJDBoverhangMin 1 \\\n                --outFilterType BySJout \\\n                --alignIntronMin 20 \\\n                --alignIntronMax 1000000 \\\n                --outFilterScoreMin 10  \\\n                --alignEndsType Extend5pOfRead1 \\\n                --twopassMode Basic \\\n                --outSAMtype BAM Unsorted\"\n    \"\"\"\n    STAR \\\\\n        --runThreadN $task.cpus \\\\\n        --runMode alignReads \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads --readFilesCommand gunzip -c \\\\\n        --outFileNamePrefix ${name}. $clip_args\n    samtools sort -@ $task.cpus -o ${name}.Aligned.sortedByCoord.out.bam ${name}.Aligned.out.bam\n    samtools index -@ $task.cpus ${name}.Aligned.sortedByCoord.out.bam\n    \"\"\"\n}", "\nprocess align {\n    tag \"$name\"\n    label 'process_high'\n    publishDir \"${params.outdir}/mapped\", mode: params.publish_dir_mode\n\n    input:\n    tuple val(name), path(reads) from ch_unmapped\n    path(index) from ch_star_index.collect()\n\n    output:\n    tuple val(name), path(\"${name}.Aligned.sortedByCoord.out.bam\"), path(\"${name}.Aligned.sortedByCoord.out.bam.bai\") into ch_aligned, ch_aligned_preseq\n    path \"*.Log.final.out\" into ch_align_mqc, ch_align_qc\n\n    script:\n    clip_args = \"--outFilterMultimapNmax 1 \\\n                --outFilterMultimapScoreRange 1 \\\n                --outSAMattributes All \\\n                --alignSJoverhangMin 8 \\\n                --alignSJDBoverhangMin 1 \\\n                --outFilterType BySJout \\\n                --alignIntronMin 20 \\\n                --alignIntronMax 1000000 \\\n                --outFilterScoreMin 10  \\\n                --alignEndsType Extend5pOfRead1 \\\n                --twopassMode Basic \\\n                --outSAMtype BAM Unsorted\"\n    \"\"\"\n    STAR \\\\\n        --runThreadN $task.cpus \\\\\n        --runMode alignReads \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads --readFilesCommand gunzip -c \\\\\n        --outFileNamePrefix ${name}. $clip_args\n\n    samtools sort -@ $task.cpus -o ${name}.Aligned.sortedByCoord.out.bam ${name}.Aligned.out.bam\n    samtools index -@ $task.cpus ${name}.Aligned.sortedByCoord.out.bam\n    \"\"\"\n}"], "list_proc": ["clairecoleman1/clipseq1/clairecoleman1__clipseq1/align", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/align", "nf-core/clipseq/nf-core__clipseq/align"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["BiocStyle"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["cutandrun"], "list_contrib": ["jordeu", "nf-core-bot", "cjfields", "ewels", "KevinMenden", "charlotte-west", "dladd", "chris-cheshire"], "nb_contrib": 8, "codes": ["\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"           , emit: yml\n    path \"software_versions_mqc.yml\"       , emit: mqc_yml\n    path \"software_versions_unique_mqc.yml\", emit: mqc_unique_yml\n    path \"local_versions.yml\"              , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    def _make_versions_unique_html(versions):\n        unique_versions = []\n\n        for process, tmp_versions in sorted(versions.items()):\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                tool_version = tool + \"=\" + version\n                if tool_version not in unique_versions:\n                    unique_versions.append(tool_version)\n\n        unique_versions.sort()\n\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions-unique tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions-unique\">\n                    <thead>\n                        <tr>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n\n        for tool_version in unique_versions:\n            tool_version_split = tool_version.split('=')\n            html.append(\"<tbody>\")\n            html.append(\n                dedent(\n                    f'''\\\\\n                    <tr>\n                        <td><samp>{tool_version_split[0]}</samp></td>\n                        <td><samp>{tool_version_split[1]}</samp></td>\n                    </tr>\n                    '''\n                )\n            )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions by Process',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    versions_mqc_unique = {\n        'id': 'software_versions_unique',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_unique_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open(\"software_versions_unique_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc_unique, f, default_flow_style=False)\n\n    with open('local_versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}"], "list_proc": ["nf-core/cutandrun/nf-core__cutandrun/CUSTOM_DUMPSOFTWAREVERSIONS"], "list_wf_names": ["nf-core/cutandrun"]}, {"nb_reuse": 5, "tools": ["TIDDIT"], "nb_own": 2, "list_own": ["vincenthhu", "nf-core"], "nb_wf": 3, "list_wf": ["modules", "raredisease", "nf-core-westest"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process TIDDIT_SV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::tiddit=2.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/tiddit:2.12.1--py38h1773678_0' :\n        'quay.io/biocontainers/tiddit:2.12.1--py38h1773678_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n    path  fai\n\n    output:\n    tuple val(meta), path(\"*.vcf\")        , emit: vcf\n    tuple val(meta), path(\"*.ploidy.tab\") , emit: ploidy\n    tuple val(meta), path(\"*.signals.tab\"), emit: signals\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta == \"dummy_file.txt\" ? \"--ref $fasta\" : \"\"\n    \"\"\"\n    tiddit \\\\\n        --sv \\\\\n        $args \\\\\n        --bam $bam \\\\\n        $reference \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf\n    touch ${prefix}.ploidy.tab\n    touch ${prefix}.signals.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process TIDDIT_COV {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::tiddit=2.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/tiddit:2.12.1--py38h1773678_0' :\n        'quay.io/biocontainers/tiddit:2.12.1--py38h1773678_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.tab\"), optional: true, emit: cov\n    tuple val(meta), path(\"*.wig\"), optional: true, emit: wig\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--ref $fasta\" : \"\"\n    \"\"\"\n    tiddit \\\\\n        --cov \\\\\n        -o $prefix \\\\\n        $args \\\\\n        --bam $bam \\\\\n        $reference\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.wig\n    touch ${prefix}.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process TIDDIT_SV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::tiddit=2.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/tiddit:2.12.1--py38h1773678_0' :\n        'quay.io/biocontainers/tiddit:2.12.1--py38h1773678_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n    path  fai\n\n    output:\n    tuple val(meta), path(\"*.vcf\")        , emit: vcf\n    tuple val(meta), path(\"*.ploidy.tab\") , emit: ploidy\n    tuple val(meta), path(\"*.signals.tab\"), emit: signals\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--ref $fasta\" : \"\"\n    \"\"\"\n    tiddit \\\\\n        --sv \\\\\n        $args \\\\\n        --bam $bam \\\\\n        $reference \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf\n    touch ${prefix}.ploidy.tab\n    touch ${prefix}.signals.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process TIDDIT_COV {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::tiddit=2.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/tiddit:2.12.1--py38h1773678_0' :\n        'quay.io/biocontainers/tiddit:2.12.1--py38h1773678_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.tab\"), optional: true, emit: cov\n    tuple val(meta), path(\"*.wig\"), optional: true, emit: wig\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--ref $fasta\" : \"\"\n    \"\"\"\n    tiddit \\\\\n        --cov \\\\\n        -o $prefix \\\\\n        $args \\\\\n        --bam $bam \\\\\n        $reference\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.wig\n    touch ${prefix}.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process TIDDIT_COV {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::tiddit=2.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/tiddit:2.12.1--py38h1773678_0' :\n        'quay.io/biocontainers/tiddit:2.12.1--py38h1773678_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.tab\"), optional: true, emit: cov\n    tuple val(meta), path(\"*.wig\"), optional: true, emit: wig\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--ref $fasta\" : \"\"\n    \"\"\"\n    tiddit \\\\\n        --cov \\\\\n        -o $prefix \\\\\n        $args \\\\\n        --bam $bam \\\\\n        $reference\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tiddit: \\$(echo \\$(tiddit 2>&1) | sed 's/^.*TIDDIT-//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/TIDDIT_SV", "nf-core/raredisease/nf-core__raredisease/TIDDIT_COV", "nf-core/modules/nf-core__modules/TIDDIT_SV", "nf-core/modules/nf-core__modules/TIDDIT_COV", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/TIDDIT_COV"], "list_wf_names": ["vincenthhu/nf-core-westest", "nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_FILTERASV {\n    tag \"${category}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    val(category)\n\n    output:\n    path(\"*.qza\")       , emit: qza\n    path \"versions.yml\" , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        IFS=',' read -r -a metacategory <<< \\\"$category\\\"\n\n        #remove samples that do not have any value\n        for j in \\\"\\${metacategory[@]}\\\"\n        do\n            qiime feature-table filter-samples \\\n                --i-table ${table} \\\n                --m-metadata-file ${metadata} \\\n                --p-where \\\"\\$j<>\\'\\'\\\" \\\n                --o-filtered-table \\$j.qza\n        done\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir beta_diversity\n        echo \"\" > \"WARNING No column in ${metadata.baseName} seemed suitable.qza\"\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process QIIME2_FILTERASV {\n    tag \"${category}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    val(category)\n\n    output:\n    path(\"*.qza\")       , emit: qza\n    path \"versions.yml\" , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        IFS=',' read -r -a metacategory <<< \\\"$category\\\"\n\n        #remove samples that do not have any value\n        for j in \\\"\\${metacategory[@]}\\\"\n        do\n            qiime feature-table filter-samples \\\n                --i-table ${table} \\\n                --m-metadata-file ${metadata} \\\n                --p-where \\\"\\$j<>\\'\\'\\\" \\\n                --o-filtered-table \\$j.qza\n        done\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir beta_diversity\n        echo \"\" > \"WARNING No column in ${metadata.baseName} seemed suitable.qza\"\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_FILTERASV", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_FILTERASV"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_DIVERSITY_BETAORD {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core)\n\n    output:\n    path(\"beta_diversity/*\"), emit: beta\n    path \"versions.yml\"     , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime emperor plot \\\n        --i-pcoa ${core} \\\n        --m-metadata-file ${metadata} \\\n        --o-visualization ${core.baseName}-vis.qzv\n    qiime tools export --input-path ${core.baseName}-vis.qzv \\\n        --output-path beta_diversity/${core.baseName}-PCoA\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_DIVERSITY_BETAORD {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core)\n\n    output:\n    path(\"beta_diversity/*\"), emit: beta\n    path \"versions.yml\"     , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n    mkdir beta_diversity\n\n    qiime emperor plot \\\n        --i-pcoa ${core} \\\n        --m-metadata-file ${metadata} \\\n        --o-visualization ${core.baseName}-vis.qzv\n    qiime tools export --input-path ${core.baseName}-vis.qzv \\\n        --output-path beta_diversity/${core.baseName}-PCoA\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_DIVERSITY_BETAORD", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_DIVERSITY_BETAORD"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess variantSelect {\n    tag \"${name}\"\n    publishDir \"${params.outdir}/GATK_VariantSelection\", mode: 'copy', \n    saveAs: {filename -> params.saveIntermediateVariants ? \"$filename\" : null }\n\n    input:\n    set val(name), file(raw_vcf), file(raw_vcf_idx) from raw_gvcfs\n\n    output:\n    set val(name), file(\"${name}_snp.vcf\"), file(\"${name}_snp.vcf.idx\") into raw_snp\n    set val(name), file(\"${name}_indels.vcf\"), file(\"${name}_indels.vcf.idx\") into raw_indels\n\n    script:\n    \"\"\"\n    gatk -T SelectVariants \\\\\n        -R $params.gfasta \\\\\n        --variant $raw_vcf \\\\\n        --out ${name}_snp.vcf \\\\\n        --selectTypeToInclude SNP\n\n    gatk -T SelectVariants \\\\\n        -R $params.gfasta \\\\\n        --variant $raw_vcf \\\\\n        --out ${name}_indels.vcf \\\\\n        --selectTypeToInclude INDEL \\\\\n        --selectTypeToInclude MIXED \\\\\n        --selectTypeToInclude MNP \\\\\n        --selectTypeToInclude SYMBOLIC \\\\\n        --selectTypeToInclude NO_VARIATION\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/variantSelect"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["IGVtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess igvtools {\n    tag \"$name\"\n                                                                             \n                                              \n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/mapped/tdfs\", mode: 'copy', pattern: \"*.tdf\"\n\n    input:\n    set val(name), file(normalized_bg) from bedgraph_tdf\n    file chrom_sizes from chrom_sizes_for_igv\n\n    output:\n    set val(name), file(\"*.tdf\") into tiled_data_ch\n\n    script:\n    \"\"\"\n    export LC_ALL=C\n    igvtools toTDF ${normalized_bg} ${name}.rcc.tdf ${chrom_sizes}\n    \"\"\"\n }"], "list_proc": ["nf-core/nascent/nf-core__nascent/igvtools"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["hlatyping"], "list_contrib": ["nvk747", "KochTobi", "nf-core-bot", "ewels", "sven1103", "maxulysse", "ggabernet", "apeltzer", "pditommaso", "christopher-mohr"], "nb_contrib": 10, "codes": [" process remap_to_hla {\n        label 'process_medium'\n\n        input:\n        path(data_index) from params.base_index_path\n        set val(pattern), file(bams) from input_data\n        output:\n        set val(pattern), \"mapped_{1,2}.bam\" into fished_reads\n\n        script:\n        def full_index = \"$data_index/$base_index_name\"\n        if (params.single_end)\n            \"\"\"\n            samtools bam2fq $bams > output_1.fastq\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index output_1.fastq > output_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -b1 output_1.bam > mapped_1.bam\n            \"\"\"\n        else\n            \"\"\"\n            samtools view -@ ${task.cpus} -h -f 0x40 $bams > output_1.bam\n            samtools view -@ ${task.cpus} -h -f 0x80 $bams > output_2.bam\n            samtools bam2fq output_1.bam > output_1.fastq\n            samtools bam2fq output_2.bam > output_2.fastq\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index output_1.fastq output_2.fastq > output.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x40 -b1 output.bam > mapped_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x80 -b1 output.bam > mapped_2.bam\n            \"\"\"\n    }"], "list_proc": ["nf-core/hlatyping/nf-core__hlatyping/remap_to_hla"], "list_wf_names": ["nf-core/hlatyping"]}, {"nb_reuse": 2, "tools": ["BUStools"], "nb_own": 2, "list_own": ["nf-core", "redst4r"], "nb_wf": 2, "list_wf": ["nf-10x-kallisto", "scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "redst4r", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 8, "codes": [" process bustools_inspect{\n                  \n     publishDir \"${params.outdir}/kallisto/bustools_metrics\", mode: \"copy\"\n\n     input:\n     file bus from kallisto_corrected_sort_to_metrics\n\n     output:\n     file \"${bus}.json\"\n\n     script:\n     \"\"\"\n     bustools inspect -o ${bus}.json ${bus}/output.corrected.sort.bus\n     \"\"\"\n }", "\nprocess bustools_inspect{\n    tag \"$bus\"\n    publishDir \"${params.outdir}/kallisto/bustools_metrics\", mode: \"copy\"\n\n    input:\n    file bus from kallisto_corrected_sort_to_metrics\n\n    output:\n    file \"${bus}.json\"\n\n    script:\n    \"\"\"\n    bustools inspect -o ${bus}.json ${bus}/output.corrected.sort.bus\n    \"\"\"\n}"], "list_proc": ["redst4r/nf-10x-kallisto/redst4r__nf-10x-kallisto/bustools_inspect", "nf-core/scrnaseq/nf-core__scrnaseq/bustools_inspect"], "list_wf_names": ["redst4r/nf-10x-kallisto", "nf-core/scrnaseq"]}, {"nb_reuse": 53, "tools": ["MultiQC"], "nb_own": 40, "list_own": ["galaxyuvri-ea", "Crone0610", "jordwil", "lifebit-ai", "qbicsoftware-archive", "h3abionet", "dfornika", "grst", "heinzlab", "UCL-BLIC", "mpozuelo", "edgano", "caspargross", "LNUc-EEMiS", "monikaBrandt", "glormph", "Gustius", "herczegrobert", "samlhao", "nf-core", "yassineS", "czbiohub", "bioinformatics-lab", "grbot", "SherineAwad", "likelet", "Dowell-Lab", "propan2one", "kerimoff", "vladsaveliev", "dalmiaa", "davismcc", "marchoeppner", "KevinMenden", "steepale", "simozhou", "maxibor", "jiangfuqing", "drpatelh", "remiolsen"], "nb_wf": 51, "list_wf": ["csrna", "ChIP-Flow", "Sample_NF", "nf-core-mutenrich", "nf-core-sgrnaquant", "nf-core-refcaller", "TCoffee-NatureProtocol-nf", "blastlca", "outlaw", "kinseq", "variantcallerbench", "UvriMetaSeq", "nf-core-radseq", "metatdenovo-dsl1", "nf-hipsci-fibro", "qtlquant", "roary", "umcaw", "nf-core-denovohybrid", "trigenome-analysis-nf", "microarray-qc-workflow", "nf-core-test", "nf-core-singlegenometese", "nf-ortholog", "wgsalign", "nf-core-lfquandenser", "CRISPR-Cas-off-target-identification", "epi-awesome", "cookiecutter", "nf-large-assembly", "nf-demux", "ownpipeline", "wgsfastqtobam", "nextflow-spid", "exoseq", "nf-core-cpo", "nf-core-example", "trinoflow", "nextflow-bowtie2", "MesKit", "h3arefgraph", "QC_demux", "enaprep", "hybrid-assembly", "nf-ginkgo", "nf-extractcoding", "RNAseq-Flow", "nf-core-mytrial", "chip-seq-pipeline", "nf-core-mynewpipeline", "omics-analyser"], "list_contrib": ["c-guzman", "zmaas", "drpatelh", "alneberg", "ewels", "jherrero", "maxulysse", "Crone0610", "Ninomoriaty", "jordwil", "Wangxin555", "dfornika", "grst", "lynn-sanford", "erikrikarddaniel", "lizheng141026", "cjfields", "AlfredUg", "abhi18av", "pditommaso", "mpozuelo", "edgano", "chenjy327", "caspargross", "Gustius", "monikaBrandt", "glormph", "senthil10", "herczegrobert", "samlhao", "lconde-ucl", "ZIWEI-WONG", "yassineS", "Niinleslie", "maallen3", "Zethson", "grbot", "PhilPalmer", "likelet", "propan2one", "kerimoff", "vladsaveliev", "sven1103", "dalmiaa", "LiuJie1117", "marchoeppner", "KevinMenden", "steepale", "davismcc", "apeltzer", "simozhou", "maxibor", "olgabot", "jambler24", "remiolsen"], "nb_contrib": 55, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('trimgalore/*') from trimgalore_results.collect()\n    file ('samtools/*') from samtools_stats.collect()\n    file ('picard/*') from picard_reports.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file ('quast_results/*') from quast_results\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('trimmomatic/*') from trim_logs.collect()\n    file ('denovo_stacks/*') from denovo_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n             'alignment/*'                              \n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n            'fastqc/*'                                           \n    file ('longranger/*') from longranger_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('trimgalore/*') from trimgalore_results.collect()\n    file ('samtools/*') from samtools_stats.collect()\n    file ('picard/*') from picard_reports.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file (fastqc:'fastqc/*') from fastqc_results.collect()\n    file ('samtools/*') from samtools_stats.collect()\n    file ('picard/*') from picard_reports.collect()\n    file ('deeptools/*') from deepTools_multiqc.collect()\n\n    output:\n    file '*multiqc_report.html' into multiqc_report\n    file '*_data' into multiqc_data\n    file '.command.err' into multiqc_stderr\n    val prefix into multiqc_prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . 2>&1\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file (fastqc:'fastqc/*') from fastqc_results.collect()\n    file ('samtools/*') from samtools_stats.collect()\n    file ('picard/*') from picard_reports.collect()\n    file ('deeptools/*') from deepTools_multiqc.collect()\n\n    output:\n    file '*multiqc_report.html' into multiqc_report\n    file '*_data' into multiqc_data\n    file '.command.err' into multiqc_stderr\n    val prefix into multiqc_prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . 2>&1\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n                          \n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file ('*/report.tsv') from quast_results.collect().ifEmpty([])\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", " process multiqc {\n\n   publishDir \"${cluster_path}/data/05_QC/${project}/multiqc/\", mode: 'copy',\n   saveAs: { filename ->\n     if (filename.endsWith(\".html\")) filename\n   }\n\n   input:\n   path multiqc_config from ch_multiqc_config\n                               \n   path ('software_versions/*') from software_versions_yaml.collect()\n   path workflow_summary from create_workflow_summary(summary)\n   path('fastqc/*') from fastqc_results.collect().ifEmpty([])\n   path('trimgalore/*') from trimgalore_trim_mqc.collect().ifEmpty([])\n   path('trimgalore/fastqc/*') from trimgalore_fastqc_mqc.collect().ifEmpty([])\n   path('star/*') from star_logs.collect().ifEmpty([])\n   path('samtools/stats/*') from bam_stats.collect().ifEmpty([])\n   path('picard/markdups/*') from picard_mrkd_results.collect().ifEmpty([])\n   path('picard/quality_score_distribution/*') from picard_distribution_results.collect().ifEmpty([])\n           'samtools/flagstat/*'                                     \n           'samtools/idxstats/*'                                      \n   path ('rseqc/bam_stat/*') from rseqc_bam.collect().ifEmpty([])\n   path ('rseqc/read_duplication/*') from rseqc_dup.collect().ifEmpty([])\n   path \"*\" from ch_image_docs\n\n   output:\n   path \"*multiqc_report.html\"\n   path \"*_data\"\n                         \n\n   script:\n   rtitle = custom_runName ? \"--title \\\"$project\\\"\" : ''\n   rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : \"$project\"\n   \"\"\"\n   multiqc . -f $rtitle $rfilename --config $multiqc_config\n   \"\"\"\n\n }", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('AdapterRemoval/*') from adapter_removal_results.collect().ifEmpty([])\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiQCconfig\n    file (fastqc:'fastqc/*') from fastqc_results.toList()\n    file ('trimgalore/*') from trimgalore_results.toList()\n    file ('gatk_base_recalibration/T*') from gatk_base_recalibration_results.toList()\n    file ('gatk_picard_duplicates/*') from markdup_results.toList()\n    file ('qualimap/*') from qualimap_results.toList()\n    file ('software_versions/*') from software_versions_yaml.toList()\n\n\n    output:\n    file '*multiqc_report.html' into multiqc_report\n    file '*_data' into multiqc_data\n    file '.command.err' into multiqc_stderr\n    val prefix into multiqc_prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiQCconfig .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    validExitStatus 0,1,143\n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/multiqc/\", mode: 'copy', pattern: \"multiqc_report.html\"\n    publishDir \"${params.outdir}/multiqc/\", mode: 'copy', pattern: \"*_data\"\n\n    when:\n    !params.skipMultiQC\n\n    input:\n    file multiqc_config\n    file (fastqc:'qc/fastqc/*') from fastqc_results.collect()\n    file ('qc/fastqc/*') from trimmed_fastqc_results.collect()\n    file ('qc/trimstats/*') from trim_stats.collect()\n    file ('qc/samtools_mapstats/*') from bam_flagstat.collect()\n    file ('qc/rseqc/*') from rseqc_results.collect()\n    file ('qc/preseq/*') from preseq_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file ('qc/hisat2_mapstats/*') from hisat2_mapstats.collect()\n    file ('qc/picard/*') from picard_stats_multiqc.collect()\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\" into multiqc_report_files\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n\n    \"\"\"\n    multiqc . -f $rtitle $rfilename --config $multiqc_config\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('fastp/*') from fastp_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n    path (fastp_results) from se_fastp_results.mix(pe_fastp_results).collect().ifEmpty([])\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . ${fastp_results}\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess multiQC {\n    validExitStatus 0,1,143\n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/multiqc/\", mode: 'copy', pattern: \"multiqc_report.html\"\n    publishDir \"${params.outdir}/multiqc/\", mode: 'copy', pattern: \"*_data\"\n\n    when:\n    !params.skipMultiQC\n\n    input:\n    file multiqc_config\n    file (fastqc:'qc/fastqc/*') from fastqc_results.collect()\n    file ('qc/fastqc/*') from trimmed_fastqc_results.collect()\n    file ('qc/trimstats/*') from trim_stats.collect()\n    file ('qc/mapstats/*') from bam_flagstat.collect()\n    file ('qc/rseqc/*') from rseqc_results.collect()\n    file ('qc/preseq/*') from preseq_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file ('qc/hisat2_mapstats/*') from hisat2_mapstats.collect()\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\" into multiqc_report_files\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n\n    \"\"\"\n    multiqc . -f $rtitle $rfilename --config $multiqc_config\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}"], "list_proc": ["grbot/nf-core-test/grbot__nf-core-test/multiqc", "lifebit-ai/roary/lifebit-ai__roary/multiqc", "jiangfuqing/CRISPR-Cas-off-target-identification/jiangfuqing__CRISPR-Cas-off-target-identification/multiqc", "UCL-BLIC/nf-ginkgo/UCL-BLIC__nf-ginkgo/multiqc", "marchoeppner/kinseq/marchoeppner__kinseq/multiqc", "nf-core/cookiecutter/nf-core__cookiecutter/multiqc", "KevinMenden/hybrid-assembly/KevinMenden__hybrid-assembly/multiqc", "kerimoff/qtlquant/kerimoff__qtlquant/multiqc", "marchoeppner/trinoflow/marchoeppner__trinoflow/multiqc", "vladsaveliev/umcaw/vladsaveliev__umcaw/multiqc", "remiolsen/nf-core-radseq/remiolsen__nf-core-radseq/multiqc", "simozhou/epi-awesome/simozhou__epi-awesome/multiqc", "remiolsen/outlaw/remiolsen__outlaw/multiqc", "SherineAwad/nf-core-mytrial/SherineAwad__nf-core-mytrial/multiqc", "UCL-BLIC/wgsalign/UCL-BLIC__wgsalign/multiqc", "dalmiaa/Sample_NF/dalmiaa__Sample_NF/multiqc", "grst/nf-core-test/grst__nf-core-test/multiqc", "heinzlab/chip-seq-pipeline/heinzlab__chip-seq-pipeline/multiqc", "yassineS/nf-demux/yassineS__nf-demux/multiqc", "likelet/MesKit/likelet__MesKit/multiqc", "drpatelh/nf-core-sgrnaquant/drpatelh__nf-core-sgrnaquant/multiqc", "jordwil/nextflow-bowtie2/jordwil__nextflow-bowtie2/multiqc", "monikaBrandt/nf-core-mynewpipeline/monikaBrandt__nf-core-mynewpipeline/multiqc", "heinzlab/csrna/heinzlab__csrna/multiqc", "propan2one/variantcallerbench/propan2one__variantcallerbench/multiqc", "caspargross/nf-core-denovohybrid/caspargross__nf-core-denovohybrid/multiqc", "herczegrobert/ownpipeline/herczegrobert__ownpipeline/multiqc", "Crone0610/nf-core-refcaller/Crone0610__nf-core-refcaller/multiqc", "mpozuelo/QC_demux/mpozuelo__QC_demux/multiqc", "LNUc-EEMiS/metatdenovo-dsl1/LNUc-EEMiS__metatdenovo-dsl1/multiqc", "qbicsoftware-archive/microarray-qc-workflow/qbicsoftware-archive__microarray-qc-workflow/multiqc", "maxibor/enaprep/maxibor__enaprep/multiqc", "galaxyuvri-ea/UvriMetaSeq/galaxyuvri-ea__UvriMetaSeq/multiqc", "h3abionet/h3arefgraph/h3abionet__h3arefgraph/multiqc", "czbiohub/blastlca/czbiohub__blastlca/multiqc", "glormph/nf-core-lfquandenser/glormph__nf-core-lfquandenser/multiqc", "steepale/nf-core-mutenrich/steepale__nf-core-mutenrich/multiqc", "Gustius/omics-analyser/Gustius__omics-analyser/multiqc", "nf-core/exoseq/nf-core__exoseq/multiqc", "steepale/wgsfastqtobam/steepale__wgsfastqtobam/multiqc", "bioinformatics-lab/nf-core-singlegenometese/bioinformatics-lab__nf-core-singlegenometese/multiqc", "davismcc/nf-hipsci-fibro/davismcc__nf-hipsci-fibro/multiqc", "bioinformatics-lab/trigenome-analysis-nf/bioinformatics-lab__trigenome-analysis-nf/multiqc", "czbiohub/nf-core-test/czbiohub__nf-core-test/multiqc", "czbiohub/nf-extractcoding/czbiohub__nf-extractcoding/multiqc", "dfornika/nf-core-cpo/dfornika__nf-core-cpo/multiqc", "Dowell-Lab/ChIP-Flow/Dowell-Lab__ChIP-Flow/multiqc", "czbiohub/nf-large-assembly/czbiohub__nf-large-assembly/multiqc", "samlhao/nextflow-spid/samlhao__nextflow-spid/multiqc", "czbiohub/nf-ortholog/czbiohub__nf-ortholog/multiqc", "edgano/TCoffee-NatureProtocol-nf/edgano__TCoffee-NatureProtocol-nf/multiqc", "Dowell-Lab/RNAseq-Flow/Dowell-Lab__RNAseq-Flow/multiQC", "grbot/nf-core-example/grbot__nf-core-example/multiqc"], "list_wf_names": ["likelet/MesKit", "steepale/wgsfastqtobam", "steepale/nf-core-mutenrich", "Dowell-Lab/ChIP-Flow", "KevinMenden/hybrid-assembly", "remiolsen/outlaw", "lifebit-ai/roary", "jiangfuqing/CRISPR-Cas-off-target-identification", "remiolsen/nf-core-radseq", "caspargross/nf-core-denovohybrid", "herczegrobert/ownpipeline", "bioinformatics-lab/nf-core-singlegenometese", "czbiohub/nf-core-test", "glormph/nf-core-lfquandenser", "czbiohub/blastlca", "dfornika/nf-core-cpo", "bioinformatics-lab/trigenome-analysis-nf", "samlhao/nextflow-spid", "LNUc-EEMiS/metatdenovo-dsl1", "heinzlab/chip-seq-pipeline", "heinzlab/csrna", "jordwil/nextflow-bowtie2", "davismcc/nf-hipsci-fibro", "grbot/nf-core-example", "kerimoff/qtlquant", "qbicsoftware-archive/microarray-qc-workflow", "vladsaveliev/umcaw", "mpozuelo/QC_demux", "marchoeppner/trinoflow", "simozhou/epi-awesome", "Crone0610/nf-core-refcaller", "yassineS/nf-demux", "Dowell-Lab/RNAseq-Flow", "Gustius/omics-analyser", "SherineAwad/nf-core-mytrial", "monikaBrandt/nf-core-mynewpipeline", "czbiohub/nf-extractcoding", "drpatelh/nf-core-sgrnaquant", "grst/nf-core-test", "UCL-BLIC/wgsalign", "marchoeppner/kinseq", "czbiohub/nf-large-assembly", "czbiohub/nf-ortholog", "h3abionet/h3arefgraph", "maxibor/enaprep", "grbot/nf-core-test", "galaxyuvri-ea/UvriMetaSeq", "dalmiaa/Sample_NF", "nf-core/cookiecutter", "edgano/TCoffee-NatureProtocol-nf", "UCL-BLIC/nf-ginkgo", "propan2one/variantcallerbench", "nf-core/exoseq"]}, {"nb_reuse": 2, "tools": ["MultiQC"], "nb_own": 2, "list_own": ["FAANG", "nf-core"], "nb_wf": 2, "list_wf": ["methylseq", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "drpatelh", "Jani-94"], "nb_contrib": 19, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_results_for_multiqc.collect().ifEmpty([])\n    file ('trimgalore/*') from ch_trim_galore_results_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_align_log_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_dedup_log_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_splitting_report_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_mbias_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_reports_results_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_summary_results_for_multiqc.collect().ifEmpty([])\n    file ('samtools/*') from ch_flagstat_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('samtools/*') from ch_samtools_stats_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('picard/*') from ch_markDups_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('methyldackel/*') from ch_methyldackel_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('qualimap/*') from ch_qualimap_results_for_multiqc.collect().ifEmpty([])\n    file ('preseq/*') from preseq_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml_for_multiqc.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file . \\\\\n        -m custom_content -m picard -m qualimap -m bismark -m samtools -m preseq -m cutadapt -m fastqc\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_results_for_multiqc.collect().ifEmpty([])\n    file ('trimgalore/*') from ch_trim_galore_results_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_align_log_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_dedup_log_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_splitting_report_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_mbias_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_reports_results_for_multiqc.collect().ifEmpty([])\n    file ('bismark/*') from ch_bismark_summary_results_for_multiqc.collect().ifEmpty([])\n    file ('samtools/*') from ch_flagstat_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('samtools/*') from ch_samtools_stats_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('picard/*') from ch_markDups_results_for_multiqc.flatten().collect().ifEmpty([])\n    file ('methyldackel/*') from ch_methyldackel_results_for_multiqc.flatten().collect().ifEmpty([])\n            'qualimap/*'                                                              \n            'preseq/*'                                             \n    file ('software_versions/*') from ch_software_versions_yaml_for_multiqc.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file . \\\\\n        -m custom_content -m picard -m qualimap -m bismark -m samtools -m preseq -m cutadapt -m fastqc\n    \"\"\"\n}"], "list_proc": ["nf-core/methylseq/nf-core__methylseq/multiqc", "FAANG/GSM-pipeline/FAANG__GSM-pipeline/multiqc"], "list_wf_names": ["FAANG/GSM-pipeline", "nf-core/methylseq"]}, {"nb_reuse": 7, "tools": ["SAMtools"], "nb_own": 6, "list_own": ["Genomic-Medicine-Linkoping", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 7, "list_wf": ["saw.sarek", "sarek_ubec", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 23, "codes": ["\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}"], "list_proc": ["nf-core/sarek/nf-core__sarek/UMIMapBamFile", "rmoran7/custom_sarek/rmoran7__custom_sarek/UMIMapBamFile", "rmoran7/dx_sarek/rmoran7__dx_sarek/UMIMapBamFile", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/UMIMapBamFile", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/UMIMapBamFile", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/UMIMapBamFile", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/UMIMapBamFile"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 19, "tools": ["TIDDIT"], "nb_own": 11, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 18, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "layer_lab_chco", "layer_lab_caw", "layer_lab_vc", "germline_somatic", "custom_sarek", "nf-core-sarek", "sarek-mirror", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "pgp-chronek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    label 'container_sarek'\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publish_dir_mode\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta) \n        file(fastaFai)\n\n    output:\n        tuple val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\"), emit: vcfTIDDIT\n        tuple file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\"), emit: tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publish_dir_mode\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta) \n        file(fastaFai)\n\n    output:\n        tuple val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\"), emit: vcfTIDDIT\n        tuple file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\"), emit: tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess CallVariantsWithTiddit {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        tuple val(idPatient), val(idSample), file(bam), file(bai)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n        tuple val(idPatient), val(idSample), val(\"TIDDIT\"), file(\"*.vcf.gz\"), file(\"*.tbi\")\n        tuple file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\")\n\n                             \n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag \"${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    label 'container_sarek'\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publish_dir_mode\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta) \n        file(fastaFai)\n\n    output:\n        tuple val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\"), emit: vcfTIDDIT\n        tuple file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\"), emit: tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess TIDDIT {\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publishDirMode\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai) from bamTIDDIT\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\") into vcfTIDDIT\n        set file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\") into tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/rmoran7__dx_sarek/TIDDIT", "cgpu/haplosarek/cgpu__haplosarek/TIDDIT", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/TIDDIT", "cgpu/sarek-mirror/cgpu__sarek-mirror/TIDDIT", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/TIDDIT", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/TIDDIT", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/TIDDIT", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/TIDDIT", "cgpu/pgp-chronek/cgpu__pgp-chronek/TIDDIT", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/TIDDIT", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/CallVariantsWithTiddit", "nf-core/sarek/nf-core__sarek/TIDDIT", "rmoran7/custom_sarek/rmoran7__custom_sarek/TIDDIT", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/TIDDIT", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/TIDDIT", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/TIDDIT", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/TIDDIT", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/TIDDIT", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/TIDDIT"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "ryanlayerlab/layer_lab_caw", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror-cache", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/haplosarek", "javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["ITSxpress"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "drpatelh", "PhilPalmer"], "nb_contrib": 15, "codes": ["process ITSX_CUTASV {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::itsx=1.1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/itsx:1.1.3--hdfd78af_1' :\n        'quay.io/biocontainers/itsx:1.1.3--hdfd78af_1' }\"\n\n    input:\n    path fasta\n    val outfile\n\n    output:\n    path outfile         , emit: fasta\n    path \"ASV_ITS_seqs.summary.txt\", emit: summary\n    path \"ASV_ITS_seqs.*fasta\", emit: fastas\n    path \"versions.yml\"  , emit: versions\n    path \"*.args.txt\"    , emit: args\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    ITSx \\\\\n        -i $fasta \\\\\n        $args \\\\\n        --cpu $task.cpus \\\\\n        -o ASV_ITS_seqs\n\n    if [ ! -s $outfile ]; then\n        echo \"ERROR: No ITS regions found by ITSx. You might want to modify --cut_its and/or --its_partial\" >&2\n        exit 1\n    fi\n\n    echo -e \"ITSx\\t$args\" > ITSx.args.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ITSx: \\$( ITSx -h 2>&1 > /dev/null | tail -n 2 | head -n 1 | cut -f 2 -d ' ' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/ITSX_CUTASV"], "list_wf_names": ["nf-core/ampliseq"]}, {"nb_reuse": 7, "tools": ["Prokka"], "nb_own": 4, "list_own": ["xiaoli-dong", "ABMicroBioinf", "nf-core", "jianhong"], "nb_wf": 6, "list_wf": ["pathogen", "mag", "magph", "shotgun", "funcscan", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "HadrienG", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "jasmezz", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 111, "codes": ["\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "process PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0' :\n        'quay.io/biocontainers/prokka:1.14.6--pl526_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        prokka: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"*.gff\"), emit: gff\n    tuple val(meta), path(\"*.gbk\"), emit: gbk\n    tuple val(meta), path(\"*.fna\"), emit: fna\n    tuple val(meta), path(\"*.faa\"), emit: faa\n    tuple val(meta), path(\"*.ffn\"), emit: ffn\n    tuple val(meta), path(\"*.sqn\"), emit: sqn\n    tuple val(meta), path(\"*.fsa\"), emit: fsa\n    tuple val(meta), path(\"*.tbl\"), emit: tbl\n    tuple val(meta), path(\"*.err\"), emit: err\n    tuple val(meta), path(\"*.log\"), emit: log\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n    mv ${prefix}/* .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"*.gff\"), emit: gff\n    tuple val(meta), path(\"*.gbk\"), emit: gbk\n    tuple val(meta), path(\"*.fna\"), emit: fna\n    tuple val(meta), path(\"*.faa\"), emit: faa\n    tuple val(meta), path(\"*.ffn\"), emit: ffn\n    tuple val(meta), path(\"*.sqn\"), emit: sqn\n    tuple val(meta), path(\"*.fsa\"), emit: fsa\n    tuple val(meta), path(\"*.tbl\"), emit: tbl\n    tuple val(meta), path(\"*.err\"), emit: err\n    tuple val(meta), path(\"*.log\"), emit: log\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n    mv ${prefix}/* .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"*.gff\"), emit: gff\n    tuple val(meta), path(\"*.gbk\"), emit: gbk\n    tuple val(meta), path(\"*.fna\"), emit: fna\n    tuple val(meta), path(\"*.faa\"), emit: faa\n    tuple val(meta), path(\"*.ffn\"), emit: ffn\n    tuple val(meta), path(\"*.sqn\"), emit: sqn\n    tuple val(meta), path(\"*.fsa\"), emit: fsa\n    tuple val(meta), path(\"*.tbl\"), emit: tbl\n    tuple val(meta), path(\"*.err\"), emit: err\n    tuple val(meta), path(\"*.log\"), emit: log\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n    mv ${prefix}/* .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "process PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0' :\n        'quay.io/biocontainers/prokka:1.14.6--pl526_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        prokka: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "process PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0' :\n        'quay.io/biocontainers/prokka:1.14.6--pl526_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        prokka: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/PROKKA", "nf-core/funcscan/nf-core__funcscan/PROKKA", "xiaoli-dong/magph/xiaoli-dong__magph/PROKKA", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/PROKKA", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/PROKKA", "nf-core/modules/nf-core__modules/PROKKA", "jianhong/shotgun/jianhong__shotgun/PROKKA"], "list_wf_names": ["jianhong/shotgun", "nf-core/funcscan", "xiaoli-dong/pathogen", "ABMicroBioinf/pathogen", "nf-core/modules", "nf-core/mag", "xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["LeeHom"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["\nprocess LEEHOM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::leehom=1.2.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/leehom:1.2.15--h29e30f7_1' :\n        'quay.io/biocontainers/leehom:1.2.15--h29e30f7_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\")          , optional: true, emit: bam\n    tuple val(meta), path(\"${prefix}.fq.gz\")        , optional: true, emit: fq_pass\n    tuple val(meta), path(\"${prefix}.fail.fq.gz\")   , optional: true, emit: fq_fail\n    tuple val(meta), path(\"${prefix}_r1.fq.gz\")     , optional: true, emit: unmerged_r1_fq_pass\n    tuple val(meta), path(\"${prefix}_r1.fail.fq.gz\"), optional: true, emit: unmerged_r1_fq_fail\n    tuple val(meta), path(\"${prefix}_r2.fq.gz\")     , optional: true, emit: unmerged_r2_fq_pass\n    tuple val(meta), path(\"${prefix}_r2.fail.fq.gz\"), optional: true, emit: unmerged_r2_fq_fail\n    tuple val(meta), path(\"*.log\")                                  , emit: log\n    path \"versions.yml\"                                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    if (reads.toString().endsWith('.bam')) {\n        \"\"\"\n        leeHom \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -o ${prefix}.bam \\\\\n            --log ${prefix}.log \\\\\n            $reads\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            leehom: $VERSION\n        END_VERSIONS\n        \"\"\"\n    } else if (meta.single_end) {\n        \"\"\"\n        leeHom \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -fq1 $reads \\\\\n            -fqo $prefix \\\\\n            --log ${prefix}.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            leehom: $VERSION\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        leeHom \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -fq1 ${reads[0]} \\\\\n            -fq2 ${reads[1]} \\\\\n            -fqo $prefix \\\\\n            --log ${prefix}.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            leehom: $VERSION\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/LEEHOM"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MASH_SKETCH {\n    tag \"$meta.id\"\n    label 'process_medium'\n    conda (params.enable_conda ? \"bioconda::mash=2.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1' :\n        'quay.io/biocontainers/mash:2.3--he348c14_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.msh\")        , emit: mash\n    tuple val(meta), path(\"*.mash_stats\") , emit: stats\n    path \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mash \\\\\n        sketch \\\\\n        $args \\\\\n        -p $task.cpus \\\\\n        -o ${prefix} \\\\\n        -r $reads \\\\\n        2> ${prefix}.mash_stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(mash --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MASH_SKETCH"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_DIVERSITY_BETA {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core), val(category)\n\n    output:\n    path(\"beta_diversity/*\"), emit: beta\n    path \"versions.yml\"     , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        IFS=',' read -r -a metacategory <<< \\\"$category\\\"\n        for j in \\\"\\${metacategory[@]}\\\"\n        do\n            qiime diversity beta-group-significance \\\n                --i-distance-matrix ${core} \\\n                --m-metadata-file ${metadata} \\\n                --m-metadata-column \\\"\\$j\\\" \\\n                --o-visualization ${core.baseName}-\\$j.qzv \\\n                --p-pairwise\n            qiime tools export --input-path ${core.baseName}-\\$j.qzv \\\n                --output-path beta_diversity/${core.baseName}-\\$j\n        done\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir beta_diversity\n        echo \"\" > \"beta_diversity/WARNING No column in ${metadata.baseName} seemed suitable.txt\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process QIIME2_DIVERSITY_BETA {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core), val(category)\n\n    output:\n    path(\"beta_diversity/*\"), emit: beta\n    path \"versions.yml\"     , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        IFS=',' read -r -a metacategory <<< \\\"$category\\\"\n        for j in \\\"\\${metacategory[@]}\\\"\n        do\n            qiime diversity beta-group-significance \\\n                --i-distance-matrix ${core} \\\n                --m-metadata-file ${metadata} \\\n                --m-metadata-column \\\"\\$j\\\" \\\n                --o-visualization ${core.baseName}-\\$j.qzv \\\n                --p-pairwise\n            qiime tools export --input-path ${core.baseName}-\\$j.qzv \\\n                --output-path beta_diversity/${core.baseName}-\\$j\n        done\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir beta_diversity\n        echo \"\" > \"beta_diversity/WARNING No column in ${metadata.baseName} seemed suitable.txt\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_DIVERSITY_BETA", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_DIVERSITY_BETA"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["Cutadapt"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.5' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.5--py39h38f01e4_0' :\n        'quay.io/biocontainers/cutadapt:3.5--py39h38f01e4_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path adapters\n\n    output:\n    tuple val(meta), path('*.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')     , emit: log\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired = meta.single_end ? \"-a file:adapters.sub.fa\" : \"-a file:adapters.sub.fa -A file:adapters.sub.fa\"\n    def trimmed = meta.single_end ? \"-o ${prefix}.fastq.gz\" : \"-o ${prefix}_1.fastq.gz -p ${prefix}_2.fastq.gz\"\n    \"\"\"\n    sed -r '/^[ACTGactg]+\\$/ s/\\$/X/g' $adapters > adapters.sub.fa\n\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $paired \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.5' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.5--py39h38f01e4_0' :\n        'quay.io/biocontainers/cutadapt:3.5--py39h38f01e4_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path adapters\n\n    output:\n    tuple val(meta), path('*.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')     , emit: log\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired = meta.single_end ? \"-a file:adapters.sub.fa\" : \"-a file:adapters.sub.fa -A file:adapters.sub.fa\"\n    def trimmed = meta.single_end ? \"-o ${prefix}.fastq.gz\" : \"-o ${prefix}_1.fastq.gz -p ${prefix}_2.fastq.gz\"\n    \"\"\"\n    sed -r '/^[ACTGactg]+\\$/ s/\\$/X/g' $adapters > adapters.sub.fa\n\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $paired \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/CUTADAPT", "nf-core/viralrecon/nf-core__viralrecon/CUTADAPT"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 2, "tools": ["QIIME", "BioMe"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_ANCOM_TAX {\n    tag \"${table.baseName} - taxonomic level: ${taxlevel}\"\n    label 'process_medium'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(table), path(taxonomy) ,val(taxlevel)\n\n    output:\n    path \"ancom/*\"      , emit: ancom\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n    mkdir ancom\n\n    # Sum data at the specified level\n    qiime taxa collapse \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-level ${taxlevel} \\\n            --o-collapsed-table lvl${taxlevel}-${table}\n\n    # Extract summarised table and output a file with the number of taxa\n    qiime tools export --input-path lvl${taxlevel}-${table} --output-path exported/\n    biom convert -i exported/feature-table.biom -o ${table.baseName}-level-${taxlevel}.feature-table.tsv --to-tsv\n\n    if [ \\$(grep -v '^#' -c ${table.baseName}-level-${taxlevel}.feature-table.tsv) -lt 2 ]; then\n        echo ${taxlevel} > ancom/\\\"WARNING Summing your data at taxonomic level ${taxlevel} produced less than two rows (taxa), ANCOM can't proceed -- did you specify a bad reference taxonomy?\\\".txt\n    else\n        qiime composition add-pseudocount \\\n                --i-table lvl${taxlevel}-${table} \\\n                --o-composition-table comp-lvl${taxlevel}-${table}\n        qiime composition ancom \\\n                --i-table comp-lvl${taxlevel}-${table} \\\n                --m-metadata-file ${metadata} \\\n                --m-metadata-column ${table.baseName} \\\n                --o-visualization comp-lvl${taxlevel}-${table.baseName}.qzv\n        qiime tools export --input-path comp-lvl${taxlevel}-${table.baseName}.qzv \\\n                --output-path ancom/Category-${table.baseName}-level-${taxlevel}\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_ANCOM_TAX {\n    tag \"${table.baseName} - taxonomic level: ${taxlevel}\"\n    label 'process_medium'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(table), path(taxonomy) ,val(taxlevel)\n\n    output:\n    path \"ancom/*\"      , emit: ancom\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n    mkdir ancom\n\n    # Sum data at the specified level\n    qiime taxa collapse \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-level ${taxlevel} \\\n            --o-collapsed-table lvl${taxlevel}-${table}\n\n    # Extract summarised table and output a file with the number of taxa\n    qiime tools export --input-path lvl${taxlevel}-${table} --output-path exported/\n    biom convert -i exported/feature-table.biom -o ${table.baseName}-level-${taxlevel}.feature-table.tsv --to-tsv\n\n    if [ \\$(grep -v '^#' -c ${table.baseName}-level-${taxlevel}.feature-table.tsv) -lt 2 ]; then\n        echo ${taxlevel} > ancom/\\\"WARNING Summing your data at taxonomic level ${taxlevel} produced less than two rows (taxa), ANCOM can't proceed -- did you specify a bad reference taxonomy?\\\".txt\n    else\n        qiime composition add-pseudocount \\\n                --i-table lvl${taxlevel}-${table} \\\n                --o-composition-table comp-lvl${taxlevel}-${table}\n        qiime composition ancom \\\n                --i-table comp-lvl${taxlevel}-${table} \\\n                --m-metadata-file ${metadata} \\\n                --m-metadata-column ${table.baseName} \\\n                --o-visualization comp-lvl${taxlevel}-${table.baseName}.qzv\n        qiime tools export --input-path comp-lvl${taxlevel}-${table.baseName}.qzv \\\n                --output-path ancom/Category-${table.baseName}-level-${taxlevel}\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_ANCOM_TAX", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_ANCOM_TAX"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 5, "tools": ["BEDTools"], "nb_own": 4, "list_own": ["harleenduggal", "raygozag", "nf-core", "mahesh-panchal"], "nb_wf": 4, "list_wf": ["RNASEQ", "rnaseq", "test_nfcore_workflow_chain", "nfcore-rnaseq"], "list_contrib": ["Emiller88", "alneberg", "FriederikeHanssen", "ewels", "drejom", "arontommi", "maxulysse", "rsuchecki", "SpikyClip", "matrulda", "ggabernet", "george-hall-ucl", "jordwil", "veeravalli", "adomingues", "colindaven", "vezzi", "lpantano", "skrakau", "chuan-wang", "ppericard", "grst", "pcantalupo", "nf-core-bot", "mvanins", "Galithil", "jun-wan", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "na399", "robsyme", "BABS-STP1", "senthil10", "drpowell", "kviljoen", "rfenouil", "jburos", "chris-cheshire", "mashehu", "raygozag", "Hammarn", "sven1103", "jemten", "paulklemm", "pranathivemuri", "marchoeppner", "mahesh-panchal", "JoseEspinosa", "apeltzer", "KevinMenden", "aanil", "silviamorins", "d4straub", "olgabot", "drpatelh", "amayer21", "zxl124"], "nb_contrib": 60, "codes": ["process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/raygozag__rnaseq/BEDTOOLS_GENOMECOV", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/BEDTOOLS_GENOMECOV", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/BEDTOOLS_GENOMECOV", "nf-core/rnaseq/nf-core__rnaseq/BEDTOOLS_GENOMECOV", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BEDTOOLS_GENOMECOV"], "list_wf_names": ["raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess recalIndels {\n    tag \"${name}\"\n    publishDir \"${params.outdir}/GATK_RecalibrateIndels\", mode: 'copy', \n    saveAs: {filename -> params.saveIntermediateVariants ? \"$filename\" : null }\n\n    input:\n    set val(name), file(raw_indel), file(raw_indel_idx) from raw_indels\n\n    output:\n    set val(name), file(\"${name}_filtered_indels.vcf\"), file(\"${name}_filtered_indels.vcf.idx\") into filtered_indels\n\n    script:\n    \"\"\"\n    gatk -T VariantRecalibrator \\\\\n        -R $params.gfasta \\\\\n        --input $raw_indel \\\\\n        --maxGaussians 4 \\\\\n        --recal_file ${name}_indel.recal \\\\\n        --tranches_file ${name}_indel.tranches \\\\\n        -resource:mills,known=false,training=true,truth=true,prior=12.0 $params.mills \\\\\n        -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 $params.dbsnp \\\\\n        -an QD -an DP -an FS -an SOR \\\\\n        -mode INDEL \n\n    gatk -T ApplyRecalibration \\\\\n        -R $params.gfasta \\\\\n        --out ${name}_filtered_indels.vcf \\\\\n        --input $raw_indel \\\\\n        --mode SNP \\\\\n        --tranches_file ${name}_indel.tranches \\\\\n        --recal_file ${name}_indel.recal \\\\\n        --ts_filter_level 99.0 \\\\\n        -mode INDEL\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/recalIndels"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["neutronstar"], "list_contrib": ["ewels", "remiolsen"], "nb_contrib": 2, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n    input:\n      file \"v_supernova.txt\" from v_supernova\n    output:\n      file \"software_versions_mqc.yaml\" into software_versions_yaml\n      file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    quast.py -v &> v_quast.txt\n    multiqc --version > v_multiqc.txt\n    run_BUSCO.py -v > v_busco.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/neutronstar/nf-core__neutronstar/get_software_versions"], "list_wf_names": ["nf-core/neutronstar"]}, {"nb_reuse": 1, "tools": ["sourmash", "SKAT", "fastPHASE", "SAMtools", "SortMeRna"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      if (filename.indexOf(\".yaml\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    bam2fasta info &> v_bam2fasta.txt\n    fastp --version &> v_fastp.txt\n    samtools --version &> v_samtools.txt\n    ska version &> v_ska.txt\n    sortmerna --version &> v_sortmerna.txt\n    sourmash -v &> v_sourmash.txt\n    pip show orpheum &> v_orpheum.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/get_software_versions"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["FastQC", "MultiQC", "SAMtools", "Bowtie", "htseqcount"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess get_software_versions {\n   publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n   saveAs: {filename ->\n       if (filename.indexOf(\".csv\") > 0) filename\n       else null\n   }\n\n   output:\n   file 'software_versions_mqc.yaml' into software_versions_yaml\n   file \"software_versions.csv\"\n\n   script:\n   java_mem = ''\n   if(task.memory){\n       tmem = task.memory.toBytes()\n       java_mem = \"-Xms${tmem} -Xmx${tmem}\"\n   }\n   \"\"\"\n   export mirtracejar=\\$(dirname \\$(which mirtrace))\n   echo $workflow.manifest.version > v_pipeline.txt\n   echo $workflow.nextflow.version > v_nextflow.txt\n   echo \\$(R --version 2>&1) > v_R.txt\n   fastqc --version > v_fastqc.txt\n   trim_galore --version > v_trim_galore.txt\n   bowtie --version > v_bowtie.txt\n   samtools --version > v_samtools.txt\n   htseq-count -h > v_htseq.txt\n   fasta_formatter -h > v_fastx.txt\n   java $java_mem -jar \\$mirtracejar/mirtrace.jar --mirtrace-wrapper-name mirtrace --version > v_mirtrace.txt\n   multiqc --version > v_multiqc.txt\n   miRDeep2.pl -h > v_mirdeep2.txt\n\n   scrape_software_versions.py > software_versions_mqc.yaml\n   \"\"\"\n}"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/get_software_versions"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 11, "tools": ["BWA", "SAMtools"], "nb_own": 8, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu"], "nb_wf": 10, "list_wf": ["saw.sarek", "pgp-chronek", "sarek_ubec", "germline_somatic", "sarek-mirror", "dx_sarek", "sarek", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess AlignReadsToReferenceSequence {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), file(inputFile1), file(inputFile2)\n        file(bwaIndex)\n        file(fasta)\n        file(fastaFai) \n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_${idRun}.bam\")\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n                                                                                          \n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m ${params.samtoolsSortMemory} - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputReads\n        file(bwaIndex) from ch_bwaIndex\n        file(fasta) from ch_fasta\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, idSample, file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: step == 'mapping'\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n        ${convertToFastq}\n        bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputReads\n        file(bwaIndex) from ch_bwaIndex\n        file(fasta) from ch_fasta\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, idSample, file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: step == 'mapping'\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n        ${convertToFastq}\n        bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputReads\n        file(bwaIndex) from ch_bwaIndex\n        file(fasta) from ch_fasta\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, idSample, file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: step == 'mapping'\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n        ${convertToFastq}\n        bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n    ${convertToFastq}\n    bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/sripaladugu__germline_somatic/MapReads", "nf-core/sarek/nf-core__sarek/MapReads", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/AlignReadsToReferenceSequence", "cgpu/pgp-chronek/cgpu__pgp-chronek/MapReads", "cgpu/sarek-mirror/cgpu__sarek-mirror/MapReads", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/MapReads", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MapReads", "rmoran7/dx_sarek/rmoran7__dx_sarek/MapReads", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/MapReads", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MapReads", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MapReads"], "list_wf_names": ["cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "rmoran7/dx_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 17, "tools": ["GATK"], "nb_own": 10, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 17, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "PGP-UK-sarek", "layer_lab_caw", "layer_lab_chco", "layer_lab_vc", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["FriederikeHanssen", "alneberg", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "rmoran7", "lescai", "cgpu", "apeltzer", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 25, "codes": ["\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n    label 'cpus_1'\n\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_ponIndex\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFiles\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'process_medium'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2PairOutput\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFilesPair\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2StatsPair\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n                                                                        \n    intervalsOptions = params.no_intervals ? params.target_bed ? \"-L ${params.target_bed}\" : \"-L ${germlineResource}\" : \"-L ${intervalBed}\"\n\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor} -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFiles\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2TN{\n    label 'container_llab'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_2'\n\n    input:\n        tuple idPatient, \n            idSampleNormal, file(bamNormal), file(baiNormal),\n            idSampleTumor, file(bamTumor), file(baiTumor), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n        file(ponSomatic)\n        file(ponSomaticIndex)\n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\"), emit: vcf\n        \n        tuple idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), emit: stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.somatic_pon ? \"--panel-of-normals ${ponSomatic}\" : \"\"\n                \n    \"\"\"\n    init.sh\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFiles\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2TN{\n    label 'container_llab'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_2'\n\n    input:\n        tuple idPatient, \n            idSampleNormal, file(bamNormal), file(baiNormal),\n            idSampleTumor, file(bamTumor), file(baiTumor), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n        file(ponSomatic)\n        file(ponSomaticIndex)\n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\"), emit: vcf\n        \n        tuple idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), emit: stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.somatic_pon ? \"--panel-of-normals ${ponSomatic}\" : \"\"\n                \n    \"\"\"\n    init.sh\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2TN{\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_2'\n\n    input:\n        tuple idPatient, \n            idSampleNormal, file(bamNormal), file(baiNormal),\n            idSampleTumor, file(bamTumor), file(baiTumor), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n        file(ponSomatic)\n        file(ponSomaticIndex)\n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\"), emit: vcf\n        \n        tuple idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), emit: stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon_somatic ? \"--panel-of-normals ${ponSomatic}\" : \"\"\n                \n    \"\"\"\n    init.sh\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFiles\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2PairOutput\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFilesPair\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2StatsPair\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor} -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n    label 'cpus_1'\n\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_ponIndex\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'process_medium'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2PairOutput\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFilesPair\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2StatsPair\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n                                                                        \n    intervalsOptions = params.no_intervals ? params.target_bed ? \"-L ${params.target_bed}\" : \"-L ${germlineResource}\" : \"-L ${intervalBed}\"\n\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor} -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}"], "list_proc": ["cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/Mutect2", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/Mutect2", "rmoran7/dx_sarek/rmoran7__dx_sarek/Mutect2", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/Mutect2", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/Mutect2TN", "cgpu/haplosarek/cgpu__haplosarek/Mutect2", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/Mutect2", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/Mutect2TN", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/Mutect2TN", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/Mutect2", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/Mutect2", "cgpu/sarek-mirror/cgpu__sarek-mirror/Mutect2", "nf-core/sarek/nf-core__sarek/Mutect2", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/Mutect2", "cgpu/pgp-chronek/cgpu__pgp-chronek/Mutect2", "rmoran7/custom_sarek/rmoran7__custom_sarek/Mutect2", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/Mutect2"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "ryanlayerlab/layer_lab_caw", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-mirror-cache", "cgpu/sarek-genomechronicler", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek", "javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["Racon"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess RACON {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n    label 'process_high_memory'\n    label 'error_retry'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'racon=1.4.20-1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/racon:1.4.20--h9a82719_1\"\n    } else {\n        container \"quay.io/biocontainers/racon:1.4.20--h9a82719_1\"\n    }\n\n    input:\n    tuple val(meta), val(reads), file(longreads), path('assembly.fasta'), path(paf)\n\n    output:\n    tuple val(meta), path('*_assembly_consensus.fasta') , emit: assembly\n    path  '*.version.txt'                     , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    racon -t \"${task.cpus}\" \"${longreads}\" \"${paf}\" \"assembly.fasta\" > ${prefix}_assembly_consensus.fasta\n\n    echo \\$(racon --version 2>&1) | sed 's/^.*v//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/RACON"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process PICARD_MERGESAMFILES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bams)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def bam_files = bams.sort()\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MergeSamFiles] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    if (bam_files.size() > 1) {\n        \"\"\"\n        picard \\\\\n            -Xmx${avail_mem}g \\\\\n            MergeSamFiles \\\\\n            $args \\\\\n            ${'--INPUT '+bam_files.join(' --INPUT ')} \\\\\n            --OUTPUT ${prefix}.bam\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            picard: \\$( echo \\$(picard MergeSamFiles --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        ln -s ${bam_files[0]} ${prefix}.bam\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            picard: \\$( echo \\$(picard MergeSamFiles --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_MERGESAMFILES"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": [" process generate_star_index {\n            tag \"$fasta\"\n            label 'process_high'\n            publishDir path: { params.save_index ? \"${params.outdir}/STAR_index\" : params.outdir },\n                saveAs: { params.save_index ? it : null }, mode: params.publish_dir_mode\n\n            input:\n            path(fasta) from ch_fasta\n            path(gtf) from ch_gtf_star\n\n            output:\n            path(\"STAR_${fasta.baseName}\") into ch_star_index\n\n            script:\n            \"\"\"\n            samtools faidx $fasta\n            NUM_BASES=`awk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n            mkdir STAR_${fasta.baseName}\n\n            STAR \\\\\n                --runMode genomeGenerate \\\\\n                --runThreadN ${task.cpus} \\\\\n                --genomeDir STAR_${fasta.baseName} \\\\\n                --genomeFastaFiles $fasta \\\\\n                --genomeSAindexNbases \\$NUM_BASES \\\\\n                --sjdbGTFfile $gtf\n            \"\"\"\n        }", " process generate_star_index_no_gtf {\n            tag \"$fasta\"\n            label 'process_high'\n            publishDir path: { params.save_index ? \"${params.outdir}/STAR_index\" : params.outdir },\n                saveAs: { params.save_index ? it : null }, mode: params.publish_dir_mode\n\n            input:\n            path(fasta) from ch_fasta\n\n            output:\n            path(\"STAR_${fasta.baseName}\") into ch_star_index\n\n            script:\n            \"\"\"\n            samtools faidx $fasta\n            NUM_BASES=`awk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n            mkdir STAR_${fasta.baseName}\n\n            STAR \\\\\n                --runMode genomeGenerate --runThreadN ${task.cpus} \\\\\n                --genomeDir STAR_${fasta.baseName} \\\\\n                --genomeFastaFiles $fasta \\\\\n                --genomeSAindexNbases \\$NUM_BASES \\\\\n            \"\"\"\n        }"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/generate_star_index", "nf-core/clipseq/nf-core__clipseq/generate_star_index_no_gtf"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 3, "tools": ["PiRaNhA"], "nb_own": 3, "list_own": ["clairecoleman1", "nf-core", "oisinmccaffrey"], "nb_wf": 3, "list_wf": ["clipseq.nextflow", "clipseq", "clipseq1"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "CharlotteAnne", "drpatelh", "clairecoleman1", "oisinmccaffrey"], "nb_contrib": 8, "codes": [" process piranha_peak_call {\n        tag \"$name\"\n        label 'process_high'\n        publishDir \"${params.outdir}/piranha\", mode: params.publish_dir_mode\n\n        when:\n        'piranha' in callers\n\n        input:\n        tuple val(name), path(xlinks) from ch_xlinks_piranha\n\n        output:\n        tuple val(name), path(\"${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\") into ch_peaks_piranha\n        path \"*.peaks.bed.gz\" into ch_piranha_qc\n\n        script:\n        bin_size_both = params.bin_size_both\n        cluster_dist = params.cluster_dist\n        \"\"\"\n        pigz -d -c $xlinks | \\\\\n        awk '{OFS=\"\\t\"}{for(i=0;i<\\$5;i++) print }' \\\\\n        > expanded.bed\n\n        Piranha \\\\\n            expanded.bed \\\\\n            -s \\\\\n            -b $bin_size_both \\\\\n            -u $cluster_dist \\\\\n            -o paraclu.bed\n\n        awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$3, \".\", \\$5, \\$6}' paraclu.bed | \\\\\n        pigz > ${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\n        \"\"\"\n    }", "\nprocess piranha_peak_call {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/piranha\", mode: 'copy'\n\n        input:\n        tuple val(name), path(xlinks) from ch_xlinks_piranha\n\n        output:\n        tuple val(name), path(\"${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\") into ch_peaks_piranha\n        path \"*.peaks.bed.gz\" into ch_piranha_qc\n\n        script:\n        bin_size_both = 3\n        cluster_dist = 3\n        \"\"\"\n        pigz -d -c $xlinks | \\\\\n        awk '{OFS=\"\\t\"}{for(i=0;i<\\$5;i++) print }' \\\\\n        > expanded.bed\n        Piranha \\\\\n            expanded.bed \\\\\n            -s \\\\\n            -b $bin_size_both \\\\\n            -u $cluster_dist \\\\\n            -o paraclu.bed\n        awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$3, \".\", \\$5, \\$6}' paraclu.bed | \\\\\n        pigz > ${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\n        \"\"\"\n}", "\nprocess piranha_peak_call {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/piranha\", mode: 'copy'\n\n        input:\n        tuple val(name), path(xlinks) from ch_xlinks_piranha\n\n        output:\n        tuple val(name), path(\"${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\") into ch_peaks_piranha\n        path \"*.peaks.bed.gz\" into ch_piranha_qc\n\n        script:\n        bin_size_both = 3\n        cluster_dist = 3\n        \"\"\"\n        pigz -d -c $xlinks | \\\\\n        awk '{OFS=\"\\t\"}{for(i=0;i<\\$5;i++) print }' \\\\\n        > expanded.bed\n        Piranha \\\\\n            expanded.bed \\\\\n            -s \\\\\n            -b $bin_size_both \\\\\n            -u $cluster_dist \\\\\n            -o paraclu.bed\n        awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$3, \".\", \\$5, \\$6}' paraclu.bed | \\\\\n        pigz > ${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\n        \"\"\"\n}"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/piranha_peak_call", "clairecoleman1/clipseq1/clairecoleman1__clipseq1/piranha_peak_call", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/piranha_peak_call"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "nf-core/clipseq"]}, {"nb_reuse": 2, "tools": ["QIIME", "BioMe"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_INASV {\n    tag \"${asv}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(asv)\n\n    output:\n    path(\"table.qza\")    , emit: qza\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    \"\"\"\n    echo -n \"#OTU Table\" | cat - \"$asv\" > biom-table.txt\n    biom convert -i biom-table.txt -o table.biom --table-type=\"OTU table\" --to-hdf5\n    qiime tools import \\\n        --input-path table.biom \\\n        --type 'FeatureTable[Frequency]' \\\n        --input-format BIOMV210Format \\\n        --output-path table.qza\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_INASV {\n    tag \"${asv}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(asv)\n\n    output:\n    path(\"table.qza\")    , emit: qza\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    \"\"\"\n    echo -n \"#OTU Table\" | cat - \"$asv\" > biom-table.txt\n    biom convert -i biom-table.txt -o table.biom --table-type=\"OTU table\" --to-hdf5\n    qiime tools import \\\n        --input-path table.biom \\\n        --type 'FeatureTable[Frequency]' \\\n        --input-format BIOMV210Format \\\n        --output-path table.qza\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_INASV", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_INASV"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["mpileup", "BCFtools"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["test_nfcore_workflow_chain", "modules", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process BCFTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.gz\")      , emit: vcf\n    tuple val(meta), path(\"*.tbi\")     , emit: tbi\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    tuple val(meta), path(\"*.mpileup\") , emit: mpileup, optional: true\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args3 = task.ext.args3 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    echo \"${meta.id}\" > sample_name.list\n\n    bcftools \\\\\n        mpileup \\\\\n        --fasta-ref $fasta \\\\\n        $args \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | bcftools call --output-type v $args2 \\\\\n        | bcftools reheader --samples sample_name.list \\\\\n        | bcftools view --output-file ${prefix}.vcf.gz --output-type z $args3\n\n    tabix -p vcf -f ${prefix}.vcf.gz\n\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.gz\")      , emit: vcf\n    tuple val(meta), path(\"*.tbi\")     , emit: tbi\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    tuple val(meta), path(\"*.mpileup\") , emit: mpileup, optional: true\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args3 = task.ext.args3 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    echo \"${meta.id}\" > sample_name.list\n\n    bcftools \\\\\n        mpileup \\\\\n        --fasta-ref $fasta \\\\\n        $args \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | bcftools call --output-type v $args2 \\\\\n        | bcftools reheader --samples sample_name.list \\\\\n        | bcftools view --output-file ${prefix}.vcf.gz --output-type z $args3\n\n    tabix -p vcf -f ${prefix}.vcf.gz\n\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.gz\")      , emit: vcf\n    tuple val(meta), path(\"*.tbi\")     , emit: tbi\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    tuple val(meta), path(\"*.mpileup\") , emit: mpileup, optional: true\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args3 = task.ext.args3 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    echo \"${meta.id}\" > sample_name.list\n\n    bcftools \\\\\n        mpileup \\\\\n        --fasta-ref $fasta \\\\\n        $args \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | bcftools call --output-type v $args2 \\\\\n        | bcftools reheader --samples sample_name.list \\\\\n        | bcftools view --output-file ${prefix}.vcf.gz --output-type z $args3\n\n    tabix -p vcf -f ${prefix}.vcf.gz\n\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BCFTOOLS_MPILEUP", "nf-core/viralrecon/nf-core__viralrecon/BCFTOOLS_MPILEUP", "nf-core/modules/nf-core__modules/BCFTOOLS_MPILEUP"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_EXTRACT {\n    tag \"${meta.FW_primer}-${meta.RV_primer}\"\n    label 'process_low'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple val(meta), path(database)\n\n    output:\n    tuple val(meta), path(\"*.qza\"), emit: qza\n    path \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    ### Import\n    qiime tools import --type \\'FeatureData[Sequence]\\' \\\n        --input-path ${database[0]} \\\n        --output-path ref-seq.qza\n    qiime tools import --type \\'FeatureData[Taxonomy]\\' \\\n        --input-format HeaderlessTSVTaxonomyFormat \\\n        --input-path ${database[1]} \\\n        --output-path ref-taxonomy.qza\n    #Extract sequences based on primers\n    qiime feature-classifier extract-reads \\\n        --i-sequences ref-seq.qza \\\n        --p-f-primer ${meta.FW_primer} \\\n        --p-r-primer ${meta.RV_primer} \\\n        --o-reads ${meta.FW_primer}-${meta.RV_primer}-ref-seq.qza \\\n        --quiet\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_EXTRACT {\n    tag \"${meta.FW_primer}-${meta.RV_primer}\"\n    label 'process_low'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple val(meta), path(database)\n\n    output:\n    tuple val(meta), path(\"*.qza\"), emit: qza\n    path \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    ### Import\n    qiime tools import --type \\'FeatureData[Sequence]\\' \\\n        --input-path ${database[0]} \\\n        --output-path ref-seq.qza\n    qiime tools import --type \\'FeatureData[Taxonomy]\\' \\\n        --input-format HeaderlessTSVTaxonomyFormat \\\n        --input-path ${database[1]} \\\n        --output-path ref-taxonomy.qza\n    #Extract sequences based on primers\n    qiime feature-classifier extract-reads \\\n        --i-sequences ref-seq.qza \\\n        --p-f-primer ${meta.FW_primer} \\\n        --p-r-primer ${meta.RV_primer} \\\n        --o-reads ${meta.FW_primer}-${meta.RV_primer}-ref-seq.qza \\\n        --quiet\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_EXTRACT", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_EXTRACT"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["MultiQC", "FastQC", "STAR", "Cutadapt", "SAMtools", "Salmon", "gffread", "htseqcount"], "nb_own": 2, "list_own": ["bhagesh-codebeast", "nf-core"], "nb_wf": 2, "list_wf": ["dualrnaseq", "nextflowdualrnaseq"], "list_contrib": ["lbarquist", "nf-core-bot", "apeltzer", "reganhayward", "bhagesh-codebeast", "bozmik"], "nb_contrib": 6, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    python --version > v_python.txt\n    R --version > v_r.txt\n    cutadapt --version > v_cutadapt.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    STAR --version > v_star.txt\n    htseq-count . . --version > v_htseq.txt\n    samtools --version > v_samtools.txt\n    gffread --version > v_gffread.txt\n    salmon --version > v_salmon.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    python --version > v_python.txt\n    R --version > v_r.txt\n    cutadapt --version > v_cutadapt.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    STAR --version > v_star.txt\n    htseq-count . . --version > v_htseq.txt\n    samtools --version > v_samtools.txt\n    gffread --version > v_gffread.txt\n    salmon --version > v_salmon.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/dualrnaseq/nf-core__dualrnaseq/get_software_versions", "bhagesh-codebeast/nextflowdualrnaseq/bhagesh-codebeast__nextflowdualrnaseq/get_software_versions"], "list_wf_names": ["nf-core/dualrnaseq", "bhagesh-codebeast/nextflowdualrnaseq"]}, {"nb_reuse": 9, "tools": ["STAR"], "nb_own": 6, "list_own": ["raygozag", "nf-core", "mahesh-panchal", "harleenduggal", "cguyomar", "goodwright"], "nb_wf": 8, "list_wf": ["rnavar", "RNASEQ", "test_nfcore_workflow_chain", "nf-ase", "nfcore-rnaseq", "modules", "imaps-nf", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "jordwil", "FelixKrueger", "rfara", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "aanil", "silviamorins", "d4straub", "SPPearce", "Midnighter", "rannick", "yuukiiwa", "samirelanduk", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "mvanins", "nebfield", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "alexharston", "Gwennid", "Jeremy1805", "marc-jones", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "cguyomar", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "CharlotteAnne", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "m3hdad", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 153, "codes": ["process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.6.1d--0' :\n        'quay.io/biocontainers/star:2.6.1d--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path \"versions.yml\"                       , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.6.1d--0' :\n        'quay.io/biocontainers/star:2.6.1d--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path \"versions.yml\"                       , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.6.1d--0' :\n        'quay.io/biocontainers/star:2.6.1d--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path \"versions.yml\"                       , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.6.1d--0' :\n        'quay.io/biocontainers/star:2.6.1d--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path \"versions.yml\"                       , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.7.9a' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0'\n    } else {\n        container 'quay.io/biocontainers/star:2.7.9a--h9ee0642_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  \"versions.yml\"                      , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n    tuple val(meta), path('*.out.junction')          , optional:true, emit: junction\n\n    script:\n    def prefix          = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def ignore_gtf      = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_platform    = params.seq_platform ? \"'PL:$params.seq_platform'\" : \"\"\n    def seq_center      = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix' $seq_platform \" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix' $seq_platform \"\n    def out_sam_type    = (options.args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (options.args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $options.args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n    if [ -f ${prefix}.SJ.out.tab ]; then\n        mv ${prefix}.SJ.out.tab ${prefix}.out.junction\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess STAR_ALIGN_WITH_JUNCTIONS {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.7.9a' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0'\n    } else {\n        container 'quay.io/biocontainers/star:2.7.9a--h9ee0642_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path junctions\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  \"versions.yml\"                      , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n    tuple val(meta), path('*.out.junction')          , optional:true, emit: junction\n\n    script:\n    def prefix          = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def ignore_gtf      = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_platform    = params.seq_platform ? \"'PL:$params.seq_platform'\" : \"\"\n    def seq_center      = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix' $seq_platform \" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix' $seq_platform \"\n    def out_sam_type    = (options.args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (options.args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        --sjdbFileChrStartEnd $junctions \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $options.args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n    if [ -f ${prefix}.SJ.out.tab ]; then\n        mv ${prefix}.SJ.out.tab ${prefix}.out.junction\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.7.9a' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0' :\n        'quay.io/biocontainers/star:2.7.9a--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    val star_ignore_sjdbgtf\n    val seq_platform\n    val seq_center\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  \"versions.yml\"                      , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n    tuple val(meta), path('*.out.junction')          , optional:true, emit: junction\n    tuple val(meta), path('*.out.sam')               , optional:true, emit: sam\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf      = star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_platform    = seq_platform ? \"'PL:$seq_platform'\" : \"\"\n    def seq_center      = seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$seq_center' 'SM:$prefix' $seq_platform \" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix' $seq_platform \"\n    def out_sam_type    = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.7.9a' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0' :\n        'quay.io/biocontainers/star:2.7.9a--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    val star_ignore_sjdbgtf\n    val seq_platform\n    val seq_center\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  \"versions.yml\"                      , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n    tuple val(meta), path('*.out.junction')          , optional:true, emit: junction\n    tuple val(meta), path('*.out.sam')               , optional:true, emit: sam\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf      = star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_platform    = seq_platform ? \"'PL:$seq_platform'\" : \"\"\n    def seq_center      = seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$seq_center' 'SM:$prefix' $seq_platform \" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix' $seq_platform \"\n    def out_sam_type    = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.7.9a' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0' :\n        'quay.io/biocontainers/star:2.7.9a--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    val star_ignore_sjdbgtf\n    val seq_platform\n    val seq_center\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  \"versions.yml\"                      , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n    tuple val(meta), path('*.out.junction')          , optional:true, emit: junction\n    tuple val(meta), path('*.out.sam')               , optional:true, emit: sam\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def ignore_gtf      = star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_platform    = seq_platform ? \"'PL:$seq_platform'\" : \"\"\n    def seq_center      = seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$seq_center' 'SM:$prefix' $seq_platform \" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix' $seq_platform \"\n    def out_sam_type    = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/STAR_ALIGN", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/STAR_ALIGN", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/STAR_ALIGN", "raygozag/rnaseq/raygozag__rnaseq/STAR_ALIGN", "cguyomar/nf-ase/cguyomar__nf-ase/STAR_ALIGN", "cguyomar/nf-ase/cguyomar__nf-ase/STAR_ALIGN_WITH_JUNCTIONS", "nf-core/rnavar/nf-core__rnavar/STAR_ALIGN", "nf-core/modules/nf-core__modules/STAR_ALIGN", "goodwright/imaps-nf/goodwright__imaps-nf/STAR_ALIGN"], "list_wf_names": ["raygozag/rnaseq", "cguyomar/nf-ase", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "goodwright/imaps-nf", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/rnavar"]}, {"nb_reuse": 6, "tools": ["Mgenome", "STAR"], "nb_own": 4, "list_own": ["harleenduggal", "raygozag", "nf-core", "mahesh-panchal"], "nb_wf": 5, "list_wf": ["RNASEQ", "modules", "test_nfcore_workflow_chain", "nfcore-rnaseq", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 146, "codes": ["process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.10a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:64aad4a4e144878400649e71f42105311be7ed87-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:64aad4a4e144878400649e71f42105311be7ed87-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.10a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:64aad4a4e144878400649e71f42105311be7ed87-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:64aad4a4e144878400649e71f42105311be7ed87-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/RSEM_PREPAREREFERENCE", "raygozag/rnaseq/raygozag__rnaseq/RSEM_PREPAREREFERENCE", "nf-core/modules/nf-core__modules/RSEM_PREPAREREFERENCE", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/RSEM_PREPAREREFERENCE", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/RSEM_PREPAREREFERENCE", "nf-core/rnaseq/nf-core__rnaseq/RSEM_PREPAREREFERENCE"], "list_wf_names": ["raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess variantAnnotatesnpEff {\n    tag \"$name\"\n    publishDir \"${params.outdir}/SNPEFF_AnnotatedVariants/\", mode: 'copy', \n    saveAs: {filename -> params.saveIntermediateVariants ? \"$filename\" : null }\n\n    input:\n    set file(phased_vcf), file(phased_vcf_ind) from combined_variants_snpEff\n\n    output:\n    file \"*.{snpeff}\" into combined_variants_gatk_snpeff\n    file '.command.log' into snpeff_stdout\n    file 'SnpEffStats.csv' into snpeff_results\n\n    script:\n    \"\"\"\n        snpEff \\\\\n        -c /usr/local/lib/snpEff/snpEff.config \\\\\n        -i vcf \\\\\n        -csvStats SnpEffStats.csv \\\\\n        -o gatk \\\\\n        -o vcf \\\\\n        -filterInterval $params.target_bed GRCh37.75 $phased_vcf \\\\\n            > ${name}_combined_phased_variants.snpeff \n        \n        # Print version number to standard out\n        echo \"GATK version \"\\$(snpEff -version 2>&1)\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/variantAnnotatesnpEff"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["Emiller88", "alneberg", "FriederikeHanssen", "ewels", "drejom", "arontommi", "maxulysse", "rsuchecki", "SpikyClip", "matrulda", "ggabernet", "george-hall-ucl", "jordwil", "veeravalli", "adomingues", "colindaven", "vezzi", "lpantano", "skrakau", "chuan-wang", "ppericard", "grst", "pcantalupo", "nf-core-bot", "mvanins", "Galithil", "jun-wan", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "na399", "robsyme", "BABS-STP1", "senthil10", "drpowell", "kviljoen", "rfenouil", "jburos", "chris-cheshire", "mashehu", "Hammarn", "sven1103", "jemten", "paulklemm", "pranathivemuri", "marchoeppner", "mahesh-panchal", "JoseEspinosa", "apeltzer", "KevinMenden", "aanil", "silviamorins", "d4straub", "olgabot", "drpatelh", "amayer21", "zxl124"], "nb_contrib": 59, "codes": ["process STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? conda_str : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        \"https://depot.galaxyproject.org/singularity/${container_id}\" :\n        \"quay.io/biocontainers/${container_id}\" }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n    path gtf\n    val star_ignore_sjdbgtf\n    val seq_platform\n    val seq_center\n    val is_aws_igenome\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  \"versions.yml\"                      , emit: versions\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n    tuple val(meta), path('*.out.junction')          , optional:true, emit: junction\n    tuple val(meta), path('*.out.sam')               , optional:true, emit: sam\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n                                                                                  \n    conda_str = \"bioconda::star=2.7.10a bioconda::samtools=1.15.1 conda-forge::gawk=5.1.0\"\n    container_id = 'mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:afaaa4c6f5b308b4b6aa2dd8e99e1466b2a6b0cd-0'\n    if (is_aws_igenome) {\n        conda_str = \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\"\n        container_id = 'mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0'\n    }\n\n    def ignore_gtf      = star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_platform    = seq_platform ? \"'PL:$seq_platform'\" : \"\"\n    def seq_center      = seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$seq_center' 'SM:$prefix' $seq_platform \" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix' $seq_platform \"\n    def out_sam_type    = (args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/rnaseq/nf-core__rnaseq/STAR_ALIGN"], "list_wf_names": ["nf-core/rnaseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process samtools_fastq_unaligned {\n    tag \"${channel_id}\"\n    publishDir \"${params.outdir}/10x-fastqs/per-channel/unaligned\", mode: params.publish_dir_mode\n    label \"mid_cpu\"\n\n    input:\n    set val(channel_id), file(bam) from tenx_bam_for_aligned_fastq_ch\n\n    output:\n    set val(channel_id), val(\"unaligned\"), file(reads) into tenx_reads_unaligned_ch\n\n    script:\n    reads = \"${channel_id}__unaligned.fastq.gz\"\n    \"\"\"\n    samtools view -f4 ${bam} \\\\\n      | grep -E '${tenx_cell_barcode_pattern}' \\\\\n      | samtools fastq --threads ${task.cpus} -T ${tenx_tags} - \\\\\n      | gzip -c - \\\\\n        > ${reads} \\\\\n      || touch ${reads}\n    \"\"\"\n                                                                                    \n                                                                        \n  }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/samtools_fastq_unaligned"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 4, "tools": ["MAP", "GenMAPP"], "nb_own": 2, "list_own": ["nf-core", "jianhong"], "nb_wf": 2, "list_wf": ["modules", "nf-core-hicar"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process GENMAP_INDEX {\n    tag '$fasta'\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' :\n        'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"genmap\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        index \\\\\n        -F $fasta \\\\\n        -I genmap\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GENMAP_INDEX {\n    tag '$fasta'\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' :\n        'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"genmap\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        index \\\\\n        -F $fasta \\\\\n        -I genmap\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GENMAP_MAPPABILITY {\n    tag '$fasta'\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' :\n        'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\"\n\n    input:\n    path index\n\n    output:\n    path \"*.wig\"        , optional:true, emit: wig\n    path \"*.bedgraph\"   , optional:true, emit: bedgraph\n    path \"*.txt\"        , optional:true, emit: txt\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        map \\\\\n        $args \\\\\n        -I $index \\\\\n        -O mappability\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GENMAP_MAPPABILITY {\n    tag '$fasta'\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' :\n        'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\"\n\n    input:\n    path index\n\n    output:\n    path \"*.wig\"        , optional:true, emit: wig\n    path \"*.bedgraph\"   , optional:true, emit: bedgraph\n    path \"*.txt\"        , optional:true, emit: txt\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        map \\\\\n        $args \\\\\n        -I $index \\\\\n        -O mappability\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/nf-core-hicar/jianhong__nf-core-hicar/GENMAP_INDEX", "nf-core/modules/nf-core__modules/GENMAP_INDEX", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/GENMAP_MAPPABILITY", "nf-core/modules/nf-core__modules/GENMAP_MAPPABILITY"], "list_wf_names": ["jianhong/nf-core-hicar", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess make_bowtie_index {\n    label 'process_medium'\n    publishDir path: { params.save_reference ? \"${params.outdir}/bowtie/reference\" : params.outdir },\n               saveAs: { params.save_reference ? it : null }, mode: 'copy'\n\n    input:\n    file mature from mature\n    file hairpin from hairpin\n\n    output:\n    file 'mature_idx.*' into mature_index_bowtie\n    file 'hairpin_idx.*' into hairpin_index_bowtie, hairpin_index_bowtie_2\n    file 'hairpin_idx.fa' into hairpin_mirtop\n\n    script:\n    \"\"\"\n    seqkit grep -r --pattern \\\".*${params.mirtrace_species}-.*\\\" $mature > mature_sps.fa\n    seqkit seq --rna2dna mature_sps.fa > mature_igenome.fa\n    fasta_formatter -w 0 -i mature_igenome.fa -o mature_idx.fa\n    # fasta_nucleotide_changer -d -i mature_igenome.fa -o mature_idx.fa\n    bowtie-build mature_idx.fa mature_idx --threads ${task.cpus}\n\n    seqkit grep -r --pattern \\\".*${params.mirtrace_species}-.*\\\" $hairpin > hairpin_sps.fa\n    seqkit seq --rna2dna hairpin_sps.fa > hairpin_igenome.fa\n    # fasta_nucleotide_changer -d -i hairpin_igenome.fa -o hairpin_idx.fa\n    fasta_formatter -w 0 -i hairpin_igenome.fa -o hairpin_idx.fa\n    bowtie-build hairpin_idx.fa hairpin_idx --threads ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/make_bowtie_index"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 5, "tools": ["STAR"], "nb_own": 4, "list_own": ["nf-core", "WhalleyT", "pilm-bioinformatics", "UMCUGenetics"], "nb_wf": 5, "list_wf": ["pipelines-nf-genomes", "NextflowModules", "pipelines-nf-circtools", "scrnaseq", "neoantigen_prediction"], "list_contrib": ["PeterBailey", "ffmmulder", "nf-core-bot", "rernst", "sawibo", "maxulysse", "sk-sahu", "melferink", "apeltzer", "ggabernet", "WhalleyT", "ellendejong", "lpantano", "olgabot", "ywilke"], "nb_contrib": 15, "codes": [" process makeSTARindex {\n        label 'high_memory'\n        tag \"$fasta\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/reference_genome\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta from ch_fasta_for_star_index\n        file gtf from gtf_makeSTARindex\n\n        output:\n        file \"star\" into star_index\n\n        script:\n        def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --runThreadN ${task.cpus} \\\\\n            --sjdbGTFfile $gtf \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            $avail_mem\n        \"\"\"\n    }", "\nprocess makeSTARindex {\n    label 'high_memory'\n    tag \"$fasta\"\n    publishDir path: { params.save_reference ? \"${params.outdir}/reference_genome/star_index\" : params.outdir },\n                saveAs: { params.save_reference ? it : null }, mode: 'copy'\n\n    input:\n    file fasta from genome_fasta_makeSTARindex\n    file gtf from gtf_makeSTARindex\n\n    output:\n    file \"star\" into star_index\n\n    when: params.aligner == 'star' && !params.star_index && params.fasta\n\n    script:\n    def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    \"\"\"\n    mkdir star\n    STAR \\\\\n        --runMode genomeGenerate \\\\\n        --runThreadN ${task.cpus} \\\\\n        --sjdbGTFfile $gtf \\\\\n        --genomeDir star/ \\\\\n        --genomeFastaFiles $fasta \\\\\n        $avail_mem\n    \"\"\"\n}", " process make_STAR_index{\n        label 'multithreaded'\n        publishDir path: { params.save_ref ? \"${params.outdir}/reference_genome\" : params.outdir },\n                   saveAs: { params.save_ref ? it : null }, mode: 'copy'\n\n        input:\n        file fasta from idx_fasta\n        file gtf from idx_gtf\n\n        output:\n        file \"star\" into star_index\n\n        script:\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --runThreadN ${task.cpus} \\\\\n            --sjdbGTFfile $gtf \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta\n        \"\"\"\n    }", " process makeSTARindex {\n        label 'high_memory'\n        tag \"$fasta\"\n        publishDir path: { \"${outdir}\" },\n                   mode: 'copy'\n\n        input:\n        file fasta from ch_fasta_for_star_index\n        file gtf from gtf_makeSTARindex\n\n        output:\n        file \"star\" into star_index\n\n        script:\n        def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --runThreadN ${task.cpus} \\\\\n            --sjdbGTFfile $gtf \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            $avail_mem\n        \"\"\"\n    }", "process GenomeGenerate {\n    tag {\"STAR GenomeGenerate ${genome_fasta.baseName} \"}\n    label 'STAR_2_7_3a'\n    label 'STAR_2_7_3a_GenomeGenerate'\n    container = 'quay.io/biocontainers/star:2.7.3a--0'\n    shell = ['/bin/bash', '-euo', 'pipefail']\n\n    input:\n        path(genome_fasta)\n        path(genome_gtf)\n    \n    \n    output:\n        path(\"${genome_fasta.baseName}\", emit: star_index)\n     \n   \n    script:\n                                                                                                                         \n        def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        mkdir ${genome_fasta.baseName}\n        STAR \\\n            --runMode genomeGenerate \\\n            --runThreadN ${task.cpus} \\\n            --sjdbGTFfile ${genome_gtf} \\\n            --genomeDir ${genome_fasta.baseName}/ \\\n            --genomeFastaFiles ${genome_fasta} \\\n            $avail_mem\n        \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-circtools/pilm-bioinformatics__pipelines-nf-circtools/makeSTARindex", "nf-core/scrnaseq/nf-core__scrnaseq/makeSTARindex", "WhalleyT/neoantigen_prediction/WhalleyT__neoantigen_prediction/make_STAR_index", "pilm-bioinformatics/pipelines-nf-genomes/pilm-bioinformatics__pipelines-nf-genomes/makeSTARindex", "UMCUGenetics/NextflowModules/UMCUGenetics__NextflowModules/GenomeGenerate"], "list_wf_names": ["UMCUGenetics/NextflowModules", "WhalleyT/neoantigen_prediction", "nf-core/scrnaseq", "pilm-bioinformatics/pipelines-nf-circtools", "pilm-bioinformatics/pipelines-nf-genomes"]}, {"nb_reuse": 15, "tools": ["GATK"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 15, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "sarek-mirror", "germline_somatic", "custom_sarek", "pgp-chronek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSample, file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from bamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'process_medium'\n\n    input:\n        set idPatient, idSample, file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from bamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? params.target_bed ? \"-L ${params.target_bed}\" : \"-L ${germlineResource}\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSample, file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from bamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'process_low'\n\n    input:\n        set idPatient, idSample, file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from bamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? params.target_bed ? \"-L ${params.target_bed}\" : \"-L ${germlineResource}\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSample}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}"], "list_proc": ["cgpu/sarek-mirror/cgpu__sarek-mirror/PileupSummariesForMutect2", "nf-core/sarek/nf-core__sarek/PileupSummariesForMutect2", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/PileupSummariesForMutect2", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/PileupSummariesForMutect2", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/PileupSummariesForMutect2", "rmoran7/dx_sarek/rmoran7__dx_sarek/PileupSummariesForMutect2", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/PileupSummariesForMutect2", "cgpu/haplosarek/cgpu__haplosarek/PileupSummariesForMutect2", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/PileupSummariesForMutect2", "rmoran7/custom_sarek/rmoran7__custom_sarek/PileupSummariesForMutect2", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/PileupSummariesForMutect2", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/PileupSummariesForMutect2", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/PileupSummariesForMutect2", "cgpu/pgp-chronek/cgpu__pgp-chronek/PileupSummariesForMutect2", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/PileupSummariesForMutect2"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-mirror-cache", "cgpu/sarek-genomechronicler", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["CANU"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess CANU {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n    label 'process_high_memory'\n    label 'error_retry'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'canu=2.1.1-2' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/canu:2.1.1--h1b792b2_2\"\n    } else {\n        container \"quay.io/biocontainers/canu:2.1.1--h1b792b2_2\"\n    }\n\n    input:\n    tuple val(meta), val(reads), file(longreads)\n\n    output:\n    tuple val(meta), path('*_assembly.fasta') , emit: assembly\n    tuple val(meta), path('*_assembly.report'), emit: log\n    path  '*.version.txt'                     , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def genomeSize  = meta.genome_size == 'NA' ? \"5m\" : \"${meta.genome_size}\"\n    \"\"\"\n    canu -p assembly -d canu_out \\\n        ${options.args} \\\n        genomeSize=\"${genomeSize}\" -nanopore \"${longreads}\" \\\n        maxThreads=\"${task.cpus}\" merylMemory=\"${task.memory.toGiga()}G\" \\\n        merylThreads=\"${task.cpus}\" hapThreads=\"${task.cpus}\" batMemory=\"${task.memory.toGiga()}G\" \\\n        redMemory=\"${task.memory.toGiga()}G\" redThreads=\"${task.cpus}\" \\\n        oeaMemory=\"${task.memory.toGiga()}G\" oeaThreads=\"${task.cpus}\" \\\n        corMemory=\"${task.memory.toGiga()}G\" corThreads=\"${task.cpus}\"\n    mv canu_out/assembly.contigs.fasta ${prefix}_assembly.fasta\n    mv canu_out/assembly.report ${prefix}_assembly.report\n\n    echo \\$(canu --version 2>&1) | sed -e 's/Canu //g' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/CANU"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 1, "tools": ["Count"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process KALLISTOBUSTOOLS_COUNT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::kb-python=0.26.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/kb-python:0.26.3--pyhdfd78af_0' :\n        'quay.io/biocontainers/kb-python:0.26.3--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  t2g\n    path  t1c\n    path  t2c\n    val   workflow_mode\n    val   technology\n\n    output:\n    tuple val(meta), path (\"*.count\"), emit: count\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def cdna     = t1c ? \"-c1 $t1c\" : ''\n    def introns  = t2c ? \"-c2 $t2c\" : ''\n    \"\"\"\n    kb \\\\\n        count \\\\\n        -t $task.cpus \\\\\n        -i $index \\\\\n        -g $t2g \\\\\n        $cdna \\\\\n        $introns \\\\\n        --workflow $workflow_mode \\\\\n        -x $technology \\\\\n        $args \\\\\n        -o ${prefix}.count \\\\\n        ${reads[0]} \\\\\n        ${reads[1]}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kallistobustools: \\$(echo \\$(kb --version 2>&1) | sed 's/^.*kb_python //;s/positional arguments.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/KALLISTOBUSTOOLS_COUNT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    tuple val(name), path(reads) from ch_fastq_fastqc_pretrim\n\n    output:\n    file \"*fastqc.{zip,html}\" into ch_fastqc_pretrim_mqc\n\n    script:\n    read_ext = reads.getName().split('\\\\.', 2)[1]\n    read_name = reads.getName().split('\\\\.', 2)[0]\n    new_reads = \"${name}_reads_fastqc.${read_ext}\"\n    new_reads_simple = \"${name}_reads_fastqc\"\n    \"\"\"\n    cp ${reads} ${new_reads}\n    fastqc --quiet --threads $task.cpus ${new_reads}\n    mv ${new_reads_simple}*.html ${name}_reads_fastqc.html\n    mv ${new_reads_simple}*.zip ${name}_reads_fastqc.zip\n    \"\"\"\n}"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/fastqc"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "SAMBLASTER"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BISCUIT_BLASTER {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::biscuit=1.0.2.20220113 bioconda::samblaster=0.1.26 bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-db16f1c237a26ea9245cf9924f858974ff321d6e:17fa66297f088a1bc7560b7b90dc273bf23f2d8c-0':\n        'quay.io/biocontainers/mulled-v2-db16f1c237a26ea9245cf9924f858974ff321d6e:17fa66297f088a1bc7560b7b90dc273bf23f2d8c-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args3 = task.ext.args3 ?: ''\n    def biscuit_cpus = (int) Math.max(Math.floor(task.cpus*0.95),1)\n    def samtools_cpus = task.cpus-biscuit_cpus\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.bis.amb\" | sed 's/.bis.amb//'`\n\n    biscuit align \\\\\n        -@ $biscuit_cpus \\\\\n        $args \\\\\n        \\$INDEX \\\\\n        $reads | \\\\\n    samblaster \\\\\n        $args2 | \\\\\n    samtools sort \\\\\n        -@ $samtools_cpus \\\\\n        $args3 \\\\\n        --write-index \\\\\n        -o ${prefix}.bam##idx##${prefix}.bam.bai\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        biscuit: \\$( biscuit version |& sed '1!d; s/^.*BISCUIT Version: //' )\n        samtools: \\$( samtools --version |& sed '1!d; s/^.*samtools //' )\n        samblaster: \\$( samblaster --version |& sed 's/^.*samblaster: Version //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BISCUIT_BLASTER"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_BARPLOT {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    path(taxonomy)\n\n    output:\n    path(\"barplot/*\")   , emit: folder\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime taxa barplot  \\\n        --i-table ${table}  \\\n        --i-taxonomy ${taxonomy}  \\\n        --m-metadata-file ${metadata}  \\\n        --o-visualization taxa-bar-plots.qzv  \\\n        --verbose\n    qiime tools export --input-path taxa-bar-plots.qzv  \\\n        --output-path barplot\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_BARPLOT {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    path(taxonomy)\n\n    output:\n    path(\"barplot/*\")   , emit: folder\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime taxa barplot  \\\n        --i-table ${table}  \\\n        --i-taxonomy ${taxonomy}  \\\n        --m-metadata-file ${metadata}  \\\n        --o-visualization taxa-bar-plots.qzv  \\\n        --verbose\n    qiime tools export --input-path taxa-bar-plots.qzv  \\\n        --output-path barplot\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_BARPLOT", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_BARPLOT"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["QIIME", "BioMe"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_EXPORT_RELASV {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n\n    output:\n    path(\"rel-table-ASV.tsv\"), emit: tsv\n    path \"versions.yml\"      , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #convert to relative abundances\n    qiime feature-table relative-frequency \\\n        --i-table ${table} \\\n        --o-relative-frequency-table relative-table-ASV.qza\n\n    #export to biom\n    qiime tools export --input-path relative-table-ASV.qza --output-path relative-table-ASV\n\n    #convert to tab separated text file \"rel-table-ASV.tsv\"\n    biom convert -i relative-table-ASV/feature-table.biom \\\n        -o rel-table-ASV.tsv --to-tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_EXPORT_RELASV {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n\n    output:\n    path(\"rel-table-ASV.tsv\"), emit: tsv\n    path \"versions.yml\"      , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #convert to relative abundances\n    qiime feature-table relative-frequency \\\n        --i-table ${table} \\\n        --o-relative-frequency-table relative-table-ASV.qza\n\n    #export to biom\n    qiime tools export --input-path relative-table-ASV.qza --output-path relative-table-ASV\n\n    #convert to tab separated text file \"rel-table-ASV.tsv\"\n    biom convert -i relative-table-ASV/feature-table.biom \\\n        -o rel-table-ASV.tsv --to-tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_EXPORT_RELASV", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_EXPORT_RELASV"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["Minia"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["test_nfcore_workflow_chain", "modules", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process MINIA {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::minia=3.2.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minia:3.2.6--h9a82719_0' :\n        'quay.io/biocontainers/minia:3.2.6--h9a82719_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.contigs.fa'), emit: contigs\n    tuple val(meta), path('*.unitigs.fa'), emit: unitigs\n    tuple val(meta), path('*.h5')        , emit: h5\n    path  \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_list = reads.join(\",\")\n    \"\"\"\n    echo \"${read_list}\" | sed 's/,/\\\\n/g' > input_files.txt\n    minia \\\\\n        $args \\\\\n        -nb-cores $task.cpus \\\\\n        -in input_files.txt \\\\\n        -out $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minia: \\$(echo \\$(minia --version 2>&1 | grep Minia) | sed 's/^.*Minia version //;')\n    END_VERSIONS\n    \"\"\"\n}", "process MINIA {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::minia=3.2.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minia:3.2.6--h9a82719_0' :\n        'quay.io/biocontainers/minia:3.2.6--h9a82719_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.contigs.fa'), emit: contigs\n    tuple val(meta), path('*.unitigs.fa'), emit: unitigs\n    tuple val(meta), path('*.h5')        , emit: h5\n    path  \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_list = reads.join(\",\")\n    \"\"\"\n    echo \"${read_list}\" | sed 's/,/\\\\n/g' > input_files.txt\n    minia \\\\\n        $args \\\\\n        -nb-cores $task.cpus \\\\\n        -in input_files.txt \\\\\n        -out $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minia: \\$(echo \\$(minia --version 2>&1 | grep Minia) | sed 's/^.*Minia version //;')\n    END_VERSIONS\n    \"\"\"\n}", "process MINIA {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::minia=3.2.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minia:3.2.6--h9a82719_0' :\n        'quay.io/biocontainers/minia:3.2.6--h9a82719_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.contigs.fa'), emit: contigs\n    tuple val(meta), path('*.unitigs.fa'), emit: unitigs\n    tuple val(meta), path('*.h5')        , emit: h5\n    path  \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_list = reads.join(\",\")\n    \"\"\"\n    echo \"${read_list}\" | sed 's/,/\\\\n/g' > input_files.txt\n    minia \\\\\n        $args \\\\\n        -nb-cores $task.cpus \\\\\n        -in input_files.txt \\\\\n        -out $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minia: \\$(echo \\$(minia --version 2>&1 | grep Minia) | sed 's/^.*Minia version //;')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/MINIA", "nf-core/modules/nf-core__modules/MINIA", "nf-core/viralrecon/nf-core__viralrecon/MINIA"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess sexdeterrmine {\n    label 'mc_small'\n    publishDir \"${params.outdir}/sex_determination\", mode: params.publish_dir_mode\n\n    input:\n    path bam from ch_prepped_for_sexdeterrmine.collect()\n    path(bed) from ch_bed_for_sexdeterrmine\n\n    output:\n    file \"SexDet.txt\"\n    file \"*.json\" into ch_sexdet_for_multiqc\n\n    when:\n    params.run_sexdeterrmine\n    \n    script:\n    def filter = bed.getName() != 'nf-core_eager_dummy.txt' ? \"-b $bed\" : ''\n    \"\"\"\n    ls *.bam >> bamlist.txt\n    samtools depth -aa -q30 -Q30 $filter -f bamlist.txt | sexdeterrmine -f bamlist.txt > SexDet.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/sexdeterrmine"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["STAR"], "nb_own": 2, "list_own": ["bhagesh-codebeast", "nf-core"], "nb_wf": 2, "list_wf": ["dualrnaseq", "nextflowdualrnaseq"], "list_contrib": ["lbarquist", "nf-core-bot", "apeltzer", "reganhayward", "bhagesh-codebeast", "bozmik"], "nb_contrib": 6, "codes": ["\tprocess STARindex_salmon_alignment {\n\t\tpublishDir \"${params.outdir}/STAR_for_salmon\", mode: params.publish_dir_mode\n\t\tstoreDir \"${params.outdir}/STAR_for_salmon\" \n\t\ttag \"STAR_index\"\n\n         \tlabel 'process_high'\n\n\t\tinput:\n\t\tfile(fasta) from host_pathogen_fasta_index\n\t\tfile(gff) from genome_gff_star_index\n\n\t\toutput:\n\t\tfile \"index/*\" into star_index_transcriptome_alignment\n\n\t\tscript:\n\t\tsjdbOverhang = params.sjdbOverhang\n\t\tstar_salmon_index_params = params.star_salmon_index_params\n\t\t\"\"\"\n\t\tmkdir index\n\t\tSTAR --runThreadN ${task.cpus} --runMode genomeGenerate --genomeDir index/ --genomeFastaFiles $fasta --sjdbGTFfile $gff --sjdbGTFfeatureExon exon --sjdbGTFtagExonParentTranscript Parent --sjdbOverhang $sjdbOverhang $star_salmon_index_params\n\t\t\"\"\"\n\t}", "\tprocess STARindex_salmon_alignment {\n\t\tpublishDir \"${params.outdir}/STAR_for_salmon\", mode: params.publish_dir_mode\n\t\tstoreDir \"${params.outdir}/STAR_for_salmon\" \n\t\ttag \"STAR_index\"\n\n         \tlabel 'process_high'\n\n\t\tinput:\n\t\tfile(fasta) from host_pathogen_fasta_index\n\t\tfile(gff) from genome_gff_star_index\n\n\t\toutput:\n\t\tfile \"index/*\" into star_index_transcriptome_alignment\n\n\t\tscript:\n\t\tsjdbOverhang = params.sjdbOverhang\n\t\tstar_salmon_index_params = params.star_salmon_index_params\n\t\t\"\"\"\n\t\tmkdir index\n\t\tSTAR --runThreadN ${task.cpus} --runMode genomeGenerate --genomeDir index/ --genomeFastaFiles $fasta --sjdbGTFfile $gff --sjdbGTFfeatureExon exon --sjdbGTFtagExonParentTranscript Parent --sjdbOverhang $sjdbOverhang $star_salmon_index_params\n\t\t\"\"\"\n\t}"], "list_proc": ["nf-core/dualrnaseq/nf-core__dualrnaseq/STARindex_salmon_alignment", "bhagesh-codebeast/nextflowdualrnaseq/bhagesh-codebeast__nextflowdualrnaseq/STARindex_salmon_alignment"], "list_wf_names": ["nf-core/dualrnaseq", "bhagesh-codebeast/nextflowdualrnaseq"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nanoseq"], "list_contrib": ["lwratten", "alneberg", "nf-core-bot", "ewels", "csawye01", "maxulysse", "KevinMenden", "cying111", "drpatelh", "yuukiiwa"], "nb_contrib": 10, "codes": ["\nprocess BEDTOOLS_GENOMECOV {\n    label 'process_medium'\n\n    conda     (params.enable_conda ? \"bioconda::bedtools=2.29.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.29.2--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.29.2--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(sizes), val(is_transcripts), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(sizes), path(\"*.bedGraph\"), emit: bedgraph\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    split = (params.protocol == 'DNA' || is_transcripts) ? \"\" : \"-split\"\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -split \\\\\n        -ibam ${bam[0]} \\\\\n        -bg \\\\\n        | bedtools sort > ${meta.id}.bedGraph\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/BEDTOOLS_GENOMECOV"], "list_wf_names": ["nf-core/nanoseq"]}, {"nb_reuse": 6, "tools": ["Salmon"], "nb_own": 4, "list_own": ["harleenduggal", "raygozag", "nf-core", "mahesh-panchal"], "nb_wf": 5, "list_wf": ["RNASEQ", "modules", "test_nfcore_workflow_chain", "nfcore-rnaseq", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 146, "codes": ["process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/raygozag__rnaseq/SALMON_INDEX", "nf-core/modules/nf-core__modules/SALMON_INDEX", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SALMON_INDEX", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SALMON_INDEX", "nf-core/rnaseq/nf-core__rnaseq/SALMON_INDEX", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SALMON_INDEX"], "list_wf_names": ["raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess variantEvaluate {\n    tag \"$name\"\n    publishDir \"${params.outdir}/GATK_VariantEvaluate\", mode: 'copy'\n\n    input:\n    set file(\"${name}_combined_variants.vcf\"), file(\"${name}_combined_variants.vcf.idx\") from combined_variants_evaluate\n\n    output:\n    file \"${name}_combined_phased_variants.eval\"\n    file \"${name}_combined_phased_variants.eval\" into gatk_variant_eval_results\n\n    script:\n    \"\"\"\n    gatk -T VariantEval \\\\\n        -R $params.gfasta \\\\\n        --eval $phased_vcf \\\\\n        --dbsnp $params.dbsnp \\\\\n        -o ${name}_combined_phased_variants.eval \\\\\n        -L $params.target \\\\\n        --doNotUseAllStandardModules \\\\\n        --evalModule TiTvVariantEvaluator \\\\\n        --evalModule CountVariants \\\\\n        --evalModule CompOverlap \\\\\n        --evalModule ValidationReport \\\\\n        --stratificationModule Filter \\\\\n        -l INFO\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/variantEvaluate"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["MultiQC", "kallisto", "STAR", "Salmon", "BUStools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 7, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version &> v_pipeline.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    salmon --version &> v_salmon.txt 2>&1 || true\n    STAR --version &> v_star.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    kallisto version &> v_kallisto.txt 2>&1 || true\n    bustools &> v_bustools.txt 2>&1 || true\n\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/scrnaseq/nf-core__scrnaseq/get_software_versions"], "list_wf_names": ["nf-core/scrnaseq"]}, {"nb_reuse": 2, "tools": ["MUMmer"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "leebrian", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["\nprocess MUMMER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::mummer=3.23\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mummer:3.23--pl5262h1b792b2_12' :\n        'quay.io/biocontainers/mummer:3.23--pl5262h1b792b2_12' }\"\n\n    input:\n    tuple val(meta), path(ref), path(query)\n\n    output:\n    tuple val(meta), path(\"*.coords\"), emit: coords\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed_ref = ref.getName().endsWith(\".gz\") ? true : false\n    def fasta_name_ref = ref.getName().replace(\".gz\", \"\")\n\n    def is_compressed_query = query.getName().endsWith(\".gz\") ? true : false\n    def fasta_name_query = query.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed_ref\" == \"true\" ]; then\n        gzip -c -d $ref > $fasta_name_ref\n    fi\n    if [ \"$is_compressed_query\" == \"true\" ]; then\n        gzip -c -d $query > $fasta_name_query\n    fi\n    mummer \\\\\n        $args \\\\\n        $fasta_name_ref \\\\\n        $fasta_name_query \\\\\n        > ${prefix}.coords\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mummer: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MUMMER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::mummer=3.23\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mummer:3.23--pl5262h1b792b2_12' :\n        'quay.io/biocontainers/mummer:3.23--pl5262h1b792b2_12' }\"\n\n    input:\n    tuple val(meta), path(ref), path(query)\n\n    output:\n    tuple val(meta), path(\"*.coords\"), emit: coords\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed_ref = ref.getName().endsWith(\".gz\") ? true : false\n    def fasta_name_ref = ref.getName().replace(\".gz\", \"\")\n\n    def is_compressed_query = query.getName().endsWith(\".gz\") ? true : false\n    def fasta_name_query = query.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed_ref\" == \"true\" ]; then\n        gzip -c -d $ref > $fasta_name_ref\n    fi\n    if [ \"$is_compressed_query\" == \"true\" ]; then\n        gzip -c -d $query > $fasta_name_query\n    fi\n    mummer \\\\\n        $args \\\\\n        $fasta_name_ref \\\\\n        $fasta_name_query \\\\\n        > ${prefix}.coords\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mummer: $VERSION\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/MUMMER", "nf-core/modules/nf-core__modules/MUMMER"], "list_wf_names": ["nf-core/modules", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 5, "tools": ["Picard", "SAMtools"], "nb_own": 4, "list_own": ["FAANG", "PI-Bioinfo", "UCL-BLIC", "nf-core"], "nb_wf": 4, "list_wf": ["thamlee-nxf-rnaseq", "methylseq", "rnaseq_variant_calling", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "lconde-ucl", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "ThamLe2601", "drpatelh", "Jani-94"], "nb_contrib": 21, "codes": [" process markDuplicates {\n            tag \"$name\"\n            publishDir \"${params.outdir}/bwa-mem_markDuplicates\", mode: params.publish_dir_mode,\n                saveAs: {filename -> filename.indexOf(\".bam\") == -1 ? \"logs/$filename\" : \"$filename\"}\n\n            input:\n            set val(name), file(bam) from ch_bam_sorted_for_markDuplicates\n                        \n\n            output:\n            set val(name), file(\"${name}.markDups.bam\") into ch_bam_dedup_for_methyldackel, ch_bam_dedup_for_qualimap, ch_bam_cgmaptools\n            set val(name), file(\"${bam.baseName}.markDups.bam.bai\") into ch_bam_index_for_methyldackel                                                              \n            file \"${bam.baseName}.markDups_metrics.txt\" into ch_markDups_results_for_multiqc\n\n            script:\n            if( !task.memory ){\n                log.info \"[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.\"\n                avail_mem = 3\n            } else {\n                avail_mem = task.memory.toGiga()\n            }\n            \"\"\"\n            picard -Xmx${avail_mem}g MarkDuplicates \\\\\n                INPUT=$bam \\\\\n                OUTPUT=${bam.baseName}.markDups.bam \\\\\n                METRICS_FILE=${bam.baseName}.markDups_metrics.txt \\\\\n                REMOVE_DUPLICATES=false \\\\\n                ASSUME_SORTED=true \\\\\n                PROGRAM_RECORD_ID='null' \\\\\n                VALIDATION_STRINGENCY=LENIENT\n            samtools index ${bam.baseName}.markDups.bam\n            \"\"\"\n        }", " process markDuplicates_bam_input {\n        tag \"${name}\"\n        publishDir \"${params.outdir}/bam_markDuplicates\", mode: params.publish_dir_mode,\n            saveAs: {filename -> \"$filename\"}\n\n    input:\n                                                  \n        set val(name), file(bam) from ch_indep_bam_for_processing      \n\n    output:\n        set val(name), file(\"*.markDups.bam\") into ch_bam_resort, ch_bam_dedup_for_qualimap_indep\n        set val(name), file(\"*.markDups.bam.bai\") into ch_bam_index_indep \n        file \"*.markDups_metrics.txt\" into ch_markDups_results_for_multiqc_indep\n            \n    script:\n        \n                                                \n        \n        if( !task.memory ){\n            log.info \"[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.\"\n            avail_mem = 3\n            } else {\n                avail_mem = task.memory.toGiga()\n            }\n\n            \"\"\"\n            picard -Xmx${avail_mem}g MarkDuplicates \\\\\n                INPUT=$bam \\\\\n                OUTPUT=${name}.markDups.bam \\\\\n                METRICS_FILE=${name}.markDups_metrics.txt \\\\\n                REMOVE_DUPLICATES=false \\\\\n                ASSUME_SORTED=true \\\\\n                PROGRAM_RECORD_ID='null' \\\\\n                VALIDATION_STRINGENCY=LENIENT\n            samtools index ${name}.markDups.bam\n            \"\"\"\n        }", "\nprocess markDuplicates {\n\n    tag \"${bam.baseName - 'Aligned.sortedByCoord.out'}\"\n    publishDir \"${params.outdir}/markDuplicates\", mode: 'copy',\n        saveAs: {filename ->\n            if (filename.indexOf(\"_metrics.txt\") > 0) \"metrics/$filename\"\n            else if (!params.saveDedupBAM && filename == \"where_are_my_files.txt\") filename\n            else if (params.saveDedupBAM && filename != \"where_are_my_files.txt\") filename\n            else null\n        }\n\n    input:\n    file bam from bam_markduplicates\n    file wherearemyfiles\n\n    output:\n    set val(\"${sample}\"), file(\"${sample}.markDups.bam\"), file(\"${sample}.markDups.bai\") into bam_md\n    file \"${sample}.markDups_metrics.txt\" into picard_resuls\n    file \"where_are_my_files.txt\"\n\n    script:\n    sample = bam.baseName - /Aligned.sortedByCoord.out/\n    \"\"\"\n    picard MarkDuplicates \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${sample}.markDups.bam \\\\\n        METRICS_FILE=${sample}.markDups_metrics.txt \\\\\n        REMOVE_DUPLICATES=false \\\\\n        ASSUME_SORTED=true \\\\\n        PROGRAM_RECORD_ID='null' \\\\\n        VALIDATION_STRINGENCY=LENIENT \\\\\n\tCREATE_INDEX=TRUE\n    \"\"\"\n}", " process markDuplicates {\n            tag \"$name\"\n            publishDir \"${params.outdir}/bwa-mem_markDuplicates\", mode: params.publish_dir_mode,\n                saveAs: {filename -> filename.indexOf(\".bam\") == -1 ? \"logs/$filename\" : \"$filename\"}\n\n            input:\n            set val(name), file(bam) from ch_bam_sorted_for_markDuplicates\n\n            output:\n            set val(name), file(\"${bam.baseName}.markDups.bam\") into ch_bam_dedup_for_methyldackel, ch_bam_dedup_for_qualimap\n            set val(name), file(\"${bam.baseName}.markDups.bam.bai\") into ch_bam_index_for_methyldackel                                                              \n            file \"${bam.baseName}.markDups_metrics.txt\" into ch_markDups_results_for_multiqc\n\n            script:\n            if( !task.memory ){\n                log.info \"[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.\"\n                avail_mem = 3\n            } else {\n                avail_mem = task.memory.toGiga()\n            }\n            \"\"\"\n            picard -Xmx${avail_mem}g MarkDuplicates \\\\\n                INPUT=$bam \\\\\n                OUTPUT=${bam.baseName}.markDups.bam \\\\\n                METRICS_FILE=${bam.baseName}.markDups_metrics.txt \\\\\n                REMOVE_DUPLICATES=false \\\\\n                ASSUME_SORTED=true \\\\\n                PROGRAM_RECORD_ID='null' \\\\\n                VALIDATION_STRINGENCY=LENIENT\n            samtools index ${bam.baseName}.markDups.bam\n            \"\"\"\n        }", "\nprocess picard {\n    tag \"picard $sampleid\"\n    publishDir \"$params.picard\", mode: 'copy'\n\n    input:\n    tuple val(sampleid), path (bam) \n\n    output:\n    path (\"${sampleid}*\"), emit: report_picard\n\n    script:\n    \"\"\" \n    picard MarkDuplicates \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${sampleid}.markDups.bam \\\\\n        METRICS_FILE=${sampleid}.markDups_metrics.txt \\\\\n        REMOVE_DUPLICATES=false \\\\\n        ASSUME_SORTED=true \\\\\n        PROGRAM_RECORD_ID='null' \\\\\n        VALIDATION_STRINGENCY=LENIENT\n    \"\"\"\n}"], "list_proc": ["FAANG/GSM-pipeline/FAANG__GSM-pipeline/markDuplicates", "FAANG/GSM-pipeline/FAANG__GSM-pipeline/markDuplicates_bam_input", "UCL-BLIC/rnaseq_variant_calling/UCL-BLIC__rnaseq_variant_calling/markDuplicates", "nf-core/methylseq/nf-core__methylseq/markDuplicates", "PI-Bioinfo/thamlee-nxf-rnaseq/PI-Bioinfo__thamlee-nxf-rnaseq/picard"], "list_wf_names": ["UCL-BLIC/rnaseq_variant_calling", "nf-core/methylseq", "PI-Bioinfo/thamlee-nxf-rnaseq", "FAANG/GSM-pipeline"]}, {"nb_reuse": 18, "tools": ["GATK"], "nb_own": 11, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 17, "list_wf": ["haplosarek", "saw.sarek", "sarek_ubec", "layer_lab_caw", "layer_lab_chco", "PGP-UK-sarek", "germline_somatic", "layer_lab_vc", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n                      \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                        \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(fasta)\n        file(fastaFai) \n        file(dict)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\")\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n                      \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                        \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(fasta)\n        file(fastaFai) \n        file(dict)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\")\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${intervalBed.baseName}_${idSample}.recal.bam \\\n        -L ${intervalBed} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    disk '70 GB'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${intervalBed.baseName}_${idSample}.recal.bam \\\n        -L ${intervalBed} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${intervalBed.baseName}_${idSample}.recal.bam \\\n        -L ${intervalBed} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n                      \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                        \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(fasta)\n        file(fastaFai) \n        file(dict)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\")\n\n    when: params.known_indels  && step != 'variantcalling' &&\n        ('haplotypecaller' in tools || \n        'mutect2' in tools ||\n        'mutect2_single' in tools ||\n        'gen_somatic_pon' in tools)\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_4'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess RecalibrateBasesInReadGroup {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n    label 'withGatkContainer'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        tuple val(idPatient), val(idSample), file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(dict)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${prefix}${idSample}.recal.bam\")\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${intervalBed.baseName}_${idSample}.recal.bam \\\n        -L ${intervalBed} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}"], "list_proc": ["Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/ApplyBQSR", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/ApplyBQSR", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/ApplyBQSR", "nf-core/sarek/nf-core__sarek/ApplyBQSR", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/ApplyBQSR", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/ApplyBQSR", "cgpu/pgp-chronek/cgpu__pgp-chronek/ApplyBQSR", "rmoran7/custom_sarek/rmoran7__custom_sarek/ApplyBQSR", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/ApplyBQSR", "cgpu/sarek-mirror/cgpu__sarek-mirror/ApplyBQSR", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/ApplyBQSR", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/ApplyBQSR", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/ApplyBQSR", "rmoran7/dx_sarek/rmoran7__dx_sarek/ApplyBQSR", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/ApplyBQSR", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/RecalibrateBasesInReadGroup", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/ApplyBQSR", "cgpu/haplosarek/cgpu__haplosarek/ApplyBQSR"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "ryanlayerlab/layer_lab_caw", "nf-core/sarek", "chelauk/test_nextflow_sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "javaidm/layer_lab_vc"]}, {"nb_reuse": 15, "tools": ["GATK"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 15, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "sarek-mirror", "germline_somatic", "custom_sarek", "pgp-chronek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["FriederikeHanssen", "alneberg", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'process_low'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(mergedPileup) from mergedPileupFile\n\n     output:\n        set idPatient, val(\"${idSample}\"), file(\"${idSample}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSample}_pileupsummaries.table \\\n        -O ${idSample}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(mergedPileup) from pairBamCalculateContamination\n\n     output:\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(mergedPileup) from pairBamCalculateContamination\n\n     output:\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'process_medium'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(mergedPileup) from mergedPileupFile\n\n     output:\n        set idPatient, val(\"${idSample}\"), file(\"${idSample}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSample}_pileupsummaries.table \\\n        -O ${idSample}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(mergedPileup) from mergedPileupFile\n\n     output:\n        set idPatient, val(\"${idSample}\"), file(\"${idSample}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSample}_pileupsummaries.table \\\n        -O ${idSample}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(mergedPileup) from mergedPileupFile\n\n     output:\n        set idPatient, val(\"${idSample}\"), file(\"${idSample}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSample}_pileupsummaries.table \\\n        -O ${idSample}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(mergedPileup) from pairBamCalculateContamination\n\n     output:\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(mergedPileup) from pairBamCalculateContamination\n\n     output:\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}"], "list_proc": ["cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/CalculateContamination", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/CalculateContamination", "cgpu/sarek-mirror/cgpu__sarek-mirror/CalculateContamination", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/CalculateContamination", "rmoran7/custom_sarek/rmoran7__custom_sarek/CalculateContamination", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/CalculateContamination", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/CalculateContamination", "rmoran7/dx_sarek/rmoran7__dx_sarek/CalculateContamination", "nf-core/sarek/nf-core__sarek/CalculateContamination", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/CalculateContamination", "cgpu/haplosarek/cgpu__haplosarek/CalculateContamination", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/CalculateContamination", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/CalculateContamination", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/CalculateContamination", "cgpu/pgp-chronek/cgpu__pgp-chronek/CalculateContamination"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-mirror-cache"]}, {"nb_reuse": 1, "tools": ["Unicycler"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess UNICYCLER {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n    label 'process_high_memory'\n    label 'error_retry'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::unicycler=0.4.8' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/unicycler:0.4.8--py38h8162308_3\"\n    } else {\n        container \"quay.io/biocontainers/unicycler:0.4.8--py38h8162308_3\"\n    }\n\n    input:\n    tuple val(meta), file(reads), file(longreads)\n\n    output:\n    tuple val(meta), path('*.scaffolds.fa'), emit: scaffolds\n    tuple val(meta), path('*.assembly.gfa'), emit: gfa\n    tuple val(meta), path('*.log')         , emit: log\n    path  '*.version.txt'                  , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if(params.assembly_type == 'long'){\n        input_reads = \"-l $longreads\"\n    } else if (params.assembly_type == 'short'){\n        input_reads = \"-1 ${reads[0]} -2 ${reads[1]}\"\n    } else if (params.assembly_type == 'hybrid'){\n        input_reads = \"-1 ${reads[0]} -2 ${reads[1]} -l $longreads\"\n    }\n    \"\"\"\n    unicycler \\\\\n        --threads $task.cpus \\\\\n        $options.args \\\\\n        $input_reads \\\\\n        --out ./\n\n    mv assembly.fasta ${prefix}.scaffolds.fa\n    mv assembly.gfa ${prefix}.assembly.gfa\n    mv unicycler.log ${prefix}.unicycler.log\n\n    echo \\$(unicycler --version 2>&1) | sed 's/^.*Unicycler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/UNICYCLER"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 2, "tools": ["vcfanno"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "raredisease"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process VCFANNO {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::vcfanno=0.3.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/vcfanno:0.3.3--h9ee0642_0':\n        'quay.io/biocontainers/vcfanno:0.3.3--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    tuple val(meta), path(vcf_uncompressed)\n    path toml\n    path resource_dir\n\n    output:\n    tuple val(meta), path(\"*_annotated.vcf\"), emit: vcf\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_vcf = vcf_uncompressed ?: vcf\n    \"\"\"\n    ln -sf $resource_dir/* \\$(pwd)\n\n    vcfanno \\\\\n        -p $task.cpus \\\\\n        $args \\\\\n        $toml \\\\\n        $input_vcf \\\\\n        > ${prefix}_annotated.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        vcfanno: \\$(echo \\$(vcfanno 2>&1 | grep version | cut -f3 -d' ' ))\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}_annotated.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        vcfanno: \\$(echo \\$(vcfanno 2>&1 | grep version | cut -f3 -d' ' ))\n    END_VERSIONS\n    \"\"\"\n}", "process VCFANNO {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::vcfanno=0.3.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/vcfanno:0.3.3--h9ee0642_0':\n        'quay.io/biocontainers/vcfanno:0.3.3--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    tuple val(meta), path(vcf_uncompressed)\n    path toml\n    path resource_dir\n\n    output:\n    tuple val(meta), path(\"*_annotated.vcf\"), emit: vcf\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_vcf = vcf_uncompressed ?: vcf\n    \"\"\"\n    ln -sf $resource_dir/* \\$(pwd)\n\n    vcfanno \\\\\n        -p $task.cpus \\\\\n        $args \\\\\n        $toml \\\\\n        $input_vcf \\\\\n        > ${prefix}_annotated.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        vcfanno: \\$(echo \\$(vcfanno 2>&1 | grep version | cut -f3 -d' ' ))\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}_annotated.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        vcfanno: \\$(echo \\$(vcfanno 2>&1 | grep version | cut -f3 -d' ' ))\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/VCFANNO", "nf-core/modules/nf-core__modules/VCFANNO"], "list_wf_names": ["nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 54, "tools": ["FastQC"], "nb_own": 40, "list_own": ["NailouZhang", "HeshamElAbd", "FriederikeHanssen", "galaxyuvri-ea", "Crone0610", "jordwil", "h3abionet", "cmatKhan", "BlackburnLab", "elowy01", "Flomics", "nriddiford", "bc2zb", "alemenze", "biggstd", "espelpz", "mpozuelo", "FAANG", "lauramble", "veitveit", "LNUc-EEMiS", "LaurenceKuhl", "chelauk", "maxibor", "luissian", "czbiohub", "nf-core", "yassineS", "nibscbioinformatics", "MicrobialGenomics", "paulstretenowich", "bioinformatics-lab", "peterk87", "HuipengL", "suzannejin", "bhargava-morampalli", "marchoeppner", "robinfchan", "jtmccr1", "BarryDigby"], "nb_wf": 51, "list_wf": ["nf-predictorthologs", "nfcorepgdb", "nf-ionampliseq", "test-nf-core-pipeline", "nf-core-refcaller", "nf-core-wombatp", "HPCBio-Refgraph_pipeline", "pgdb", "bamtofastq", "humgen", "nf-core-hlatyping", "testpipeline", "cdnaseqont-nextflow", "demultiplexing", "magicuniqueamplicons", "UvriMetaSeq", "joao-test", "metatdenovo-dsl1", "nf-core-rw", "nf-core-gsfworkflow", "methylseq", "taranispip", "trigenome-analysis-nf", "nf-simulaternaseq", "genebygenebact", "nf-core-lohcator", "nf-core-singlegenometese", "MGI_demux", "rnaseq-vizfada", "nf-ortholog", "nfcore_test", "bisulfite_align_nf", "nf-core-assemblybacterias", "nf-core-testworkflow", "nf-proportionality", "nf-demux", "circ", "nf-core-virome", "nf-core-disambiguate", "smrnaseq", "nf-core-deviptcore", "trinoflow", "nextflow-bowtie2", "nf-core-viralrecon", "GSM-pipeline", "nf-extractcoding", "TTrichiura_Tubulin", "babysarek", "bowtie2-lca", "nf-core-phylofunk", "shrnacount"], "list_contrib": ["HeshamElAbd", "xec-cm", "ypriverol", "phue", "alneberg", "alesssia", "ewels", "FriederikeHanssen", "evanfloden", "maxulysse", "Crone0610", "ggabernet", "matrulda", "FelixKrueger", "jordwil", "colindaven", "lpantano", "chuan-wang", "cmatKhan", "erikrikarddaniel", "lcabus-flomics", "elowy01", "nf-core-bot", "nriddiford", "cjfields", "bleazard", "ErikDanielsson", "sirselim", "AlfredUg", "alemenze", "biggstd", "espelpz", "abhi18av", "pditommaso", "pericsson", "mpozuelo", "sdjebali", "lauramble", "robsyme", "veitveit", "mpozuelo-flomics", "noirot", "LaurenceKuhl", "mjsteinbaugh", "chelauk", "nvk747", "maxibor", "kkowalden", "luissian", "yuantianhpc", "yassineS", "grendon", "bluegenes", "lekhakaranam", "Jani-94", "MicrobialGenomics", "jcurado-flomics", "mashehu", "peterk87", "suzannejin", "Hammarn", "gdevailly", "sven1103", "jemten", "lescai", "marchoeppner", "KevinMenden", "pranathivemuri", "apeltzer", "telatin", "robinfchan", "phoenixAja", "olgabot", "drpatelh", "BarryDigby", "kstawiski"], "nb_contrib": 76, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results_for_multiqc\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", " process fastqc {\n        if (params.custom_container) container \"${params.custom_container}\"\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/fastqc\", mode: 'copy', overwrite: true,\n            saveAs: { filename ->\n                        filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                    }\n\n        input:\n        set val(name), file(reads) from ch_read_files_for_fastqc\n\n        output:\n        file '*_fastqc.{zip,html}' into ch_fastqc_results_for_multiqc\n\n        script:\n        \"\"\"\n        fastqc --quiet --threads $task.cpus $reads\n        \"\"\"\n    }", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC-Pretrim ${id}\"\n    executor               myExecutor\n    cpus                   2\n    queue                  params.myQueue\n    memory                 \"12 GB\"\n    module                 \"FastQC/0.11.8-Java-1.8.0_152\"\n    publishDir             \"${resultsPath}/FASTQC-Pretrim\"\n\n    input:\n    set val(id), file(reads) from merge_fastqc_ch\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc_post {\n    tag \"FASTQC-Posttrim ${id}\"\n    executor               myExecutor\n    clusterOptions         params.clusterAcct     \n    cpus                   4\n    queue                  params.myQueue\n    memory                 \"12 GB\"\n    module                 \"FastQC/0.11.8-Java-1.8.0_152\"\n    publishDir             \"${resultsPath}/FASTQC-Posttrim\"\n\n    input:\n    set val(id), file(pereads), file(sereads) from trim_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_trimmed_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $pereads $sereads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", " process fastqc {\n     tag \"$sample\"\n     label 'process_medium'\n     publishDir \"${cluster_path}/data/04_pfastq/${platform}/${run_id}/${lane}/${user}/fastqc/${sample}\", mode: 'copy',\n     saveAs: { filename ->\n       filename.endsWith(\".zip\") ? \"zips/$filename\" : filename\n     }\n\n     input:\n     set val(row), val(sample), path(reads), val(index), val(run_id), val(lane), val(protocol), val(platform), val(user) from ch_fastqc\n\n     output:\n     set path(\"*_fastqc.{zip,html}\"), val(run_id), val(lane), val(platform), val(user) into fastqc_results\n\n     script:\n     \"\"\"\n     fastqc --quiet --threads $task.cpus $reads\n     \"\"\"\n   }", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", " process fastqc {\n     tag \"$sample\"\n     label 'process_medium'\n     publishDir \"${params.outdir}/${run_id}/${lane}/4-fastqc/${sample}\", mode: 'copy',\n     saveAs: { filename ->\n       filename.endsWith(\".zip\") ? \"zips/$filename\" : filename\n     }\n\n     input:\n     set val(sample), path(reads), val(run_id), val(lane) from ch_fastqc\n\n     output:\n     set path(\"*_fastqc.{zip,html}\"), val(run_id), val(lane) into fastqc_results\n\n     script:\n     \"\"\"\n     fastqc --quiet --threads $task.cpus $reads\n     \"\"\"\n   }", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results_for_multiqc\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "process FASTQC {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n      saveAs: { filename ->\n                    filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n              }\n\n  input:\n  tuple val(sample), path(reads)\n\n  output:\n  path \"*_fastqc.{zip,html}\"\n\n  script:\n  \"\"\"\n  fastqc --quiet --threads $task.cpus $reads\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess computeFastQCInput{\n  tag \"$name\"\n  label 'process_medium'\n\n  input:\n  set val(name), file(bam) from bam_files_fastqc\n\n  output:\n  file \"*.{zip,html}\" into ch_fastqc_reports_mqc_input_bam\n\n  script:\n  \"\"\"\n  fastqc --quiet --threads $task.cpus $bam\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess pairedEndReadsQC{\n    label 'process_medium'\n    tag \"$read1\"\n\n    input:\n    set file(read1), file(read2) from read_qc\n\n    output:\n    file \"*.{zip,html}\" into ch_fastqc_reports_mqc_pe\n\n    when:\n    !params.no_read_QC\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $read1 $read2\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    label 'process_low'\n    tag \"$reads\"\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    file reads from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", " process fastqc {\n      tag \"$name\"\n      label 'process_medium'\n      publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n          saveAs: { filename ->\n                        filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                  }\n\n      input:\n      set val(name), file(reads) from ch_read_files_fastqc\n\n      output:\n      file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n      script:\n      \"\"\"\n      fastqc --quiet --threads $task.cpus $reads\n      \"\"\"\n  }"], "list_proc": ["czbiohub/nf-simulaternaseq/czbiohub__nf-simulaternaseq/fastqc", "Flomics/joao-test/Flomics__joao-test/fastqc", "HeshamElAbd/nf-core-deviptcore/HeshamElAbd__nf-core-deviptcore/fastqc", "elowy01/nf-core-testworkflow/elowy01__nf-core-testworkflow/fastqc", "luissian/nf-core-assemblybacterias/luissian__nf-core-assemblybacterias/fastqc", "chelauk/nf-core-hlatyping/chelauk__nf-core-hlatyping/fastqc", "nf-core/methylseq/nf-core__methylseq/fastqc", "suzannejin/nf-proportionality/suzannejin__nf-proportionality/fastqc", "NailouZhang/nf-core-virome/NailouZhang__nf-core-virome/fastqc", "marchoeppner/trinoflow/marchoeppner__trinoflow/fastqc", "bc2zb/nf-core-disambiguate/bc2zb__nf-core-disambiguate/fastqc", "maxibor/test-nf-core-pipeline/maxibor__test-nf-core-pipeline/fastqc", "HuipengL/nfcore_test/HuipengL__nfcore_test/fastqc", "veitveit/nf-core-wombatp/veitveit__nf-core-wombatp/fastqc", "alemenze/magicuniqueamplicons/alemenze__magicuniqueamplicons/fastqc", "nf-core/testpipeline/nf-core__testpipeline/fastqc", "robinfchan/bisulfite_align_nf/robinfchan__bisulfite_align_nf/fastqc", "yassineS/nf-demux/yassineS__nf-demux/fastqc", "h3abionet/HPCBio-Refgraph_pipeline/h3abionet__HPCBio-Refgraph_pipeline/fastqc", "MicrobialGenomics/TTrichiura_Tubulin/MicrobialGenomics__TTrichiura_Tubulin/fastqc", "h3abionet/HPCBio-Refgraph_pipeline/h3abionet__HPCBio-Refgraph_pipeline/fastqc_post", "jordwil/nextflow-bowtie2/jordwil__nextflow-bowtie2/fastqc", "mpozuelo/MGI_demux/mpozuelo__MGI_demux/fastqc", "bhargava-morampalli/cdnaseqont-nextflow/bhargava-morampalli__cdnaseqont-nextflow/fastqc", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/fastqc", "Crone0610/nf-core-refcaller/Crone0610__nf-core-refcaller/fastqc", "mpozuelo/demultiplexing/mpozuelo__demultiplexing/fastqc", "LNUc-EEMiS/metatdenovo-dsl1/LNUc-EEMiS__metatdenovo-dsl1/fastqc", "jtmccr1/nf-core-phylofunk/jtmccr1__nf-core-phylofunk/fastqc", "LaurenceKuhl/shrnacount/LaurenceKuhl__shrnacount/fastqc", "FAANG/GSM-pipeline/FAANG__GSM-pipeline/fastqc", "cmatKhan/nf-core-rw/cmatKhan__nf-core-rw/fastqc", "peterk87/nf-ionampliseq/peterk87__nf-ionampliseq/FASTQC", "FriederikeHanssen/babysarek/FriederikeHanssen__babysarek/fastqc", "BarryDigby/circ/BarryDigby__circ/fastqc", "biggstd/nf-core-gsfworkflow/biggstd__nf-core-gsfworkflow/fastqc", "galaxyuvri-ea/UvriMetaSeq/galaxyuvri-ea__UvriMetaSeq/fastqc", "FriederikeHanssen/bamtofastq/FriederikeHanssen__bamtofastq/computeFastQCInput", "paulstretenowich/nf-core-viralrecon/paulstretenowich__nf-core-viralrecon/fastqc", "FriederikeHanssen/bamtofastq/FriederikeHanssen__bamtofastq/pairedEndReadsQC", "espelpz/genebygenebact/espelpz__genebygenebact/fastqc", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/fastqc", "FriederikeHanssen/shrnacount/FriederikeHanssen__shrnacount/fastqc", "espelpz/taranispip/espelpz__taranispip/fastqc", "nriddiford/nf-core-lohcator/nriddiford__nf-core-lohcator/fastqc", "bioinformatics-lab/nf-core-singlegenometese/bioinformatics-lab__nf-core-singlegenometese/fastqc", "bioinformatics-lab/trigenome-analysis-nf/bioinformatics-lab__trigenome-analysis-nf/fastqc", "czbiohub/nf-extractcoding/czbiohub__nf-extractcoding/fastqc", "BlackburnLab/nfcorepgdb/BlackburnLab__nfcorepgdb/fastqc", "maxibor/bowtie2-lca/maxibor__bowtie2-lca/fastqc", "BlackburnLab/pgdb/BlackburnLab__pgdb/fastqc", "czbiohub/nf-ortholog/czbiohub__nf-ortholog/fastqc", "nf-core/smrnaseq/nf-core__smrnaseq/fastqc", "czbiohub/nf-predictorthologs/czbiohub__nf-predictorthologs/fastqc"], "list_wf_names": ["MicrobialGenomics/TTrichiura_Tubulin", "lauramble/rnaseq-vizfada", "mpozuelo/demultiplexing", "czbiohub/nf-simulaternaseq", "BlackburnLab/nfcorepgdb", "bc2zb/nf-core-disambiguate", "chelauk/nf-core-hlatyping", "HeshamElAbd/nf-core-deviptcore", "Flomics/joao-test", "elowy01/nf-core-testworkflow", "maxibor/bowtie2-lca", "bioinformatics-lab/nf-core-singlegenometese", "peterk87/nf-ionampliseq", "HuipengL/nfcore_test", "LaurenceKuhl/shrnacount", "nf-core/methylseq", "bioinformatics-lab/trigenome-analysis-nf", "LNUc-EEMiS/metatdenovo-dsl1", "NailouZhang/nf-core-virome", "espelpz/taranispip", "jordwil/nextflow-bowtie2", "nriddiford/nf-core-lohcator", "nf-core/smrnaseq", "marchoeppner/trinoflow", "Crone0610/nf-core-refcaller", "yassineS/nf-demux", "suzannejin/nf-proportionality", "espelpz/genebygenebact", "czbiohub/nf-extractcoding", "BlackburnLab/pgdb", "paulstretenowich/nf-core-viralrecon", "maxibor/test-nf-core-pipeline", "FriederikeHanssen/bamtofastq", "bhargava-morampalli/cdnaseqont-nextflow", "cmatKhan/nf-core-rw", "FAANG/GSM-pipeline", "FriederikeHanssen/babysarek", "czbiohub/nf-ortholog", "veitveit/nf-core-wombatp", "luissian/nf-core-assemblybacterias", "biggstd/nf-core-gsfworkflow", "alemenze/magicuniqueamplicons", "mpozuelo/MGI_demux", "galaxyuvri-ea/UvriMetaSeq", "jtmccr1/nf-core-phylofunk", "robinfchan/bisulfite_align_nf", "nf-core/testpipeline", "h3abionet/HPCBio-Refgraph_pipeline", "czbiohub/nf-predictorthologs", "FriederikeHanssen/shrnacount", "nibscbioinformatics/humgen", "BarryDigby/circ"]}, {"nb_reuse": 2, "tools": ["MSI", "MSIsensor"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MSISENSOR_MSI {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::msisensor=0.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/msisensor:0.5--hb3646a4_2' :\n        'quay.io/biocontainers/msisensor:0.5--hb3646a4_2' }\"\n\n    input:\n    tuple val(meta), path(normal_bam), path(normal_bai), path(tumor_bam), path(tumor_bai), val(metascan), path(homopolymers)\n\n    output:\n    tuple val(meta), path(\"${prefix}\")         , emit: output\n    tuple val(meta), path(\"${prefix}_dis\")     , emit: output_dis\n    tuple val(meta), path(\"${prefix}_germline\"), emit: output_germline\n    tuple val(meta), path(\"${prefix}_somatic\") , emit: output_somatic\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    msisensor \\\\\n        msi \\\\\n        -d $homopolymers \\\\\n        -n $normal_bam \\\\\n        -t $tumor_bam \\\\\n        -o $prefix \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        msisensor: \\$(msisensor 2>&1 | sed -nE 's/Version:\\\\sv([0-9]\\\\.[0-9])/\\\\1/ p')\n    END_VERSIONS\n    \"\"\"\n}", "process MSISENSOR_SCAN {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::msisensor=0.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/msisensor:0.5--hb3646a4_2' :\n        'quay.io/biocontainers/msisensor:0.5--hb3646a4_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tab\"), emit: txt\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    msisensor \\\\\n        scan \\\\\n        -d $fasta \\\\\n        -o ${prefix}.msisensor_scan.tab \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        msisensor: \\$(msisensor 2>&1 | sed -nE 's/Version:\\\\sv([0-9]\\\\.[0-9])/\\\\1/ p')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MSISENSOR_MSI", "nf-core/modules/nf-core__modules/MSISENSOR_SCAN"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 3, "tools": ["BCFtools"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["modules", "test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process BCFTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools stats $args $vcf > ${prefix}.bcftools_stats.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools stats $args $vcf > ${prefix}.bcftools_stats.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools stats $args $vcf > ${prefix}.bcftools_stats.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/BCFTOOLS_STATS", "nf-core/modules/nf-core__modules/BCFTOOLS_STATS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BCFTOOLS_STATS"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_TREE {\n    label 'process_medium'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(repseq)\n\n    output:\n    path(\"rooted-tree.qza\"), emit: qza\n    path(\"tree.nwk\")       , emit: nwk\n    path \"versions.yml\"    , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime alignment mafft \\\n        --i-sequences ${repseq} \\\n        --o-alignment aligned-rep-seqs.qza \\\n        --p-n-threads ${task.cpus}\n    qiime alignment mask \\\n        --i-alignment aligned-rep-seqs.qza \\\n        --o-masked-alignment masked-aligned-rep-seqs.qza\n    qiime phylogeny fasttree \\\n        --i-alignment masked-aligned-rep-seqs.qza \\\n        --p-n-threads ${task.cpus} \\\n        --o-tree unrooted-tree.qza\n    qiime phylogeny midpoint-root \\\n        --i-tree unrooted-tree.qza \\\n        --o-rooted-tree rooted-tree.qza\n    qiime tools export --input-path rooted-tree.qza  \\\n        --output-path phylogenetic_tree\n    cp phylogenetic_tree/tree.nwk .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_TREE {\n    label 'process_medium'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(repseq)\n\n    output:\n    path(\"rooted-tree.qza\"), emit: qza\n    path(\"tree.nwk\")       , emit: nwk\n    path \"versions.yml\"    , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime alignment mafft \\\n        --i-sequences ${repseq} \\\n        --o-alignment aligned-rep-seqs.qza \\\n        --p-n-threads ${task.cpus}\n    qiime alignment mask \\\n        --i-alignment aligned-rep-seqs.qza \\\n        --o-masked-alignment masked-aligned-rep-seqs.qza\n    qiime phylogeny fasttree \\\n        --i-alignment masked-aligned-rep-seqs.qza \\\n        --p-n-threads ${task.cpus} \\\n        --o-tree unrooted-tree.qza\n    qiime phylogeny midpoint-root \\\n        --i-tree unrooted-tree.qza \\\n        --o-rooted-tree rooted-tree.qza\n    qiime tools export --input-path rooted-tree.qza  \\\n        --output-path phylogenetic_tree\n    cp phylogenetic_tree/tree.nwk .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_TREE", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_TREE"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 6, "tools": ["AIVAR", "Consensus", "SAMtools", "mpileup"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["test_nfcore_workflow_chain", "modules", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process IVAR_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.fa\")      , emit: fasta\n    tuple val(meta), path(\"*.qual.txt\"), emit: qual\n    tuple val(meta), path(\"*.mpileup\") , optional:true, emit: mpileup\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        --reference $fasta \\\\\n        $args2 \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            consensus \\\\\n            $args \\\\\n            -p $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.fa\")      , emit: fasta\n    tuple val(meta), path(\"*.qual.txt\"), emit: qual\n    tuple val(meta), path(\"*.mpileup\") , optional:true, emit: mpileup\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        --reference $fasta \\\\\n        $args2 \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            consensus \\\\\n            $args \\\\\n            -p $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_VARIANTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n    path  fai\n    path  gff\n    val   save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.tsv\")    , emit: tsv\n    tuple val(meta), path(\"*.mpileup\"), optional:true, emit: mpileup\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def features = gff ? \"-g $gff\" : \"\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        $args2 \\\\\n        --reference $fasta \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            variants \\\\\n            $args \\\\\n            $features \\\\\n            -r $fasta \\\\\n            -p $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_VARIANTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n    path  fai\n    path  gff\n    val   save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.tsv\")    , emit: tsv\n    tuple val(meta), path(\"*.mpileup\"), optional:true, emit: mpileup\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def features = gff ? \"-g $gff\" : \"\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        $args2 \\\\\n        --reference $fasta \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            variants \\\\\n            $args \\\\\n            $features \\\\\n            -r $fasta \\\\\n            -p $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.fa\")      , emit: fasta\n    tuple val(meta), path(\"*.qual.txt\"), emit: qual\n    tuple val(meta), path(\"*.mpileup\") , optional:true, emit: mpileup\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        --reference $fasta \\\\\n        $args2 \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            consensus \\\\\n            $args \\\\\n            -p $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_VARIANTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n    path  fai\n    path  gff\n    val   save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.tsv\")    , emit: tsv\n    tuple val(meta), path(\"*.mpileup\"), optional:true, emit: mpileup\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def features = gff ? \"-g $gff\" : \"\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        $args2 \\\\\n        --reference $fasta \\\\\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            variants \\\\\n            $args \\\\\n            $features \\\\\n            -r $fasta \\\\\n            -p $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/IVAR_CONSENSUS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/IVAR_CONSENSUS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/IVAR_VARIANTS", "nf-core/modules/nf-core__modules/IVAR_VARIANTS", "nf-core/modules/nf-core__modules/IVAR_CONSENSUS", "nf-core/viralrecon/nf-core__viralrecon/IVAR_VARIANTS"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess kraken {\n  tag \"$prefix\"\n  label 'mc_huge'\n  publishDir \"${params.outdir}/metagenomic_classification/kraken\", mode: params.publish_dir_mode\n\n  when:\n  params.run_metagenomic_screening && params.run_bam_filtering && params.bam_unmapped_type == 'fastq' && params.metagenomic_tool == 'kraken'\n\n  input:\n  path(fastq) from ch_input_for_metagenomic_kraken.map { it[7] }\n  path(krakendb) from ch_krakendb\n\n  output:\n  file \"*.kraken.out\" into ch_kraken_out\n  tuple prefix, path(\"*.kraken2_report\") into ch_kraken_report, ch_kraken_for_multiqc\n\n  script:\n  prefix = fastq.baseName\n  out = prefix+\".kraken.out\"\n  kreport = prefix+\".kraken2_report\"\n  kreport_old = prefix+\".kreport\"\n\n  \"\"\"\n  kraken2 --db ${krakendb} --threads ${task.cpus} --output $out --report-minimizer-data --report $kreport $fastq\n  cut -f1-3,6-8 $kreport > $kreport_old\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/kraken"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["FastQC", "BEDTools", "SAMtools", "preseq", "IGVtools", "HISAT2"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess get_software_versions {\n    validExitStatus 0,1,127\n    publishDir \"${params.outdir}/software_versions/\", mode: 'copy', pattern: '*.txt'\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    bbversion.sh --version > v_bbduk.txt\n    hisat2 --version > v_hisat2.txt\n    samtools --version > v_samtools.txt\n    fastq-dump --version > v_fastq-dump.txt\n    preseq > v_preseq.txt\n    seqkit version > v_seqkit.txt\n    bedtools --version > v_bedtools.txt\n    export LC_ALL=C\n    igvtools version > v_igv-tools.txt\n\n    # Can't call this before running MultiQC or it breaks it\n    read_distribution.py --version > v_rseqc.txt\n\n    for X in `ls *.txt`; do\n        cat \\$X >> all_versions.txt;\n    done\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/nascent/nf-core__nascent/get_software_versions"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 7, "tools": ["Salmon"], "nb_own": 5, "list_own": ["raygozag", "vincenthhu", "nf-core", "mahesh-panchal", "harleenduggal"], "nb_wf": 6, "list_wf": ["RNASEQ", "test_nfcore_workflow_chain", "modules", "nfcore-rnaseq", "nf-core-westest", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "jordwil", "FelixKrueger", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "aanil", "silviamorins", "d4straub", "SPPearce", "Midnighter", "rannick", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "mvanins", "nebfield", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "vincenthhu", "lassefolkersen", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 147, "codes": ["process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["harleenduggal/RNASEQ/harleenduggal__RNASEQ/SALMON_QUANT", "nf-core/rnaseq/nf-core__rnaseq/SALMON_QUANT", "raygozag/rnaseq/raygozag__rnaseq/SALMON_QUANT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SALMON_QUANT", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/SALMON_QUANT", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SALMON_QUANT", "nf-core/modules/nf-core__modules/SALMON_QUANT"], "list_wf_names": ["raygozag/rnaseq", "vincenthhu/nf-core-westest", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["BWA", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess bwamem {\n    tag \"$name\"\n\n    input:\n    set val(name), file(reads) from trimmed_reads\n    file(bwa_index) from bwa_index\n\n    output:\n    set val(name), file(\"${name}_bwa.bam\") into samples_sorted_bam\n    file '.command.log' into bwa_stdout\n\n\n    script:\n    def avail_mem = task.memory ? \"-m ${task.memory.toMega().intdiv(task.cpus)}M\" : ''\n    rg=\"\\'@RG\\\\tID:${params.run_id}\\\\tSM:${params.run_id}\\\\tPL:illumina\\'\"\n\n    \"\"\"\n    bwa mem \\\\\n    -R $rg \\\\\n    -t ${task.cpus} \\\\\n    $params.gfasta \\\\\n    $reads \\\\\n    | samtools ${avail_mem} sort -O bam - > $outfile ${name}_bwa.bam\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/bwamem"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 7, "codes": ["\nprocess build_kallisto_index {\n    tag \"$fasta\"\n    label 'mid_memory'\n    publishDir path: { params.save_reference ? \"${params.outdir}/reference_genome/kallisto_index\" : params.outdir },\n                saveAs: { params.save_reference ? it : null }, mode: 'copy'\n    input:\n    file fasta from transcriptome_fasta_kallisto.mix(transcriptome_fasta_kallisto_extracted)\n\n    output:\n    file \"${name}.idx\" into kallisto_index\n\n    when: params.aligner == 'kallisto' && !params.kallisto_index\n\n    script:\n    if(\"${fasta}\".endsWith('.gz')){\n      name = \"${fasta.baseName}\"\n      unzip = \"gunzip -f ${fasta}\"\n    } else {\n      unzip = \"\"\n      name = \"${fasta}\"\n    }\n    \"\"\"\n    $unzip\n    kallisto index -i ${name}.idx -k 31 $name\n    \"\"\"\n}"], "list_proc": ["nf-core/scrnaseq/nf-core__scrnaseq/build_kallisto_index"], "list_wf_names": ["nf-core/scrnaseq"]}, {"nb_reuse": 9, "tools": ["BWA", "SAMtools"], "nb_own": 6, "list_own": ["vincenthhu", "nf-core", "CDCgov", "csf-ngs", "cidgoh", "jianhong"], "nb_wf": 7, "list_wf": ["mycosnp-nf", "cidgoh_qc", "nf-core-hicar", "modules", "nf-core-westest", "controldna", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "anwarMZ", "idot", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "duanjunhyq", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "leebrian", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 114, "codes": ["process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:8110a70be2bfe7f75a2ea7f2a89cda4cc7732095-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:8110a70be2bfe7f75a2ea7f2a89cda4cc7732095-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_SAMPE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:8110a70be2bfe7f75a2ea7f2a89cda4cc7732095-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:8110a70be2bfe7f75a2ea7f2a89cda4cc7732095-0' }\"\n\n    input:\n    tuple val(meta), path(reads), path(sai)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-r ${meta.read_group}\" : \"\"\n\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa sampe \\\\\n        $args \\\\\n        $read_group \\\\\n        \\$INDEX \\\\\n        $sai \\\\\n        $reads | samtools sort -@ ${task.cpus - 1} -O bam - > ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_SAMSE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:8110a70be2bfe7f75a2ea7f2a89cda4cc7732095-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:8110a70be2bfe7f75a2ea7f2a89cda4cc7732095-0' }\"\n\n    input:\n    tuple val(meta), path(reads), path(sai)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-r ${meta.read_group}\" : \"\"\n\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa samse \\\\\n        $args \\\\\n        $read_group \\\\\n        \\$INDEX \\\\\n        $sai \\\\\n        $reads | samtools sort -@ ${task.cpus - 1} -O bam - > ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:c56a3aabc8d64e52d5b9da1e8ecec2031668596d-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:c56a3aabc8d64e52d5b9da1e8ecec2031668596d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BWA_MEM", "nf-core/ssds/nf-core__ssds/BWA_MEM", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/BWA_MEM", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BWA_MEM", "nf-core/modules/nf-core__modules/BWA_SAMPE", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/BWA_MEM", "csf-ngs/controldna/csf-ngs__controldna/BWA_MEM", "nf-core/modules/nf-core__modules/BWA_SAMSE", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/BWA_MEM"], "list_wf_names": ["jianhong/nf-core-hicar", "csf-ngs/controldna", "cidgoh/cidgoh_qc", "vincenthhu/nf-core-westest", "nf-core/ssds", "nf-core/modules", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 4, "tools": ["Flipper", "restrict"], "nb_own": 2, "list_own": ["nf-core", "jianhong"], "nb_wf": 2, "list_wf": ["modules", "nf-core-hicar"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process PAIRTOOLS_FLIP {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(sam)\n    path chromsizes\n\n    output:\n    tuple val(meta), path(\"*.flip.gz\"), emit: flip\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        flip \\\\\n        -c $chromsizes \\\\\n        $args \\\\\n        -o ${prefix}.flip.gz \\\\\n        $sam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}", "process PAIRTOOLS_RESTRICT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(pairs)\n    path frag\n\n    output:\n    tuple val(meta), path(\"*.pairs.gz\"), emit: restrict\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        restrict \\\\\n        -f $frag \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $pairs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}", "process PAIRTOOLS_RESTRICT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(pairs)\n    path frag\n\n    output:\n    tuple val(meta), path(\"*.pairs.gz\"), emit: restrict\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        restrict \\\\\n        -f $frag \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $pairs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}", "process PAIRTOOLS_FLIP {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(sam)\n    path chromsizes\n\n    output:\n    tuple val(meta), path(\"*.flip.gz\"), emit: flip\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        flip \\\\\n        -c $chromsizes \\\\\n        $args \\\\\n        -o ${prefix}.flip.gz \\\\\n        $sam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PAIRTOOLS_FLIP", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/PAIRTOOLS_RESTRICT", "nf-core/modules/nf-core__modules/PAIRTOOLS_RESTRICT", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/PAIRTOOLS_FLIP"], "list_wf_names": ["jianhong/nf-core-hicar", "nf-core/modules"]}, {"nb_reuse": 11, "tools": ["SAMtools"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 10, "list_wf": ["saw.sarek", "sarek_ubec", "PGP-UK-sarek", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bamMergeBamRecalNoInt\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bamRecalNoInt\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bamRecalQCnoInt\n        set idPatient, idSample into bamRecalTSVnoInt\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bamMergeBamRecalNoInt\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bamRecalNoInt\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bamRecalQCnoInt\n        set idPatient, idSample into bamRecalTSVnoInt\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexRecalibratedSampleReadGoup {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.bam\")\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\")\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.bam\")\n        tuple val(idPatient), val(idSample)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/IndexBamRecal", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/IndexBamRecal", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/IndexBamRecal", "nf-core/sarek/nf-core__sarek/IndexBamRecal", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/IndexBamRecal", "rmoran7/custom_sarek/rmoran7__custom_sarek/IndexBamRecal", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/IndexBamRecal", "rmoran7/dx_sarek/rmoran7__dx_sarek/IndexBamRecal", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/IndexBamRecal", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/IndexRecalibratedSampleReadGoup", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/IndexBamRecal"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 4, "tools": ["Prokka"], "nb_own": 4, "list_own": ["gongyh", "avantonder", "nf-core", "peterk87"], "nb_wf": 4, "list_wf": ["assembleBAC", "nf-iav-illumina", "nf-core-scp", "bacass"], "list_contrib": ["gongyh", "rivera10", "bewt85", "nf-core-bot", "ewels", "avantonder", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh", "peterk87"], "nb_contrib": 14, "codes": ["\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    prefix           = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def strain       = options.strain ?: \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        --strain $strain \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/PROKKA", "peterk87/nf-iav-illumina/peterk87__nf-iav-illumina/PROKKA", "gongyh/nf-core-scp/gongyh__nf-core-scp/PROKKA", "avantonder/assembleBAC/avantonder__assembleBAC/PROKKA"], "list_wf_names": ["nf-core/bacass", "gongyh/nf-core-scp", "peterk87/nf-iav-illumina", "avantonder/assembleBAC"]}, {"nb_reuse": 4, "tools": ["BCFtools"], "nb_own": 2, "list_own": ["nibscbioinformatics", "nf-core"], "nb_wf": 2, "list_wf": ["bactmap", "nf-core-viralevo"], "list_contrib": ["alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "kaurravneet4123", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess BCFTOOLS_FILTER {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"*.version.txt\"        , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bcftools filter \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $options.args \\\\\n        $vcf\n\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_INDEX {\n    tag \"$vcf\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    path vcf \n\n    output:\n    path \"*.csi\", emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def filename = \"$vcf\".tokenize('_')[0]\n    def caller   = (\"$vcf\".contains(\"_ivar\")) ? \"ivar\" :  (\"$vcf\".contains(\"lofreq\")) ? \"lofreq\" : ''\n   \"\"\"\n    bcftools index $vcf\n\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_VIEW {\n    tag \"$vcf\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    path vcf\n\n    output:\n    path \"*.vcf.gz\", emit: vcf\n    path \"*.version.txt\"     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def filename = \"$vcf\".tokenize('_')[0]\n    def caller   = (\"$vcf\".contains(\"_ivar\")) ? \"ivar\" :  (\"$vcf\".contains(\"lofreq\")) ? \"lofreq\" : ''\n    \"\"\"\n    bcftools view \\\\\n        $vcf \\\\\n        $options.args \\\\\n        -o ${filename}_${caller}.vcf.gz\n\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_NORM {\n    tag \"$vcf\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    path vcf\n\n    output:\n    path(\"*.vcf\") , emit: vcf\n    path  \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def filename = \"$vcf\".tokenize('_')[0]\n    def caller   = (\"$vcf\".contains(\"_ivar\")) ? \"ivar\" :  (\"$vcf\".contains(\"lofreq\")) ? \"lofreq\" : ''\n    \"\"\"\n    bcftools norm \\\\\n        $options.args \\\\\n        $vcf \\\\\n        > ${filename}_${caller}.vcf\n\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bactmap/nf-core__bactmap/BCFTOOLS_FILTER", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/BCFTOOLS_INDEX", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/BCFTOOLS_VIEW", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/BCFTOOLS_NORM"], "list_wf_names": ["nf-core/bactmap", "nibscbioinformatics/nf-core-viralevo"]}, {"nb_reuse": 1, "tools": ["Skewer"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess SKEWER {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"skewer=0.2.2-3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/skewer:0.2.2--hc9558a2_3\"\n    } else {\n        container \"quay.io/biocontainers/skewer:0.2.2--hc9558a2_3\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*_trm-cmb.R{1,2}.fastq.gz\"), emit: reads\n    path(\"*.log\")                                     , emit: log\n    path \"*.version.txt\"                              , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    \"\"\"\n    # loop over readunits in pairs per sample\n    pairno=0\n    echo \"${reads[0]} ${reads[1]}\" | xargs -n2 | while read fq1 fq2; do\n        skewer $options.args -t ${task.cpus} \\$fq1 \\$fq2;\n    done\n\n    # gzip, because skewer's -z returns an error\n    gzip *.fastq\n\n    cat \\$(ls *trimmed-pair1.fastq.gz | sort) >> ${meta.id}_trm-cmb.R1.fastq.gz\n    cat \\$(ls *trimmed-pair2.fastq.gz | sort) >> ${meta.id}_trm-cmb.R2.fastq.gz\n\n    echo \\$(skewer --version 2>&1) | sed 's/^.*skewer version: //; s/ .*//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/SKEWER"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 6, "tools": ["Picard"], "nb_own": 3, "list_own": ["csf-ngs", "vincenthhu", "nf-core"], "nb_wf": 4, "list_wf": ["controldna", "modules", "ssds", "nf-core-westest"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "idot", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process PICARD_SORTSAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    val sort_order\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        SortSam \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SORT_ORDER $sort_order\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_SORTSAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::picard=2.25.7' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.25.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.25.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    val sort_order\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        SortSam \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SORT_ORDER $sort_order\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_SORTBAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    val sort_order\n    val outname\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}_${outname}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        SortSam \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SORT_ORDER $sort_order\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_SORTSAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    val sort_order\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        SortSam \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SORT_ORDER $sort_order\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_SORTSAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    val sort_order\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        SortSam \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SORT_ORDER $sort_order\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_SORTVCF {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path reference\n    path sequence_dict\n\n    output:\n    tuple val(meta), path(\"*_sorted.vcf.gz\"), emit: vcf\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def seq_dict = sequence_dict ? \"--SEQUENCE_DICTIONARY $sequence_dict\" : \"\"\n    def reference = reference ? \"--REFERENCE_SEQUENCE $reference\" : \"\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortVcf] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n\n    \"\"\"\n    picard \\\\\n        SortVcf \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $vcf \\\\\n        $args \\\\\n        $seq_dict \\\\\n        $reference \\\\\n        --OUTPUT ${prefix}_sorted.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortVcf --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}_sorted.vcf.gz\n    touch ${prefix}.bam.bai\n    touch ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortVcf --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_SORTSAM", "nf-core/ssds/nf-core__ssds/PICARD_SORTSAM", "csf-ngs/controldna/csf-ngs__controldna/PICARD_SORTBAM", "csf-ngs/controldna/csf-ngs__controldna/PICARD_SORTSAM", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/PICARD_SORTSAM", "nf-core/modules/nf-core__modules/PICARD_SORTVCF"], "list_wf_names": ["nf-core/ssds", "csf-ngs/controldna", "nf-core/modules", "vincenthhu/nf-core-westest"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process GATK4_MERGEBAMALIGNMENT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(aligned), path(unmapped)\n    path  fasta\n    path  dict\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK MergeBamAlignment] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" MergeBamAlignment \\\\\n        --UNMAPPED_BAM $unmapped \\\\\n        --ALIGNED_BAM $aligned \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --REFERENCE_SEQUENCE $fasta \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_MERGEBAMALIGNMENT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["JSpecies", "Taxa", "PCFamily"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process DADA2_ADDSPECIES {\n    tag \"${taxtable},${database}\"\n    label 'process_high'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    path(taxtable)\n    path(database)\n    val(outfile)\n\n    output:\n    path(outfile)       , emit: tsv\n    path \"versions.yml\" , emit: versions\n    path \"*.args.txt\"   , emit: args\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    suppressPackageStartupMessages(library(dada2))\n    set.seed(100) # Initialize random number generator for reproducibility\n\n    taxtable <- readRDS(\\\"$taxtable\\\")\n\n    tx <- addSpecies(taxtable, \\\"$database\\\", $args, verbose=TRUE)\n\n    # Create a table with specified column order\n    tmp <- data.frame(row.names(tx)) # To separate ASV_ID from sequence\n    taxa <- data.frame(\n        ASV_ID = tx[,\"ASV_ID\"],\n        Domain = tx[,\"Domain\"],\n        Kingdom = tx[,\"Kingdom\"],\n        Phylum = tx[,\"Phylum\"],\n        Class = tx[,\"Class\"],\n        Order = tx[,\"Order\"],\n        Family = tx[,\"Family\"],\n        Genus = tx[,\"Genus\"],\n        Species = tx[,\"Species\"],\n        confidence = tx[,\"confidence\"],\n        sequence = tmp[,],\n        row.names=row.names(tmp)\n    )\n\n    write.table(taxa, file = \\\"$outfile\\\", sep = \"\\\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE, na = '')\n\n    write.table('addSpecies\\t$args', file = \"addSpecies.args.txt\", row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n    \"\"\"\n}", "process DADA2_ADDSPECIES {\n    tag \"${taxtable},${database}\"\n    label 'process_high'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    path(taxtable)\n    path(database)\n    val(outfile)\n\n    output:\n    path(outfile)       , emit: tsv\n    path \"versions.yml\" , emit: versions\n    path \"*.args.txt\"   , emit: args\n\n    script:\n    def args = task.ext.args ?: ''\n    def seed = task.ext.seed ?: '100'\n    \"\"\"\n    #!/usr/bin/env Rscript\n    suppressPackageStartupMessages(library(dada2))\n    set.seed($seed) # Initialize random number generator for reproducibility\n\n    taxtable <- readRDS(\\\"$taxtable\\\")\n\n    tx <- addSpecies(taxtable, \\\"$database\\\", $args, verbose=TRUE)\n\n    # Create a table with specified column order\n    tmp <- data.frame(row.names(tx)) # To separate ASV_ID from sequence\n    taxa <- data.frame(\n        ASV_ID = tx[,\"ASV_ID\"],\n        Domain = tx[,\"Domain\"],\n        Kingdom = tx[,\"Kingdom\"],\n        Phylum = tx[,\"Phylum\"],\n        Class = tx[,\"Class\"],\n        Order = tx[,\"Order\"],\n        Family = tx[,\"Family\"],\n        Genus = tx[,\"Genus\"],\n        Species = tx[,\"Species\"],\n        confidence = tx[,\"confidence\"],\n        sequence = tmp[,],\n        row.names=row.names(tmp)\n    )\n\n    write.table(taxa, file = \\\"$outfile\\\", sep = \"\\\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE, na = '')\n\n    write.table('addSpecies\\t$args\\nseed\\t$seed', file = \"addSpecies.args.txt\", row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/DADA2_ADDSPECIES", "nf-core/ampliseq/nf-core__ampliseq/DADA2_ADDSPECIES"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["snpEff"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process SNPEFF_ANN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path  db\n    path  config\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.vcf\")      , emit: vcf\n    tuple val(meta), path(\"*.csv\")      , emit: csv\n    tuple val(meta), path(\"*.genes.txt\"), emit: txt\n    tuple val(meta), path(\"*.html\")     , emit: html\n    path \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        ${fasta.baseName} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        $args \\\\\n        $vcf \\\\\n        -csvStats ${prefix}.snpeff.csv \\\\\n        > ${prefix}.snpeff.vcf\n    mv snpEff_summary.html ${prefix}.snpeff.summary.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}", "process SNPEFF_ANN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path  db\n    path  config\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.vcf\")      , emit: vcf\n    tuple val(meta), path(\"*.csv\")      , emit: csv\n    tuple val(meta), path(\"*.genes.txt\"), emit: txt\n    tuple val(meta), path(\"*.html\")     , emit: html\n    path \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        ${fasta.baseName} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        $args \\\\\n        $vcf \\\\\n        -csvStats ${prefix}.snpeff.csv \\\\\n        > ${prefix}.snpeff.vcf\n    mv snpEff_summary.html ${prefix}.snpeff.summary.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/SNPEFF_ANN", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SNPEFF_ANN"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_INSEQ {\n    tag \"${seq}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(seq)\n\n    output:\n    path(\"rep-seqs.qza\"), emit: qza\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    qiime tools import \\\n        --input-path \"$seq\" \\\n        --type 'FeatureData[Sequence]' \\\n        --output-path rep-seqs.qza\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_INSEQ {\n    tag \"${seq}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(seq)\n\n    output:\n    path(\"rep-seqs.qza\"), emit: qza\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    qiime tools import \\\n        --input-path \"$seq\" \\\n        --type 'FeatureData[Sequence]' \\\n        --output-path rep-seqs.qza\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_INSEQ", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_INSEQ"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["Unicycler"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["test_nfcore_workflow_chain", "modules", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process UNICYCLER {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::unicycler=0.4.8' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/unicycler:0.4.8--py38h8162308_3' :\n        'quay.io/biocontainers/unicycler:0.4.8--py38h8162308_3' }\"\n\n    input:\n    tuple val(meta), path(shortreads), path(longreads)\n\n    output:\n    tuple val(meta), path('*.scaffolds.fa.gz'), emit: scaffolds\n    tuple val(meta), path('*.assembly.gfa.gz'), emit: gfa\n    tuple val(meta), path('*.log')            , emit: log\n    path  \"versions.yml\"                      , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def short_reads = shortreads ? ( meta.single_end ? \"-s $shortreads\" : \"-1 ${shortreads[0]} -2 ${shortreads[1]}\" ) : \"\"\n    def long_reads  = longreads ? \"-l $longreads\" : \"\"\n    \"\"\"\n    unicycler \\\\\n        --threads $task.cpus \\\\\n        $args \\\\\n        $short_reads \\\\\n        $long_reads \\\\\n        --out ./\n\n    mv assembly.fasta ${prefix}.scaffolds.fa\n    gzip -n ${prefix}.scaffolds.fa\n    mv assembly.gfa ${prefix}.assembly.gfa\n    gzip -n ${prefix}.assembly.gfa\n    mv unicycler.log ${prefix}.unicycler.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        unicycler: \\$(echo \\$(unicycler --version 2>&1) | sed 's/^.*Unicycler v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process UNICYCLER {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::unicycler=0.4.8' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/unicycler:0.4.8--py38h8162308_3' :\n        'quay.io/biocontainers/unicycler:0.4.8--py38h8162308_3' }\"\n\n    input:\n    tuple val(meta), path(shortreads), path(longreads)\n\n    output:\n    tuple val(meta), path('*.scaffolds.fa.gz'), emit: scaffolds\n    tuple val(meta), path('*.assembly.gfa.gz'), emit: gfa\n    tuple val(meta), path('*.log')            , emit: log\n    path  \"versions.yml\"                      , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def short_reads = shortreads ? ( meta.single_end ? \"-s $shortreads\" : \"-1 ${shortreads[0]} -2 ${shortreads[1]}\" ) : \"\"\n    def long_reads  = longreads ? \"-l $longreads\" : \"\"\n    \"\"\"\n    unicycler \\\\\n        --threads $task.cpus \\\\\n        $args \\\\\n        $short_reads \\\\\n        $long_reads \\\\\n        --out ./\n\n    mv assembly.fasta ${prefix}.scaffolds.fa\n    gzip -n ${prefix}.scaffolds.fa\n    mv assembly.gfa ${prefix}.assembly.gfa\n    gzip -n ${prefix}.assembly.gfa\n    mv unicycler.log ${prefix}.unicycler.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        unicycler: \\$(echo \\$(unicycler --version 2>&1) | sed 's/^.*Unicycler v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process UNICYCLER {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::unicycler=0.4.8' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/unicycler:0.4.8--py38h8162308_3' :\n        'quay.io/biocontainers/unicycler:0.4.8--py38h8162308_3' }\"\n\n    input:\n    tuple val(meta), path(shortreads), path(longreads)\n\n    output:\n    tuple val(meta), path('*.scaffolds.fa.gz'), emit: scaffolds\n    tuple val(meta), path('*.assembly.gfa.gz'), emit: gfa\n    tuple val(meta), path('*.log')            , emit: log\n    path  \"versions.yml\"                      , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def short_reads = shortreads ? ( meta.single_end ? \"-s $shortreads\" : \"-1 ${shortreads[0]} -2 ${shortreads[1]}\" ) : \"\"\n    def long_reads  = longreads ? \"-l $longreads\" : \"\"\n    \"\"\"\n    unicycler \\\\\n        --threads $task.cpus \\\\\n        $args \\\\\n        $short_reads \\\\\n        $long_reads \\\\\n        --out ./\n\n    mv assembly.fasta ${prefix}.scaffolds.fa\n    gzip -n ${prefix}.scaffolds.fa\n    mv assembly.gfa ${prefix}.assembly.gfa\n    gzip -n ${prefix}.assembly.gfa\n    mv unicycler.log ${prefix}.unicycler.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        unicycler: \\$(echo \\$(unicycler --version 2>&1) | sed 's/^.*Unicycler v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/UNICYCLER", "nf-core/modules/nf-core__modules/UNICYCLER", "nf-core/viralrecon/nf-core__viralrecon/UNICYCLER"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess multiqc {\n    label 'sc_medium'\n\n    publishDir \"${params.outdir}/multiqc\", mode: params.publish_dir_mode\n\n    input:\n    file multiqc_config from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file software_versions_mqc from software_versions_yaml.collect().ifEmpty([])\n    file logo from ch_eager_logo\n    file ('fastqc_raw/*') from ch_prefastqc_for_multiqc.collect().ifEmpty([])\n    path('fastqc/*') from ch_fastqc_after_clipping.collect().ifEmpty([])\n    file ('adapter_removal/*') from ch_adapterremoval_logs.collect().ifEmpty([])\n    file ('mapping/bt2/*') from ch_bt2_for_multiqc.collect().ifEmpty([])\n    file ('flagstat/*') from ch_flagstat_for_multiqc.collect().ifEmpty([])\n    file ('flagstat_filtered/*') from ch_bam_filtered_flagstat_for_multiqc.collect().ifEmpty([])\n    file ('preseq/*') from ch_preseq_for_multiqc.collect().ifEmpty([])\n    file ('damageprofiler/dmgprof*/*') from ch_damageprofiler_results.collect().ifEmpty([])\n    file ('qualimap/qualimap*/*') from ch_qualimap_results.collect().ifEmpty([])\n    file ('markdup/*') from ch_markdup_results_for_multiqc.collect().ifEmpty([])\n    file ('dedup*/*') from ch_dedup_results_for_multiqc.collect().ifEmpty([])\n    file ('fastp/*') from ch_fastp_for_multiqc.collect().ifEmpty([])\n    file ('sexdeterrmine/*') from ch_sexdet_for_multiqc.collect().ifEmpty([])\n    file ('mutnucratio/*') from ch_mtnucratio_for_multiqc.collect().ifEmpty([])\n    file ('endorspy/*') from ch_endorspy_for_multiqc.collect().ifEmpty([])\n    file ('multivcfanalyzer/*') from ch_multivcfanalyzer_for_multiqc.collect().ifEmpty([])\n    file ('fastp_lowcomplexityfilter/*') from ch_metagenomic_complexity_filter_for_multiqc.collect().ifEmpty([])\n    file ('malt/*') from ch_malt_for_multiqc.collect().ifEmpty([])\n    file ('kraken/*') from ch_kraken_for_multiqc.collect().ifEmpty([])\n    file ('hops/*') from ch_hops_for_multiqc.collect().ifEmpty([])\n    file ('nuclear_contamination/*') from ch_nuclear_contamination_for_multiqc.collect().ifEmpty([])\n    file ('genotyping/*') from ch_eigenstrat_snp_cov_for_multiqc.collect().ifEmpty([])\n    file ('bcftools_stats') from ch_bcftools_stats_for_multiqc.collect().ifEmpty([])\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    \n    def custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $multiqc_config $custom_config_file .\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/multiqc"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 23, "tools": ["SAMtools"], "nb_own": 13, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "BrianLohman", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "AnneCharlotteMichel", "Irvibena", "nf-core", "madhyastha", "KlausVG", "luslab"], "nb_wf": 14, "list_wf": ["saw.sarek", "luslab-nf-modules", "Reprohackathon", "gatk-nextflow-sample", "sarek_ubec", "reproHack", "germline_somatic", "Groupe10_Hackaton", "custom_sarek", "dx_sarek", "sarek", "star-rsem-scrnaseq-nf", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["BrianLohman", "alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "mjmansfi", "pcantalupo", "szilvajuhos", "nf-core-bot", "alexthiery", "AnneCharlotteMichel", "jfnavarro", "Rhine-cats", "madhyastha", "Thadouard", "jackmo375", "Linxian0816", "chelauk", "adrlar", "lconde-ucl", "amchakra", "charlotte-west", "malinlarsson", "charles-plessy", "marc-jones", "chris-cheshire", "candiceh08", "sidorov-si", "ffmmulder", "rmoran7", "vivianepasteau", "lescai", "SimiliSerpent", "apeltzer", "baptisteruiz", "ArnaudM22", "KlausVG", "olgabot", "idelim09", "davidmasp"], "nb_contrib": 44, "codes": ["\nprocess umitools_dedup {\n    label \"min_cores\"\n    label \"avg_mem\"\n    label \"regular_queue\"\n\n    tag \"${meta.sample_id}\"\n    \n    publishDir \"${params.outdir}/${opts.publish_dir}\",\n        mode: \"copy\", \n        overwrite: true,\n        saveAs: { filename ->\n                      if (opts.publish_results == \"none\") null\n                      else filename }\n\n                                     \n    container 'quay.io/biocontainers/mulled-v2-509311a44630c01d9cb7d2ac5727725f51ea43af:b4c5bc18f99e35cd061328b98f501aa7b9717308-0'\n\n    input:\n        val(opts)\n        tuple val(meta), path(bam), path(bai)\n       \n    output:\n        tuple val(meta), path(\"${prefix}.bam\"), path(\"${prefix}.bam.bai\"), emit: dedupBam\n        path \"*.log\", emit: report\n\n    script:\n\n           \n    prefix = opts.suffix ? \"${meta.sample_id}${opts.suffix}\" : \"${meta.sample_id}\"\n\n    args = \"--log=${prefix}.log \"\n\n    if(opts.args && opts.args != '') {\n        ext_args = opts.args\n        args += ext_args.trim()\n    }\n\n                        \n    dedup_command = \"umi_tools dedup ${args} -I ${bam[0]} -S ${prefix}.bam --output-stats=${prefix}\"\n\n          \n    if (params.verbose){\n        println (\"[MODULE] umi_tools/dedup command: \" + dedup_command)\n    }\n\n           \n    \"\"\"\n    ${dedup_command}\n    samtools index ${prefix}.bam\n    \"\"\"\n}", "\nprocess indexBamFiles {\n\tpublishDir 'results/bam_index'\n\n\tinput:\n\tfile bam from bam_indexBamFiles\n\n\toutput:\n\ttuple file(\"${bam}.bai\"), file(\"${bam}\") into bamindex\n\n\t\"\"\"\n\tsamtools index *.bam\n\t\"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess samtools_index {\n    container \"${containers.samtools}\"\n    cpus 8\n    memory \"32 GB\"\n\n    if (params.output != 'NONE') {\n      publishDir \"${output_dir}\", enabled: params.output != 'NONE'\n    }\n\n  input:\n    file \"${sample_id}.bam\" from bam_file\n  \n  output:\n    file \"${sample_id}.bam.bai\" into bai_file\n  \n  script:\n  \"\"\"\n  samtools index \\\n        ${sample_id}.bam\n  \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess indexBam {\n\n\tpublishDir \"results/bam_files/\"\n\n\tinput:\n\tfile bam from mapped_fastq_1\n\n\toutput:\n\tfile \"*.bai\" into sam_fastq_files\n\n\tscript:\n\t\"\"\"\n\tsamtools index *.bam\n\t\"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess GetIndexOfAlignedSampleReadGroup {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.bam\")\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.bam.bai\")\n        tuple val(idPatient), val(idSample)\n\n    when: save_bam_mapped\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess samFastq {\n    publishDir \"bai/\"\n\n    input:\n    file bam from mapped_fastq_files_1\n \n    output:\n    file \"*.bai\" into sam_fastq_files\n \n    script:\n    \"\"\"\n    samtools index *.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess index {\n    module 'samtools/1.12'\n    tag \"${pair_id}\"\n    publishDir \"${params.outdir}/bams\", mode:\"copy\"\n\n    input:\n      tuple val(pair_id), path(bam)\n\n    output:\n      path(\"${pair_id}.bam\")\n      path(\"${pair_id}.bam.bai\")    \n\n    script:\n      \"\"\"\n      samtools index ${pair_id}.bam\n      \"\"\"\n}"], "list_proc": ["luslab/luslab-nf-modules/luslab__luslab-nf-modules/umitools_dedup", "KlausVG/reproHack/KlausVG__reproHack/indexBamFiles", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/IndexBamMergedForSentieon", "madhyastha/gatk-nextflow-sample/madhyastha__gatk-nextflow-sample/samtools_index", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/IndexBamFile", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/IndexBamMergedForSentieon", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/IndexBamFile", "Irvibena/Groupe10_Hackaton/Irvibena__Groupe10_Hackaton/indexBam", "nf-core/sarek/nf-core__sarek/IndexBamMergedForSentieon", "nf-core/sarek/nf-core__sarek/IndexBamFile", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GetIndexOfAlignedSampleReadGroup", "rmoran7/custom_sarek/rmoran7__custom_sarek/IndexBamMergedForSentieon", "rmoran7/custom_sarek/rmoran7__custom_sarek/IndexBamFile", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/IndexBamMergedForSentieon", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/IndexBamFile", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/IndexBamMergedForSentieon", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/IndexBamFile", "AnneCharlotteMichel/Reprohackathon/AnneCharlotteMichel__Reprohackathon/samFastq", "rmoran7/dx_sarek/rmoran7__dx_sarek/IndexBamMergedForSentieon", "rmoran7/dx_sarek/rmoran7__dx_sarek/IndexBamFile", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/IndexBamMergedForSentieon", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/IndexBamFile", "BrianLohman/star-rsem-scrnaseq-nf/BrianLohman__star-rsem-scrnaseq-nf/index"], "list_wf_names": ["Irvibena/Groupe10_Hackaton", "UMCUGenetics/sarek_ubec", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "chelauk/test_nextflow_sarek", "madhyastha/gatk-nextflow-sample", "nf-core/sarek", "AnneCharlotteMichel/Reprohackathon", "BrianLohman/star-rsem-scrnaseq-nf", "luslab/luslab-nf-modules", "rmoran7/dx_sarek", "KlausVG/reproHack", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["HISAT2"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess hisat2 {\n                                                                           \n                                                                      \n                                                            \n    tag \"$name\"\n    validExitStatus 0,143\n\n    input:\n    val(indices) from hisat2_indices.first()\n    set val(name), file(trimmed_reads) from trimmed_reads_hisat2\n\n    output:\n    set val(name), file(\"*.sam\") into hisat2_sam\n\n    script:\n    index_base = indices[0].toString() - ~/.\\d.ht2/\n    if (!params.singleEnd) {\n        \"\"\"\n        echo ${name}\n    \n        hisat2  -p ${task.cpus} \\\n                --very-sensitive \\\n                --no-spliced-alignment \\\n                -x ${index_base} \\\n                -1 ${name}_R1.trim.fastq \\\n                -2 ${name}_R2.trim.fastq\n                > ${name}.sam\n        \"\"\"\n    } else {\n        \"\"\"\n        echo ${name}\n    \n        hisat2  -p ${task.cpus} \\\n                --very-sensitive \\\n                --no-spliced-alignment \\\n                -x ${index_base}\\\n                -U ${trimmed_reads} \\\n                > ${name}.sam\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/nascent/nf-core__nascent/hisat2"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 6, "tools": ["BWA"], "nb_own": 5, "list_own": ["c3g", "nf-core", "CDCgov", "cidgoh", "jianhong"], "nb_wf": 6, "list_wf": ["mycosnp-nf", "cidgoh_qc", "genflow-dnaseq", "nf-core-hicar", "modules", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "anwarMZ", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "duanjunhyq", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' :\n        'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' :\n        'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' :\n        'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' :\n        'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $options.args \\\\\n        $fasta \\\\\n        -p bwa/${fasta.baseName}\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' :\n        'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BWA_INDEX", "nf-core/ssds/nf-core__ssds/BWA_INDEX", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BWA_INDEX", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/BWA_INDEX", "c3g/genflow-dnaseq/c3g__genflow-dnaseq/BWA_INDEX", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/BWA_INDEX"], "list_wf_names": ["jianhong/nf-core-hicar", "cidgoh/cidgoh_qc", "nf-core/ssds", "nf-core/modules", "c3g/genflow-dnaseq", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 2, "tools": ["PredictPA"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "funcscan"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "jasmezz", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["\nprocess DEEPARG_PREDICT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::deeparg=1.0.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/deeparg:1.0.2--pyhdfd78af_1' :\n        'quay.io/biocontainers/deeparg:1.0.2--pyhdfd78af_1' }\"\n      \n                                                                                                                                                              \n                                                                      \n      \n    containerOptions { \"${workflow.containerEngine}\" == 'singularity' ? '-B $(which bash):/usr/local/lib/python2.7/site-packages/Theano-0.8.2-py2.7.egg-info/PKG-INFO' : '' }\n\n    input:\n    tuple val(meta), path(fasta), val(model)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*.align.daa\")            , emit: daa\n    tuple val(meta), path(\"*.align.daa.tsv\")        , emit: daa_tsv\n    tuple val(meta), path(\"*.mapping.ARG\")          , emit: arg\n    tuple val(meta), path(\"*.mapping.potential.ARG\"), emit: potential_arg\n    path \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    deeparg \\\\\n        predict \\\\\n        $args \\\\\n        -i $fasta \\\\\n        -o ${prefix} \\\\\n        -d $db \\\\\n        --model $model\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        deeparg: $VERSION\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess DEEPARG_PREDICT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::deeparg=1.0.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/deeparg:1.0.2--pyhdfd78af_1' :\n        'quay.io/biocontainers/deeparg:1.0.2--pyhdfd78af_1' }\"\n      \n                                                                                                                                                              \n                                                                      \n      \n    containerOptions { \"${workflow.containerEngine}\" == 'singularity' ? '-B $(which bash):/usr/local/lib/python2.7/site-packages/Theano-0.8.2-py2.7.egg-info/PKG-INFO' : '' }\n\n    input:\n    tuple val(meta), path(fasta), val(model)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*.align.daa\")            , emit: daa\n    tuple val(meta), path(\"*.align.daa.tsv\")        , emit: daa_tsv\n    tuple val(meta), path(\"*.mapping.ARG\")          , emit: arg\n    tuple val(meta), path(\"*.mapping.potential.ARG\"), emit: potential_arg\n    path \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    deeparg \\\\\n        predict \\\\\n        $args \\\\\n        -i $fasta \\\\\n        -o ${prefix} \\\\\n        -d $db \\\\\n        --model $model\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        deeparg: $VERSION\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/funcscan/nf-core__funcscan/DEEPARG_PREDICT", "nf-core/modules/nf-core__modules/DEEPARG_PREDICT"], "list_wf_names": ["nf-core/funcscan", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["kallisto"], "nb_own": 2, "list_own": ["nf-core", "redst4r"], "nb_wf": 2, "list_wf": ["nf-10x-kallisto", "scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "redst4r", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 8, "codes": [" process kallisto {\n                          \n                                                                    \n\n     input:\n                                                           \n     file(reads) from combined_flat\n     file index from kallisto_index.collect()\n\n     output:\n     file \"bus_output\" into kallisto_bus_to_sort\n     file \"kallisto.log\" into kallisto_log_for_multiqc\n\n     script:\n     \"\"\"\n     echo $index\n     kallisto bus \\\\\n         -i $index \\\\\n         -o bus_output/ \\\\\n         -x ${params.chemistry} \\\\\n         -t ${params.cpus} \\\\\n         $reads | tee kallisto.log\n     \"\"\"\n }", "\nprocess kallisto {\n    tag \"$name\"\n    label 'mid_memory'\n    publishDir \"${params.outdir}/kallisto/raw_bus\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from read_files_kallisto\n    file index from kallisto_index.collect()\n\n    output:\n    file \"${name}_bus_output\" into kallisto_bus_to_sort\n    file \"${name}_kallisto.log\" into kallisto_log_for_multiqc\n\n    when: params.aligner == 'kallisto'\n\n    script:\n    \"\"\"\n    kallisto bus \\\\\n        -i $index \\\\\n        -o ${name}_bus_output/ \\\\\n        -x ${params.type}${params.chemistry} \\\\\n        -t ${task.cpus} \\\\\n        $reads | tee ${name}_kallisto.log\n    \"\"\"\n}"], "list_proc": ["redst4r/nf-10x-kallisto/redst4r__nf-10x-kallisto/kallisto", "nf-core/scrnaseq/nf-core__scrnaseq/kallisto"], "list_wf_names": ["redst4r/nf-10x-kallisto", "nf-core/scrnaseq"]}, {"nb_reuse": 9, "tools": ["TIDDIT", "GATK", "FastQC", "MultiQC", "QualiMap", "SAMtools", "BWA", "FreeBayes", "VCFtools", "MSIsensor", "BCFtools", "FREEC", "snpEff"], "nb_own": 7, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 8, "list_wf": ["saw.sarek", "sarek_ubec", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 23, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n                              \n    aligner=\"bwa-mem2\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess Get_software_versions {\n    publishDir path:\"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n        file \"software_versions.csv\"\n\n    when: !('versions' in skipQC)\n\n    script:\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    # TODO platypus will not output a version\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n        file \"software_versions.csv\"\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess GetSoftwareVersions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml'\n                                      \n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    ${params.sarekDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n        file \"software_versions.csv\"\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n        file \"software_versions.csv\"\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/rmoran7__custom_sarek/get_software_versions", "rmoran7/dx_sarek/rmoran7__dx_sarek/get_software_versions", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/Get_software_versions", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/get_software_versions", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/get_software_versions", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GetSoftwareVersions", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/get_software_versions", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/get_software_versions", "nf-core/sarek/nf-core__sarek/get_software_versions"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["VSEARCH"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["airrflow"], "list_contrib": ["tbugfinder", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "subwaystation"], "nb_contrib": 8, "codes": ["process PRESTO_CLUSTERSETS {\n    tag \"$meta.id\"\n    label \"process_long_parallelized\"\n\n    conda (params.enable_conda ? \"bioconda::presto=0.7.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/presto:0.7.0--pyhdfd78af_0' :\n        'quay.io/biocontainers/presto:0.7.0--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(R1), path(R2)\n\n    output:\n    tuple val(meta), path(\"*_R1_cluster-pass.fastq\"), path(\"*_R2_cluster-pass.fastq\"), emit: reads\n    path \"*_command_log.txt\", emit: logs\n    path \"*.log\"\n    path \"*.tab\", emit: log_tab\n    path(\"versions.yml\"), emit: versions\n\n    script:\n    \"\"\"\n    ClusterSets.py set --nproc ${task.cpus} -s $R1 --outname ${meta.id}_R1 --exec vsearch --log ${meta.id}_R1.log > \"${meta.id}_command_log.txt\"\n    ClusterSets.py set --nproc ${task.cpus} -s $R2 --outname ${meta.id}_R2 --exec vsearch --log ${meta.id}_R2.log >> \"${meta.id}_command_log.txt\"\n    ParseLog.py -l \"${meta.id}_R1.log\" \"${meta.id}_R2.log\" -f ID BARCODE SEQCOUNT CLUSTERS\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        presto: \\$( ClusterSets.py --version | awk -F' '  '{print \\$2}' )\n        vsearch: \\$( vsearch --version &> vsearch.txt; cat vsearch.txt | head -n 1 | grep -o 'v[0-9\\\\.]\\\\+' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/airrflow/nf-core__airrflow/PRESTO_CLUSTERSETS"], "list_wf_names": ["nf-core/airrflow"]}, {"nb_reuse": 1, "tools": ["Diamond"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process DIAMOND_MAKEDB {\n    tag \"$fasta\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::diamond=2.0.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/diamond:2.0.15--hb97b32f_0' :\n        'quay.io/biocontainers/diamond:2.0.15--hb97b32f_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"${fasta}.dmnd\", emit: db\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    diamond \\\\\n        makedb \\\\\n        --threads $task.cpus \\\\\n        --in  $fasta \\\\\n        -d $fasta \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        diamond: \\$(diamond --version 2>&1 | tail -n 1 | sed 's/^diamond version //')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/DIAMOND_MAKEDB"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 6, "tools": ["MLST"], "nb_own": 5, "list_own": ["ABMicroBioinf", "happykhan", "nf-core", "bactopia", "xiaoli-dong"], "nb_wf": 5, "list_wf": ["pathogen", "magph", "nf-klebtest", "modules", "bactopia"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "Accio", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "fmaguire", "ntoda03", "emnilsson", "happykhan", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "TGotwig"], "nb_contrib": 110, "codes": ["\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/mlst:2.19.0--hdfd78af_1\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mlst $options.args --label ${prefix} --threads $task.cpus $fasta > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "process MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1' :\n        'quay.io/biocontainers/mlst:2.19.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mlst: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/mlst:2.19.0--hdfd78af_1\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mlst $options.args --label ${prefix} --threads $task.cpus $fasta > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "process MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1' :\n        'quay.io/biocontainers/mlst:2.19.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mlst: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1' :\n        'quay.io/biocontainers/mlst:2.19.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\" , emit: logs, optional: true\n    path \".command.*\"  , emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $options.args \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mlst: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/mlst:2.19.0--hdfd78af_1\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}"], "list_proc": ["ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/MLST", "happykhan/nf-klebtest/happykhan__nf-klebtest/MLST", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/MLST", "nf-core/modules/nf-core__modules/MLST", "bactopia/bactopia/bactopia__bactopia/MLST", "xiaoli-dong/magph/xiaoli-dong__magph/MLST"], "list_wf_names": ["xiaoli-dong/pathogen", "happykhan/nf-klebtest", "ABMicroBioinf/pathogen", "bactopia/bactopia", "nf-core/modules", "xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bactmap"], "list_contrib": ["alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh"], "nb_contrib": 8, "codes": ["\nprocess MASH_SKETCH {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    conda (params.enable_conda ? \"bioconda::mash=2.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1\"\n    } else {\n        container \"quay.io/biocontainers/mash:2.3--he348c14_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.msh\")        , emit: mash\n    tuple val(meta), path(\"*.mash_stats\") , emit: stats\n    path \"*.version.txt\"                  , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mash \\\\\n        sketch \\\\\n        $options.args \\\\\n        -p $task.cpus \\\\\n        -o ${prefix} \\\\\n        -r $reads \\\\\n        2> ${prefix}.mash_stats\n    echo \\$(mash --version 2>&1) > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bactmap/nf-core__bactmap/MASH_SKETCH"], "list_wf_names": ["nf-core/bactmap"]}, {"nb_reuse": 8, "tools": ["Picard"], "nb_own": 4, "list_own": ["vincenthhu", "csf-ngs", "nf-core", "mahesh-panchal"], "nb_wf": 7, "list_wf": ["raredisease", "viralrecon", "test_nfcore_workflow_chain", "modules", "nf-core-westest", "controldna", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "idot", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 115, "codes": ["process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.CollectMultipleMetrics \\\\\n        --REFERENCE_SEQUENCE $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.CollectMultipleMetrics.alignment_summary_metrics\n    touch ${prefix}.CollectMultipleMetrics.insert_size_metrics\n    touch ${prefix}.CollectMultipleMetrics.quality_distribution.pdf\n    touch ${prefix}.CollectMultipleMetrics.base_distribution_by_cycle_metrics\n    touch ${prefix}.CollectMultipleMetrics.quality_by_cycle_metrics\n    touch ${prefix}.CollectMultipleMetrics.read_length_histogram.pdf\n    touch ${prefix}.CollectMultipleMetrics.base_distribution_by_cycle.pdf\n    touch ${prefix}.CollectMultipleMetrics.quality_by_cycle.pdf\n    touch ${prefix}.CollectMultipleMetrics.insert_size_histogram.pdf\n    touch ${prefix}.CollectMultipleMetrics.quality_distribution_metrics\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard CollectMultipleMetrics --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::picard=2.25.7' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.25.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.25.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.CollectMultipleMetrics.alignment_summary_metrics\n    touch ${prefix}.CollectMultipleMetrics.insert_size_metrics\n    touch ${prefix}.CollectMultipleMetrics.quality_distribution.pdf\n    touch ${prefix}.CollectMultipleMetrics.base_distribution_by_cycle_metrics\n    touch ${prefix}.CollectMultipleMetrics.quality_by_cycle_metrics\n    touch ${prefix}.CollectMultipleMetrics.read_length_histogram.pdf\n    touch ${prefix}.CollectMultipleMetrics.base_distribution_by_cycle.pdf\n    touch ${prefix}.CollectMultipleMetrics.quality_by_cycle.pdf\n    touch ${prefix}.CollectMultipleMetrics.insert_size_histogram.pdf\n    touch ${prefix}.CollectMultipleMetrics.quality_distribution_metrics\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard CollectMultipleMetrics --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTMULTIPLEMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    tuple val(meta), path(\"*.pdf\")    , emit: pdf\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectMultipleMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectMultipleMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTWGSMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectWgsMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectWgsMetrics \\\\\n        $args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.CollectWgsMetrics.coverage_metrics \\\\\n        REFERENCE_SEQUENCE=$fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectWgsMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_COLLECTMULTIPLEMETRICS", "nf-core/ssds/nf-core__ssds/PICARD_COLLECTMULTIPLEMETRICS", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/PICARD_COLLECTMULTIPLEMETRICS", "nf-core/raredisease/nf-core__raredisease/PICARD_COLLECTMULTIPLEMETRICS", "nf-core/viralrecon/nf-core__viralrecon/PICARD_COLLECTMULTIPLEMETRICS", "csf-ngs/controldna/csf-ngs__controldna/PICARD_COLLECTMULTIPLEMETRICS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/PICARD_COLLECTMULTIPLEMETRICS", "csf-ngs/controldna/csf-ngs__controldna/PICARD_COLLECTWGSMETRICS"], "list_wf_names": ["csf-ngs/controldna", "nf-core/raredisease", "nf-core/ssds", "vincenthhu/nf-core-westest", "nf-core/modules", "nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["Filter"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process LOFREQ_FILTER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::lofreq=2.1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/lofreq:2.1.5--py38h588ecb2_4' :\n        'quay.io/biocontainers/lofreq:2.1.5--py38h588ecb2_4' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    lofreq \\\\\n        filter \\\\\n        $args \\\\\n        -i $vcf \\\\\n        -o ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        lofreq: \\$(echo \\$(lofreq version 2>&1) | sed 's/^version: //; s/ *commit.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/LOFREQ_FILTER"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Bowtie"], "nb_own": 3, "list_own": ["clairecoleman1", "nf-core", "oisinmccaffrey"], "nb_wf": 3, "list_wf": ["clipseq.nextflow", "clipseq", "clipseq1"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "CharlotteAnne", "drpatelh", "clairecoleman1", "oisinmccaffrey"], "nb_contrib": 8, "codes": [" process premap {\n        tag \"$name\"\n        label 'process_high'\n        publishDir \"${params.outdir}/premap\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(name), path(reads) from ch_trimmed\n        path(index) from ch_bt2_index.collect()\n\n        output:\n        tuple val(name), path(\"${name}.unmapped.fastq.gz\") into ch_unmapped\n        tuple val(name), path(\"${name}.premapped.bam\"), path(\"${name}.premapped.bam.bai\")\n        path \"*.log\" into ch_premap_mqc, ch_premap_qc\n\n        script:\n        \"\"\"\n        bowtie2 -p $task.cpus -x ${index[0].simpleName} --un-gz ${name}.unmapped.fastq.gz -U $reads 2> ${name}.premap.log | \\\n        samtools sort -@ $task.cpus /dev/stdin > ${name}.premapped.bam && \\\n        samtools index -@ $task.cpus ${name}.premapped.bam\n        \"\"\"\n    }", "\nprocess premap {\n\n    publishDir \"${params.outdir}/premap\", mode: 'copy'\n\n    tag \"$key\"\n\n    input:\n\n    tuple val(key), file(reads) from mapping_reads\n    path(index) from ch_bt2_index.collect()\n\n    output:\n    tuple val(key), path(\"${key}.unmapped.fq.gz\") into ch_unmapped\n    tuple val(key), path(\"${key}.premapped.bam\"), path(\"${key}.premapped.bam.bai\")\n    path \"*.log\" into ch_premap_mqc, ch_premap_qc\n\n    script:\n    \"\"\"\n    bowtie2 -p $task.cpus -x ${index[0].simpleName} --un-gz ${key}.unmapped.fq.gz -U $reads 2> ${key}.premap.log | \\\n    samtools sort -@ $task.cpus /dev/stdin > ${key}.premapped.bam && \\\n    samtools index -@ $task.cpus ${key}.premapped.bam\n    \"\"\"\n}", "\nprocess premap {\n\n    publishDir \"${params.outdir}/premap\", mode: 'copy'\n\n    tag \"$key\"\n\n    input:\n\n    tuple val(key), file(reads) from mapping_reads\n    path(index) from ch_bt2_index.collect()\n\n    output:\n    tuple val(key), path(\"${key}.unmapped.fq.gz\") into ch_unmapped\n    tuple val(key), path(\"${key}.premapped.bam\"), path(\"${key}.premapped.bam.bai\")\n    path \"*.log\" into ch_premap_mqc, ch_premap_qc\n\n    script:\n    \"\"\"\n    bowtie2 -p $task.cpus -x ${index[0].simpleName} --un-gz ${key}.unmapped.fq.gz -U $reads 2> ${key}.premap.log | \\\n    samtools sort -@ $task.cpus /dev/stdin > ${key}.premapped.bam && \\\n    samtools index -@ $task.cpus ${key}.premapped.bam\n    \"\"\"\n}"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/premap", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/premap", "clairecoleman1/clipseq1/clairecoleman1__clipseq1/premap"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "nf-core/clipseq"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_TRAIN {\n    tag \"${meta.FW_primer}-${meta.RV_primer}\"\n    label 'process_high'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple val(meta), path(qza)\n\n    output:\n    path(\"*-classifier.qza\"), emit: qza\n    path \"versions.yml\"    , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #Train classifier\n    qiime feature-classifier fit-classifier-naive-bayes \\\n        --i-reference-reads ${meta.FW_primer}-${meta.RV_primer}-ref-seq.qza \\\n        --i-reference-taxonomy ref-taxonomy.qza \\\n        --o-classifier ${meta.FW_primer}-${meta.RV_primer}-classifier.qza \\\n        --quiet\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_TRAIN {\n    tag \"${meta.FW_primer}-${meta.RV_primer}\"\n    label 'process_high'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple val(meta), path(qza)\n\n    output:\n    path(\"*-classifier.qza\"), emit: qza\n    path \"versions.yml\"    , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #Train classifier\n    qiime feature-classifier fit-classifier-naive-bayes \\\n        --i-reference-reads ${meta.FW_primer}-${meta.RV_primer}-ref-seq.qza \\\n        --i-reference-taxonomy ref-taxonomy.qza \\\n        --o-classifier ${meta.FW_primer}-${meta.RV_primer}-classifier.qza \\\n        --quiet\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_TRAIN", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_TRAIN"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["Bandage"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["modules", "test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process BANDAGE_IMAGE {\n    tag \"${meta.id}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::bandage=0.8.1' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bandage:0.8.1--hc9558a2_2' :\n        'quay.io/biocontainers/bandage:0.8.1--hc9558a2_2' }\"\n\n    input:\n    tuple val(meta), path(gfa)\n\n    output:\n    tuple val(meta), path('*.png'), emit: png\n    tuple val(meta), path('*.svg'), emit: svg\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    Bandage image $gfa ${prefix}.png $args\n    Bandage image $gfa ${prefix}.svg $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bandage: \\$(echo \\$(Bandage --version 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BANDAGE_IMAGE {\n    tag \"${meta.id}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::bandage=0.8.1' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bandage:0.8.1--hc9558a2_2' :\n        'quay.io/biocontainers/bandage:0.8.1--hc9558a2_2' }\"\n\n    input:\n    tuple val(meta), path(gfa)\n\n    output:\n    tuple val(meta), path('*.png'), emit: png\n    tuple val(meta), path('*.svg'), emit: svg\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    Bandage image $gfa ${prefix}.png $args\n    Bandage image $gfa ${prefix}.svg $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bandage: \\$(echo \\$(Bandage --version 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BANDAGE_IMAGE {\n    tag \"${meta.id}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::bandage=0.8.1' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bandage:0.8.1--hc9558a2_2' :\n        'quay.io/biocontainers/bandage:0.8.1--hc9558a2_2' }\"\n\n    input:\n    tuple val(meta), path(gfa)\n\n    output:\n    tuple val(meta), path('*.png'), emit: png\n    tuple val(meta), path('*.svg'), emit: svg\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    Bandage image $gfa ${prefix}.png $args\n    Bandage image $gfa ${prefix}.svg $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bandage: \\$(echo \\$(Bandage --version 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/BANDAGE_IMAGE", "nf-core/modules/nf-core__modules/BANDAGE_IMAGE", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BANDAGE_IMAGE"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess qualimap {\n    label 'mc_small'\n    tag \"${samplename}\"\n    publishDir \"${params.outdir}/qualimap\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_qualimap\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_addlibmerge_for_qualimap\n    file fasta from ch_fasta_for_qualimap.collect()\n    path snpcapture_bed from ch_snpcapture_bed \n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*\") into ch_qualimap_results\n\n    script:\n    def snpcap = snpcapture_bed.getName() != 'nf-core_eager_dummy.txt' ? \"-gff ${snpcapture_bed}\" : ''\n    \"\"\"\n    qualimap bamqc -bam $bam -nt ${task.cpus} -outdir . -outformat \"HTML\" ${snpcap} --java-mem-size=${task.memory.toGiga()}G\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/qualimap"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["heinzlab", "nf-core"], "nb_wf": 2, "list_wf": ["smrnaseq", "smrna-seq-pipeline"], "list_contrib": ["c-guzman", "ewels", "maxulysse", "lpantano", "chuan-wang", "sirselim", "lcabus-flomics", "nf-core-bot", "ErikDanielsson", "pericsson", "sdjebali", "pditommaso", "mjsteinbaugh", "Hammarn", "jemten", "KevinMenden", "apeltzer", "drpatelh", "kstawiski"], "nb_contrib": 19, "codes": [" process bowtie_unmapped {\n        label 'process_ignore'\n        label 'process_medium'\n        tag \"${input_files[0].baseName}\"\n        publishDir \"${params.outdir}/bowtie_ref/unmapped\", mode: 'copy'\n\n        input:\n        file input_files from bowtie_bam_for_unmapped.toSortedList()\n\n        output:\n        file 'unmapped_refgenome.txt' into bowtie_unmapped\n\n        script:\n        \"\"\"\n        for i in $input_files\n        do\n          printf \"\\${i}\\t\"\n          samtools view -c -f0x4 \\${i}\n        done > unmapped_refgenome.txt\n        \"\"\"\n    }", " process bowtie2_unmapped {\n        tag \"${input_files[0].baseName}\"\n        publishDir \"${params.outdir}/bowtie2/unmapped\", mode: 'copy'\n\n        input:\n        file input_files from bowtie2_bam_for_unmapped.toSortedList()\n\n        output:\n        file 'unmapped_refgenome.txt' into bowtie2_unmapped\n\n        script:\n        \"\"\"\n        for i in $input_files\n        do\n          printf \"\\${i}\\t\"\n          samtools view -c -f0x4 \\${i}\n        done > unmapped_refgenome.txt\n        \"\"\"\n    }"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/bowtie_unmapped", "heinzlab/smrna-seq-pipeline/heinzlab__smrna-seq-pipeline/bowtie2_unmapped"], "list_wf_names": ["nf-core/smrnaseq", "heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess genotypegvcfs{\n    tag \"${name}\"\n    publishDir \"${params.outdir}/GATK_GenotypeGVCFs/\", mode: 'copy', \n    saveAs: {filename -> params.saveIntermediateVariants ? \"$filename\" : null }\n\n    input:\n    set val(name), file(raw_vcf), file(raw_vcf_idx) from raw_variants\n\n    output:\n    set val(name), file(\"${name}_gvcf.vcf\"), file(\"${name}_gvcf.vcf.idx\") into raw_gvcfs\n\n    script:\n    \"\"\"\n    gatk -T GenotypeGVCFs \\\\\n    -R $params.gfasta \\\\\n    --variant $raw_vcf \\\\\n    -nt $task.cpus \\\\\n    -o ${name}_gvcf.vcf \\\\\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/genotypegvcfs"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess dreg_prep {\n    validExitStatus 0,143\n    errorStrategy 'ignore'\n    tag \"$name\"\n    publishDir \"${params.outdir}/mapped/dreg_input\", mode: 'copy', pattern: \"*.bw\"\n\n    input:\n    set val(name), file(bam_file) from sorted_bams_for_dreg_prep\n    file(chr_sizes) from chrom_sizes_for_bed\n\n    output:\n        set val(name), file(\"*.bw\") into dreg_bigwig\n\n    script:\n    \"\"\"\n\n    echo \"Creating BigWigs suitable as inputs to dREG\"\n\n    bedtools bamtobed -i ${bam_file} | awk 'BEGIN{OFS=\"\\t\"} (\\$5 > 0){print \\$0}' | \\\n    awk 'BEGIN{OFS=\"\\t\"} (\\$6 == \"+\") {print \\$1,\\$2,\\$2+1,\\$4,\\$5,\\$6}; (\\$6 == \"-\") {print \\$1, \\$3-1,\\$3,\\$4,\\$5,\\$6}' \\\n    > ${name}.dreg.bed\n    sortBed -i ${name}.dreg.bed > ${name}.dreg.sort.bed\n\n    echo positive strand processed to bedGraph\n\n    bedtools genomecov -bg -i ${name}.dreg.sort.bed -g ${chr_sizes} -strand + > ${name}.pos.bedGraph\n    sortBed -i ${name}.pos.bedGraph > ${name}.pos.sort.bedGraph\n    bedtools genomecov -bg -i ${name}.dreg.sort.bed -g ${chr_sizes} -strand - \\\n    | awk 'BEGIN{OFS=\"\\t\"} {print \\$1,\\$2,\\$3,-1*\\$4}' > ${name}.neg.bedGraph\n    sortBed -i ${name}.neg.bedGraph > ${name}.neg.sort.bedGraph\n\n    echo negative strand processed to bedGraph\n\n    ${params.bedGraphToBigWig} ${name}.pos.sort.bedGraph ${chr_sizes} ${name}.pos.bw\n    ${params.bedGraphToBigWig} ${name}.neg.sort.bedGraph ${chr_sizes} ${name}.neg.bw\n\n    echo bedGraph to bigwig done\n    \"\"\"\n }"], "list_proc": ["nf-core/nascent/nf-core__nascent/dreg_prep"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 11, "tools": ["RNASEQR", "QualiMap"], "nb_own": 7, "list_own": ["chelauk", "raygozag", "vincenthhu", "nf-core", "mahesh-panchal", "CDCgov", "harleenduggal"], "nb_wf": 9, "list_wf": ["raredisease", "mycosnp-nf", "RNASEQ", "test_nfcore_workflow_chain", "nf-core-blasr", "modules", "nfcore-rnaseq", "nf-core-westest", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "mciprianoCDC", "santiagorevale", "ppericard", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "cjjossart", "pcantalupo", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "vincenthhu", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "chelauk", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 151, "codes": ["process QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess QUALIMAP_BAMQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1\"\n    } else {\n        container \"quay.io/biocontainers/qualimap:2.2.2d--1\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bam) \n    path gff\n    val use_gff\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = use_gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $options.args \\\\\n        -bam $bam \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n    stub:\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    unset DISPLAY\n    mkdir ${prefix}\n    touch ${prefix}/thing.txt\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(qualimap java_options=\"-Djava.awt.headless=true\" 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n}", "process QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_BAMQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path gff\n    val use_gff\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = use_gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $args \\\\\n        -bam $bam \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_BAMQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path gff\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $args \\\\\n        -bam $bam \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mkdir -p $prefix/css\n    mkdir $prefix/images_qualimapReport\n    mkdir $prefix/raw_data_qualimapReport\n    cd $prefix/css\n    touch agogo.css\n    touch basic.css\n    touch bgtop.png\n    touch comment-close.png\n    touch doctools.js\n    touch down-pressed.png\n    touch jquery.js\n    touch plus.png\n    touch qualimap_logo_small.png\n    touch searchtools.js\n    touch up.png\n    touch websupport.js\n    touch ajax-loader.gif\n    touch bgfooter.png\n    touch comment-bright.png\n    touch comment.png\n    touch down.png\n    touch file.png\n    touch minus.png\n    touch pygments.css\n    touch report.css\n    touch underscore.js\n    touch up-pressed.png\n    cd ../images_qualimapReport/\n    touch genome_coverage_0to50_histogram.png\n    touch genome_coverage_quotes.png\n    touch genome_insert_size_across_reference.png\n    touch genome_mapping_quality_histogram.png\n    touch genome_uniq_read_starts_histogram.png\n    touch genome_coverage_across_reference.png\n    touch genome_gc_content_per_window.png\n    touch genome_insert_size_histogram.png\n    touch genome_reads_clipping_profile.png\n    touch genome_coverage_histogram.png\n    touch genome_homopolymer_indels.png\n    touch genome_mapping_quality_across_reference.png\n    touch genome_reads_content_per_read_position.png\n    cd ../raw_data_qualimapReport\n    touch coverage_across_reference.txt\n    touch genome_fraction_coverage.txt\n    touch insert_size_histogram.txt\n    touch mapped_reads_nucleotide_content.txt\n    touch coverage_histogram.txt\n    touch homopolymer_indels.txt\n    touch mapped_reads_clipping_profile.txt\n    touch mapping_quality_across_reference.txt\n    touch duplication_rate_histogram.txt\n    touch insert_size_across_reference.txt\n    touch mapped_reads_gc-content_distribution.txt\n    touch mapping_quality_histogram.txt\n    cd ../\n    touch genome_results.txt\n    touch qualimapReport.html\n    cd ../\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_BAMQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path gff\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $args \\\\\n        -bam $bam \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mkdir -p $prefix/css\n    mkdir $prefix/images_qualimapReport\n    mkdir $prefix/raw_data_qualimapReport\n    cd $prefix/css\n    touch agogo.css\n    touch basic.css\n    touch bgtop.png\n    touch comment-close.png\n    touch doctools.js\n    touch down-pressed.png\n    touch jquery.js\n    touch plus.png\n    touch qualimap_logo_small.png\n    touch searchtools.js\n    touch up.png\n    touch websupport.js\n    touch ajax-loader.gif\n    touch bgfooter.png\n    touch comment-bright.png\n    touch comment.png\n    touch down.png\n    touch file.png\n    touch minus.png\n    touch pygments.css\n    touch report.css\n    touch underscore.js\n    touch up-pressed.png\n    cd ../images_qualimapReport/\n    touch genome_coverage_0to50_histogram.png\n    touch genome_coverage_quotes.png\n    touch genome_insert_size_across_reference.png\n    touch genome_mapping_quality_histogram.png\n    touch genome_uniq_read_starts_histogram.png\n    touch genome_coverage_across_reference.png\n    touch genome_gc_content_per_window.png\n    touch genome_insert_size_histogram.png\n    touch genome_reads_clipping_profile.png\n    touch genome_coverage_histogram.png\n    touch genome_homopolymer_indels.png\n    touch genome_mapping_quality_across_reference.png\n    touch genome_reads_content_per_read_position.png\n    cd ../raw_data_qualimapReport\n    touch coverage_across_reference.txt\n    touch genome_fraction_coverage.txt\n    touch insert_size_histogram.txt\n    touch mapped_reads_nucleotide_content.txt\n    touch coverage_histogram.txt\n    touch homopolymer_indels.txt\n    touch mapped_reads_clipping_profile.txt\n    touch mapping_quality_across_reference.txt\n    touch duplication_rate_histogram.txt\n    touch insert_size_across_reference.txt\n    touch mapped_reads_gc-content_distribution.txt\n    touch mapping_quality_histogram.txt\n    cd ../\n    touch genome_results.txt\n    touch qualimapReport.html\n    cd ../\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_BAMQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path gff\n    val use_gff\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = use_gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $args \\\\\n        -bam $bam \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/QUALIMAP_RNASEQ", "chelauk/nf-core-blasr/chelauk__nf-core-blasr/QUALIMAP_BAMQC", "raygozag/rnaseq/raygozag__rnaseq/QUALIMAP_RNASEQ", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/QUALIMAP_BAMQC", "nf-core/raredisease/nf-core__raredisease/QUALIMAP_BAMQC", "nf-core/modules/nf-core__modules/QUALIMAP_BAMQC", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/QUALIMAP_RNASEQ", "nf-core/modules/nf-core__modules/QUALIMAP_RNASEQ", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/QUALIMAP_RNASEQ", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/QUALIMAP_BAMQC", "nf-core/rnaseq/nf-core__rnaseq/QUALIMAP_RNASEQ"], "list_wf_names": ["raygozag/rnaseq", "chelauk/nf-core-blasr", "nf-core/raredisease", "vincenthhu/nf-core-westest", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 3, "tools": ["SAMtools", "QualiMap"], "nb_own": 3, "list_own": ["FAANG", "nf-core", "robinfchan"], "nb_wf": 3, "list_wf": ["methylseq", "bisulfite_align_nf", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "robinfchan", "drpatelh", "Jani-94"], "nb_contrib": 20, "codes": ["\nprocess qualimap {\n    tag \"$name\"\n    publishDir \"${params.outdir}/qualimap\", mode: params.publish_dir_mode\n\n    input:\n    set val(name), file(bam) from ch_bam_dedup_for_qualimap\n\n    output:\n    file \"${bam.baseName}_qualimap\" into ch_qualimap_results_for_multiqc\n\n    script:\n    gcref = params.genome.toString().startsWith('GRCh') ? '-gd HUMAN' : ''\n    gcref = params.genome.toString().startsWith('GRCm') ? '-gd MOUSE' : ''\n    def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n    def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n    \"\"\"\n    samtools sort $bam \\\\\n        -@ ${task.cpus} $sort_mem \\\\\n        -o ${bam.baseName}.sorted.bam\n    qualimap bamqc $gcref \\\\\n        -bam ${bam.baseName}.sorted.bam \\\\\n        -outdir ${bam.baseName}_qualimap \\\\\n        --collect-overlap-pairs \\\\\n        --java-mem-size=${task.memory.toGiga()}G \\\\\n        -nt ${task.cpus}\n    \"\"\"\n}", "\nprocess qualimap {\n    tag \"$name\"\n    publishDir \"${params.outdir}/qualimap\", mode: params.publish_dir_mode\n\n    input:\n    set val(name), file(bam) from ch_bam_dedup_for_qualimap\n\n    output:\n    file \"${bam.baseName}_qualimap\" into ch_qualimap_results_for_multiqc\n\n    script:\n    gcref = params.genome.toString().startsWith('GRCh') ? '-gd HUMAN' : ''\n    gcref = params.genome.toString().startsWith('GRCm') ? '-gd MOUSE' : ''\n    def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n    def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n    \"\"\"\n    samtools sort $bam \\\\\n        -@ ${task.cpus} $sort_mem \\\\\n        -o ${bam.baseName}.sorted.bam\n    qualimap bamqc $gcref \\\\\n        -bam ${bam.baseName}.sorted.bam \\\\\n        -outdir ${bam.baseName}_qualimap \\\\\n        --collect-overlap-pairs \\\\\n        --java-mem-size=${task.memory.toGiga()}G \\\\\n        -nt ${task.cpus}\n    \"\"\"\n}", " process qualimap {\n        if (params.custom_container) container \"${params.custom_container}\"\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/qualimap\", mode: 'copy', overwrite: true\n\n        input:\n        set val(name), file(bam) from ch_bam_dedup_for_qualimap\n\n        output:\n        file \"${bam.baseName}_qualimap\" into ch_qualimap_results_for_multiqc\n\n        script:\n        def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n        def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n        \"\"\"\n        samtools sort $bam \\\n            -@ ${task.cpus} $sort_mem \\\n            -o ${bam.baseName}.sorted.bam\n        qualimap bamqc -bam ${bam.baseName}.sorted.bam \\\n            -outdir ${bam.baseName}_qualimap \\\n            --collect-overlap-pairs \\\n            --java-mem-size=${task.memory.toGiga()}G \\\n            -nt ${task.cpus}\n        \"\"\"\n    }"], "list_proc": ["nf-core/methylseq/nf-core__methylseq/qualimap", "FAANG/GSM-pipeline/FAANG__GSM-pipeline/qualimap", "robinfchan/bisulfite_align_nf/robinfchan__bisulfite_align_nf/qualimap"], "list_wf_names": ["robinfchan/bisulfite_align_nf", "FAANG/GSM-pipeline", "nf-core/methylseq"]}, {"nb_reuse": 16, "tools": ["FastQC"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 15, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess GetUnmappedBamQualityReport {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: 'copy'\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), path(\"${idSample}_${idRun}.bam\")\n\n    output:\n        path(\"*.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBAMFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBAMFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBAMFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBAMFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBAMFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["nf-core/sarek/nf-core__sarek/FastQCBAM", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/FastQCBAM", "rmoran7/custom_sarek/rmoran7__custom_sarek/FastQCBAM", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GetUnmappedBamQualityReport", "cgpu/pgp-chronek/cgpu__pgp-chronek/FastQCBAM", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/FastQCBAM", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/FastQCBAM", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/FastQCBAM", "cgpu/sarek-mirror/cgpu__sarek-mirror/FastQCBAM", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/FastQCBAM", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/FastQCBAM", "rmoran7/dx_sarek/rmoran7__dx_sarek/FastQCBAM", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/FastQCBAM", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/FastQCBAM", "cgpu/haplosarek/cgpu__haplosarek/FastQCBAM", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/FastQCBAM"], "list_wf_names": ["cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror-cache", "cgpu/sarek-mirror", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 17, "tools": ["GATK"], "nb_own": 10, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "UCL-BLIC", "lifebit-ai"], "nb_wf": 16, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "sarek-mirror", "germline_somatic", "custom_sarek", "pgp-chronek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeVariantsFromGatk {\n    label 'withGatkContainer'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        tuple val(idPatient), val(idSample), val(variantCaller), file(intervalBed), file(gvcf)\n        file(dbsnp)\n        file(dbsnpIndex)\n        file(dict)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n    tuple val(idPatient), val(idSample), val(\"HaplotypeCaller\"), file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.vcf\")\n\n                                      \n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = isChannelActive(dbsnp) ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: !(params.noGVCF) && ('haplotypecaller' in tools)\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess RunGenotypeGVCFs {\n  tag {idSample + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSample, file(intervalBed), file(gvcf) from vcfsToGenotype\n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.dbsnp,\n      referenceMap.dbsnpIndex\n    ])\n\n  output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into hcGenotypedVCF\n\n  when: 'haplotypecaller' in tools && !params.onlyQC\n\n  script:\n                                                                                 \n  \"\"\"\n  gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    IndexFeatureFile -F ${gvcf}\n\n  gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    GenotypeGVCFs \\\n    -R ${genomeFile} \\\n    -L ${intervalBed} \\\n    -D ${dbsnp} \\\n    -V ${gvcf} \\\n    -O ${intervalBed.baseName}_${idSample}.vcf\n  \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: !(params.noGVCF) && ('haplotypecaller' in tools)\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}"], "list_proc": ["chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/GenotypeGVCFs", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GenotypeVariantsFromGatk", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/GenotypeGVCFs", "cgpu/haplosarek/cgpu__haplosarek/GenotypeGVCFs", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/GenotypeGVCFs", "cgpu/sarek-mirror/cgpu__sarek-mirror/GenotypeGVCFs", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/GenotypeGVCFs", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/GenotypeGVCFs", "rmoran7/custom_sarek/rmoran7__custom_sarek/GenotypeGVCFs", "nf-core/sarek/nf-core__sarek/GenotypeGVCFs", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GenotypeGVCFs", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/GenotypeGVCFs", "cgpu/pgp-chronek/cgpu__pgp-chronek/GenotypeGVCFs", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/RunGenotypeGVCFs", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/GenotypeGVCFs", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/GenotypeGVCFs", "rmoran7/dx_sarek/rmoran7__dx_sarek/GenotypeGVCFs"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "rmoran7/dx_sarek", "sickle-in-africa/saw.sarek", "UCL-BLIC/Sarek_v2.3.FIX1", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-mirror-cache"]}, {"nb_reuse": 1, "tools": ["Count"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "drpatelh", "PhilPalmer"], "nb_contrib": 15, "codes": ["process DADA2_QUALITY {\n    tag \"$meta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    path \"${meta}_qual_stats.pdf\"            , emit: pdf\n    tuple val(meta), path(\"*_qual_stats.tsv\"), emit: tsv\n    path \"versions.yml\"                      , emit: versions\n    path \"*.args.txt\"                        , emit: args\n    path \"*plotQualityProfile.txt\"           , emit: warning\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    suppressPackageStartupMessages(library(dada2))\n    suppressPackageStartupMessages(library(ShortRead))\n\n    readfiles <- sort(list.files(\".\", pattern = \".fastq.gz\", full.names = TRUE))\n\n    #make list of number of sequences\n    readfiles_length <- countLines(readfiles) / 4\n    sum_readfiles_length <- sum(readfiles_length)\n\n    #use only the first x files when read number gets above 2147483647, read numbers above that do not fit into an INT and crash the process!\n    if ( sum_readfiles_length > 2147483647 ) {\n        max_files = length(which(cumsum(readfiles_length) <= 2147483647 ))\n        write.table(max_files, file = paste0(\"WARNING Only \",max_files,\" of \",length(readfiles),\" files and \",sum(readfiles_length[1:max_files]),\" of \",sum_readfiles_length,\" reads were used for ${meta} plotQualityProfile.txt\"), row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n        readfiles <- readfiles[1:max_files]\n    } else {\n        max_files <- length(readfiles)\n        write.table(max_files, file = paste0(max_files,\" files were used for ${meta} plotQualityProfile.txt\"), row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    }\n\n    plot <- plotQualityProfile(readfiles$args)\n    data <- plot\\$data\n\n    #aggregate data for each sequencing cycle\n    df <- data.frame(\n        Count = aggregate(data\\$Count, list(data\\$Cycle), sum),\n        Median = aggregate(rep(data\\$Score, data\\$Count), list(rep(data\\$Cycle, data\\$Count)), median)[2]\n    )\n    colnames(df) <- c(\"Cycle\", \"Count\", \"Median\")\n\n    #write output\n    write.table( t(df), file = paste0(\"${meta}_qual_stats\",\".tsv\"), sep = \"\\t\", row.names = TRUE, col.names = FALSE, quote = FALSE)\n    pdf(paste0(\"${meta}_qual_stats\",\".pdf\"))\n    plot\n    dev.off()\n\n    write.table(paste0('plotQualityProfile\\t$args\\nmax_files\\t',max_files), file = \"plotQualityProfile.args.txt\", row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")),paste0(\"    ShortRead: \", packageVersion(\"ShortRead\")) ), \"versions.yml\")\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/DADA2_QUALITY"], "list_wf_names": ["nf-core/ampliseq"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process KALLISTO_INDEX {\n    tag \"$fasta\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::kallisto=0.46.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/kallisto:0.46.2--h4f7b962_1' :\n        'quay.io/biocontainers/kallisto:0.46.2--h4f7b962_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"kallisto\" , emit: idx\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    kallisto \\\\\n        index \\\\\n        $args \\\\\n        -i kallisto \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kallisto: \\$(echo \\$(kallisto 2>&1) | sed 's/^kallisto //; s/Usage.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/KALLISTO_INDEX"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["RAxML-NG"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bactmap"], "list_contrib": ["alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh"], "nb_contrib": 8, "codes": ["\nprocess RAXMLNG {\n    label 'process_high'\n    label 'process_long'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.0.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/raxml-ng:1.0.2--h7447c1b_0\"\n    } else {\n        container \"quay.io/biocontainers/raxml-ng:1.0.2--h7447c1b_0\"\n    }\n\n    input:\n    path alignment\n\n    output:\n    path \"*.raxml.bestTree\", emit: phylogeny\n    path \"*.raxml.support\" , optional:true, emit: phylogeny_bootstrapped\n    path \"*.version.txt\"   , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    raxml-ng \\\\\n        $options.args \\\\\n        --msa $alignment \\\\\n        --threads $task.cpus \\\\\n        --prefix output\n\n    echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bactmap/nf-core__bactmap/RAXMLNG"], "list_wf_names": ["nf-core/bactmap"]}, {"nb_reuse": 1, "tools": ["Count"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process CELLRANGER_COUNT {\n    tag \"$meta.gem\"\n    label 'process_high'\n\n    if (params.enable_conda) {\n        exit 1, \"Conda environments cannot be used when using the Cell Ranger tool. Please use docker or singularity containers.\"\n    }\n    container \"nfcore/cellranger:6.1.2\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  reference\n\n    output:\n    path(\"sample-${meta.gem}/outs/*\"), emit: outs\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def sample_arg = meta.samples.unique().join(\",\")\n    def reference_name = reference.name\n    \"\"\"\n    cellranger \\\\\n        count \\\\\n        --id='sample-${meta.gem}' \\\\\n        --fastqs=. \\\\\n        --transcriptome=$reference_name \\\\\n        --sample=$sample_arg \\\\\n        --localcores=$task.cpus \\\\\n        --localmem=${task.memory.toGiga()} \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cellranger: \\$(echo \\$( cellranger --version 2>&1) | sed 's/^.*[^0-9]\\\\([0-9]*\\\\.[0-9]*\\\\.[0-9]*\\\\).*\\$/\\\\1/' )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    mkdir -p \"sample-${meta.gem}/outs/\"\n    touch sample-${meta.gem}/outs/fake_file.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cellranger: \\$(echo \\$( cellranger --version 2>&1) | sed 's/^.*[^0-9]\\\\([0-9]*\\\\.[0-9]*\\\\.[0-9]*\\\\).*\\$/\\\\1/' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/CELLRANGER_COUNT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 4, "tools": ["preseq"], "nb_own": 4, "list_own": ["lengfei5", "clairecoleman1", "nf-core", "oisinmccaffrey"], "nb_wf": 4, "list_wf": ["clipseq.nextflow", "atacseq_nf", "clipseq", "clipseq1"], "list_contrib": ["mashehu", "nf-core-bot", "lengfei5", "ewels", "maxulysse", "drewjbeh", "apeltzer", "ggabernet", "charlotte-west", "amchakra", "oisinmccaffrey", "jinmingda", "drpatelh", "clairecoleman1", "CharlotteAnne"], "nb_contrib": 15, "codes": ["\nprocess preseq {\n    tag \"$name\"\n    publishDir \"${params.outdir}/preseq\", mode: 'copy'\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_aligned_preseq\n\n    output:\n    path '*.ccurve.txt' into ch_preseq_mqc\n    path '*.log'\n\n    script:\n    \"\"\"\n    preseq lc_extrap \\\\\n        -output ${name}.ccurve.txt \\\\\n        -verbose \\\\\n        -bam \\\\\n        -seed 42 \\\\\n        $bam\n    cp .command.err ${name}.command.log\n    \"\"\"\n}", "\nprocess MERGED_LIB_PRESEQ {\n    tag \"$name\"\n    label 'process_low'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/preseq\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_preseq\n\n    input:\n    tuple val(name), path(bam) from ch_mlib_bam_preseq\n\n    output:\n    path '*.ccurve.txt' into ch_mlib_preseq_mqc\n    path '*.log'\n\n    script:\n    prefix = \"${name}.mLb.mkD\"\n    pe = params.single_end ? '' : '-pe'\n    \"\"\"\n    preseq lc_extrap \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        -verbose \\\\\n        -bam \\\\\n        $pe \\\\\n        -seed 1 \\\\\n        ${bam[0]}\n    cp .command.err ${prefix}.command.log\n    \"\"\"\n}", "\nprocess preseq {\n    tag \"$name\"\n    label 'process_low'\n    publishDir \"${params.outdir}/preseq\", mode: params.publish_dir_mode\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_aligned_preseq\n\n    output:\n    path '*.ccurve.txt' into ch_preseq_mqc\n    path '*.log'\n\n    script:\n    \"\"\"\n    preseq lc_extrap \\\\\n        -output ${name}.ccurve.txt \\\\\n        -verbose \\\\\n        -bam \\\\\n        -seed 42 \\\\\n        $bam\n    cp .command.err ${name}.command.log\n    \"\"\"\n}", "\nprocess preseq {\n    tag \"$name\"\n    publishDir \"${params.outdir}/preseq\", mode: 'copy'\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_aligned_preseq\n\n    output:\n    path '*.ccurve.txt' into ch_preseq_mqc\n    path '*.log'\n\n    script:\n    \"\"\"\n    preseq lc_extrap \\\\\n        -output ${name}.ccurve.txt \\\\\n        -verbose \\\\\n        -bam \\\\\n        -seed 42 \\\\\n        $bam\n    cp .command.err ${name}.command.log\n    \"\"\"\n}"], "list_proc": ["clairecoleman1/clipseq1/clairecoleman1__clipseq1/preseq", "lengfei5/atacseq_nf/lengfei5__atacseq_nf/MERGED_LIB_PRESEQ", "nf-core/clipseq/nf-core__clipseq/preseq", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/preseq"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "lengfei5/atacseq_nf", "nf-core/clipseq"]}, {"nb_reuse": 2, "tools": ["QIIME", "BioMe"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_EXPORT_ABSOLUTE {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n    path(repseq)\n    path(taxonomy)\n    val(tax_agglom_min)\n    val(tax_agglom_max)\n\n    output:\n    path(\"rep-seq.fasta\")            , emit: fasta\n    path(\"feature-table.tsv\")        , emit: tsv\n    path(\"feature-table.biom\")       , emit: biom\n    path(\"seven_number_summary.tsv\") , emit: summary\n    path(\"descriptive_stats.tsv\")    , emit: descr\n    path(\"abs-abund-table-*.tsv\")    , emit: abundtable\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #produce raw count table in biom format \"table/feature-table.biom\"\n    qiime tools export --input-path ${table}  \\\n        --output-path table\n    cp table/feature-table.biom .\n\n    #produce raw count table \"table/feature-table.tsv\"\n    biom convert -i table/feature-table.biom \\\n        -o feature-table.tsv  \\\n        --to-tsv\n\n    #produce representative sequence fasta file \"sequences.fasta\"\n    qiime feature-table tabulate-seqs  \\\n        --i-data ${repseq}  \\\n        --o-visualization rep-seqs.qzv\n    qiime tools export --input-path rep-seqs.qzv  \\\n        --output-path representative_sequences\n    cp representative_sequences/sequences.fasta rep-seq.fasta\n    cp representative_sequences/*.tsv .\n\n    ##on several taxa level\n    array=(\\$(seq ${tax_agglom_min} 1 ${tax_agglom_max}))\n    for i in \\${array[@]}\n    do\n        #collapse taxa\n        qiime taxa collapse \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-level \\$i \\\n            --o-collapsed-table table-\\$i.qza\n        #export to biom\n        qiime tools export --input-path table-\\$i.qza \\\n            --output-path table-\\$i\n        #convert to tab separated text file\n        biom convert \\\n            -i table-\\$i/feature-table.biom \\\n            -o abs-abund-table-\\$i.tsv --to-tsv\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_EXPORT_ABSOLUTE {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n    path(repseq)\n    path(taxonomy)\n    val(tax_agglom_min)\n    val(tax_agglom_max)\n\n    output:\n    path(\"rep-seq.fasta\")            , emit: fasta\n    path(\"feature-table.tsv\")        , emit: tsv\n    path(\"feature-table.biom\")       , emit: biom\n    path(\"seven_number_summary.tsv\") , emit: summary\n    path(\"descriptive_stats.tsv\")    , emit: descr\n    path(\"abs-abund-table-*.tsv\")    , emit: abundtable\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #produce raw count table in biom format \"table/feature-table.biom\"\n    qiime tools export --input-path ${table}  \\\n        --output-path table\n    cp table/feature-table.biom .\n\n    #produce raw count table \"table/feature-table.tsv\"\n    biom convert -i table/feature-table.biom \\\n        -o feature-table.tsv  \\\n        --to-tsv\n\n    #produce representative sequence fasta file \"sequences.fasta\"\n    qiime feature-table tabulate-seqs  \\\n        --i-data ${repseq}  \\\n        --o-visualization rep-seqs.qzv\n    qiime tools export --input-path rep-seqs.qzv  \\\n        --output-path representative_sequences\n    cp representative_sequences/sequences.fasta rep-seq.fasta\n    cp representative_sequences/*.tsv .\n\n    ##on several taxa level\n    array=(\\$(seq ${tax_agglom_min} 1 ${tax_agglom_max}))\n    for i in \\${array[@]}\n    do\n        #collapse taxa\n        qiime taxa collapse \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-level \\$i \\\n            --o-collapsed-table table-\\$i.qza\n        #export to biom\n        qiime tools export --input-path table-\\$i.qza \\\n            --output-path table-\\$i\n        #convert to tab separated text file\n        biom convert \\\n            -i table-\\$i/feature-table.biom \\\n            -o abs-abund-table-\\$i.tsv --to-tsv\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_EXPORT_ABSOLUTE", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_EXPORT_ABSOLUTE"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["AIVAR"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["modules", "test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process IVAR_TRIM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path bed\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path('*.log'), emit: log\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    ivar trim \\\\\n        $args \\\\\n        -i $bam \\\\\n        -b $bed \\\\\n        -p $prefix \\\\\n        > ${prefix}.ivar.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_TRIM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path bed\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path('*.log'), emit: log\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    ivar trim \\\\\n        $args \\\\\n        -i $bam \\\\\n        -b $bed \\\\\n        -p $prefix \\\\\n        > ${prefix}.ivar.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process IVAR_TRIM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ivar=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ivar:1.3.1--h089eab3_0' :\n        'quay.io/biocontainers/ivar:1.3.1--h089eab3_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path bed\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path('*.log'), emit: log\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    ivar trim \\\\\n        $args \\\\\n        -i $bam \\\\\n        -b $bed \\\\\n        -p $prefix \\\\\n        > ${prefix}.ivar.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ivar: \\$(echo \\$(ivar version 2>&1) | sed 's/^.*iVar version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/IVAR_TRIM", "nf-core/viralrecon/nf-core__viralrecon/IVAR_TRIM", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/IVAR_TRIM"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess genotyping_hc {\n  label 'mc_small'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/genotyping\", mode: params.publish_dir_mode\n\n  when:\n  params.run_genotyping && params.genotyping_tool == 'hc'\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_damagemanipulation_for_genotyping_hc\n  file fasta from ch_fasta_for_genotyping_hc.collect()\n  file fai from ch_fai_for_hc.collect()\n  file dict from ch_dict_for_hc.collect()\n\n  output: \n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*vcf.gz\") into ch_hc_for_bcftools_stats\n\n  script:\n  if (!params.gatk_dbsnp)\n    \"\"\"\n    gatk HaplotypeCaller --java-options \"-Xmx${task.memory.toGiga()}G\" -R ${fasta} -I ${bam} -O ${samplename}.haplotypecaller.vcf -stand-call-conf ${params.gatk_call_conf} --sample-ploidy ${params.gatk_ploidy} --output-mode ${params.gatk_hc_out_mode} --emit-ref-confidence ${params.gatk_hc_emitrefconf}\n    bgzip -@ ${task.cpus} ${samplename}.haplotypecaller.vcf\n    \"\"\"\n\n  else if (params.gatk_dbsnp)\n    \"\"\"\n    gatk HaplotypeCaller --java-options \"-Xmx${task.memory.toGiga()}G\" -R ${fasta} -I ${bam} -O ${samplename}.haplotypecaller.vcf --dbsnp ${params.gatk_dbsnp} -stand-call-conf ${params.gatk_call_conf} --sample_ploidy ${params.gatk_ploidy} --output_mode ${params.gatk_hc_out_mode} --emit-ref-confidence ${params.gatk_hc_emitrefconf}\n    bgzip -@  ${task.cpus} ${samplename}.haplotypecaller.vcf\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/genotyping_hc"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["bedGraphToBigWig"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nanoseq"], "list_contrib": ["lwratten", "alneberg", "nf-core-bot", "ewels", "csawye01", "maxulysse", "KevinMenden", "cying111", "drpatelh", "yuukiiwa"], "nb_contrib": 10, "codes": ["\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'meta.id') }\n\n    conda     (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    } else {\n        container \"quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    }\n\n    input:\n    tuple val(meta), path(sizes), path(bedgraph)\n\n    output:\n    tuple val(meta), path(sizes), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    \"\"\"\n    bedGraphToBigWig $bedgraph $sizes ${meta.id}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo $VERSION)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/UCSC_BEDGRAPHTOBIGWIG"], "list_wf_names": ["nf-core/nanoseq"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess recalSNPs {\n    tag \"${name}\"\n    publishDir \"${params.outdir}/GATK_RecalibrateSNPs/\", mode: 'copy', \n    saveAs: {filename -> params.saveIntermediateVariants ? \"$filename\" : null }\n\n    input:\n    set val(name), file(raw_snp), file(raw_snp_idx) from raw_snp\n\n    output:\n    set val(name), file(\"${sample}_filtered_snp.vcf\"), file(\"${sample}_filtered_snp.vcf.idx\") into filtered_snp\n\n    script:\n    \"\"\"\n    gatk -T VariantRecalibrator \\\\\n        -R $params.gfasta \\\\\n        --input $raw_snp \\\\\n        --maxGaussians 4 \\\\\n        --recal_file ${name}_snp.recal \\\\\n        --tranches_file ${name}_snp.tranches \\\\\n        -resource:omni,known=false,training=true,truth=true,prior=12.0 $params.omni \\\\\n        -resource:1000G,known=false,training=true,truth=false,prior=10.0 $params.thousandg \\\\\n        -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 $params.dbsnp \\\\\n        --mode SNP \\\\\n        -an QD \\\\\n        -an FS \\\\\n        -an MQ\n\n    gatk -T ApplyRecalibration \\\\\n        -R $params.gfasta \\\\\n        --out ${name}_filtered_snp.vcf \\\\\n        --input $raw_snp \\\\\n        --mode SNP \\\\\n        --tranches_file ${name}_snp.tranches \\\\\n        --recal_file ${name}_snp.recal \\\\\n        --ts_filter_level 99.5 \\\\\n        -mode SNP \n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/recalSNPs"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    validExitStatus 0,1,143\n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/multiqc/\", mode: 'copy', pattern: \"multiqc_report.html\"\n    publishDir \"${params.outdir}/multiqc/\", mode: 'copy', pattern: \"*_data\"\n\n    when:\n    !params.skipMultiQC\n\n    input:\n    file multiqc_config from ch_multiqc_config.collect()\n    file (fastqc:'qc/fastqc/*') from fastqc_results.collect()\n    file ('qc/fastqc/*') from trimmed_fastqc_results.collect()\n    file ('qc/trimstats/*') from trim_stats.collect()\n    file ('qc/mapstats/*') from bam_flagstat.collect()\n    file ('qc/rseqc/*') from rseqc_results.collect()\n    file ('qc/preseq/*') from preseq_results.collect()\n    file ('software_versions/*') from software_versions_yaml\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\" into multiqc_report_files\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n\n\n    \"\"\"\n    export PATH=~/.local/bin:$PATH\n\n    multiqc . -f $rtitle $rfilename --config $multiqc_config\n    \"\"\"\n}"], "list_proc": ["nf-core/nascent/nf-core__nascent/multiqc"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 2, "tools": ["PAIR"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process SEQKIT_PAIR {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::seqkit=2.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqkit:2.1.0--h9ee0642_0':\n        'quay.io/biocontainers/seqkit:2.1.0--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.paired.fastq.gz\")                  , emit: reads\n    tuple val(meta), path(\"*.unpaired.fastq.gz\"), optional: true, emit: unpaired_reads\n    path \"versions.yml\"                                         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    seqkit \\\\\n        pair \\\\\n        -1 ${reads[0]} \\\\\n        -2 ${reads[1]} \\\\\n        $args \\\\\n        --threads $task.cpus\n\n    # gzip fastq\n    find . -maxdepth 1 -name \"*.fastq\" -exec gzip {} \\;\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqkit: \\$( seqkit | sed '3!d; s/Version: //' )\n    END_VERSIONS\n    \"\"\"\n}", "process SEQKIT_PAIR {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::seqkit=2.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqkit:2.1.0--h9ee0642_0':\n        'quay.io/biocontainers/seqkit:2.1.0--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.paired.fastq.gz\")                  , emit: reads\n    tuple val(meta), path(\"*.unpaired.fastq.gz\"), optional: true, emit: unpaired_reads\n    path \"versions.yml\"                                         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    seqkit \\\\\n        pair \\\\\n        -1 ${reads[0]} \\\\\n        -2 ${reads[1]} \\\\\n        $args \\\\\n        --threads $task.cpus\n\n    # gzip fastq\n    find . -maxdepth 1 -name \"*.fastq\" -exec gzip {} \\;\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqkit: \\$( seqkit | sed '3!d; s/Version: //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SEQKIT_PAIR", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SEQKIT_PAIR"], "list_wf_names": ["CDCgov/mycosnp-nf", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["MultiQC", "SAMtools"], "nb_own": 2, "list_own": ["keng404", "nf-core"], "nb_wf": 2, "list_wf": ["nextflow_test", "hlatyping"], "list_contrib": ["nvk747", "KochTobi", "keng404", "nf-core-bot", "ewels", "sven1103", "maxulysse", "ggabernet", "apeltzer", "pditommaso", "christopher-mohr"], "nb_contrib": 11, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    yara_mapper --help  &> v_yara.txt 2>&1 || true\n    cat \\$(which OptiTypePipeline.py) &> v_optitype.txt 2>&1 ||\u00a0true\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    yara_mapper --help  &> v_yara.txt 2>&1 || true\n    cat \\$(which OptiTypePipeline.py) &> v_optitype.txt 2>&1 ||\u00a0true\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/hlatyping/nf-core__hlatyping/get_software_versions", "keng404/nextflow_test/keng404__nextflow_test/get_software_versions"], "list_wf_names": ["nf-core/hlatyping", "keng404/nextflow_test"]}, {"nb_reuse": 7, "tools": ["SAMtools", "SAMBLASTER"], "nb_own": 6, "list_own": ["Genomic-Medicine-Linkoping", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 7, "list_wf": ["saw.sarek", "sarek_ubec", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 23, "codes": ["\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/sarek/nf-core__sarek/GroupReadsByUmi", "rmoran7/custom_sarek/rmoran7__custom_sarek/GroupReadsByUmi", "rmoran7/dx_sarek/rmoran7__dx_sarek/GroupReadsByUmi", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/GroupReadsByUmi", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GroupReadsByUmi", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/GroupReadsByUmi", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/GroupReadsByUmi"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process PICARD_COLLECTWGSMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*_metrics\"), emit: metrics\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectWgsMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectWgsMetrics \\\\\n        $args \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.CollectWgsMetrics.coverage_metrics \\\\\n        --REFERENCE_SEQUENCE $fasta\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CollectWgsMetrics --version 2>&1 | grep -o 'Version.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_COLLECTWGSMETRICS"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 29, "tools": ["BWA"], "nb_own": 24, "list_own": ["lifebit-ai", "GMS6804-master", "ABMicroBioinf", "MarieGurke", "melnel000", "UCL-BLIC", "didillysquat", "marcocrotti", "chelauk", "nibscbioinformatics", "nf-core", "jamez-eh", "TheJacksonLaboratory", "bcgsc", "DLBPointon", "esrice", "SergFern", "jianhong", "StaPH-B", "njspix", "palfalvi", "cgpu", "MaddalenaCella", "jambler24"], "nb_wf": 29, "list_wf": ["TMBur", "Sarek_CBIO", "kopp_2021_fowl", "nanoporeseq", "nf-fh-pcp-wes-align", "humgen", "nfmap", "radseq-processing-nf", "haplosarek", "sarek-mirror-cache", "bac_pangenome", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "Sarek", "nf-tb", "pgp-chronek", "sarek-mirror", "Variant_Calling", "test_nextflow_sarek", "sarek-genomechronicler", "nextflowTutorial", "mycosnp", "mmrSVD", "GenomeChronicler-Sarek-nf", "exoseq", "ref_genomes", "workflows", "nf-lcWGS-mapping-and-imputation", "annotation-pipeline-nextflow"], "list_contrib": ["alneberg", "RichardCorbett", "arontommi", "ewels", "maxulysse", "pallolason", "szilvajuhos", "MarieGurke", "Sebastian-D", "bleazard", "pditommaso", "didillysquat", "xiaoli-dong", "marcocrotti", "chelauk", "marcelm", "senthil10", "jamez-eh", "malinlarsson", "BrianSanderson", "DLBPointon", "esrice", "SergFern", "jianhong", "njspix", "J35P312", "jongtaek-kim", "palfalvi", "lescai", "cgpu", "apeltzer", "waffle-iron", "MaddalenaCella", "LeuThrAsp", "jtk622", "jambler24"], "nb_contrib": 36, "codes": ["\nprocess index_ref {\n\n  label 'RAM_high'\n\n  input:\n  file(ref) from ref_index_ch\n\n  output:\n  tuple file(ref), file('*') into ref_mapPE_ch\n  tuple file(ref), file('*') into ref_mapSE_ch\n\n  script:\n  \"\"\"\n    bwa index $ref\n  \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.*\") into bwaIndexes\n\n  when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n  script:\n  \"\"\"\n  bwa index ${fasta}\n  \"\"\"\n}", "\nprocess genome_index {\n\n\ttag \"$genome\"\n\tpublishDir params.resultGenome, mode: params.saveMode\n\t\n\tinput:\n\tfile genome \n\t\n\toutput:\n\tpath '*' into genome_index_ch\n\t\n\tscript:\n\t\n\t\"\"\"\n\tbwa index ${genome}\t\n\t\"\"\"\n\n}", " process bwa_indexing{\n            container 'biocontainers/bwa:v0.7.17_cv1'\n            if (workflow.containerEngine == 'docker'){\n                containerOptions '-u $(id -u):$(id -g)'\n            }\n            cpus params.mapping_threads\n\n            input:\n            path ref_genome from params.ref\n\n            output:\n            tuple path(ref_genome), path(\"*{.sa,.amb,.ann,.pac,.bwt}\") into ngm_index_ch\n\n            script:\n            \"\"\"\n            bwa index ${ref_genome}\n            \"\"\"\n        }", "\nprocess bwa_index_reference {\n\ttag \"$fasta\"\n\tinput:\n\t\tpath(fasta) \n\n\toutput:\n\t\tpath(\"${fasta}.*\") \n\t\t\n\tscript:\n\t\"\"\"\n\t\t/usr/TMB/bwa index ${fasta}\n\t\"\"\"\n}", "\nprocess indexRefGenome {\n                                                           \n    input:\n      path genome from params.ref_genome\n    output:\n      file '*' into bwa_index\n\n    script:\n    \"\"\"\n    bwa index ${genome}\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.*\") into bwaIndexes\n\n    when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess bwa_index {\n    executor 'pbs'\n    queue params.queue\n    cpus 1\n    time '72h'\n    conda params.conda_path+'/index'\n    publishDir \"${params.out_dir}/${id}/bwa\", mode: 'copy'\n\n    input:\n        tuple val(id), path(ref)\n\n    output:\n        path(\"*\", includeInputs: true)\n\n    script:\n    \"\"\"\n        bwa index ${ref}\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.*\") into bwaIndexes\n\n  when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n  script:\n  \"\"\"\n  bwa index ${fasta}\n  \"\"\"\n}", "process bwa_index {\n\n  label 'small_job'\n\n  conda \"$baseDir/conda-envs/bwa-samtools-env.yaml\"\n\n                                                                \n\n  input:\n    path assembly\n    val options\n\n  output:\n    path \"${assembly}.*\", emit: index\n\n  script:\n    \"\"\"\n    bwa index $options ${assembly}\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.*\") into bwa_built\n\n    when: !(params.bwa) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaForBWA\n\n  output:\n    file(\"*.{amb,ann,bwt,pac,sa}\") into bwaIndexes\n\n  script:\n\n  \"\"\"\n  bwa index ${f_reference}\n  \"\"\"\n}", "\nprocess PREPARE_GENOME {\n  publishDir \"${params.outdir}/${params.options.publish_dir}\", mode: 'copy'\n  conda (params.conda ? \"bioconda::bwa=0.7.17\" : null)\n\n  input:\n    path genome\n  output:\n    tuple path(genome), path(\"*\"), emit: bwa_index\n\n  script:\n\n  \"\"\"\n  bwa index $genome\n  \"\"\"\n}", "\nprocess bwa_index {\n    tag \"BWA_INDEX on $ref\"\n    publishDir \"${params.outdir}/data\", mode: 'copy'\n\n    input:\n    path(ref)\n\n    output:\n    tuple path(\"ref.fna.sa\"), path(\"ref.fna.bwt\"), path(\"ref.fna.pac\"), path(\"ref.fna.amb\"), path(\"ref.fna.ann\")                                                                         \n\n    script:\n    \"\"\"\n    bwa index ${ref}\n    \"\"\"\n}", "\tprocess BuildBWAindexes {\n\t\ttag \"${fasta}\"\n\t\tlabel 'bwa'\n\t\tpublishDir params.outdir, mode: 'copy'\n\t\tinput:\n\t\t\tfile(fasta) from ch_fasta\n\t\toutput:\n\t\t\tfile(\"${fasta}.*\") into bwa_built\n\t\twhen: !(params.bwa) && params.fasta && params.fastq1\n\t\tscript:\n\t\t\"\"\"\n\t\tbwa index ${fasta}\n\t\t\"\"\"\n\t}", "\nprocess BuildBWAindexes {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.*\") into bwaIndexes\n\n  when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n  script:\n  \"\"\"\n  bwa index ${fasta}\n  \"\"\"\n}", "\nprocess BuildBWAindexes {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.*\") into bwaIndexes\n\n    when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess prepare_genome_bwa {\n  tag \"$genome.baseName\"\n\n  input:\n      file genome from genome_file\n  output:\n      file \"${genome}.amb\" into genome_bwa_amb\n      file \"${genome}.ann\" into genome_bwa_ann\n      file \"${genome}.bwt\" into genome_bwa_bwt\n      file \"${genome}.pac\" into genome_bwa_pac\n      file \"${genome}.sa\" into genome_bwa_sa\n\n  script:\n  \"\"\"\n  bwa index $genome\n  \"\"\"\n}", "\nprocess BuildBWAindexes {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.*\") into bwaIndexes\n\n    when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    bwa index ${fasta}\n    \"\"\"\n}", " process makeBWAIndex {\n        tag \"$params.gfasta\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/reference_genome\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta from fasta_for_bwa_index\n\n        output:\n        file \"*.{amb,ann,bwt,pac,sa}\" into bwa_index\n\n        script:\n        \"\"\"\n        bwa index $fasta\n        \"\"\"\n    }", "\nprocess bwa_index {\n\tcontainer \"fredhutch/bwa:0.7.17\"\n\terrorStrategy 'retry'\n\tmaxRetries 30\n\tlabel 'small'\n\n\tinput:\n        file combined_reference\n\n     \toutput:\n\t    file \"*.{amb,ann,bwt,pac,sa,alt}\"\n\n        script:\n        \"\"\"\n        bwa index $combined_reference\n        \"\"\"\t\n}", "\nprocess BuildBWAindexes {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaForBWA\n\n  output:\n    file(\"*.{amb,ann,bwt,pac,sa}\") into bwaIndexes\n\n  script:\n\n  \"\"\"\n  bwa index ${f_reference}\n  \"\"\"\n}", "\nprocess bwaIndex {\n    publishDir \"/home/ldotrang/C_auris_testing/nfBWAref/bwa_reference_output/bwa_index\", mode: 'copy'\n    input:\n    file (reference) from reference_seq3\n\n                                                             \n                                          \n                                           \n    output:\n    file(\"*\") into bwa_index\n\n    shell:\n    \"\"\"\n    bwa index ${reference}\n    \"\"\"\n}", "\nprocess bwa_index {\n    publishDir 'bwa_index'\n    module 'bwa/bwa-0.7.17'\n\n    input:\n    file reference from reference_file\n\n    output:\n    file \"${reference}.*\" into reference_index\n\n    \"\"\" bwa index ${reference} \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.*\") into bwaIndexes\n\n  when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n  script:\n  \"\"\"\n  bwa index ${fasta}\n  \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.*\") into bwaIndexes\n\n  when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n  script:\n  \"\"\"\n  bwa index ${fasta}\n  \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: params.publishDirMode\n\n  input:\n    file(f_reference) from ch_fastaForBWA\n\n  output:\n    file(\"*.{amb,ann,bwt,pac,sa}\") into bwaIndexes\n\n  script:\n  \"\"\"\n  bwa index ${f_reference}\n  \"\"\"\n}", "\nprocess index {\n    label 'index'\n    publishDir params.index, mode:'copy'\n    \n    input:\n        path reference from params.reference\n     \n    output:\n        file \"*.{amb,ann,bwt,pac,sa}\" into index_ch\n\n    script:       \n    \"\"\"\n    bwa index $reference \n    \"\"\"\n}", " process Custom_genome_indexing {\n    tag \"Indexes reference file using the specified aligner\"\n    label 'big_mem'\n    label 'generic'\n\n    publishDir \"data/custom_reference\", mode: 'copy'\n\n    input:\n    file reference_file from ch_reference\n\n    output:\n    file \"*\"\n\n    script:\n\n    if(params.aln == 'bwa'){\n\n    \"\"\"\n      bwa index ${reference_file[0]}\n    \"\"\"\n\n    }else if(params.aln == 'bowtie2'){\n\n    \"\"\"\n      bowtie2-build ${reference_file[0]} ${prefixRef}.bowtie2\n    \"\"\"\n    }\n  }"], "list_proc": ["MarieGurke/nfmap/MarieGurke__nfmap/index_ref", "cgpu/sarek-mirror/cgpu__sarek-mirror/BuildBWAindexes", "marcocrotti/radseq-processing-nf/marcocrotti__radseq-processing-nf/genome_index", "didillysquat/kopp_2021_fowl/didillysquat__kopp_2021_fowl/bwa_indexing", "bcgsc/TMBur/bcgsc__TMBur/bwa_index_reference", "ABMicroBioinf/nf-tb/ABMicroBioinf__nf-tb/indexRefGenome", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BuildBWAindexes", "njspix/ref_genomes/njspix__ref_genomes/bwa_index", "cgpu/pgp-chronek/cgpu__pgp-chronek/BuildBWAindexes", "palfalvi/nanoporeseq/palfalvi__nanoporeseq/bwa_index", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BuildBWAindexes", "melnel000/Sarek_CBIO/melnel000__Sarek_CBIO/BuildBWAindexes", "jianhong/nextflowTutorial/jianhong__nextflowTutorial/PREPARE_GENOME", "DLBPointon/annotation-pipeline-nextflow/DLBPointon__annotation-pipeline-nextflow/bwa_index", "TheJacksonLaboratory/mmrSVD/TheJacksonLaboratory__mmrSVD/BuildBWAindexes", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/BuildBWAindexes", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BuildBWAindexes", "jambler24/bac_pangenome/jambler24__bac_pangenome/prepare_genome_bwa", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/BuildBWAindexes", "nf-core/exoseq/nf-core__exoseq/makeBWAIndex", "jamez-eh/nf-fh-pcp-wes-align/jamez-eh__nf-fh-pcp-wes-align/bwa_index", "GMS6804-master/Sarek/GMS6804-master__Sarek/BuildBWAindexes", "StaPH-B/mycosnp/StaPH-B__mycosnp/bwaIndex", "esrice/workflows/esrice__workflows/bwa_index", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/BuildBWAindexes", "cgpu/haplosarek/cgpu__haplosarek/BuildBWAindexes", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/BuildBWAindexes", "MaddalenaCella/nf-lcWGS-mapping-and-imputation/MaddalenaCella__nf-lcWGS-mapping-and-imputation/index", "SergFern/Variant_Calling/SergFern__Variant_Calling/Custom_genome_indexing"], "list_wf_names": ["ABMicroBioinf/nf-tb", "GMS6804-master/Sarek", "didillysquat/kopp_2021_fowl", "lifebit-ai/GenomeChronicler-Sarek-nf", "cgpu/PGP-UK-sarek", "MaddalenaCella/nf-lcWGS-mapping-and-imputation", "cgpu/sarek-mirror", "MarieGurke/nfmap", "bcgsc/TMBur", "cgpu/haplosarek", "njspix/ref_genomes", "chelauk/test_nextflow_sarek", "esrice/workflows", "SergFern/Variant_Calling", "jianhong/nextflowTutorial", "jambler24/bac_pangenome", "cgpu/sarek-mirror-cache", "marcocrotti/radseq-processing-nf", "cgpu/sarek-genomechronicler", "StaPH-B/mycosnp", "cgpu/pgp-chronek", "jamez-eh/nf-fh-pcp-wes-align", "DLBPointon/annotation-pipeline-nextflow", "palfalvi/nanoporeseq", "melnel000/Sarek_CBIO", "UCL-BLIC/Sarek_v2.3.FIX1", "TheJacksonLaboratory/mmrSVD", "nf-core/exoseq", "nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "drpatelh", "PhilPalmer"], "nb_contrib": 15, "codes": ["process QIIME2_DIVERSITY_ADONIS {\n    tag \"${core.baseName} - ${params.qiime_adonis_formula}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core)\n\n    output:\n    path(\"adonis/*\")     , emit: html\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def formula = params.qiime_adonis_formula\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime diversity adonis \\\\\n        --p-n-jobs $task.cpus \\\\\n        --i-distance-matrix ${core} \\\\\n        --m-metadata-file ${metadata} \\\\\n        --o-visualization ${core.baseName}_adonis.qzv \\\\\n        $args \\\\\n        --p-formula \"${formula}\"\n    qiime tools export --input-path ${core.baseName}_adonis.qzv \\\\\n        --output-path adonis/${core.baseName}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_DIVERSITY_ADONIS"], "list_wf_names": ["nf-core/ampliseq"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process GATK4_GATHERPILEUPSUMMARIES {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n\n    input:\n    tuple val(meta), path(pileup)\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.pileupsummaries.table\"), emit: table\n    path \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = pileup.collect{ \"--I $it\" }.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GatherPileupSummaries] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GatherPileupSummaries \\\\\n        $input_list \\\\\n        --O ${prefix}.pileupsummaries.table \\\\\n        --sequence-dictionary $dict \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_GATHERPILEUPSUMMARIES"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 9, "tools": ["BiocStyle"], "nb_own": 8, "list_own": ["erikrikarddaniel", "ABMicroBioinf", "nf-core", "asthara10", "bactopia", "xiaoli-dong", "PGScatalog", "LNUc-EEMiS"], "nb_wf": 8, "list_wf": ["pathogen", "magph", "magmap", "pgsc_calc", "metatdenovo", "nanoseq", "bactopia", "insertseq"], "list_contrib": ["yuukiiwa", "alneberg", "ewels", "Mxrcon", "maxulysse", "rpetit3", "lwratten", "erikrikarddaniel", "nf-core-bot", "nebfield", "fmaguire", "csawye01", "Accio", "cying111", "xiaoli-dong", "emnilsson", "KevinMenden", "asthara10", "TGotwig", "drpatelh", "smlmbrt"], "nb_contrib": 21, "codes": ["\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.safe_load(f) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "process DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n    import yaml\n    import platform\n    from textwrap import dedent\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n    module_versions = {}\n    module_versions[\"${task.process.tokenize(':').last()}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    \n    script:\n    println(versions)\n\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        print(\"*********************\")\n        print(f.name)\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n                                                                                                  \n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\", emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n    import datetime\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"custom_dumpsoftwareversions\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\",\n        \"date\": datetime.datetime.now()\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    \n    script:\n    println(versions)\n\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        print(\"*********************\")\n        print(f.name)\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    \n    script:\n    println(versions)\n\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        print(\"*********************\")\n        print(f.name)\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}"], "list_proc": ["erikrikarddaniel/magmap/erikrikarddaniel__magmap/CUSTOM_DUMPSOFTWAREVERSIONS", "PGScatalog/pgsc_calc/PGScatalog__pgsc_calc/DUMPSOFTWAREVERSIONS", "nf-core/nanoseq/nf-core__nanoseq/CUSTOM_DUMPSOFTWAREVERSIONS", "xiaoli-dong/magph/xiaoli-dong__magph/CUSTOM_DUMPSOFTWAREVERSIONS", "bactopia/bactopia/bactopia__bactopia/CUSTOM_DUMPSOFTWAREVERSIONS", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/CUSTOM_DUMPSOFTWAREVERSIONS", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/CUSTOM_DUMPSOFTWAREVERSIONS", "asthara10/insertseq/asthara10__insertseq/CUSTOM_DUMPSOFTWAREVERSIONS", "LNUc-EEMiS/metatdenovo/LNUc-EEMiS__metatdenovo/CUSTOM_DUMPSOFTWAREVERSIONS"], "list_wf_names": ["PGScatalog/pgsc_calc", "erikrikarddaniel/magmap", "xiaoli-dong/pathogen", "ABMicroBioinf/pathogen", "bactopia/bactopia", "nf-core/nanoseq", "asthara10/insertseq", "xiaoli-dong/magph", "LNUc-EEMiS/metatdenovo"]}, {"nb_reuse": 4, "tools": ["preseq"], "nb_own": 2, "list_own": ["Dowell-Lab", "nf-core"], "nb_wf": 4, "list_wf": ["Nascent-Flow", "ChIP-Flow", "nascent", "RNAseq-Flow"], "list_contrib": ["zmaas", "lynn-sanford", "ignaciot", "apeltzer", "maallen3"], "nb_contrib": 5, "codes": ["\nprocess preseq {\n    tag \"$name\"\n    memory '20 GB'\n    time '8h'\n    validExitStatus 0,1\n    publishDir \"${params.outdir}/qc/preseq/\", mode: 'copy', pattern: \"*.txt\"\n    \n    when:\n    !params.skippreseq\n\n    input:\n    set val(name), file(bam_file) from sorted_bams_for_preseq\n    file(bam_indices) from sorted_bam_indices_for_preseq\n\n    output:\n    file(\"*.txt\") into preseq_results\n\n    script:\n    \"\"\"\n    preseq c_curve -B -o ${name}.c_curve.txt \\\n           ${bam_file}\n\n    preseq lc_extrap -B -o ${name}.lc_extrap.txt \\\n           ${bam_file}\n    \"\"\"\n }", "\nprocess preseq {\n    tag \"$name\"\n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/qc/preseq/\", mode: 'copy', pattern: \"*.txt\"\n\n    input:\n    set val(name), file(bam_file) from sorted_bams_for_preseq\n    file(bam_indices) from sorted_bam_indices_for_preseq\n\n    output:\n    file(\"*.txt\") into preseq_results\n\n    script:\n    \"\"\"\n\n    preseq c_curve -B -o ${name}.c_curve.txt \\\n           ${bam_file}\n\n    preseq lc_extrap -B -o ${name}.lc_extrap.txt \\\n           ${bam_file}\n    \"\"\"\n }", "\nprocess preseq {\n    tag \"$name\"\n    memory '20 GB'\n    time '8h'\n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/qc/preseq/\", mode: 'copy', pattern: \"*.txt\"\n\n    input:\n    set val(name), file(bam_file) from sorted_bams_for_preseq\n    file(bam_indices) from sorted_bam_indices_for_preseq\n\n    output:\n    file(\"*.txt\") into preseq_results\n\n    script:\n    \"\"\"\n    preseq c_curve -B -o ${name}.c_curve.txt \\\n           ${bam_file}\n\n    preseq lc_extrap -B -o ${name}.lc_extrap.txt \\\n           ${bam_file}\n    \"\"\"\n }", "\nprocess preseq {\n    tag \"$name\"\n    memory '20 GB'\n    time '8h'\n    errorStrategy 'ignore'\n    publishDir \"${params.outdir}/qc/preseq/\", mode: 'copy', pattern: \"*.txt\"\n    \n    when:\n    !params.skippreseq && !params.skipAllQC    \n\n    input:\n    set val(name), file(bam_file) from sorted_bams_for_preseq\n    file(bam_indices) from sorted_bam_indices_for_preseq\n\n    output:\n    file(\"*.txt\") into preseq_results\n\n    script:\n    \"\"\"\n    preseq c_curve -B -o ${name}.c_curve.txt \\\n           ${bam_file}\n    preseq lc_extrap -B -o ${name}.lc_extrap.txt \\\n           ${bam_file}\n    \"\"\"\n }"], "list_proc": ["Dowell-Lab/ChIP-Flow/Dowell-Lab__ChIP-Flow/preseq", "nf-core/nascent/nf-core__nascent/preseq", "Dowell-Lab/RNAseq-Flow/Dowell-Lab__RNAseq-Flow/preseq", "Dowell-Lab/Nascent-Flow/Dowell-Lab__Nascent-Flow/preseq"], "list_wf_names": ["Dowell-Lab/RNAseq-Flow", "Dowell-Lab/ChIP-Flow", "Dowell-Lab/Nascent-Flow", "nf-core/nascent"]}, {"nb_reuse": 2, "tools": ["Taxa", "PCFamily"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process DADA2_TAXONOMY {\n    tag \"${fasta},${database}\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    path(fasta)\n    path(database)\n    val(outfile)\n\n    output:\n    path(outfile), emit: tsv\n    path( \"ASV_tax.rds\" ), emit: rds\n    path \"versions.yml\"  , emit: versions\n    path \"*.args.txt\"    , emit: args\n\n    script:\n    def args = task.ext.args ?: ''\n    def seed = task.ext.seed ?: '100'\n    \"\"\"\n    #!/usr/bin/env Rscript\n    suppressPackageStartupMessages(library(dada2))\n    set.seed($seed) # Initialize random number generator for reproducibility\n\n    seq <- getSequences(\\\"$fasta\\\", collapse = TRUE, silence = FALSE)\n    taxa <- assignTaxonomy(seq, \\\"$database\\\", taxLevels = c(\"Domain\", \"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\"), $args, multithread = $task.cpus, verbose=TRUE, outputBootstraps = TRUE)\n\n    # Make a data frame, add ASV_ID from seq, set confidence to the bootstrap for the most specific taxon and reorder columns before writing to file\n    tx <- data.frame(ASV_ID = names(seq), taxa, sequence = row.names(taxa\\$tax), row.names = names(seq))\n    tx\\$confidence <- with(tx,\n        ifelse(!is.na(tax.Genus), boot.Genus,\n            ifelse(!is.na(tax.Family), boot.Family,\n                ifelse(!is.na(tax.Order), boot.Order,\n                    ifelse(!is.na(tax.Class), boot.Class,\n                        ifelse(!is.na(tax.Phylum), boot.Phylum,\n                            ifelse(!is.na(tax.Kingdom), boot.Kingdom,\n                                ifelse(!is.na(tax.Domain), boot.Domain, 0)\n                            )\n                        )\n                    )\n                )\n            )\n        )\n    )/100\n    taxa_export <- data.frame(\n        ASV_ID = tx\\$ASV_ID,\n        Domain = tx\\$tax.Domain,\n        Kingdom = tx\\$tax.Kingdom,\n        Phylum = tx\\$tax.Phylum,\n        Class = tx\\$tax.Class,\n        Order = tx\\$tax.Order,\n        Family = tx\\$tax.Family,\n        Genus = tx\\$tax.Genus,\n        confidence = tx\\$confidence,\n        sequence = tx\\$sequence,\n        row.names = names(seq)\n    )\n\n    write.table(taxa_export, file = \\\"$outfile\\\", sep = \"\\\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE, na = '')\n\n    # Save a version with rownames for addSpecies\n    taxa_export <- cbind( ASV_ID = tx\\$ASV_ID, taxa\\$tax, confidence = tx\\$confidence)\n    saveRDS(taxa_export, \"ASV_tax.rds\")\n\n    write.table('assignTaxonomy\\t$args\\nseed\\t$seed', file = \"assignTaxonomy.args.txt\", row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n    \"\"\"\n}", "process DADA2_TAXONOMY {\n    tag \"${fasta},${database}\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    path(fasta)\n    path(database)\n    val(outfile)\n\n    output:\n    path(outfile), emit: tsv\n    path( \"ASV_tax.rds\" ), emit: rds\n    path \"versions.yml\"  , emit: versions\n    path \"*.args.txt\"    , emit: args\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    suppressPackageStartupMessages(library(dada2))\n    set.seed(100) # Initialize random number generator for reproducibility\n\n    seq <- getSequences(\\\"$fasta\\\", collapse = TRUE, silence = FALSE)\n    taxa <- assignTaxonomy(seq, \\\"$database\\\", taxLevels = c(\"Domain\", \"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\"), $args, multithread = $task.cpus, verbose=TRUE, outputBootstraps = TRUE)\n\n    # Make a data frame, add ASV_ID from seq, set confidence to the bootstrap for the most specific taxon and reorder columns before writing to file\n    tx <- data.frame(ASV_ID = names(seq), taxa, sequence = row.names(taxa\\$tax), row.names = names(seq))\n    tx\\$confidence <- with(tx,\n        ifelse(!is.na(tax.Genus), boot.Genus,\n            ifelse(!is.na(tax.Family), boot.Family,\n                ifelse(!is.na(tax.Order), boot.Order,\n                    ifelse(!is.na(tax.Class), boot.Class,\n                        ifelse(!is.na(tax.Phylum), boot.Phylum,\n                            ifelse(!is.na(tax.Kingdom), boot.Kingdom,\n                                ifelse(!is.na(tax.Domain), boot.Domain, 0)\n                            )\n                        )\n                    )\n                )\n            )\n        )\n    )/100\n    taxa_export <- data.frame(\n        ASV_ID = tx\\$ASV_ID,\n        Domain = tx\\$tax.Domain,\n        Kingdom = tx\\$tax.Kingdom,\n        Phylum = tx\\$tax.Phylum,\n        Class = tx\\$tax.Class,\n        Order = tx\\$tax.Order,\n        Family = tx\\$tax.Family,\n        Genus = tx\\$tax.Genus,\n        confidence = tx\\$confidence,\n        sequence = tx\\$sequence,\n        row.names = names(seq)\n    )\n\n    write.table(taxa_export, file = \\\"$outfile\\\", sep = \"\\\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE, na = '')\n\n    # Save a version with rownames for addSpecies\n    taxa_export <- cbind( ASV_ID = tx\\$ASV_ID, taxa\\$tax, confidence = tx\\$confidence)\n    saveRDS(taxa_export, \"ASV_tax.rds\")\n\n    write.table('assignTaxonomy\\t$args', file = \"assignTaxonomy.args.txt\", row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/DADA2_TAXONOMY", "laclac102/ampliseq/laclac102__ampliseq/DADA2_TAXONOMY"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["MultiDataSet"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["modules", "test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process NEXTCLADE_DATASETGET {\n    tag \"$dataset\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::nextclade=1.10.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/nextclade:1.10.2--h9ee0642_0' :\n        'quay.io/biocontainers/nextclade:1.10.2--h9ee0642_0' }\"\n\n    input:\n    val dataset\n    val reference\n    val tag\n\n    output:\n    path \"$prefix\"     , emit: dataset\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${dataset}\"\n    def fasta = reference ? \"--reference ${reference}\" : ''\n    def version = tag ? \"--tag ${tag}\" : ''\n    \"\"\"\n    nextclade \\\\\n        dataset \\\\\n        get \\\\\n        $args \\\\\n        --name $dataset \\\\\n        $fasta \\\\\n        $version \\\\\n        --output-dir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        nextclade: \\$(nextclade --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process NEXTCLADE_DATASETGET {\n    tag \"$dataset\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::nextclade=1.10.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/nextclade:1.10.2--h9ee0642_0' :\n        'quay.io/biocontainers/nextclade:1.10.2--h9ee0642_0' }\"\n\n    input:\n    val dataset\n    val reference\n    val tag\n\n    output:\n    path \"$prefix\"     , emit: dataset\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${dataset}\"\n    def fasta = reference ? \"--reference ${reference}\" : ''\n    def version = tag ? \"--tag ${tag}\" : ''\n    \"\"\"\n    nextclade \\\\\n        dataset \\\\\n        get \\\\\n        $args \\\\\n        --name $dataset \\\\\n        $fasta \\\\\n        $version \\\\\n        --output-dir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        nextclade: \\$(nextclade --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process NEXTCLADE_DATASETGET {\n    tag \"$dataset\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::nextclade=1.10.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/nextclade:1.10.2--h9ee0642_0' :\n        'quay.io/biocontainers/nextclade:1.10.2--h9ee0642_0' }\"\n\n    input:\n    val dataset\n    val reference\n    val tag\n\n    output:\n    path \"$prefix\"     , emit: dataset\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${dataset}\"\n    def fasta = reference ? \"--reference ${reference}\" : ''\n    def version = tag ? \"--tag ${tag}\" : ''\n    \"\"\"\n    nextclade \\\\\n        dataset \\\\\n        get \\\\\n        $args \\\\\n        --name $dataset \\\\\n        $fasta \\\\\n        $version \\\\\n        --output-dir $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        nextclade: \\$(nextclade --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/NEXTCLADE_DATASETGET", "nf-core/viralrecon/nf-core__viralrecon/NEXTCLADE_DATASETGET", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/NEXTCLADE_DATASETGET"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess genotyping_pileupcaller {\n  label 'mc_small'\n  tag \"${strandedness}\"\n  publishDir \"${params.outdir}/genotyping\", mode: params.publish_dir_mode\n\n  when:\n  params.run_genotyping && params.genotyping_tool == 'pileupcaller'\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, bam, bai from ch_prepped_for_pileupcaller_double.mix(ch_prepped_for_pileupcaller_single)\n  file fasta from ch_fasta_for_genotyping_pileupcaller.collect()\n  file fai from ch_fai_for_pileupcaller.collect()\n  file dict from ch_dict_for_pileupcaller.collect()\n  path(bed) from ch_bed_for_pileupcaller.collect()\n  path(snp) from ch_snp_for_pileupcaller.collect().dump(tag: \"pileupcaller_snp_file\")\n\n  output:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"pileupcaller.${strandedness}.*\") into ch_for_eigenstrat_snp_coverage\n\n  script:\n  def use_bed = bed.getName() != 'nf-core_eager_dummy.txt' ? \"-l ${bed}\" : ''\n  def use_snp = snp.getName() != 'nf-core_eager_dummy2.txt' ? \"-f ${snp}\" : ''\n\n  def transitions_mode = strandedness == \"single\" ? \"\" : \"${params.pileupcaller_transitions_mode}\" == 'SkipTransitions' ? \"--skipTransitions\" : \"${params.pileupcaller_transitions_mode}\" == 'TransitionsMissing' ? \"--transitionsMissing\" : \"\"\n  def caller = \"--${params.pileupcaller_method}\"\n  def ssmode = strandedness == \"single\" ? \"--singleStrandMode\" : \"\"\n  def bam_list = bam.flatten().join(\" \")\n  def sample_names = samplename.flatten().join(\",\")\n  def map_q = params.pileupcaller_min_map_quality\n  def base_q = params.pileupcaller_min_base_quality\n\n  \"\"\"\n  samtools mpileup -B --ignore-RG -q ${map_q} -Q ${base_q} ${use_bed} -f ${fasta} ${bam_list} | pileupCaller ${caller} ${ssmode} ${transitions_mode} --sampleNames ${sample_names} ${use_snp} -e pileupcaller.${strandedness}\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/genotyping_pileupcaller"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["PavriLab", "nf-core"], "nb_wf": 2, "list_wf": ["repliseq-nf", "nascent"], "list_contrib": ["t-neumann", "ignaciot", "dmalzl", "apeltzer"], "nb_contrib": 4, "codes": [" process make_chromosome_sizes {\n      tag \"$fasta\"\n      publishDir path: { params.saveReference ? \"${params.outdir}/reference_genome\" : params.outdir },\n              saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n      input:\n      file fasta from genome_fasta\n\n      output:\n      file(\"${fasta}.sizes\") into chrom_sizes_ch\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n      \"\"\"\n  }", "\nprocess MakeGenomeFilter {\n    tag \"$fasta\"\n\n    input:\n    file fasta from fastaForGenomeSizes\n\n    output:\n\n    file \"*.sizes\" into chromSizesChannel\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n    \"\"\"\n}"], "list_proc": ["nf-core/nascent/nf-core__nascent/make_chromosome_sizes", "PavriLab/repliseq-nf/PavriLab__repliseq-nf/MakeGenomeFilter"], "list_wf_names": ["nf-core/nascent", "PavriLab/repliseq-nf"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nanoseq"], "list_contrib": ["lwratten", "alneberg", "nf-core-bot", "ewels", "csawye01", "maxulysse", "KevinMenden", "cying111", "drpatelh", "yuukiiwa"], "nb_contrib": 10, "codes": ["\nprocess BEDTOOLS_BAMBED {\n    label 'process_medium'\n\n    conda     (params.enable_conda ? \"bioconda::bedtools=2.29.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.29.2--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.29.2--hc088bd4_0\"\n    }\n\n    when:\n    !params.skip_alignment && !params.skip_bigbed && (params.protocol == 'directRNA' || params.protocol == 'cDNA')\n\n    input:\n    tuple val(meta), path(sizes), val(is_transcripts), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(sizes), path(\"*.bed12\") , emit: bed12\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    \"\"\"\n    bedtools \\\\\n        bamtobed \\\\\n        -bed12 \\\\\n        -cigar \\\\\n        -i ${bam[0]} \\\\\n        | bedtools sort > ${meta.id}.bed12\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/BEDTOOLS_BAMBED"], "list_wf_names": ["nf-core/nanoseq"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["exoseq"], "list_contrib": ["senthil10", "alneberg", "ewels", "maxulysse", "apeltzer"], "nb_contrib": 5, "codes": ["\nprocess combineVariants {\n    tag \"$name\"\n    publishDir \"${params.outdir}/GATK_CombineVariants/\", mode: 'copy', \n    saveAs: {filename -> params.saveIntermediateVariants ? \"$filename\" : null }\n\n    input:\n    set file(fsnp), file(fsnp_idx), file(findel), file(findel_idx) from variants_filtered\n\n    output:\n    set file(\"${name}_combined_variants.vcf\"), file(\"${name}_combined_variants.vcf.idx\") into (combined_variants_evaluate,combined_variants_snpEff, combined_variants_gatk)\n\n    script:\n    \"\"\"\n    gatk -T CombineVariants \\\\\n        -R $params.gfasta \\\\\n        --out ${name}_combined_variants.vcf \\\\\n        --genotypemergeoption PRIORITIZE \\\\\n        --variant:${name}_SNP_filtered $fsnp \\\\\n        --variant:${name}_indels_filtered $findel \\\\\n        --rod_priority_list ${name}_SNP_filtered,${name}_indels_filtered\n    \"\"\"\n}"], "list_proc": ["nf-core/exoseq/nf-core__exoseq/combineVariants"], "list_wf_names": ["nf-core/exoseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["neutronstar"], "list_contrib": ["ewels", "remiolsen"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n    input:\n    file ('supernova/') from supernova_results2.collect()\n    file ('busco/') from busco_results.collect()\n    file ('quast/') from quast_results.collect()\n    file ('software_versions/') from software_versions_yaml.toList()\n    file(mqc_config) from Channel.fromPath(\"${params.multiqc_config}\")\n\n    output:\n    file \"*multiqc_report.html\"\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc -i ${custom_runName} -f -s  --config ${mqc_config} .\n    \"\"\"\n}"], "list_proc": ["nf-core/neutronstar/nf-core__neutronstar/multiqc"], "list_wf_names": ["nf-core/neutronstar"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process GATK4_SELECTVARIANTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_idx)\n\n    output:\n    tuple val(meta), path(\"*.selectvariants.vcf.gz\")       , emit: vcf\n    tuple val(meta), path(\"*.selectvariants.vcf.gz.tbi\")   , emit: tbi\n    path \"versions.yml\"\t\t                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK VariantFiltration] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}G\" SelectVariants \\\\\n        --variant $vcf \\\\\n        --output ${prefix}.selectvariants.vcf.gz \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_SELECTVARIANTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_idx)\n\n    output:\n    tuple val(meta), path(\"*.selectvariants.vcf.gz\")       , emit: vcf\n    tuple val(meta), path(\"*.selectvariants.vcf.gz.tbi\")   , emit: tbi\n    path \"versions.yml\"\t\t                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK VariantFiltration] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}G\" SelectVariants \\\\\n        -V $vcf \\\\\n        -O ${prefix}.selectvariants.vcf.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_SELECTVARIANTS", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/GATK4_SELECTVARIANTS"], "list_wf_names": ["CDCgov/mycosnp-nf", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Bismark"], "nb_own": 2, "list_own": ["FAANG", "nf-core"], "nb_wf": 2, "list_wf": ["methylseq", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "drpatelh", "Jani-94"], "nb_contrib": 19, "codes": [" process bismark_align {\n        tag \"$name\"\n        publishDir \"${params.outdir}/bismark_alignments\", mode: params.publish_dir_mode,\n            saveAs: {filename ->\n                if( filename.indexOf(\".fq.gz\") > 0 ) \"unmapped/$filename\"\n                else if( filename.indexOf(\"report.txt\") > 0 ) \"logs/$filename\"\n                else if( (!params.save_align_intermeds && !params.skip_deduplication && !params.rrbs).every() && filename == \"where_are_my_files.txt\" ) filename\n                else if( (params.save_align_intermeds || params.skip_deduplication || params.rrbs).any() && filename != \"where_are_my_files.txt\" ) filename\n                else null\n            }\n\n        input:\n        set val(name), file(reads) from ch_trimmed_reads_for_alignment\n        file index from ch_bismark_index_for_bismark_align.collect()\n        file wherearemyfiles from ch_wherearemyfiles_for_bismark_align.collect()\n        file knownsplices from ch_splicesites_for_bismark_hisat_align.collect().ifEmpty([])\n\n        output:\n        set val(name), file(\"*.bam\") into ch_bam_for_bismark_deduplicate, ch_bam_for_bismark_summary, ch_bam_for_preseq\n        set val(name), file(\"*report.txt\") into ch_bismark_align_log_for_bismark_report, ch_bismark_align_log_for_bismark_summary, ch_bismark_align_log_for_multiqc\n        file \"*.fq.gz\" optional true\n        file \"where_are_my_files.txt\"\n\n        script:\n                                               \n        input = params.single_end ? reads : \"-1 ${reads[0]} -2 ${reads[1]}\"\n\n                                 \n        aligner = params.aligner == \"bismark_hisat\" ? \"--hisat2\" : \"--bowtie2\"\n\n                                            \n        splicesites = params.aligner == \"bismark_hisat\" && params.known_splices ? \"--known-splicesite-infile <(hisat2_extract_splice_sites.py ${knownsplices})\" : ''\n        pbat = params.pbat ? \"--pbat\" : ''\n        non_directional = params.single_cell || params.zymo || params.non_directional ? \"--non_directional\" : ''\n        unmapped = params.unmapped ? \"--unmapped\" : ''\n        mismatches = params.relax_mismatches ? \"--score_min L,0,-${params.num_mismatches}\" : ''\n        soft_clipping = params.local_alignment ? \"--local\" : ''\n        minins = bismark_minins ? \"--minins $bismark_minins\" : ''\n        maxins = bismark_maxins ? \"--maxins $bismark_maxins\" : ''\n\n                                                                                           \n        multicore = ''\n        if( task.cpus ){\n                                                                                  \n            if( params.single_cell || params.zymo || params.non_directional ){\n                cpu_per_multicore = 5\n                mem_per_multicore = (18.GB).toBytes()\n            } else {\n                cpu_per_multicore = 3\n                mem_per_multicore = (13.GB).toBytes()\n            }\n                                                                       \n            if(params.bismark_align_cpu_per_multicore) {\n                cpu_per_multicore = (params.bismark_align_cpu_per_multicore as int)\n            }\n            if(params.bismark_align_mem_per_multicore) {\n                mem_per_multicore = (params.bismark_align_mem_per_multicore as nextflow.util.MemoryUnit).toBytes()\n            }\n                                                                             \n            ccore = ((task.cpus as int) / cpu_per_multicore) as int\n                                                                                                                \n            try {\n                tmem = (task.memory as nextflow.util.MemoryUnit).toBytes()\n                mcore = (tmem / mem_per_multicore) as int\n                ccore = Math.min(ccore, mcore)\n            } catch (all) {\n                log.debug \"Warning: Not able to define bismark align multicore based on available memory\"\n            }\n            if( ccore > 1 ){\n              multicore = \"--multicore $ccore\"\n            }\n        }\n\n                       \n        \"\"\"\n        bismark $input \\\\\n            $aligner \\\\\n            --bam $pbat $non_directional $unmapped $mismatches $multicore $minins $maxins \\\\\n            --genome $index \\\\\n            $reads \\\\\n            $soft_clipping \\\\\n            $splicesites\n        \"\"\"\n    }", " process bismark_align {\n        tag \"$name\"\n        publishDir \"${params.outdir}/bismark_alignments\", mode: params.publish_dir_mode,\n            saveAs: {filename ->\n                if( filename.indexOf(\".fq.gz\") > 0 ) \"unmapped/$filename\"\n                else if( filename.indexOf(\"report.txt\") > 0 ) \"logs/$filename\"\n                else if( (!params.save_align_intermeds && !params.skip_deduplication && !params.rrbs).every() && filename == \"where_are_my_files.txt\" ) filename\n                else if( (params.save_align_intermeds || params.skip_deduplication || params.rrbs).any() && filename != \"where_are_my_files.txt\" ) filename\n                else null\n            }\n\n        input:\n        set val(name), file(reads) from ch_trimmed_reads_for_alignment\n        file index from ch_bismark_index_for_bismark_align.collect()\n        file wherearemyfiles from ch_wherearemyfiles_for_bismark_align.collect()\n        file knownsplices from ch_splicesites_for_bismark_hisat_align.collect().ifEmpty([])\n\n        output:\n        set val(name), file(\"*.bam\") into ch_bam_for_bismark_deduplicate, ch_bam_for_bismark_summary, ch_bam_for_preseq\n        set val(name), file(\"*report.txt\") into ch_bismark_align_log_for_bismark_report, ch_bismark_align_log_for_bismark_summary, ch_bismark_align_log_for_multiqc\n        file \"*.fq.gz\" optional true\n        file \"where_are_my_files.txt\"\n\n        script:\n                                               \n        input = params.single_end ? reads : \"-1 ${reads[0]} -2 ${reads[1]}\"\n\n                                 \n        aligner = params.aligner == \"bismark_hisat\" ? \"--hisat2\" : \"--bowtie2\"\n\n                                            \n        splicesites = params.aligner == \"bismark_hisat\" && params.known_splices ? \"--known-splicesite-infile <(hisat2_extract_splice_sites.py ${knownsplices})\" : ''\n        pbat = params.pbat ? \"--pbat\" : ''\n        non_directional = params.single_cell || params.zymo || params.non_directional ? \"--non_directional\" : ''\n        unmapped = params.unmapped ? \"--unmapped\" : ''\n        mismatches = params.relax_mismatches ? \"--score_min L,0,-${params.num_mismatches}\" : ''\n        soft_clipping = params.local_alignment ? \"--local\" : ''\n        minins = bismark_minins ? \"--minins $bismark_minins\" : ''\n        maxins = bismark_maxins ? \"--maxins $bismark_maxins\" : ''\n\n                                                                                           \n        multicore = ''\n        if( task.cpus ){\n                                                                                  \n            if( params.single_cell || params.zymo || params.non_directional ){\n                cpu_per_multicore = 5\n                mem_per_multicore = (18.GB).toBytes()\n            } else {\n                cpu_per_multicore = 3\n                mem_per_multicore = (13.GB).toBytes()\n            }\n                                                                       \n            if(params.bismark_align_cpu_per_multicore) {\n                cpu_per_multicore = (params.bismark_align_cpu_per_multicore as int)\n            }\n            if(params.bismark_align_mem_per_multicore) {\n                mem_per_multicore = (params.bismark_align_mem_per_multicore as nextflow.util.MemoryUnit).toBytes()\n            }\n                                                                             \n            ccore = ((task.cpus as int) / cpu_per_multicore) as int\n                                                                                                                \n            try {\n                tmem = (task.memory as nextflow.util.MemoryUnit).toBytes()\n                mcore = (tmem / mem_per_multicore) as int\n                ccore = Math.min(ccore, mcore)\n            } catch (all) {\n                log.debug \"Warning: Not able to define bismark align multicore based on available memory\"\n            }\n            if( ccore > 1 ){\n              multicore = \"--multicore $ccore\"\n            }\n        }\n\n                       \n        \"\"\"\n        bismark $input \\\\\n            $aligner \\\\\n            --bam $pbat $non_directional $unmapped $mismatches $multicore $minins $maxins \\\\\n            --genome $index \\\\\n            $soft_clipping \\\\\n            $splicesites\n        \"\"\"\n    }"], "list_proc": ["FAANG/GSM-pipeline/FAANG__GSM-pipeline/bismark_align", "nf-core/methylseq/nf-core__methylseq/bismark_align"], "list_wf_names": ["nf-core/methylseq", "FAANG/GSM-pipeline"]}, {"nb_reuse": 8, "tools": ["GATK"], "nb_own": 6, "list_own": ["Genomic-Medicine-Linkoping", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 7, "list_wf": ["saw.sarek", "sarek_ubec", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 23, "codes": ["\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicatesInSampleReadGroup {\n    label 'cpus_16'\n    label 'withGatkContainer'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.bam\")\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\")\n        tuple val(idPatient), val(idSample)\n        file (\"${idSample}.bam.metrics\") optional true\n\n    when: !(params.skip_markduplicates)\n\n    script:\n                                                                                                                                                                                    \n    markdup_java_options = params.markdup_java_options\n    metrics = 'markduplicates' in getInputSkipQC() ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n\n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/sripaladugu__germline_somatic/MarkDuplicates", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MarkDuplicatesInSampleReadGroup", "nf-core/sarek/nf-core__sarek/MarkDuplicates", "rmoran7/custom_sarek/rmoran7__custom_sarek/MarkDuplicates", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MarkDuplicates", "rmoran7/dx_sarek/rmoran7__dx_sarek/MarkDuplicates", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MarkDuplicates", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MarkDuplicates"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["DFAST"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess DFAST {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"dfast=1.2.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/dfast:1.2.14--h2e03b76_0\"\n    } else {\n        container \"quay.io/biocontainers/dfast:1.2.14--h2e03b76_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    file (config)\n\n    output:\n    tuple val(meta), path(\"RESULT*\"), emit: reads\n    path \"*.version.txt\"            , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    \"\"\"\n    dfast_file_downloader.py --protein dfast --dbroot .\n    dfast --genome ${fasta} --config $config\n    dfast --version | sed -e \"s/DFAST ver. //g\"  > \"${software}.version.txt\"\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/DFAST"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process GATK4_CREATESOMATICPANELOFNORMALS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(genomicsdb)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CreateSomaticPanelOfNormals] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CreateSomaticPanelOfNormals \\\\\n        --variant gendb://$genomicsdb \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_CREATESOMATICPANELOFNORMALS"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["snpEff"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process SNPEFF_BUILD {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    path fasta\n    path gff\n\n    output:\n    path 'snpeff_db'   , emit: db\n    path '*.config'    , emit: config\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def basename = fasta.baseName\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    mkdir -p snpeff_db/genomes/\n    cd snpeff_db/genomes/\n    ln -s ../../$fasta ${basename}.fa\n\n    cd ../../\n    mkdir -p snpeff_db/${basename}/\n    cd snpeff_db/${basename}/\n    ln -s ../../$gff genes.gff\n\n    cd ../../\n    echo \"${basename}.genome : ${basename}\" > snpeff.config\n\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        build \\\\\n        -config snpeff.config \\\\\n        -dataDir ./snpeff_db \\\\\n        -gff3 \\\\\n        -v \\\\\n        ${basename}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}", "process SNPEFF_BUILD {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    path fasta\n    path gff\n\n    output:\n    path 'snpeff_db'   , emit: db\n    path '*.config'    , emit: config\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def basename = fasta.baseName\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    mkdir -p snpeff_db/genomes/\n    cd snpeff_db/genomes/\n    ln -s ../../$fasta ${basename}.fa\n\n    cd ../../\n    mkdir -p snpeff_db/${basename}/\n    cd snpeff_db/${basename}/\n    ln -s ../../$gff genes.gff\n\n    cd ../../\n    echo \"${basename}.genome : ${basename}\" > snpeff.config\n\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        build \\\\\n        -config snpeff.config \\\\\n        -dataDir ./snpeff_db \\\\\n        -gff3 \\\\\n        -v \\\\\n        ${basename}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/SNPEFF_BUILD", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SNPEFF_BUILD"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["preseq"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess preseq {\n    label 'sc_tiny'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/preseq\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_preseq\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(input) from ch_input_for_preseq\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"${input.baseName}.preseq\") into ch_preseq_for_multiqc\n\n    script:\n    pe_mode = params.skip_collapse && seqtype == \"PE\" ? '-P' : ''\n    if(!params.skip_deduplication && params.preseq_mode == 'c_curve' && params.dedupper == \"dedup\"){\n    \"\"\"\n    preseq c_curve -s ${params.preseq_step_size} -o ${input.baseName}.preseq -H ${input}\n    \"\"\"\n    } else if( !params.skip_deduplication && params.preseq_mode == 'c_curve' && params.dedupper == \"markduplicates\"){\n    \"\"\"\n    preseq c_curve -s ${params.preseq_step_size} -o ${input.baseName}.preseq -B ${input} ${pe_mode}\n    \"\"\"\n    } else if ( params.skip_deduplication && params.preseq_mode == 'c_curve' ) {\n    \"\"\"\n    preseq c_curve -s ${params.preseq_step_size} -o ${input.baseName}.preseq -B ${input} ${pe_mode}\n    \"\"\"\n    } else if(!params.skip_deduplication && params.preseq_mode == 'lc_extrap' && params.dedupper == \"dedup\"){\n    \"\"\"\n    preseq lc_extrap -s ${params.preseq_step_size} -o ${input.baseName}.preseq -H ${input} -n ${params.preseq_bootstrap} -e ${params.preseq_maxextrap} -cval ${params.preseq_cval} -x ${params.preseq_terms}\n    \"\"\"\n    } else if( !params.skip_deduplication && params.preseq_mode == 'lc_extrap' && params.dedupper == \"markduplicates\"){\n    \"\"\"\n    preseq lc_extrap -s ${params.preseq_step_size} -o ${input.baseName}.preseq -B ${input} ${pe_mode} -n ${params.preseq_bootstrap} -e ${params.preseq_maxextrap} -cval ${params.preseq_cval} -x ${params.preseq_terms}\n    \"\"\"\n    } else if ( params.skip_deduplication && params.preseq_mode == 'lc_extrap' ) {\n    \"\"\"\n    preseq lc_extrap -s ${params.preseq_step_size} -o ${input.baseName}.preseq -B ${input} ${pe_mode} -n ${params.preseq_bootstrap} -e ${params.preseq_maxextrap} -cval ${params.preseq_cval} -x ${params.preseq_terms}\n    \"\"\"\n    }\n}"], "list_proc": ["nf-core/eager/nf-core__eager/preseq"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess bcftools_stats {\n  label  'mc_small'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/bcftools/stats\", mode: params.publish_dir_mode\n\n  when: \n  params.run_bcftools_stats\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(vcf) from ch_ug_for_bcftools_stats.mix(ch_hc_for_bcftools_stats,ch_fb_for_bcftools_stats)\n  file fasta from ch_fasta_for_bcftools_stats.collect()\n\n  output:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.vcf.stats\") into ch_bcftools_stats_for_multiqc\n\n  script:\n  \"\"\"\n  bcftools stats *.vcf.gz -F ${fasta} > ${samplename}.vcf.stats\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/bcftools_stats"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 3, "tools": ["Centrifuge"], "nb_own": 3, "list_own": ["cidgoh", "cancerbioinformatics", "nf-core"], "nb_wf": 3, "list_wf": ["cidgoh_qc", "mag", "PATCH-pipeline"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "radhika-kataria", "duanjunhyq", "maxibor", "skrakau", "jfy133", "anwarMZ"], "nb_contrib": 17, "codes": ["\nprocess CENTRIFUGE {\n    tag \"${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::centrifuge=1.0.4_beta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/centrifuge:1.0.4_beta--he513fc3_5\"\n    } else {\n        container \"quay.io/biocontainers/centrifuge:1.0.4_beta--he513fc3_5\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(db)\n\n    output:\n    tuple val(\"centrifuge\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path \"report.txt\"                                        , emit: report\n    path \"kreport.txt\"                                       , emit: kreport\n    path '*.version.txt'                                     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    centrifuge -x \"${db_name}\" \\\n        -p ${task.cpus} \\\n        --report-file report.txt \\\n        -S results.txt \\\n        $input\n    centrifuge-kreport -x \"${db_name}\" results.txt > kreport.txt\n    cat results.txt | cut -f 1,3 > results.krona\n\n    centrifuge --version | sed -n 1p | sed 's/^.*centrifuge-class version //' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess CENTRIFUGE {\n    tag \"${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::centrifuge=1.0.4_beta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/centrifuge:1.0.4_beta--he513fc3_5\"\n    } else {\n        container \"quay.io/biocontainers/centrifuge:1.0.4_beta--he513fc3_5\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(db)\n\n    output:\n    tuple val(\"centrifuge\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path \"report.txt\"                                        , emit: report\n    path \"kreport.txt\"                                       , emit: kreport\n    path '*.version.txt'                                     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    centrifuge -x \"${db_name}\" \\\n        -p ${task.cpus} \\\n        --report-file report.txt \\\n        -S results.txt \\\n        $input\n    centrifuge-kreport -x \"${db_name}\" results.txt > kreport.txt\n    cat results.txt | cut -f 1,3 > results.krona\n\n    centrifuge --version | head -n 1 | sed 's/^.*centrifuge-class version //' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess CENTRIFUGE {\n    tag \"${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::centrifuge=1.0.4_beta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/centrifuge:1.0.4_beta--he513fc3_5\"\n    } else {\n        container \"quay.io/biocontainers/centrifuge:1.0.4_beta--he513fc3_5\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(db)\n\n    output:\n    tuple val(\"centrifuge\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path \"report.txt\"                                        , emit: report\n    path \"kreport.txt\"                                       , emit: kreport\n    path '*.version.yml'                                     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    centrifuge -x \"${db_name}\" \\\n        -p ${task.cpus} \\\n        --report-file report.txt \\\n        -S results.txt \\\n        $input\n    centrifuge-kreport -x \"${db_name}\" results.txt > kreport.txt\n    cat results.txt | cut -f 1,3 > results.krona\n    centrifuge --version | sed -n 1p | sed 's/^.*centrifuge-class version //' > ${software}.version.yml\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/CENTRIFUGE", "cancerbioinformatics/PATCH-pipeline/cancerbioinformatics__PATCH-pipeline/CENTRIFUGE", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/CENTRIFUGE"], "list_wf_names": ["cidgoh/cidgoh_qc", "nf-core/mag", "cancerbioinformatics/PATCH-pipeline"]}, {"nb_reuse": 1, "tools": ["StringTie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nanoseq"], "list_contrib": ["lwratten", "alneberg", "nf-core-bot", "ewels", "csawye01", "maxulysse", "KevinMenden", "cying111", "drpatelh", "yuukiiwa"], "nb_contrib": 10, "codes": ["\nprocess STRINGTIE2 {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n                                                         \n    conda     (params.enable_conda ? \"bioconda::stringtie=2.1.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/stringtie:2.1.4--h7e0af3c_0\"\n    } else {\n        container \"quay.io/biocontainers/stringtie:2.1.4--h7e0af3c_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta), path(gtf), path(bam)\n\n    output:\n    path \"*.stringtie.gtf\"       , emit: stringtie_gtf\n    path  \"versions.yml\"         , emit: versions\n\n    script:\n    \"\"\"\n    stringtie \\\\\n        -L \\\\\n        -G $gtf \\\\\n        -o ${meta.id}.stringtie.gtf $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(stringtie --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/STRINGTIE2"], "list_wf_names": ["nf-core/nanoseq"]}, {"nb_reuse": 3, "tools": ["GATK"], "nb_own": 2, "list_own": ["rmoran7", "nf-core"], "nb_wf": 3, "list_wf": ["sarek", "custom_sarek", "dx_sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 21, "codes": ["\nprocess Mutect2Single {\n    tag \"${idSampleTumor}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from singleBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") into mutect2SingleOutput\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\") optional true into intervalStatsFilesSingle\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") optional true into mutect2StatsSingle\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}.vcf\n    \"\"\"\n}", "\nprocess Mutect2Single {\n    tag \"${idSampleTumor}-${intervalBed.baseName}\"\n\n    label 'process_medium'\n\n    input:\n        set idPatient, idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from singleBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") into mutect2SingleOutput\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\") optional true into intervalStatsFilesSingle\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") optional true into mutect2StatsSingle\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}.vcf\n    \"\"\"\n}", "\nprocess Mutect2Single {\n    tag \"${idSampleTumor}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    memory '32 GB'\n\n    input:\n        set idPatient, idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from singleBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") into mutect2SingleOutput\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\") optional true into intervalStatsFilesSingle\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") optional true into mutect2StatsSingle\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}.vcf\n    \"\"\"\n}"], "list_proc": ["nf-core/sarek/nf-core__sarek/Mutect2Single", "rmoran7/dx_sarek/rmoran7__dx_sarek/Mutect2Single", "rmoran7/custom_sarek/rmoran7__custom_sarek/Mutect2Single"], "list_wf_names": ["rmoran7/dx_sarek", "nf-core/sarek", "rmoran7/custom_sarek"]}, {"nb_reuse": 2, "tools": ["RapidNJ"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["\nprocess RAPIDNJ {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::rapidnj=2.3.2 conda-forge::biopython=1.78\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-805c6e0f138f952f9c61cdd57c632a1a263ea990:3c52e4c8da6b3e4d69b9ca83fa4d366168898179-0' :\n        'quay.io/biocontainers/mulled-v2-805c6e0f138f952f9c61cdd57c632a1a263ea990:3c52e4c8da6b3e4d69b9ca83fa4d366168898179-0' }\"\n\n    input:\n    path alignment\n\n    output:\n    path \"*.sth\"       , emit: stockholm_alignment\n    path \"*.tre\"       , emit: phylogeny\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    python \\\\\n        -c 'from Bio import SeqIO; SeqIO.convert(\"$alignment\", \"fasta\", \"alignment.sth\", \"stockholm\")'\n\n    rapidnj \\\\\n        alignment.sth \\\\\n        $args \\\\\n        -i sth \\\\\n        -c $task.cpus \\\\\n        -x rapidnj_phylogeny.tre\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rapidnj: $VERSION\n        biopython: \\$(python -c \"import Bio; print(Bio.__version__)\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess RAPIDNJ {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::rapidnj=2.3.2 conda-forge::biopython=1.78\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-805c6e0f138f952f9c61cdd57c632a1a263ea990:3c52e4c8da6b3e4d69b9ca83fa4d366168898179-0' :\n        'quay.io/biocontainers/mulled-v2-805c6e0f138f952f9c61cdd57c632a1a263ea990:3c52e4c8da6b3e4d69b9ca83fa4d366168898179-0' }\"\n\n    input:\n    path alignment\n\n    output:\n    path \"*.sth\"       , emit: stockholm_alignment\n    path \"*.tre\"       , emit: phylogeny\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    python \\\\\n        -c 'from Bio import SeqIO; SeqIO.convert(\"$alignment\", \"fasta\", \"alignment.sth\", \"stockholm\")'\n\n    rapidnj \\\\\n        alignment.sth \\\\\n        $args \\\\\n        -i sth \\\\\n        -c $task.cpus \\\\\n        -x rapidnj_phylogeny.tre\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rapidnj: $VERSION\n        biopython: \\$(python -c \"import Bio; print(Bio.__version__)\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/RAPIDNJ", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/RAPIDNJ"], "list_wf_names": ["CDCgov/mycosnp-nf", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["FAANG", "nf-core"], "nb_wf": 2, "list_wf": ["methylseq", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "drpatelh", "Jani-94"], "nb_contrib": 19, "codes": [" process samtools_sort_index_flagstat {\n        tag \"$name\"\n        publishDir \"${params.outdir}/bam_processing\", mode: params.publish_dir_mode,\n            saveAs: {filename ->\n                if(filename.indexOf(\"report.txt\") > 0) \"logs/$filename\"\n                else if( (!params.save_align_intermeds && !params.skip_deduplication && !params.rrbs).every() && filename == \"where_are_my_files.txt\") filename\n                else if( (params.save_align_intermeds || params.skip_deduplication || params.rrbs).any() && filename != \"where_are_my_files.txt\") filename\n                else null\n            }\n\n        input:\n        set val(name), file(bam) from ch_bam_for_samtools_sort_index_flagstat\n        file wherearemyfiles from ch_wherearemyfiles_for_samtools_sort_index_flagstat.collect()\n\n        output:\n        set val(name), file(\"${bam.baseName}.sorted.bam\") into ch_indep_bam_sorted, ch_bam_sorted_for_markDuplicates\n        set val(name), file(\"${bam.baseName}.sorted.bam.bai\") into ch_bam_index, ch_indep_bam_index\n        file \"${bam.baseName}_flagstat_report.txt\" into ch_flagstat_results_for_multiqc\n        file \"${bam.baseName}_stats_report.txt\" into ch_samtools_stats_results_for_multiqc\n        file \"where_are_my_files.txt\"\n\n        script:\n        def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n        def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n        \"\"\"\n        samtools sort $bam \\\\\n            -@ ${task.cpus} $sort_mem \\\\\n            -o ${bam.baseName}.sorted.bam\n        samtools index ${bam.baseName}.sorted.bam\n        samtools flagstat ${bam.baseName}.sorted.bam > ${bam.baseName}_flagstat_report.txt\n        samtools stats ${bam.baseName}.sorted.bam > ${bam.baseName}_stats_report.txt\n        \"\"\"\n    }", " process samtools_sort_index_flagstat {\n        tag \"$name\"\n        publishDir \"${params.outdir}/bwa-mem_alignments\", mode: params.publish_dir_mode,\n            saveAs: {filename ->\n                if(filename.indexOf(\"report.txt\") > 0) \"logs/$filename\"\n                else if( (!params.save_align_intermeds && !params.skip_deduplication && !params.rrbs).every() && filename == \"where_are_my_files.txt\") filename\n                else if( (params.save_align_intermeds || params.skip_deduplication || params.rrbs).any() && filename != \"where_are_my_files.txt\") filename\n                else null\n            }\n\n        input:\n        set val(name), file(bam) from ch_bam_for_samtools_sort_index_flagstat\n        file wherearemyfiles from ch_wherearemyfiles_for_samtools_sort_index_flagstat.collect()\n\n        output:\n        set val(name), file(\"${bam.baseName}.sorted.bam\") into ch_bam_sorted_for_markDuplicates\n        set val(name), file(\"${bam.baseName}.sorted.bam.bai\") into ch_bam_index\n        file \"${bam.baseName}_flagstat_report.txt\" into ch_flagstat_results_for_multiqc\n        file \"${bam.baseName}_stats_report.txt\" into ch_samtools_stats_results_for_multiqc\n        file \"where_are_my_files.txt\"\n\n        script:\n        def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n        def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n        \"\"\"\n        samtools sort $bam \\\\\n            -@ ${task.cpus} $sort_mem \\\\\n            -o ${bam.baseName}.sorted.bam\n        samtools index ${bam.baseName}.sorted.bam\n        samtools flagstat ${bam.baseName}.sorted.bam > ${bam.baseName}_flagstat_report.txt\n        samtools stats ${bam.baseName}.sorted.bam > ${bam.baseName}_stats_report.txt\n        \"\"\"\n    }"], "list_proc": ["FAANG/GSM-pipeline/FAANG__GSM-pipeline/samtools_sort_index_flagstat", "nf-core/methylseq/nf-core__methylseq/samtools_sort_index_flagstat"], "list_wf_names": ["nf-core/methylseq", "FAANG/GSM-pipeline"]}, {"nb_reuse": 19, "tools": ["GATK"], "nb_own": 11, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 18, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "layer_lab_chco", "layer_lab_caw", "layer_lab_vc", "germline_somatic", "custom_sarek", "nf-core-sarek", "sarek-mirror", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "pgp-chronek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n    echo true\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publish_dir_mode, overwrite: false\n\n    input:\n        tuple idPatient, idSample, file(recal)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.table\"), emit: recal_table\n                                                     \n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample, val(\"${idSample}.md.bam\"), val(\"${idSample}.md.bai\"), val(\"${idSample}.recal.table\") into (recalTableTSV, recalTableSampleTSV)\n\n    when: step == 'mapping'\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n    echo true\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publish_dir_mode, overwrite: false\n\n    input:\n        tuple idPatient, idSample, file(recal)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.table\"), emit: recal_table\n                                                     \n\n    when: params.known_indels  && step != 'variantcalling' &&\n        ('haplotypecaller' in tools || \n            'mutect2' in tools ||\n            'mutect2_single' in tools ||\n            'gen_somatic_pon' in tools)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample, val(\"${idSample}.md.bam\"), val(\"${idSample}.md.bai\"), val(\"${idSample}.recal.table\") into (recalTableTSV, recalTableSampleTSV)\n\n    when: step == 'mapping'\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess MergeBaseRecalibrationReportsForSample {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n    label 'withGatkContainer'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        tuple val(idPatient), val(idSample), file(recal)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.table\")\n        tuple val(idPatient), val(idSample)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample, val(\"${idSample}.md.bam\"), val(\"${idSample}.md.bai\"), val(\"${idSample}.recal.table\") into (recalTableTSV, recalTableSampleTSV)\n\n    when: step == 'mapping'\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n    echo true\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publish_dir_mode, overwrite: false\n\n    input:\n        tuple idPatient, idSample, file(recal)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.table\"), emit: recal_table\n                                                     \n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample, val(\"${idSample}.md.bam\"), val(\"${idSample}.md.bai\"), val(\"${idSample}.recal.table\") into (recalTableTSV, recalTableSampleTSV)\n\n    when: step == 'mapping'\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample, val(\"${idSample}.md.bam\"), val(\"${idSample}.md.bai\"), val(\"${idSample}.recal.table\") into (recalTableTSV, recalTableSampleTSV)\n\n    when: step == 'mapping'\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/GatherBQSRReports", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/GatherBQSRReports", "cgpu/sarek-mirror/cgpu__sarek-mirror/GatherBQSRReports", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/GatherBQSRReports", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/GatherBQSRReports", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/GatherBQSRReports", "cgpu/pgp-chronek/cgpu__pgp-chronek/GatherBQSRReports", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/GatherBQSRReports", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergeBaseRecalibrationReportsForSample", "nf-core/sarek/nf-core__sarek/GatherBQSRReports", "rmoran7/custom_sarek/rmoran7__custom_sarek/GatherBQSRReports", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/GatherBQSRReports", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/GatherBQSRReports", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GatherBQSRReports", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/GatherBQSRReports", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/GatherBQSRReports", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/GatherBQSRReports", "rmoran7/dx_sarek/rmoran7__dx_sarek/GatherBQSRReports", "cgpu/haplosarek/cgpu__haplosarek/GatherBQSRReports"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "chelauk/test_nextflow_sarek", "nf-core/sarek", "ryanlayerlab/layer_lab_caw", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror-cache", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek", "javaidm/layer_lab_vc"]}, {"nb_reuse": 9, "tools": ["Bowtie"], "nb_own": 6, "list_own": ["ABMicroBioinf", "nf-core", "mahesh-panchal", "xiaoli-dong", "goodwright", "jianhong"], "nb_wf": 7, "list_wf": ["magph", "shotgun", "viralrecon", "modules", "imaps-nf", "test_nfcore_workflow_chain", "cutandrun"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "dladd", "rfara", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "samirelanduk", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "cjfields", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "xiaoli-dong", "alexharston", "Gwennid", "Jeremy1805", "charlotte-west", "marc-jones", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "CharlotteAnne", "jianhong", "mashehu", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 123, "codes": ["process BOWTIE_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie=1.3.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie:1.3.0--py38hed8969a_1' :\n        'quay.io/biocontainers/bowtie:1.3.0--py38hed8969a_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie'       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie\n    bowtie-build --threads $task.cpus $fasta bowtie/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie: \\$(echo \\$(bowtie --version 2>&1) | sed 's/^.*bowtie-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie=1.3.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie:1.3.0--py38hed8969a_1' :\n        'quay.io/biocontainers/bowtie:1.3.0--py38hed8969a_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie'       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie\n    bowtie-build --threads $task.cpus $fasta bowtie/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie: \\$(echo \\$(bowtie --version 2>&1) | sed 's/^.*bowtie-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py39hbb4e92a_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py39hbb4e92a_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0'\n    } else {\n        container 'quay.io/biocontainers/bowtie2:2.4.4--py36hd4290be_0'\n    }\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $options.args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py39hbb4e92a_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py39hbb4e92a_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py36hd4290be_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py39hbb4e92a_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["goodwright/imaps-nf/goodwright__imaps-nf/BOWTIE_BUILD", "nf-core/modules/nf-core__modules/BOWTIE_BUILD", "ABMicroBioinf/magph/ABMicroBioinf__magph/BOWTIE2_BUILD", "nf-core/modules/nf-core__modules/BOWTIE2_BUILD", "nf-core/cutandrun/nf-core__cutandrun/BOWTIE2_BUILD", "jianhong/shotgun/jianhong__shotgun/BOWTIE2_BUILD", "nf-core/viralrecon/nf-core__viralrecon/BOWTIE2_BUILD", "xiaoli-dong/magph/xiaoli-dong__magph/BOWTIE2_BUILD", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BOWTIE2_BUILD"], "list_wf_names": ["jianhong/shotgun", "nf-core/cutandrun", "nf-core/modules", "goodwright/imaps-nf", "nf-core/viralrecon", "ABMicroBioinf/magph", "mahesh-panchal/test_nfcore_workflow_chain", "xiaoli-dong/magph"]}, {"nb_reuse": 2, "tools": ["BWA"], "nb_own": 2, "list_own": ["cidgoh", "nf-core"], "nb_wf": 2, "list_wf": ["cidgoh_qc", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "anwarMZ", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "duanjunhyq", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process BWA_ALN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--h5bf99c6_8' :\n        'quay.io/biocontainers/bwa:0.7.17--h5bf99c6_8' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.sai\"), emit: sai\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    if (meta.single_end) {\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n        bwa aln \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f ${prefix}.sai \\\\\n            \\$INDEX \\\\\n            ${reads}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n        bwa aln \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f ${prefix}.1.sai \\\\\n            \\$INDEX \\\\\n            ${reads[0]}\n\n        bwa aln \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f ${prefix}.2.sai \\\\\n            \\$INDEX \\\\\n            ${reads[1]}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BWA_ALN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--h5bf99c6_8' :\n        'quay.io/biocontainers/bwa:0.7.17--h5bf99c6_8' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.sai\"), emit: sai\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    if (meta.single_end) {\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n        bwa aln \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f ${prefix}.sai \\\\\n            \\$INDEX \\\\\n            ${reads}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n        bwa aln \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f ${prefix}.1.sai \\\\\n            \\$INDEX \\\\\n            ${reads[0]}\n\n        bwa aln \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f ${prefix}.2.sai \\\\\n            \\$INDEX \\\\\n            ${reads[1]}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/BWA_ALN", "nf-core/modules/nf-core__modules/BWA_ALN"], "list_wf_names": ["cidgoh/cidgoh_qc", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["nanopolish"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["bacass"], "list_contrib": ["rivera10", "bewt85", "nf-core-bot", "ewels", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "d4straub", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess NANOPOLISH {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n    label 'process_high_memory'\n    label 'error_retry'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'nanopolish=0.13.2-5' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/nanopolish:0.13.2--h8cec615_5\"\n    } else {\n        container \"quay.io/biocontainers/nanopolish:0.13.2--h8cec615_5\"\n    }\n\n    input:\n    tuple val(meta), val(reads), file(longreads), file(assembly), file(bam), file(bai), file(fast5)\n\n    output:\n    tuple val(meta), file('polished_genome.fa'), emit: assembly\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    nanopolish index -d \"${fast5}\" \"${longreads}\"\n\n    nanopolish variants \\\n        --consensus \\\n        -o polished.vcf \\\n        -r \"${longreads}\" \\\n        -b \"${bam}\" \\\n        -g \"${assembly}\" \\\n        -t \"${task.cpus}\" \\\n        --min-candidate-frequency 0.1\n\n    nanopolish vcf2fasta -g \"${assembly}\" polished.vcf > polished_genome.fa\n\n    nanopolish --version | sed -e \"s/nanopolish version //g\" | head -n 1 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bacass/nf-core__bacass/NANOPOLISH"], "list_wf_names": ["nf-core/bacass"]}, {"nb_reuse": 77, "tools": ["Salmon", "FastQC", "FeatureCounts", "MultiQC"], "nb_own": 61, "list_own": ["NailouZhang", "HeshamElAbd", "drpatelh", "FriederikeHanssen", "galaxyuvri-ea", "canceromics", "Crone0610", "qbicsoftware-archive", "ssun1116", "dfornika", "kingzhuky", "cmatKhan", "grst", "MDU-PHL", "BlackburnLab", "elowy01", "nriddiford", "Flomics", "bc2zb", "biggstd", "espelpz", "edgano", "lauramble", "LaurenceKuhl", "veitveit", "mbosio85", "monikaBrandt", "glormph", "Gustius", "chelauk", "herczegrobert", "samlhao", "juneb4869", "luissian", "nf-core", "czbiohub", "nibscbioinformatics", "yassineS", "MicrobialGenomics", "paulstretenowich", "bioinformatics-lab", "grbot", "SherineAwad", "HuipengL", "likelet", "suzannejin", "bhargava-morampalli", "propan2one", "vladsaveliev", "kerimoff", "dalmiaa", "davismcc", "marchoeppner", "steepale", "apeltzer", "subwaystation", "maxibor", "jiangfuqing", "jtmccr1", "BarryDigby", "leipzig"], "nb_wf": 73, "list_wf": ["nfcorepgdb", "Sample_NF", "test-nf-core-pipeline", "nf-core-mutenrich", "nf-core-refcaller", "nf-core-sgrnaquant", "nf-core-wombatp", "TCoffee-NatureProtocol-nf", "pgdb", "MeRIPseqPipe", "blastlca", "humgen", "nf-core-hlatyping", "testpipeline", "cdnaseqont-nextflow", "kinseq", "variantcallerbench", "UvriMetaSeq", "joao-test", "IGCG-featureCounts", "nf-hipsci-fibro", "nf-core-gsfworkflow", "qtlquant", "nf-core-rw", "umcaw", "taranispip", "kovid-trees-nf", "meripseqpipe", "trigenome-analysis-nf", "microarray-qc-workflow", "genebygenebact", "nf-simulaternaseq", "nf-core-lohcator", "crisprvar", "nf-core-test", "nf-core-singlegenometese", "rnaseq-vizfada", "nf-ortholog", "nfcore_test", "nf-core-lfquandenser", "CRISPR-Cas-off-target-identification", "m6a", "nf-core-assemblybacterias", "nf-core-testworkflow", "cookiecutter", "nf-large-assembly", "nf-demux", "scrna-seq", "nf-proportionality", "ownpipeline", "wgsfastqtobam", "circ", "nf-core-virome", "bowtie2-lca", "nf-core-disambiguate", "pangenome", "nf-core-cpo", "nf-core-example", "nf-core-deviptcore", "trinoflow", "MesKit", "nf-core-viralrecon", "nf-extractcoding", "TTrichiura_Tubulin", "new_meripseqpipe", "nf-core-mytrial", "nf-core-abricate", "nf-core-mynewpipeline", "omics-analyser", "babysarek", "nf-core-gatkcohortcall", "nf-core-phylofunk", "shrnacount"], "list_contrib": ["HeshamElAbd", "xec-cm", "heuermh", "ypriverol", "alneberg", "FriederikeHanssen", "ewels", "remiolsen", "evanfloden", "maxulysse", "canceromics", "Crone0610", "ggabernet", "Ninomoriaty", "AndreaGuarracino", "ssun1116", "kingzhuky", "dfornika", "Wangxin555", "cmatKhan", "grst", "elowy01", "nf-core-bot", "nriddiford", "bleazard", "lizheng141026", "AlfredUg", "biggstd", "espelpz", "abhi18av", "pditommaso", "edgano", "lauramble", "LaurenceKuhl", "chenjy327", "mbosio85", "monikaBrandt", "PeterBailey", "veitveit", "glormph", "chelauk", "Gustius", "herczegrobert", "juneb4869", "luissian", "ZIWEI-WONG", "yassineS", "Eletham", "Niinleslie", "MicrobialGenomics", "Zethson", "jcurado-flomics", "grbot", "likelet", "suzannejin", "andersgs", "propan2one", "vladsaveliev", "kerimoff", "sven1103", "dalmiaa", "lescai", "marchoeppner", "steepale", "davismcc", "LiuJie1117", "apeltzer", "telatin", "subwaystation", "maxibor", "olgabot", "drpatelh", "BarryDigby", "leipzig"], "nb_contrib": 74, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    python ${baseDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                       \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    salmon --version > v_salmon.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    python ${baseDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    python ${baseDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    python ${baseDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    featureCounts --version > v_featurecounts.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    python ${baseDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["HeshamElAbd/nf-core-deviptcore/HeshamElAbd__nf-core-deviptcore/get_software_versions", "elowy01/nf-core-testworkflow/elowy01__nf-core-testworkflow/get_software_versions", "marchoeppner/kinseq/marchoeppner__kinseq/get_software_versions", "nf-core/cookiecutter/nf-core__cookiecutter/get_software_versions", "nf-core/crisprvar/nf-core__crisprvar/get_software_versions", "marchoeppner/trinoflow/marchoeppner__trinoflow/get_software_versions", "bc2zb/nf-core-disambiguate/bc2zb__nf-core-disambiguate/get_software_versions", "vladsaveliev/umcaw/vladsaveliev__umcaw/get_software_versions", "HuipengL/nfcore_test/HuipengL__nfcore_test/get_software_versions", "nf-core/testpipeline/nf-core__testpipeline/get_software_versions", "grst/nf-core-test/grst__nf-core-test/get_software_versions", "propan2one/variantcallerbench/propan2one__variantcallerbench/get_software_versions", "bhargava-morampalli/cdnaseqont-nextflow/bhargava-morampalli__cdnaseqont-nextflow/get_software_versions", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/get_software_versions", "Crone0610/nf-core-refcaller/Crone0610__nf-core-refcaller/get_software_versions", "qbicsoftware-archive/microarray-qc-workflow/qbicsoftware-archive__microarray-qc-workflow/get_software_versions", "biggstd/nf-core-gsfworkflow/biggstd__nf-core-gsfworkflow/get_software_versions", "leipzig/m6a/leipzig__m6a/get_software_versions", "czbiohub/blastlca/czbiohub__blastlca/get_software_versions", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/get_software_versions", "espelpz/genebygenebact/espelpz__genebygenebact/get_software_versions", "espelpz/taranispip/espelpz__taranispip/get_software_versions", "czbiohub/nf-core-test/czbiohub__nf-core-test/get_software_versions", "czbiohub/nf-extractcoding/czbiohub__nf-extractcoding/get_software_versions", "maxibor/bowtie2-lca/maxibor__bowtie2-lca/get_software_versions", "czbiohub/nf-large-assembly/czbiohub__nf-large-assembly/get_software_versions", "czbiohub/nf-ortholog/czbiohub__nf-ortholog/get_software_versions", "czbiohub/nf-simulaternaseq/czbiohub__nf-simulaternaseq/get_software_versions", "jiangfuqing/CRISPR-Cas-off-target-identification/jiangfuqing__CRISPR-Cas-off-target-identification/get_software_versions", "jiangfuqing/scrna-seq/jiangfuqing__scrna-seq/get_software_versions", "maxibor/test-nf-core-pipeline/maxibor__test-nf-core-pipeline/get_software_versions", "SherineAwad/nf-core-mytrial/SherineAwad__nf-core-mytrial/get_software_versions", "dalmiaa/Sample_NF/dalmiaa__Sample_NF/get_software_versions", "mbosio85/nf-core-gatkcohortcall/mbosio85__nf-core-gatkcohortcall/get_software_versions", "yassineS/nf-demux/yassineS__nf-demux/get_software_versions", "LaurenceKuhl/shrnacount/LaurenceKuhl__shrnacount/get_software_versions", "MDU-PHL/kovid-trees-nf/MDU-PHL__kovid-trees-nf/get_software_versions", "ssun1116/meripseqpipe/ssun1116__meripseqpipe/get_software_versions", "steepale/nf-core-mutenrich/steepale__nf-core-mutenrich/get_software_versions", "nriddiford/nf-core-lohcator/nriddiford__nf-core-lohcator/get_software_versions", "steepale/wgsfastqtobam/steepale__wgsfastqtobam/get_software_versions", "bioinformatics-lab/nf-core-singlegenometese/bioinformatics-lab__nf-core-singlegenometese/get_software_versions", "davismcc/nf-hipsci-fibro/davismcc__nf-hipsci-fibro/get_software_versions", "bioinformatics-lab/trigenome-analysis-nf/bioinformatics-lab__trigenome-analysis-nf/get_software_versions", "dfornika/nf-core-cpo/dfornika__nf-core-cpo/get_software_versions", "subwaystation/pangenome/subwaystation__pangenome/get_software_versions", "suzannejin/nf-proportionality/suzannejin__nf-proportionality/get_software_versions", "Flomics/joao-test/Flomics__joao-test/get_software_versions", "chelauk/nf-core-hlatyping/chelauk__nf-core-hlatyping/get_software_versions", "likelet/MesKit/likelet__MesKit/get_software_versions", "MicrobialGenomics/TTrichiura_Tubulin/MicrobialGenomics__TTrichiura_Tubulin/get_software_versions", "monikaBrandt/nf-core-mynewpipeline/monikaBrandt__nf-core-mynewpipeline/get_software_versions", "herczegrobert/ownpipeline/herczegrobert__ownpipeline/get_software_versions", "jtmccr1/nf-core-phylofunk/jtmccr1__nf-core-phylofunk/get_software_versions", "FriederikeHanssen/babysarek/FriederikeHanssen__babysarek/get_software_versions", "BarryDigby/circ/BarryDigby__circ/get_software_versions", "galaxyuvri-ea/UvriMetaSeq/galaxyuvri-ea__UvriMetaSeq/get_software_versions", "juneb4869/new_meripseqpipe/juneb4869__new_meripseqpipe/get_software_versions", "paulstretenowich/nf-core-viralrecon/paulstretenowich__nf-core-viralrecon/get_software_versions", "FriederikeHanssen/shrnacount/FriederikeHanssen__shrnacount/get_software_versions", "BlackburnLab/nfcorepgdb/BlackburnLab__nfcorepgdb/get_software_versions", "BlackburnLab/pgdb/BlackburnLab__pgdb/get_software_versions", "luissian/nf-core-assemblybacterias/luissian__nf-core-assemblybacterias/get_software_versions", "kerimoff/qtlquant/kerimoff__qtlquant/get_software_versions", "NailouZhang/nf-core-virome/NailouZhang__nf-core-virome/get_software_versions", "veitveit/nf-core-wombatp/veitveit__nf-core-wombatp/get_software_versions", "canceromics/MeRIPseqPipe/canceromics__MeRIPseqPipe/get_software_versions", "drpatelh/nf-core-sgrnaquant/drpatelh__nf-core-sgrnaquant/get_software_versions", "apeltzer/IGCG-featureCounts/apeltzer__IGCG-featureCounts/get_software_versions", "cmatKhan/nf-core-rw/cmatKhan__nf-core-rw/get_software_versions", "glormph/nf-core-lfquandenser/glormph__nf-core-lfquandenser/get_software_versions", "Gustius/omics-analyser/Gustius__omics-analyser/get_software_versions", "kingzhuky/meripseqpipe/kingzhuky__meripseqpipe/get_software_versions", "samlhao/nf-core-abricate/samlhao__nf-core-abricate/get_software_versions", "edgano/TCoffee-NatureProtocol-nf/edgano__TCoffee-NatureProtocol-nf/get_software_versions", "grbot/nf-core-example/grbot__nf-core-example/get_software_versions", "grbot/nf-core-test/grbot__nf-core-test/get_software_versions"], "list_wf_names": ["lauramble/rnaseq-vizfada", "BlackburnLab/nfcorepgdb", "chelauk/nf-core-hlatyping", "HeshamElAbd/nf-core-deviptcore", "czbiohub/nf-core-test", "glormph/nf-core-lfquandenser", "subwaystation/pangenome", "qbicsoftware-archive/microarray-qc-workflow", "kerimoff/qtlquant", "drpatelh/nf-core-sgrnaquant", "paulstretenowich/nf-core-viralrecon", "bhargava-morampalli/cdnaseqont-nextflow", "czbiohub/nf-ortholog", "nf-core/testpipeline", "leipzig/m6a", "FriederikeHanssen/shrnacount", "czbiohub/nf-simulaternaseq", "Flomics/joao-test", "elowy01/nf-core-testworkflow", "dfornika/nf-core-cpo", "mbosio85/nf-core-gatkcohortcall", "bioinformatics-lab/trigenome-analysis-nf", "grbot/nf-core-example", "nf-core/crisprvar", "grst/nf-core-test", "maxibor/test-nf-core-pipeline", "czbiohub/nf-large-assembly", "galaxyuvri-ea/UvriMetaSeq", "grbot/nf-core-test", "dalmiaa/Sample_NF", "nf-core/cookiecutter", "likelet/MesKit", "steepale/wgsfastqtobam", "MicrobialGenomics/TTrichiura_Tubulin", "jiangfuqing/CRISPR-Cas-off-target-identification", "nriddiford/nf-core-lohcator", "MDU-PHL/kovid-trees-nf", "apeltzer/IGCG-featureCounts", "Gustius/omics-analyser", "SherineAwad/nf-core-mytrial", "espelpz/genebygenebact", "czbiohub/nf-extractcoding", "ssun1116/meripseqpipe", "veitveit/nf-core-wombatp", "edgano/TCoffee-NatureProtocol-nf", "propan2one/variantcallerbench", "nibscbioinformatics/humgen", "steepale/nf-core-mutenrich", "bc2zb/nf-core-disambiguate", "maxibor/bowtie2-lca", "bioinformatics-lab/nf-core-singlegenometese", "herczegrobert/ownpipeline", "HuipengL/nfcore_test", "czbiohub/blastlca", "LaurenceKuhl/shrnacount", "jiangfuqing/scrna-seq", "NailouZhang/nf-core-virome", "espelpz/taranispip", "juneb4869/new_meripseqpipe", "davismcc/nf-hipsci-fibro", "vladsaveliev/umcaw", "kingzhuky/meripseqpipe", "marchoeppner/trinoflow", "Crone0610/nf-core-refcaller", "yassineS/nf-demux", "suzannejin/nf-proportionality", "monikaBrandt/nf-core-mynewpipeline", "BlackburnLab/pgdb", "cmatKhan/nf-core-rw", "samlhao/nf-core-abricate", "marchoeppner/kinseq", "FriederikeHanssen/babysarek", "luissian/nf-core-assemblybacterias", "biggstd/nf-core-gsfworkflow", "canceromics/MeRIPseqPipe", "jtmccr1/nf-core-phylofunk", "BarryDigby/circ"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_DIVERSITY_ALPHA {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core), val(category)\n\n    output:\n    path(\"alpha_diversity/*\"), emit: alpha\n    path \"versions.yml\"      , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        qiime diversity alpha-group-significance \\\n            --i-alpha-diversity ${core} \\\n            --m-metadata-file ${metadata} \\\n            --o-visualization ${core.baseName}-vis.qzv\n        qiime tools export --input-path ${core.baseName}-vis.qzv \\\n            --output-path \"alpha_diversity/${core.baseName}\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir alpha_diversity\n        echo \"\" > \"alpha_diversity/WARNING No column in ${metadata.baseName} seemed suitable.txt\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process QIIME2_DIVERSITY_ALPHA {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core), val(category)\n\n    output:\n    path(\"alpha_diversity/*\"), emit: alpha\n    path \"versions.yml\"      , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        qiime diversity alpha-group-significance \\\n            --i-alpha-diversity ${core} \\\n            --m-metadata-file ${metadata} \\\n            --o-visualization ${core.baseName}-vis.qzv\n        qiime tools export --input-path ${core.baseName}-vis.qzv \\\n            --output-path \"alpha_diversity/${core.baseName}\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir alpha_diversity\n        echo \"\" > \"alpha_diversity/WARNING No column in ${metadata.baseName} seemed suitable.txt\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_DIVERSITY_ALPHA", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_DIVERSITY_ALPHA"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["SAMtools", "mpileup"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process MAKE_BED_MASK {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.9.5 bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1a35167f7a491c7086c13835aaa74b39f1f43979:6b5cffa1187cfccf2dc983ed3b5359d49b999eb0-0' :\n        'quay.io/biocontainers/mulled-v2-1a35167f7a491c7086c13835aaa74b39f1f43979:6b5cffa1187cfccf2dc983ed3b5359d49b999eb0-0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(vcf)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.bed\")    , emit: bed\n    tuple val(meta), path(\"*.mpileup\"), optional:true, emit: mpileup\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:                                                                         \n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: 10\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        $args \\\\\n        --reference $fasta \\\\\n        $bam \\\\\n        $mpileup  \\\\\n        | awk -v OFS='\\\\t' '{print \\$1, \\$2-1, \\$2, \\$4}' | awk '\\$4 < $args2' > lowcov_positions.txt\n\n    make_bed_mask.py \\\\\n        $vcf \\\\\n        lowcov_positions.txt \\\\\n        ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}", "process MAKE_BED_MASK {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.9.5 bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1a35167f7a491c7086c13835aaa74b39f1f43979:6b5cffa1187cfccf2dc983ed3b5359d49b999eb0-0' :\n        'quay.io/biocontainers/mulled-v2-1a35167f7a491c7086c13835aaa74b39f1f43979:6b5cffa1187cfccf2dc983ed3b5359d49b999eb0-0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(vcf)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.bed\")    , emit: bed\n    tuple val(meta), path(\"*.mpileup\"), optional:true, emit: mpileup\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:                                                                         \n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: 10\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        $args \\\\\n        --reference $fasta \\\\\n        $bam \\\\\n        $mpileup  \\\\\n        | awk -v OFS='\\\\t' '{print \\$1, \\$2-1, \\$2, \\$4}' | awk '\\$4 < $args2' > lowcov_positions.txt\n\n    make_bed_mask.py \\\\\n        $vcf \\\\\n        lowcov_positions.txt \\\\\n        ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/MAKE_BED_MASK", "nf-core/viralrecon/nf-core__viralrecon/MAKE_BED_MASK"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 3, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["modules", "test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process SAMTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n    input:\n    tuple val(meta), path(input), path(intervals)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.mpileup\"), emit: mpileup\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def intervals = intervals ? \"-l ${intervals}\" : \"\"\n    \"\"\"\n    samtools mpileup \\\\\n        --fasta-ref $fasta \\\\\n        --output ${prefix}.mpileup \\\\\n        $args \\\\\n        $input\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.mpileup\"), emit: mpileup\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools mpileup \\\\\n        --fasta-ref $fasta \\\\\n        --output ${prefix}.mpileup \\\\\n        $args \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.mpileup\"), emit: mpileup\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools mpileup \\\\\n        --fasta-ref $fasta \\\\\n        --output ${prefix}.mpileup \\\\\n        $args \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SAMTOOLS_MPILEUP", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_MPILEUP", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_MPILEUP"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["MapDamage", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess mapdamage_rescaling {\n\n    label 'sc_small'\n    tag \"${libraryid}\"\n\n    publishDir \"${params.outdir}/damage_rescaling\", mode: params.publish_dir_mode\n\n    when:\n    params.run_mapdamage_rescaling\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_rmdup_for_damagerescaling\n    file fasta from ch_fasta_for_damagerescaling.collect()\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*_rescaled.bam\"), path(\"*rescaled.bam.{bai,csi}\") into ch_output_from_damagerescaling\n\n    script:\n    def base = \"${bam.baseName}\"\n    def singlestranded = strandedness == \"single\" ? '--single-stranded' : ''\n    def size = params.large_ref ? '-c' : ''\n    \"\"\"\n    mapDamage -i ${bam} -r ${fasta} --rescale --rescale-out ${base}_rescaled.bam --rescale-length-5p ${params.rescale_length_5p} --rescale-length-3p=${params.rescale_length_3p} ${singlestranded}\n    samtools index ${base}_rescaled.bam ${size}\n    \"\"\"\n\n}"], "list_proc": ["nf-core/eager/nf-core__eager/mapdamage_rescaling"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["MapDamage"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MAPDAMAGE2 {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::mapdamage2=2.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mapdamage2:2.2.1--pyr40_0' :\n        'quay.io/biocontainers/mapdamage2:2.2.1--pyr40_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"results_*/Runtime_log.txt\")                                    ,emit: runtime_log\n    tuple val(meta), path(\"results_*/Fragmisincorporation_plot.pdf\"), optional: true      ,emit: fragmisincorporation_plot\n    tuple val(meta), path(\"results_*/Length_plot.pdf\"), optional: true                    ,emit: length_plot\n    tuple val(meta), path(\"results_*/misincorporation.txt\"), optional: true               ,emit: misincorporation\n    tuple val(meta), path(\"results_*/lgdistribution.txt\"), optional: true                 ,emit: lgdistribution\n    tuple val(meta), path(\"results_*/dnacomp.txt\"), optional: true                        ,emit: dnacomp\n    tuple val(meta), path(\"results_*/Stats_out_MCMC_hist.pdf\"), optional: true            ,emit: stats_out_mcmc_hist\n    tuple val(meta), path(\"results_*/Stats_out_MCMC_iter.csv\"), optional: true            ,emit: stats_out_mcmc_iter\n    tuple val(meta), path(\"results_*/Stats_out_MCMC_trace.pdf\"), optional: true           ,emit: stats_out_mcmc_trace\n    tuple val(meta), path(\"results_*/Stats_out_MCMC_iter_summ_stat.csv\"), optional: true  ,emit: stats_out_mcmc_iter_summ_stat\n    tuple val(meta), path(\"results_*/Stats_out_MCMC_post_pred.pdf\"), optional: true       ,emit: stats_out_mcmc_post_pred\n    tuple val(meta), path(\"results_*/Stats_out_MCMC_correct_prob.csv\"), optional: true    ,emit: stats_out_mcmc_correct_prob\n    tuple val(meta), path(\"results_*/dnacomp_genome.csv\"), optional: true                 ,emit: dnacomp_genome\n    tuple val(meta), path(\"results_*/rescaled.bam\"), optional: true                       ,emit: rescaled\n    tuple val(meta), path(\"results_*/5pCtoT_freq.txt\"), optional: true                    ,emit: pctot_freq\n    tuple val(meta), path(\"results_*/3pGtoA_freq.txt\"), optional: true                    ,emit: pgtoa_freq\n    tuple val(meta), path(\"results_*/*.fasta\"), optional: true                            ,emit: fasta\n    tuple val(meta), path(\"*/\"), optional: true                                           ,emit: folder\n    path \"versions.yml\",emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mapDamage \\\\\n            $args \\\\\n            -i $bam \\\\\n            -r $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mapdamage2: \\$(echo \\$(mapDamage --version))\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MAPDAMAGE2"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 5, "tools": ["BWA", "SAMtools"], "nb_own": 5, "list_own": ["ajodeh-juma", "tamara-hodgetts", "nf-core", "Akazhiel", "remiolsen"], "nb_wf": 5, "list_wf": ["viclara", "bactmap", "hicscaff", "NeoPred-NF", "nf-atac-seq"], "list_contrib": ["ajodeh-juma", "alexandregilardet", "thanhleviet", "tamara-hodgetts", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "Akazhiel", "drpatelh", "remiolsen"], "nb_contrib": 12, "codes": ["\nprocess BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:eabfac3657eda5818bae4090db989e3d41b01542-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:eabfac3657eda5818bae4090db989e3d41b01542-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $options.args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $options.args2 -@ $task.cpus -bhS -o ${prefix}.bam -\n\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n                                     \n                                         \n                                                                                                                                                                     \n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def split_cpus = Math.floor(task.cpus/2)\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}.${part}\" : \"${meta.id}.\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \n    println \"Mapping ${meta.patient}\"\n\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n    bwa mem \\\\\n        -t ${split_cpus} \\\\\n        $options.args \\\\\n        $read_group \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $options.args2 --threads ${split_cpus} -o ${prefix}bam\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $options.args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $options.args2 -@ $task.cpus -bhS -o ${prefix}.bam -\n\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def split_cpus = Math.floor(task.cpus/2)\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $options.args \\\\\n        $read_group \\\\\n        -t ${split_cpus} \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $options.args2 -@ ${split_cpus} -bhS -o ${prefix}.bam -\n\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $options.args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $options.args2 -@ $task.cpus -bhS -o ${prefix}.bam -\n\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/bactmap/nf-core__bactmap/BWA_MEM", "Akazhiel/NeoPred-NF/Akazhiel__NeoPred-NF/BWA_MEM", "remiolsen/hicscaff/remiolsen__hicscaff/BWA_MEM", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/BWA_MEM", "ajodeh-juma/viclara/ajodeh-juma__viclara/BWA_MEM"], "list_wf_names": ["Akazhiel/NeoPred-NF", "ajodeh-juma/viclara", "remiolsen/hicscaff", "nf-core/bactmap", "tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 1, "tools": ["ANGSD", "SAMtools", "contaminationX"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": [" process nuclear_contamination{\n    label 'sc_small'\n    tag \"${samplename}\"\n    publishDir \"${params.outdir}/nuclear_contamination\", mode: params.publish_dir_mode\n\n    when:\n    params.run_nuclear_contamination\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(input), path(bai) from ch_for_nuclear_contamination\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path('*.X.contamination.out') into ch_from_nuclear_contamination\n\n    script:\n    \"\"\"\n    samtools index ${input}\n    angsd -i ${input} -r ${params.contamination_chrom_name}:5000000-154900000 -doCounts 1 -iCounts 1 -minMapQ 30 -minQ 30 -out ${libraryid}.doCounts\n    contamination -a ${libraryid}.doCounts.icnts.gz -h ${projectDir}/assets/angsd_resources/HapMapChrX.gz 2> ${libraryid}.X.contamination.out\n    \"\"\"\n }"], "list_proc": ["nf-core/eager/nf-core__eager/nuclear_contamination"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nanoseq"], "list_contrib": ["lwratten", "alneberg", "nf-core-bot", "ewels", "csawye01", "maxulysse", "KevinMenden", "cying111", "drpatelh", "yuukiiwa"], "nb_contrib": 10, "codes": ["\nprocess SUBREAD_FEATURECOUNTS {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n                                                         \n    conda     (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0\"\n    } else {\n        container \"quay.io/biocontainers/subread:2.0.1--hed695b0_0\"\n    }\n\n    input:\n    path gtf\n    path bams\n\n    output:\n    path \"counts_gene.txt\"               , emit: gene_counts\n    path \"counts_transcript.txt\"         , emit: transcript_counts\n    path \"counts_gene.txt.summary\"       , emit: featurecounts_gene_multiqc\n    path \"counts_transcript.txt.summary\" , emit: featurecounts_transcript_multiqc\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    \"\"\"\n    featureCounts \\\\\n        -L \\\\\n        -O \\\\\n        -f \\\\\n        -g gene_id \\\\\n        -t exon \\\\\n        -T $task.cpus \\\\\n        -a $gtf \\\\\n        -o counts_gene.txt \\\\\n        $bams\n\n    featureCounts \\\\\n        -L \\\\\n        -O \\\\\n        -f \\\\\n        --primary \\\\\n        --fraction \\\\\n        -F GTF \\\\\n        -g transcript_id \\\\\n        -t transcript \\\\\n        --extraAttributes gene_id \\\\\n        -T $task.cpus \\\\\n        -a $gtf \\\\\n        -o counts_transcript.txt \\\\\n        $bams\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/SUBREAD_FEATURECOUNTS"], "list_wf_names": ["nf-core/nanoseq"]}, {"nb_reuse": 2, "tools": ["FastTree"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process FASTTREE {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fasttree=2.1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fasttree:2.1.10--h516909a_4' :\n        'quay.io/biocontainers/fasttree:2.1.10--h516909a_4' }\"\n\n    input:\n    path alignment\n\n    output:\n    path \"*.tre\",         emit: phylogeny\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    fasttree \\\\\n        $args \\\\\n        -log fasttree_phylogeny.tre.log \\\\\n        -nt $alignment \\\\\n        > fasttree_phylogeny.tre\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fasttree: \\$(fasttree -help 2>&1 | head -1  | sed 's/^FastTree \\\\([0-9\\\\.]*\\\\) .*\\$/\\\\1/')\n    END_VERSIONS\n    \"\"\"\n}", "process FASTTREE {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fasttree=2.1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fasttree:2.1.10--h516909a_4' :\n        'quay.io/biocontainers/fasttree:2.1.10--h516909a_4' }\"\n\n    input:\n    path alignment\n\n    output:\n    path \"*.tre\",         emit: phylogeny\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    fasttree \\\\\n        $args \\\\\n        -log fasttree_phylogeny.tre.log \\\\\n        -nt $alignment \\\\\n        > fasttree_phylogeny.tre\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fasttree: \\$(fasttree -help 2>&1 | head -1  | sed 's/^FastTree \\\\([0-9\\\\.]*\\\\) .*\\$/\\\\1/')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/FASTTREE", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/FASTTREE"], "list_wf_names": ["CDCgov/mycosnp-nf", "nf-core/modules"]}, {"nb_reuse": 4, "tools": ["GATK"], "nb_own": 2, "list_own": ["cguyomar", "nf-core"], "nb_wf": 4, "list_wf": ["modules", "rnavar", "nf-ase", "raredisease"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "cguyomar", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor"], "nb_contrib": 108, "codes": ["process GATK4_CREATESEQUENCEDICTIONARY {\n    tag \"$fasta\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.dict\"       , emit: dict\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    def avail_mem = 6\n    if (!task.memory) {\n        log.info '[GATK CreateSequenceDictionary] Available memory not known - defaulting to 6GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CreateSequenceDictionary \\\\\n        --REFERENCE $fasta \\\\\n        --URI $fasta \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch test.dict\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_CREATESEQUENCEDICTIONARY {\n    tag \"$fasta\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.dict\"       , emit: dict\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    def avail_mem = 6\n    if (!task.memory) {\n        log.info '[GATK CreateSequenceDictionary] Available memory not known - defaulting to 6GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CreateSequenceDictionary \\\\\n        --REFERENCE $fasta \\\\\n        --URI $fasta \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch test.dict\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_CREATESEQUENCEDICTIONARY {\n    tag \"$fasta\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.dict\"       , emit: dict\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    def avail_mem = 6\n    if (!task.memory) {\n        log.info '[GATK CreateSequenceDictionary] Available memory not known - defaulting to 6GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CreateSequenceDictionary \\\\\n        --REFERENCE $fasta \\\\\n        --URI $fasta \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch test.dict\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess GATK4_CREATESEQUENCEDICTIONARY {\n    tag \"$fasta\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.0.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/gatk4:4.2.0.0--0\"\n    } else {\n        container \"quay.io/biocontainers/gatk4:4.2.0.0--0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.dict\"        , emit: dict\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    def avail_mem = 6\n    if (!task.memory) {\n        log.info '[GATK] Available memory not known - defaulting to 6GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        CreateSequenceDictionary \\\\\n        --REFERENCE $fasta \\\\\n        --URI $fasta \\\\\n        $options.args\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/GATK4_CREATESEQUENCEDICTIONARY", "nf-core/modules/nf-core__modules/GATK4_CREATESEQUENCEDICTIONARY", "nf-core/rnavar/nf-core__rnavar/GATK4_CREATESEQUENCEDICTIONARY", "cguyomar/nf-ase/cguyomar__nf-ase/GATK4_CREATESEQUENCEDICTIONARY"], "list_wf_names": ["cguyomar/nf-ase", "nf-core/rnavar", "nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 2, "tools": ["Picard"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process PICARD_FIXMATEINFORMATION {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def STRINGENCY = task.ext.stringency ?: \"STRICT\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard FixMateInformation] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        FixMateInformation \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT ${bam} \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --VALIDATION_STRINGENCY ${STRINGENCY}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard FixMateInformation --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_FIXMATEINFORMATION {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.9--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.9--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def STRINGENCY = task.ext.stringency ?: \"STRICT\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard FixMateInformation] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        FixMateInformation \\\\\n        -Xmx${avail_mem}g \\\\\n        -I ${bam} \\\\\n        -O ${prefix}.bam \\\\\n        --VALIDATION_STRINGENCY ${STRINGENCY}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard FixMateInformation --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_FIXMATEINFORMATION", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/PICARD_FIXMATEINFORMATION"], "list_wf_names": ["CDCgov/mycosnp-nf", "nf-core/modules"]}, {"nb_reuse": 51, "tools": ["FastQC"], "nb_own": 30, "list_own": ["heuermh", "CDCgov", "ggabernet", "harleenduggal", "salzmanlab", "cidgoh", "erikrikarddaniel", "ABMicroBioinf", "vincenthhu", "xiaoli-dong", "LNUc-EEMiS", "chelauk", "ksumngs", "nf-core", "marc-jones", "cancerbioinformatics", "mashehu", "jianhong", "raygozag", "gwright99", "c3g", "marchoeppner", "laclac102", "priyanka-surana", "asthara10", "MrMarkW", "mahesh-panchal", "csf-ngs", "drpatelh", "goodwright"], "nb_wf": 46, "list_wf": ["pathogen", "cidgoh_qc", "genflow-dnaseq", "gwas", "metatdenovo", "nf-core-trimmomatic", "cutandrun", "rnaseq", "ReadZS", "nanostring", "RNASEQ", "nf-core-fetchdata", "nf-core-umipreprocessing", "viralrecon", "nf-core-mutectplatypus", "nf-core-dragen", "test_nfcore_workflow_chain", "modules", "nfcore-rnaseq", "taxprofiler", "nextflow-example", "controldna", "hic", "rnavar", "v-met", "raredisease", "mycosnp-nf", "magmap", "shotgun", "nf-core-hlatyping2", "esga2", "imaps-nf", "ampliseq", "nf-core-readtwoalign", "16S_pipeline", "nf-core_test", "insertseq", "elixir", "magph", "airrflow", "PATCH-pipeline", "nf-core-hicar", "nf-core-blasr", "nf-core-westest", "spatialtranscriptomics", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "tbugfinder", "drejom", "SpikyClip", "ktrns", "jordwil", "FelixKrueger", "salzmanlab", "xingaulaglag", "rfara", "dladd", "kmurat1", "chuan-wang", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "silviamorins", "d4straub", "aanil", "Midnighter", "SPPearce", "yuukiiwa", "samirelanduk", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "mciprianoCDC", "santiagorevale", "anwarMZ", "ppericard", "idot", "kevbrick", "mvanins", "nebfield", "eameyer", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "jcurado-flomics", "ErikaKvalem", "PhilPalmer", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "MrMarkW", "MiguelJulia", "fullama", "kaurravneet4123", "amayer21", "DiegoBrambilla", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "asafpr", "adomingues", "saramonzon", "cjjossart", "pcantalupo", "cjfields", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "xiaoli-dong", "BABS-STP1", "senthil10", "kviljoen", "alexharston", "Gwennid", "Jeremy1805", "charlotte-west", "marc-jones", "chris-cheshire", "oschwengers", "i-pletenev", "asthara10", "jtangrot", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "kaitlinchaung", "ggabernet", "duanjunhyq", "vezzi", "mjcipriano", "skrakau", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "leebrian", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "chelauk", "projectoriented", "praveenraj2018", "tamuanand", "sdomanskyi", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "CharlotteAnne", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "radhika-kataria", "m3hdad", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 186, "codes": ["\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: version\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n    stub:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}_1.fastq.gz\n    touch ${prefix}_2.fastq.gz\n    touch ${prefix}.zip\n    touch ${prefix}.html\n\n    cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: version\n\n    script:\n                                                                          \n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: version\n\n    script:\n                                                                          \n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["chelauk/nf-core-blasr/chelauk__nf-core-blasr/FASTQC", "marc-jones/nextflow-example/marc-jones__nextflow-example/FASTQC", "marchoeppner/esga2/marchoeppner__esga2/FASTQC", "nf-core/spatialtranscriptomics/nf-core__spatialtranscriptomics/FASTQC", "cancerbioinformatics/PATCH-pipeline/cancerbioinformatics__PATCH-pipeline/FASTQC", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/FASTQC", "chelauk/nf-core-mutectplatypus/chelauk__nf-core-mutectplatypus/FASTQC", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/FASTQC", "nf-core/raredisease/nf-core__raredisease/FASTQC", "nf-core/ssds/nf-core__ssds/FASTQC", "nf-core/taxprofiler/nf-core__taxprofiler/FASTQC", "ABMicroBioinf/magph/ABMicroBioinf__magph/FASTQC", "chelauk/nf-core-trimmomatic/chelauk__nf-core-trimmomatic/FASTQC", "laclac102/ampliseq/laclac102__ampliseq/FASTQC", "gwright99/nf-core-dragen/gwright99__nf-core-dragen/FASTQC", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/FASTQC", "drpatelh/nf-core-fetchdata/drpatelh__nf-core-fetchdata/FASTQC", "nf-core/cutandrun/nf-core__cutandrun/FASTQC", "nf-core/rnaseq/nf-core__rnaseq/FASTQC", "ggabernet/elixir/ggabernet__elixir/FASTQC", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/FASTQC", "chelauk/nf-core-umipreprocessing/chelauk__nf-core-umipreprocessing/FASTQC", "priyanka-surana/hic/priyanka-surana__hic/FASTQC", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/FASTQC", "jianhong/16S_pipeline/jianhong__16S_pipeline/FASTQC", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/FASTQC", "nf-core/rnavar/nf-core__rnavar/FASTQC", "LNUc-EEMiS/magmap/LNUc-EEMiS__magmap/FASTQC", "heuermh/nf-core-hlatyping2/heuermh__nf-core-hlatyping2/FASTQC", "nf-core/airrflow/nf-core__airrflow/FASTQC", "nf-core/viralrecon/nf-core__viralrecon/FASTQC", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/FASTQC", "asthara10/insertseq/asthara10__insertseq/FASTQC", "csf-ngs/controldna/csf-ngs__controldna/FASTQC", "LNUc-EEMiS/metatdenovo/LNUc-EEMiS__metatdenovo/FASTQC", "mashehu/nf-core_test/mashehu__nf-core_test/FASTQC", "MrMarkW/nf-core-readtwoalign/MrMarkW__nf-core-readtwoalign/FASTQC", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/FASTQC", "nf-core/ampliseq/nf-core__ampliseq/FASTQC", "nf-core/modules/nf-core__modules/FASTQC", "salzmanlab/ReadZS/salzmanlab__ReadZS/FASTQC", "nf-core/gwas/nf-core__gwas/FASTQC", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/FASTQC", "raygozag/rnaseq/raygozag__rnaseq/FASTQC", "xiaoli-dong/magph/xiaoli-dong__magph/FASTQC", "ksumngs/v-met/ksumngs__v-met/FASTQC", "nf-core/nanostring/nf-core__nanostring/FASTQC", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/FASTQC", "jianhong/shotgun/jianhong__shotgun/FASTQC", "goodwright/imaps-nf/goodwright__imaps-nf/FASTQC", "c3g/genflow-dnaseq/c3g__genflow-dnaseq/FASTQC"], "list_wf_names": ["nf-core/airrflow", "MrMarkW/nf-core-readtwoalign", "ggabernet/elixir", "heuermh/nf-core-hlatyping2", "harleenduggal/RNASEQ", "nf-core/ampliseq", "harleenduggal/nfcore-rnaseq", "goodwright/imaps-nf", "ABMicroBioinf/magph", "c3g/genflow-dnaseq", "nf-core/rnavar", "chelauk/nf-core-mutectplatypus", "LNUc-EEMiS/magmap", "csf-ngs/controldna", "cidgoh/cidgoh_qc", "ABMicroBioinf/pathogen", "chelauk/nf-core-blasr", "nf-core/raredisease", "nf-core/ssds", "vincenthhu/nf-core-westest", "nf-core/modules", "nf-core/nanostring", "nf-core/rnaseq", "asthara10/insertseq", "CDCgov/mycosnp-nf", "gwright99/nf-core-dragen", "jianhong/nf-core-hicar", "jianhong/shotgun", "nf-core/spatialtranscriptomics", "jianhong/16S_pipeline", "erikrikarddaniel/magmap", "nf-core/gwas", "nf-core/taxprofiler", "nf-core/cutandrun", "nf-core/viralrecon", "marchoeppner/esga2", "mashehu/nf-core_test", "priyanka-surana/hic", "mahesh-panchal/test_nfcore_workflow_chain", "xiaoli-dong/magph", "LNUc-EEMiS/metatdenovo", "ksumngs/v-met", "marc-jones/nextflow-example", "laclac102/ampliseq", "xiaoli-dong/pathogen", "raygozag/rnaseq", "salzmanlab/ReadZS", "chelauk/nf-core-trimmomatic", "drpatelh/nf-core-fetchdata", "chelauk/nf-core-umipreprocessing", "cancerbioinformatics/PATCH-pipeline"]}, {"nb_reuse": 11, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "rnavar"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor"], "nb_contrib": 107, "codes": ["process GATK4_VARIANTRECALIBRATOR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    tuple path(vcfs), path(tbis), val(labels)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.recal\")   , emit: recal\n    tuple val(meta), path(\"*.idx\")     , emit: idx\n    tuple val(meta), path(\"*.tranches\"), emit: tranches\n    tuple val(meta), path(\"*plots.R\")  , emit: plots, optional:true\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference_command = fasta ? \"--reference $fasta \" : ''\n    def resource_command = labels.collect{\"--resource:$it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK VariantRecalibrator] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" VariantRecalibrator \\\\\n        --variant $vcf \\\\\n        --output ${prefix}.recal \\\\\n        --tranches-file ${prefix}.tranches \\\\\n        $reference_command \\\\\n        $resource_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_COMBINEGVCFS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_idx)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.combined.g.vcf.gz\"), emit: combined_gvcf\n    path \"versions.yml\"                         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = vcf.collect{\"--variant $it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK COMBINEGVCFS] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CombineGVCFs \\\\\n        $input_list \\\\\n        --output ${prefix}.combined.g.vcf.gz \\\\\n        --reference ${fasta} \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_FILTERMUTECTCALLS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_tbi), path(stats), path(orientationbias), path(segmentation), path(table), val(estimate)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\")            , emit: vcf\n    tuple val(meta), path(\"*.vcf.gz.tbi\")        , emit: tbi\n    tuple val(meta), path(\"*.filteringStats.tsv\"), emit: stats\n    path \"versions.yml\"                          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def orientationbias_command = orientationbias ? orientationbias.collect{\"--orientation-bias-artifact-priors $it\"}.join(' ') : ''\n    def segmentation_command    = segmentation    ? segmentation.collect{\"--tumor-segmentation $it\"}.join(' ')                  : ''\n    def estimate_command        = estimate        ? \" --contamination-estimate ${estimate} \"                                    : ''\n    def table_command           = table           ? \" --contamination-table ${table} \"                                          : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK FilterMutectCalls] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" FilterMutectCalls \\\\\n        --variant $vcf \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        $orientationbias_command \\\\\n        $segmentation_command \\\\\n        $estimate_command \\\\\n        $table_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_VARIANTFILTRATION {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"\t\t         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK VariantFiltration] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}G\" VariantFiltration \\\\\n        --variant $vcf \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_BASERECALIBRATOR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n    path  known_sites\n    path  known_sites_tbi\n\n    output:\n    tuple val(meta), path(\"*.table\"), emit: table\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n    def sites_command = known_sites.collect{\"--known-sites $it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK BaseRecalibrator] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" BaseRecalibrator  \\\\\n        --input $input \\\\\n        --output ${prefix}.table \\\\\n        --reference $fasta \\\\\n        $interval_command \\\\\n        $sites_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_VARIANTFILTRATION {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"\t\t         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK VariantFiltration] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}G\" VariantFiltration \\\\\n        --variant $vcf \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GENOTYPEGVCFS {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(gvcf), path(gvcf_index), path(intervals), path(intervals_index)\n    path  fasta\n    path  fai\n    path  dict\n    path  dbsnp\n    path  dbsnp_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path  \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def gvcf_command = gvcf.name.endsWith(\".vcf\") || gvcf.name.endsWith(\".vcf.gz\") ? \"$gvcf\" : \"gendb://$gvcf\"\n    def dbsnp_command = dbsnp ? \"--dbsnp $dbsnp\" : \"\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenotypeGVCFs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GenotypeGVCFs \\\\\n        --variant $gvcf_command \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        $interval_command \\\\\n        $dbsnp_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_HAPLOTYPECALLER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n    path  dbsnp\n    path  dbsnp_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , optional:true, emit: tbi\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def dbsnp_command = dbsnp ? \"--dbsnp $dbsnp\" : \"\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK HaplotypeCaller] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" HaplotypeCaller \\\\\n        --input $input \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        $dbsnp_command \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_APPLYVQSR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_tbi), path(recal), path(recal_index), path(tranches)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference_command = fasta ? \"--reference $fasta\" : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK ApplyVQSR] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" ApplyVQSR \\\\\n        --variant ${vcf} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $reference_command \\\\\n        --tranches-file $tranches \\\\\n        --recal-file $recal \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_BASERECALIBRATOR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n    path  known_sites\n    path  known_sites_tbi\n\n    output:\n    tuple val(meta), path(\"*.table\"), emit: table\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n    def sites_command = known_sites.collect{\"--known-sites $it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK BaseRecalibrator] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" BaseRecalibrator  \\\\\n        --input $input \\\\\n        --output ${prefix}.table \\\\\n        --reference $fasta \\\\\n        $interval_command \\\\\n        $sites_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_SPLITNCIGARREADS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK SplitNCigarReads] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" SplitNCigarReads \\\\\n        --input $bam \\\\\n        --output ${prefix}.bam \\\\\n        --reference $fasta \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_VARIANTRECALIBRATOR", "nf-core/modules/nf-core__modules/GATK4_COMBINEGVCFS", "nf-core/modules/nf-core__modules/GATK4_FILTERMUTECTCALLS", "nf-core/modules/nf-core__modules/GATK4_VARIANTFILTRATION", "nf-core/rnavar/nf-core__rnavar/GATK4_BASERECALIBRATOR", "nf-core/rnavar/nf-core__rnavar/GATK4_VARIANTFILTRATION", "nf-core/modules/nf-core__modules/GATK4_GENOTYPEGVCFS", "nf-core/modules/nf-core__modules/GATK4_HAPLOTYPECALLER", "nf-core/modules/nf-core__modules/GATK4_APPLYVQSR", "nf-core/modules/nf-core__modules/GATK4_BASERECALIBRATOR", "nf-core/modules/nf-core__modules/GATK4_SPLITNCIGARREADS"], "list_wf_names": ["nf-core/rnavar", "nf-core/modules"]}, {"nb_reuse": 4, "tools": ["SAMtools"], "nb_own": 3, "list_own": ["csf-ngs", "vincenthhu", "nf-core"], "nb_wf": 4, "list_wf": ["controldna", "modules", "raredisease", "nf-core-westest"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "idot", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:38aed4501da19db366dc7c8d52d31d94e760cfaf-0' :\n        'quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:38aed4501da19db366dc7c8d52d31d94e760cfaf-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 \\\\\n        mem \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 -@ $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' :\n        'quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 \\\\\n        mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 -@ $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' :\n        'quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 \\\\\n        mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 -@ $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:38aed4501da19db366dc7c8d52d31d94e760cfaf-0' :\n        'quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:38aed4501da19db366dc7c8d52d31d94e760cfaf-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 \\\\\n        mem \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 -@ $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BWAMEM2_MEM", "csf-ngs/controldna/csf-ngs__controldna/BWAMEM2_MEM", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/BWAMEM2_MEM", "nf-core/raredisease/nf-core__raredisease/BWAMEM2_MEM"], "list_wf_names": ["csf-ngs/controldna", "nf-core/raredisease", "nf-core/modules", "vincenthhu/nf-core-westest"]}, {"nb_reuse": 1, "tools": ["LiftOver"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["\nprocess UCSC_LIFTOVER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-liftover=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-liftover:377--h0b8a92a_3' :\n        'quay.io/biocontainers/ucsc-liftover:377--h0b8a92a_3' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path(chain)\n\n    output:\n    tuple val(meta), path(\"*.lifted.bed\")  , emit: lifted\n    tuple val(meta), path(\"*.unlifted.bed\"), emit: unlifted\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    \"\"\"\n    liftOver \\\\\n        $args \\\n        $bed \\\\\n        $chain \\\\\n        ${prefix}.lifted.bed \\\\\n        ${prefix}.unlifted.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/UCSC_LIFTOVER"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 42, "tools": ["MultiQC"], "nb_own": 34, "list_own": ["NailouZhang", "HeshamElAbd", "FriederikeHanssen", "cmatKhan", "BlackburnLab", "elowy01", "Flomics", "keng404", "nriddiford", "bc2zb", "biggstd", "espelpz", "lauramble", "veitveit", "LaurenceKuhl", "chelauk", "luissian", "czbiohub", "nf-core", "nibscbioinformatics", "khigashi1987", "Jeremy1805", "MicrobialGenomics", "paulstretenowich", "javaidm", "peterk87", "HuipengL", "suzannejin", "bhargava-morampalli", "lengfei5", "maxibor", "jtmccr1", "BarryDigby", "qbic-pipelines"], "nb_wf": 41, "list_wf": ["nfcorepgdb", "cellranger", "obsolete_layer_lab_caw", "nf-ionampliseq", "test-nf-core-pipeline", "hlatyping", "nf-core-wombatp", "pgdb", "bamtofastq", "humgen", "nf-core-hlatyping", "testpipeline", "cdnaseqont-nextflow", "layer_lab_dna_seq_vc", "clipseq", "joao-test", "nf-core-rw", "nf-core-gsfworkflow", "atacseq_nf", "taranispip", "nf-simulaternaseq", "genebygenebact", "CUTRUN_Nextflow", "nf-core-lohcator", "rnaseq-vizfada", "nf-core-influenzangs", "nfcore_test", "nf-core-assemblybacterias", "nf-core-testworkflow", "nf-proportionality", "circ", "nf-core-virome", "nf-core-disambiguate", "nf-core-deviptcore", "nf-core-viralrecon", "nextflow_test", "TTrichiura_Tubulin", "babysarek", "bowtie2-lca", "nf-core-phylofunk", "shrnacount"], "list_contrib": ["HeshamElAbd", "xec-cm", "ypriverol", "FriederikeHanssen", "ewels", "ryanlayer", "evanfloden", "maxulysse", "matrulda", "ggabernet", "cmatKhan", "KochTobi", "elowy01", "keng404", "nf-core-bot", "bleazard", "nriddiford", "biggstd", "espelpz", "pditommaso", "lauramble", "veitveit", "LaurenceKuhl", "olgabot", "chelauk", "nvk747", "luissian", "drewjbeh", "khigashi1987", "amchakra", "charlotte-west", "Jeremy1805", "MicrobialGenomics", "jcurado-flomics", "christopher-mohr", "CharlotteAnne", "peterk87", "javaidm", "mashehu", "suzannejin", "lengfei5", "sven1103", "lescai", "telatin", "apeltzer", "maxibor", "jinmingda", "drpatelh", "BarryDigby"], "nb_contrib": 49, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file mqc_custom_config from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    label 'process_low'\n    publishDir \"${params.outdir}/multiqc\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_pretrim_mqc.collect().ifEmpty([])\n    file ('cutadapt/*') from ch_cutadapt_mqc.collect().ifEmpty([])\n    file ('premap/*') from ch_premap_mqc.collect().ifEmpty([])\n    file ('mapped/*') from ch_align_mqc.collect().ifEmpty([])\n    path ('preseq/*') from ch_preseq_mqc.collect().ifEmpty([])\n    path ('rseqc/*') from ch_rseqc_mqc.collect().ifEmpty([])\n    file ('clipqc/*') from ch_clipqc_mqc.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "process MULTIQC {\n  publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n  input:\n  path(multiqc_config)\n  path(mqc_custom_config)\n  path('fastqc/*')\n  path('samtools/*')\n  path('mosdepth/*')\n  path('mash_screen/*')\n  path('bcftools/*')\n  path('edlib/*')\n  path('consensus/*')\n  path('software_versions/*')\n  path(workflow_summary)\n\n  output:\n  path \"*multiqc_report.html\", emit: multiqc_report\n  path \"*_data\"\n  path \"multiqc_plots\"\n\n  script:\n  custom_runName = params.name\n  if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n    custom_runName = workflow.runName\n  }\n  rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n  rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n  custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                     \n  \"\"\"\n  multiqc -f $rtitle $rfilename $custom_config_file .\n  \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess RunMultiQC {\n    publishDir \"${OUT_DIR}/multiqc\", mode: 'copy'\n\n    input:\n    file (fastqc:'fastqc/*')\n    file ('gatk_base_recalibration/*')\n    file ('gatk_variant_eval/*')\n    \n    output:\n    file '*multiqc_report.html'\n    file '*_data'\n    file '.command.err'\n    val prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = CUSTOM_RUN_NAME ? \"--title \\\"$CUSTOM_RUN_NAME\\\"\" : ''\n    rfilename = CUSTOM_RUN_NAME ? \"--filename \" + CUSTOM_RUN_NAME.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename  . 2>&1\n    \n    \"\"\"\n}", "\nprocess MULTIQC {\n    tag \"$name\"\n    publishDir \"${params.outdir}/multiqc/\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_multiqc && params.macs_gsize && params.blacklist\n\n    input:\n    path (multiqc_config) from ch_multiqc_config\n    path workflow_summary from ch_workflow_summary.collectFile(name: 'workflow_summary_mqc.yaml')\n    path ('fastqc/*') from ch_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trim_1st/fastqc/*') from ch_trim1st_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trim_2nd/fastqc/*') from ch_trim2nd_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('alignment/*') from ch_flagstat_mqc.collect()\n    path ('alignment/picard_metrics/*') from ch_picard_metrics_mqc.collect()\n    path ('macs/*') from ch_macs_mqc.collect().ifEmpty([])\n    path ('deeptools/*') from ch_plotprofile_mqc.collect().ifEmpty([])\n\n    output:\n    path '*multiqc_report.html'\n    path '*_data'\n\n    script:\n    rtitle = \"--title \\\"$params.name\\\"\"\n    rfilename = \"--filename \" + params.name + \"_multiqc_report\"\n    \"\"\"\n    multiqc . -f $rtitle $rfilename\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n    file flagstats from ch_bam_flagstat_mqc.collect()\n    file stats from ch_bam_stats_mqc.collect()\n    file idxstats from ch_bam_idxstat_mqc.collect()\n    file fastqc_bam from ch_fastqc_reports_mqc_input_bam.collect().ifEmpty([])\n    file fastqc_se from ch_fastqc_reports_mqc_se.collect().ifEmpty([]) \n    file fastqc_pe from ch_fastqc_reports_mqc_pe.collect().ifEmpty([])\n\n    output:\n    file \"*multiqc_report.html\"\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f -s $rtitle $rfilename $multiqc_config . \n    \"\"\"\n\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess MULTIQC {\n    publishDir \"${params.outdir}/multiqc/${PEAK_TYPE}\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_multiqc\n\n    input:\n    path (multiqc_config) from ch_multiqc_config\n    path (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n\n    path ('software_versions/*') from ch_software_versions_mqc.collect()\n    path workflow_summary from ch_workflow_summary.collectFile(name: 'workflow_summary_mqc.yaml')\n\n    path ('fastqc/*') from ch_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trimgalore/*') from ch_trimgalore_results_mqc.collect().ifEmpty([])\n    path ('trimgalore/fastqc/*') from ch_trimgalore_fastqc_reports_mqc.collect().ifEmpty([])\n\n    path ('alignment/library/*') from ch_sort_bam_flagstat_mqc.collect()\n\n    path ('alignment/mergedLibrary/*') from ch_mlib_bam_stats_mqc.collect()\n    path ('alignment/mergedLibrary/*') from ch_mlib_rm_orphan_flagstat_mqc.collect{it[1]}\n    path ('alignment/mergedLibrary/*') from ch_mlib_rm_orphan_stats_mqc.collect()\n    path ('alignment/mergedLibrary/picard_metrics/*') from ch_mlib_bam_metrics_mqc.collect()\n    path ('alignment/mergedLibrary/picard_metrics/*') from ch_mlib_collectmetrics_mqc.collect()\n    path ('macs/mergedLibrary/*') from ch_mlib_macs_mqc.collect().ifEmpty([])\n    path ('macs/mergedLibrary/*') from ch_mlib_peak_qc_mqc.collect().ifEmpty([])\n    path ('macs/mergedLibrary/consensus/*') from ch_mlib_macs_consensus_counts_mqc.collect().ifEmpty([])\n    path ('macs/mergedLibrary/consensus/*') from ch_mlib_macs_consensus_deseq_mqc.collect().ifEmpty([])\n    path ('preseq/*') from ch_mlib_preseq_mqc.collect().ifEmpty([])\n    path ('deeptools/*') from ch_mlib_plotprofile_mqc.collect().ifEmpty([])\n    path ('deeptools/*') from ch_mlib_plotfingerprint_mqc.collect().ifEmpty([])\n\n    path ('alignment/mergedReplicate/*') from ch_mrep_bam_flagstat_mqc.collect{it[1]}.ifEmpty([])\n    path ('alignment/mergedReplicate/*') from ch_mrep_bam_stats_mqc.collect().ifEmpty([])\n    path ('alignment/mergedReplicate/*') from ch_mrep_bam_metrics_mqc.collect().ifEmpty([])\n    path ('macs/mergedReplicate/*') from ch_mrep_macs_mqc.collect().ifEmpty([])\n    path ('macs/mergedReplicate/*') from ch_mrep_peak_qc_mqc.collect().ifEmpty([])\n    path ('macs/mergedReplicate/consensus/*') from ch_mrep_macs_consensus_counts_mqc.collect().ifEmpty([])\n    path ('macs/mergedReplicate/consensus/*') from ch_mrep_macs_consensus_deseq_mqc.collect().ifEmpty([])\n\n    output:\n    path '*multiqc_report.html' into ch_multiqc_report\n    path '*_data'\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename $custom_config_file\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file mqc_custom_config from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publishDirMode\n\n    input:\n    file multiqc_config from ch_multiqc_config\n    file mqc_custom_config from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('trimqc/*') from trimqc_results.collect().ifEmpty([])\n    file ('trimrep/*') from trim_reports.collect().ifEmpty([])\n    file ('picardmetrics/*') from picardmetrics.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess RunMultiQC {\n    publishDir \"${OUT_DIR}/multiqc\", mode: 'copy', overwrite: false\n\n    input:\n    file (fastqc:'fastqc/*')\n    file ('gatk_base_recalibration/*')\n    file ('gatk_variant_eval/*')\n    \n    output:\n    file '*multiqc_report.html'\n    file '*_data'\n    file '.command.err'\n    val prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = CUSTOM_RUN_NAME ? \"--title \\\"$CUSTOM_RUN_NAME\\\"\" : ''\n    rfilename = CUSTOM_RUN_NAME ? \"--filename \" + CUSTOM_RUN_NAME.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename  . 2>&1\n    \n    \"\"\"\n}"], "list_proc": ["keng404/nextflow_test/keng404__nextflow_test/multiqc", "czbiohub/nf-simulaternaseq/czbiohub__nf-simulaternaseq/multiqc", "Flomics/joao-test/Flomics__joao-test/multiqc", "HeshamElAbd/nf-core-deviptcore/HeshamElAbd__nf-core-deviptcore/multiqc", "elowy01/nf-core-testworkflow/elowy01__nf-core-testworkflow/multiqc", "luissian/nf-core-assemblybacterias/luissian__nf-core-assemblybacterias/multiqc", "chelauk/nf-core-hlatyping/chelauk__nf-core-hlatyping/multiqc", "nf-core/clipseq/nf-core__clipseq/multiqc", "suzannejin/nf-proportionality/suzannejin__nf-proportionality/multiqc", "NailouZhang/nf-core-virome/NailouZhang__nf-core-virome/multiqc", "bc2zb/nf-core-disambiguate/bc2zb__nf-core-disambiguate/multiqc", "maxibor/test-nf-core-pipeline/maxibor__test-nf-core-pipeline/multiqc", "HuipengL/nfcore_test/HuipengL__nfcore_test/multiqc", "veitveit/nf-core-wombatp/veitveit__nf-core-wombatp/multiqc", "nf-core/testpipeline/nf-core__testpipeline/multiqc", "MicrobialGenomics/TTrichiura_Tubulin/MicrobialGenomics__TTrichiura_Tubulin/multiqc", "bhargava-morampalli/cdnaseqont-nextflow/bhargava-morampalli__cdnaseqont-nextflow/multiqc", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/multiqc", "qbic-pipelines/cellranger/qbic-pipelines__cellranger/multiqc", "peterk87/nf-ionampliseq/peterk87__nf-ionampliseq/MULTIQC", "jtmccr1/nf-core-phylofunk/jtmccr1__nf-core-phylofunk/multiqc", "LaurenceKuhl/shrnacount/LaurenceKuhl__shrnacount/multiqc", "cmatKhan/nf-core-rw/cmatKhan__nf-core-rw/multiqc", "javaidm/obsolete_layer_lab_caw/javaidm__obsolete_layer_lab_caw/RunMultiQC", "khigashi1987/CUTRUN_Nextflow/khigashi1987__CUTRUN_Nextflow/MULTIQC", "FriederikeHanssen/babysarek/FriederikeHanssen__babysarek/multiqc", "BarryDigby/circ/BarryDigby__circ/multiqc", "biggstd/nf-core-gsfworkflow/biggstd__nf-core-gsfworkflow/multiqc", "paulstretenowich/nf-core-viralrecon/paulstretenowich__nf-core-viralrecon/multiqc", "FriederikeHanssen/bamtofastq/FriederikeHanssen__bamtofastq/multiqc", "espelpz/genebygenebact/espelpz__genebygenebact/multiqc", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/multiqc", "FriederikeHanssen/shrnacount/FriederikeHanssen__shrnacount/multiqc", "espelpz/taranispip/espelpz__taranispip/multiqc", "lengfei5/atacseq_nf/lengfei5__atacseq_nf/MULTIQC", "nriddiford/nf-core-lohcator/nriddiford__nf-core-lohcator/multiqc", "BlackburnLab/nfcorepgdb/BlackburnLab__nfcorepgdb/multiqc", "nf-core/hlatyping/nf-core__hlatyping/multiqc", "BlackburnLab/pgdb/BlackburnLab__pgdb/multiqc", "maxibor/bowtie2-lca/maxibor__bowtie2-lca/multiqc", "Jeremy1805/nf-core-influenzangs/Jeremy1805__nf-core-influenzangs/multiqc", "javaidm/layer_lab_dna_seq_vc/javaidm__layer_lab_dna_seq_vc/RunMultiQC"], "list_wf_names": ["lengfei5/atacseq_nf", "khigashi1987/CUTRUN_Nextflow", "MicrobialGenomics/TTrichiura_Tubulin", "lauramble/rnaseq-vizfada", "czbiohub/nf-simulaternaseq", "nf-core/clipseq", "BlackburnLab/nfcorepgdb", "bc2zb/nf-core-disambiguate", "chelauk/nf-core-hlatyping", "HeshamElAbd/nf-core-deviptcore", "Flomics/joao-test", "elowy01/nf-core-testworkflow", "maxibor/bowtie2-lca", "peterk87/nf-ionampliseq", "HuipengL/nfcore_test", "LaurenceKuhl/shrnacount", "nriddiford/nf-core-lohcator", "NailouZhang/nf-core-virome", "espelpz/taranispip", "nf-core/hlatyping", "suzannejin/nf-proportionality", "espelpz/genebygenebact", "BlackburnLab/pgdb", "keng404/nextflow_test", "paulstretenowich/nf-core-viralrecon", "qbic-pipelines/cellranger", "maxibor/test-nf-core-pipeline", "javaidm/obsolete_layer_lab_caw", "FriederikeHanssen/bamtofastq", "bhargava-morampalli/cdnaseqont-nextflow", "cmatKhan/nf-core-rw", "FriederikeHanssen/babysarek", "veitveit/nf-core-wombatp", "luissian/nf-core-assemblybacterias", "javaidm/layer_lab_dna_seq_vc", "biggstd/nf-core-gsfworkflow", "Jeremy1805/nf-core-influenzangs", "jtmccr1/nf-core-phylofunk", "nf-core/testpipeline", "FriederikeHanssen/shrnacount", "nibscbioinformatics/humgen", "BarryDigby/circ"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_ALPHARAREFACTION {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    path(tree)\n    path(stats)\n\n    output:\n    path(\"alpha-rarefaction/*\"), emit: rarefaction\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    maxdepth=\\$(count_table_minmax_reads.py $stats maximum 2>&1)\n\n    #check values\n    if [ \\\"\\$maxdepth\\\" -gt \\\"75000\\\" ]; then maxdepth=\\\"75000\\\"; fi\n    if [ \\\"\\$maxdepth\\\" -gt \\\"5000\\\" ]; then maxsteps=\\\"250\\\"; else maxsteps=\\$((maxdepth/20)); fi\n    qiime diversity alpha-rarefaction  \\\n        --i-table ${table}  \\\n        --i-phylogeny ${tree}  \\\n        --p-max-depth \\$maxdepth  \\\n        --m-metadata-file ${metadata}  \\\n        --p-steps \\$maxsteps  \\\n        --p-iterations 10  \\\n        --o-visualization alpha-rarefaction.qzv\n    qiime tools export --input-path alpha-rarefaction.qzv  \\\n        --output-path alpha-rarefaction\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_ALPHARAREFACTION {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(metadata)\n    path(table)\n    path(tree)\n    path(stats)\n\n    output:\n    path(\"alpha-rarefaction/*\"), emit: rarefaction\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    maxdepth=\\$(count_table_minmax_reads.py $stats maximum 2>&1)\n\n    #check values\n    if [ \\\"\\$maxdepth\\\" -gt \\\"75000\\\" ]; then maxdepth=\\\"75000\\\"; fi\n    if [ \\\"\\$maxdepth\\\" -gt \\\"5000\\\" ]; then maxsteps=\\\"250\\\"; else maxsteps=\\$((maxdepth/20)); fi\n    qiime diversity alpha-rarefaction  \\\n        --i-table ${table}  \\\n        --i-phylogeny ${tree}  \\\n        --p-max-depth \\$maxdepth  \\\n        --m-metadata-file ${metadata}  \\\n        --p-steps \\$maxsteps  \\\n        --p-iterations 10  \\\n        --o-visualization alpha-rarefaction.qzv\n    qiime tools export --input-path alpha-rarefaction.qzv  \\\n        --output-path alpha-rarefaction\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_ALPHARAREFACTION", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_ALPHARAREFACTION"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["Scoary"], "nb_own": 2, "list_own": ["bactopia", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "bactopia"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "Accio", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "fmaguire", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "TGotwig"], "nb_contrib": 108, "codes": ["\nprocess SCOARY {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/scoary:1.6.16--py_2' :\n        'quay.io/biocontainers/scoary:1.6.16--py_2' }\"\n\n    input:\n    tuple val(meta), path(genes)\n    path(traits)\n\n    output:\n    tuple val(meta), path(\"*.csv\"), emit: csv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"scoary\"\n    \"\"\"\n    scoary \\\\\n        $options.args \\\\\n        --no-time \\\\\n        --threads $task.cpus \\\\\n        --traits $traits \\\\\n        --genes $genes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        scoary: \\$( scoary --version 2>&1 )\n    END_VERSIONS\n    \"\"\"\n}", "process SCOARY {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::scoary=1.6.16\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/scoary:1.6.16--py_2' :\n        'quay.io/biocontainers/scoary:1.6.16--py_2' }\"\n\n    input:\n    tuple val(meta), path(genes), path(traits)\n    path(tree)\n\n    output:\n    tuple val(meta), path(\"*.csv\"), emit: csv\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def newick_tree = tree ? \"-n ${tree}\" : \"\"\n    \"\"\"\n    scoary \\\\\n        $args \\\\\n        --no-time \\\\\n        --threads $task.cpus \\\\\n        --traits $traits \\\\\n        --genes $genes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        scoary: \\$( scoary --version 2>&1 )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["bactopia/bactopia/bactopia__bactopia/SCOARY", "nf-core/modules/nf-core__modules/SCOARY"], "list_wf_names": ["bactopia/bactopia", "nf-core/modules"]}, {"nb_reuse": 3, "tools": ["ARTIC"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 3, "list_wf": ["modules", "test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process ARTIC_GUPPYPLEX {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::artic=1.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/artic:1.2.1--py_0' :\n        'quay.io/biocontainers/artic:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(fastq_dir)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    artic \\\\\n        guppyplex \\\\\n        $args \\\\\n        --directory $fastq_dir \\\\\n        --output ${prefix}.fastq\n\n    pigz -p $task.cpus *.fastq\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        artic: \\$(artic --version 2>&1 | sed 's/^.*artic //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process ARTIC_GUPPYPLEX {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::artic=1.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/artic:1.2.1--py_0' :\n        'quay.io/biocontainers/artic:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(fastq_dir)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    artic \\\\\n        guppyplex \\\\\n        $args \\\\\n        --directory $fastq_dir \\\\\n        --output ${prefix}.fastq\n\n    pigz -p $task.cpus *.fastq\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        artic: \\$(artic --version 2>&1 | sed 's/^.*artic //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process ARTIC_GUPPYPLEX {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::artic=1.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/artic:1.2.1--py_0' :\n        'quay.io/biocontainers/artic:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(fastq_dir)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    artic \\\\\n        guppyplex \\\\\n        $args \\\\\n        --directory $fastq_dir \\\\\n        --output ${prefix}.fastq\n\n    pigz -p $task.cpus *.fastq\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        artic: \\$(artic --version 2>&1 | sed 's/^.*artic //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/ARTIC_GUPPYPLEX", "nf-core/modules/nf-core__modules/ARTIC_GUPPYPLEX", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/ARTIC_GUPPYPLEX"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 59, "tools": ["SAMtools"], "nb_own": 42, "list_own": ["Genomic-Medicine-Linkoping", "b600a", "doudoufeishangtian", "lifebit-ai", "h3abionet", "GMS6804-master", "oisinmccaffrey", "isugifNF", "UMCUGenetics", "mvanins", "csiro-crop-informatics", "sickle-in-africa", "liameabbott", "genomic-medicine-sweden", "melnel000", "erikwaskiewicz", "nf-modules", "UCL-BLIC", "FAANG", "ampatchlab", "WarrenLab", "chelauk", "samlhao", "CRG-CNAG", "nf-core", "nibscbioinformatics", "gates-mri-bioinformatics", "bcgsc", "DLBPointon", "esrice", "SergFern", "clairecoleman1", "CENMIG", "StaPH-B", "rmoran7", "sripaladugu", "AgBC-UoP", "elyadlezmi", "cgpu", "bioinfo-pf-curie", "baiyuanxiang", "jambler24"], "nb_wf": 57, "list_wf": ["TMBur", "sarek_ubec", "HPCBio-Refgraph_pipeline", "Sarek_CBIO", "germline_somatic", "repset", "nf-pdx", "freebayes", "dx_sarek", "humgen", "samtools", "sarek-mirror-cache", "clipseq", "haplosarek", "CalliNGS-NF", "bac_pangenome", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "eager", "clipseq1", "RareDisease_RNA_workflow", "Sarek", "annotationMaker", "pgp-chronek", "sarek-mirror", "shortread-polish-nf", "methylseq", "test_nextflow_sarek", "sarek-genomechronicler", "ngs_nextflow", "HipSTR", "ctdna-project", "clipseq.nextflow", "Variant_Calling", "nf-rnasnv", "saw.sarek", "stress_granule_RNA_manuscript", "reference-database", "custom_sarek", "tuber", "mycosnp", "sarek", "GenomeChronicler-Sarek-nf", "snpplet", "nextflow-spid", "saw.snv-indel", "exoseq", "RNA2CM", "GangSTR", "long-reads", "nf-dnaseq", "GSM-pipeline", "WGSpipe-nf", "workflows", "annotation-pipeline-nextflow", "mapNclean-nf", "nf-core-sarek"], "list_contrib": ["FelixKrueger", "alexhbnr", "oisinmccaffrey", "pallolason", "aidaanva", "jfnavarro", "adrlar", "amchakra", "kojix2", "ffmmulder", "rmoran7", "lescai", "cgpu", "phue", "alesssia", "FriederikeHanssen", "b600a", "maxulysse", "pprieto", "rsuchecki", "doudoufeishangtian", "colindaven", "jfy133", "ythaworn", "szilvajuhos", "Sebastian-D", "mvanins", "jackmo375", "noirot", "stekaz", "grendon", "clairecoleman1", "emi80", "PhilPalmer", "jongtaek-kim", "Hammarn", "gdevailly", "sven1103", "jemten", "elyadlezmi", "Jani-94", "alexandregilardet", "ewels", "evanfloden", "shyaman", "pcantalupo", "cjfields", "TCLamnidis", "alexwhan", "IdoBar", "senthil10", "olgabot", "kkowalden", "samlhao", "scarlhoff", "lconde-ucl", "isugif", "yuantianhpc", "charlotte-west", "esrice", "SergFern", "Remi-Montagne", "ashildv", "CENMIG", "nservant", "sc13-bioinf", "LeuThrAsp", "drpatelh", "bhbhjs", "alneberg", "RichardCorbett", "arontommi", "anvlasova", "ggabernet", "skrakau", "BrunoGrandePhD", "ZandraFagernas", "nf-core-bot", "bleazard", "liameabbott", "erikwaskiewicz", "abhi18av", "pditommaso", "robsyme", "nvk747", "chelauk", "projectoriented", "marcelm", "j23414", "malinlarsson", "charles-plessy", "DLBPointon", "mashehu", "CharlotteAnne", "J35P312", "mdeloger", "apeltzer", "snewhouse", "waffle-iron", "maxibor", "jtk622", "jambler24", "davidmasp"], "nb_contrib": 103, "codes": ["\nprocess buildSamtoolsIndex {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n    container params.samtoolsImage\n\n\twhen:\n\t!file(params.referenceSequence['path'] + '.fai').exists()\n\n\tscript:\n\t\"\"\"\n\tsamtools faidx ${params.referenceSequence['path']}\n\t\"\"\"\n}", " process makeFastaIndex {\n        tag \"$fasta\"\n        publishDir path: \"${params.outdir}/reference_genome\", saveAs: { params.save_reference ? it : null }, mode: params.publish_dir_mode\n\n        input:\n        file fasta from ch_fasta_for_makeFastaIndex\n\n        output:\n        file \"${fasta}.fai\" into ch_fasta_index_for_methyldackel\n\n        script:\n        \"\"\"\n        samtools faidx $fasta\n        \"\"\"\n    }", "\nprocess index_fasta{\n\n    input:\n        path fasta\n\n    output:\n        path('*.fai'), emit: fai\n\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess samtools_faidx {\n  echo true\n  tag { fasta }\n  publishDir params.outdir,  mode: 'copy', overwrite: false\n\ninput:\n  file fasta\n\noutput:\n  file '*.fai' into samtools_fai\n\n\"\"\"\nsamtools faidx $fasta\n\"\"\"\n}", "\nprocess index_fasta {\n    publishDir \"${params.genomes_directory}/fasta\", \\\n        pattern: \"reference.fa.gz.{fai,gzi}\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(fasta)\n\n    output:\n    path(\"reference.fa.gz.fai\")\n    path(\"reference.fa.gz.gzi\")\n\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.fai\") into fastaFaiBuilt\n\n  when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "\nprocess samtools_index_reference {\n\ttag \"$fasta\"\n\tinput:\n\t\tpath(fasta) \n\n\toutput:\n\t\tpath(\"${fasta}.fai\") \n        \n\tscript:\n\t\"\"\"\n\t\t/usr/TMB/samtools faidx ${fasta}\n    \"\"\"\n}", " process indexFasta {\n         \n        publishDir \"$projectDir/data\", mode: 'copy'\n        \n        input:\n        file fasta from fasta4Indexing \n         \n        output: \n        path '*' into fastaIndex\n\n        \"\"\"\n        samtools faidx $fasta \n        \"\"\"  }", "\nprocess generate_fai{\n\n  tag \"$ref\"\n\n    \tinput:\n   \tpath(ref) from ch_ref_fai\n\n   \toutput:\n   \tpath(\"*.fai\") into (ch_fai_crosslinks, ch_fai_piranha_motif)\n\n    \tscript:\n    \t\"\"\"\n    \tsamtools faidx $ref\n    \t\"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess prepare_genome{\n    tag                    { \"PREP:${genome}\" }\n    executor               myExecutor\n    clusterOptions         params.clusterAcct \n    cpus                   defaultCPU\n    queue                  params.myQueue\n    memory                 \"$defaultMemory GB\"\n    module                 \"SAMtools/1.12-IGB-gcc-8.2.0\"\n    storeDir               genomeStore\n    \n    input:\n    file genome from genome_file\n\n    output:\n    file \"*.fai\" into genome_index_ch\n    \n    script:\n    \"\"\"\n    samtools faidx ${genome}\n    \"\"\"\n}", " process preprocess_genome {\n\n\t\t\ttag \"${fasta}\"\n\t\t\tcontainer 'lifebitai/preprocessingvctools'\n\n      input:\n      file fasta from fastaToFai\n\n      output:\n      file(\"${fasta}.fai\") into fai\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      \"\"\"\n  }", "\nprocess BuildFastaFai {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fastaFaiBuilt\n\n    when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess makeFastaIndex {\n    label 'sc_small'\n    tag \"${fasta}\"\n    publishDir path: \"${params.outdir}/reference_genome/fasta_index\", mode: params.publish_dir_mode, saveAs: { filename -> \n            if (params.save_reference) filename \n            else if(!params.save_reference && filename == \"where_are_my_files.txt\") filename\n            else null\n    }\n    \n    when: !params.fasta_index && params.fasta && ( params.mapper == 'bwaaln' || params.mapper == 'bwamem' || params.mapper == 'circularmapper')\n\n    input:\n    path fasta from ch_fasta_for_faidx\n    path where_are_my_files\n\n    output:\n    path \"*.fai\" into ch_fasta_faidx_index\n    path \"where_are_my_files.txt\"\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.fai\") into fastaFaiBuilt\n\n  when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "\nprocess PREPARE_GENOME_SAMTOOLS { \n  tag \"$ref_genome.baseName\"\n \n  input: \n    path ref_genome\n \n  output: \n    path \"${ref_genome}.fai\"\n  \n  script:\n  \"\"\"\n  samtools faidx $ref_genome\n  \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess indexFasta {\n  label 'samtools'\n  label 'lowCpu'\n  label 'lowMem'\n\n  publishDir \"${params.outDir}/genome\", mode: 'copy',\n    saveAs: {filename -> if (filename.indexOf(\".log\") > 0) \"logs/$filename\" else filename}\n\n  input:\n  file(fasta) from chFasta\n\n  output:\n  set file(fasta), file(\"*.fai\") into chFastaDict, chFastaSize, chFastaEffgsize\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "\nprocess BuildSAMToolsIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaForSAMTools\n\n  output:\n    file(\"*.fai\") into ch_samtoolsIndex\n\n  script:\n  \"\"\"\n  samtools faidx ${f_reference}\n  \"\"\"\n}", " process makeFastaIndex {\n        tag \"$fasta\"\n        publishDir path: \"${params.outdir}/reference_genome\", saveAs: { params.save_reference ? it : null }, mode: params.publish_dir_mode\n\n        input:\n        file fasta from ch_fasta_for_makeFastaIndex\n\n        output:\n        file \"${fasta}.fai\" into ch_fasta_index_for_methyldackel\n\n        script:\n        \"\"\"\n        samtools faidx $fasta\n        \"\"\"\n    }", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", " process preprocess_genome {\n\n\t\t\ttag \"${fasta}\"\n\t\t\tcontainer 'lifebitai/preprocessingvctools'\n\n      input:\n      file fasta from fastaToFai\n\n      output:\n      file(\"${fasta}.fai\") into fai\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      \"\"\"\n  }", "\nprocess createFastaIndex {\n\n  container = \"$samtools19_container\"\n\n  publishDir \"${params.outdir}\", mode: 'copy', pattern: '*fai'\n\n  input:\n  path genome from genome_samtools\n\n\n  output:\n  file(\"*.fai\") into freebayes_fai\n  file(\"*.fai\") into freebayes_fai2\n\n  script:\n\n  \"\"\"\nsamtools faidx ${genome} \n\n  \"\"\"\n\n}", "\nprocess PREPARE_GENOME_SAMTOOLS { \n  tag \"$genome.baseName\"\n \n  input: \n    path genome\n \n  output: \n    path \"${genome}.fai\"\n  \n  script:\n  \"\"\"\n  samtools faidx ${genome}\n  \"\"\"\n}", "\nprocess samtools_faidx {\n    tag \"SAMTOOLS FAIDX on $ref\"\n    publishDir \"${params.outdir}/bwa_aligned\", mode: 'copy'\n\n    input:\n    path(ref)\n\n    output:\n    path \"${ref}.fai\", emit: out_faidx\n\n    script:\n    \"\"\"\n    samtools faidx ${ref}\n    \"\"\"\n}", "\nprocess samtools_faidx {\n\n    tag { fasta.name }\n\n    label 'samtools'\n\n    publishDir(\n        path: \"${params.publish_dir}/${task.process.replaceAll(':', '/')}\",\n        enabled: params.publish_everything || params.publish_samtools_faidx,\n        mode: params.publish_mode,\n    )\n\n    input:\n    path fasta\n\n    output:\n    path \"${fasta}.fai\"\n\n    \"\"\"\n    samtools faidx \"${fasta}\"\n    \"\"\"\n}", "\nprocess faidxGenomeFASTA {\n  tag(\"${refmeta}\")\n  label 'samtools'\n\n  input:\n    set val(refmeta), file(infiles) from stagedReferences\n\n  output:\n    set val(refmeta), file(\"${ref}.fai\") into genomeIndicesForReadCoordinateConversion\n    set val(refmeta), file(ref), file(\"${ref}.fai\") into genomesForIndexing, genomesForRnfSimReads\n    set val(refmeta), file(ref), file(\"${ref}.fai\"), file(\"${gff}\") optional true into referencesForTranscriptomeExtraction                  \n\n  script:\n  ref = infiles[infiles.findIndexOf { fname -> fname =~ /\\.fasta$/ }]\n  gffIdx = infiles.findIndexOf { fname -> fname =~ /\\.gff$/ }\n  gff = gffIdx >= 0 ? infiles[gffIdx] : 'NO_GFF_HERE'\n  \"\"\"\n  samtools faidx ${ref}\n  \"\"\"\n}", "\nprocess faidxTranscriptomeFASTA {\n  tag(\"${refmeta}\")\n  label 'samtools'\n\n  input:\n    set val(refmeta), file(fa) from extractedTranscriptomes\n\n  output:\n    set val(refmeta), file(fa), file(\"${fa}.fai\") into transcriptomesForIndexing, transcriptomesForRnfSimReads\n\n  script:\n  \"\"\"\n  samtools faidx ${fa}\n  \"\"\"\n}", " process index_genome{\n    cpus 1\n    time '2h'\n    memory '5G'\n\n    publishDir \"${referenceOut}/original/\", mode:'copy', overwrite: true\n\n    tag \"indexing ${params.genomeFasta}\"\n\n    input:\n    file(genomeFasta)\n    file(genomeAnnotations)\n\n    output:\n    set file(genomeFasta), file(\"*.fai\") into genome_euk_tRNA, genome_mit_tRNA, genome_prepare, genome_annotations\n    file(\"${genomeAnnotations}\")\n    file(\"*.fai\") into geome_fai\n    file(\"${genomeFasta}\") into genome_fasta\n\n    script:\n    \"\"\"\n    samtools faidx ${genomeFasta}\n    \"\"\"\n  }", "\nprocess BuildFastaFai {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.fai\") into fastaFaiBuilt\n\n  when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fastaFaiBuilt\n\n    when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", " process preprocess_fasta_index {\n        publishDir \"$baseDir/data/test_data/input/\", mode: \"copy\"\n\n        input:\n            file(fasta) from genome_fasta_file\n\n        output:\n            file(\"${fasta.name + '.fai'}\") into genome_fasta_index\n\n        script:\n        \"\"\"\n        samtools faidx $fasta > ${fasta.name + '.fai'}\n        \"\"\"\n    }", "\nprocess prepare_genome_samtools {\n  tag \"$genome.baseName\"\n\n  input:\n      file genome from genome_file\n\n  output:\n      file \"${genome}.fai\" into genome_index_ch\n\n  script:\n  \"\"\"\n  samtools faidx ${genome}\n  \"\"\"\n}", "\nprocess faidx {\n    publishDir params.faidxResultsDir, mode: params.saveMode\n    container 'quay.io/biocontainers/samtools:1.10--h2e538c0_3'\n\n    when:\n    params.faidx\n\n    input:\n    path refFasta from ch_refFasta\n\n    output:\n    file('*.fai') into ch_out_faidx\n\n    script:\n\n    \"\"\"\n    samtools faidx $params.refFasta\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAI_INDEX_FOR_INPUT_GENOME_FASTA {\n\n\tinput:\n\tpath fasta\n\toutput:\n\tpath (\"*.fai\"), emit: genome_fai\n\tscript:\n\t\"\"\"\n\tsamtools faidx $fasta\n\t\"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fastaFaiBuilt\n\n    when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess index_fasta {\n    publishDir \"${params.genomes_directory}/fasta\", \\\n        pattern: \"reference.fa.gz.{fai,gzi}\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(fasta)\n\n    output:\n    path(\"reference.fa.gz.fai\")\n    path(\"reference.fa.gz.gzi\")\n\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess index_samtools {\n\n  input:\n  file reference from fasta_ch1\n\n  output:\n  file \"${reference}*\" into index_ch1\n\n  script:\n  \"\"\"\n  samtools faidx $reference\n  \"\"\"\n      }", " process makeFastaIndex {\n        tag \"$params.gfasta\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/reference_genome\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta from fasta_for_samtools_index\n\n        output:\n        file \"*.fai\" into samtools_index\n\n        script:\n        \"\"\"\n        samtools faidx $fasta\n        \"\"\"\n    }", "\nprocess PREPARE_GENOME_SAMTOOLS { \n  tag \"$ref_genome.baseName\"\n \n  input: \n    path ref_genome\n \n  output: \n    path \"${ref_genome}.fai\"\n  \n  script:\n  \"\"\"\n  samtools faidx $ref_genome\n  \"\"\"\n}", "\nprocess faidx {\n    input:\n    path assembly\n\n    output:\n    path \"${assembly}.fai\", emit: fai\n\n    \"\"\"\n    samtools faidx $assembly\n    \"\"\"\n}", "\nprocess samtools_faidx {\n\n    tag { fasta.name }\n\n    label 'samtools'\n\n    publishDir(\n        path: \"${params.publish_dir}/${task.process.replaceAll(':', '/')}\",\n        enabled: params.publish_everything || params.publish_samtools_faidx,\n        mode: params.publish_mode,\n    )\n\n    input:\n    path fasta\n\n    output:\n    path \"${fasta}.fai\"\n\n    \"\"\"\n    samtools faidx \"${fasta}\"\n    \"\"\"\n}", " process preprocess_fai {\n      tag \"${fasta}\"\n      container 'lifebitai/samtools:latest'\n\n      input:\n      file(fasta) from fasta_to_index\n\n      output:\n      file(\"${fasta}.fai\") into (fai_clairvoyante, fai_sniffles)\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      \"\"\"\n  }", "\nprocess samtools_faidx {\n    storeDir params.refdir\n\n    label 'samtools'\n\n    input:\n    file ref_fasta\n\n    output:\n    file \"${ref_fasta}.fai\" into ref_faidx\n\n    \"\"\"\n    samtools faidx \"${ref_fasta}\"\n    \"\"\"\n}", "\nprocess BuildSAMToolsIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaForSAMTools\n\n  output:\n    file(\"*.fai\") into ch_samtoolsIndex\n\n  script:\n  \"\"\"\n  samtools faidx ${f_reference}\n  \"\"\"\n}", "\nprocess indexReference_2{\n    publishDir \"/home/ldotrang/C_auris_testing/nfBWAref/bwa_reference_output\", mode: 'copy'\n                                               \n    input:\n    file (masked_ref) from masked_reference_seq_idx\n\n    output:\n    file(\"*\") into indexed_reference_fasta\n\n    shell:\n    \"\"\"\n    samtools faidx ${masked_ref}\n    \"\"\"\n}", "\nprocess samtools_faidx {\n    module 'samtools/samtools-1.9'\n\n    input:\n    file reference from reference_file\n\n    output:\n    file \"${reference}.fai\" into faidx\n\n    \"\"\" samtools faidx ${reference} \"\"\"\n}", "\nprocess BuildFastaFai {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.fai\") into fastaFaiBuilt\n\n  when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "\nprocess generate_fai{\n\n  tag \"$ref\"\n\n    \tinput:\n   \tpath(ref) from ch_ref_fai\n\n   \toutput:\n   \tpath(\"*.fai\") into (ch_fai_crosslinks, ch_fai_icount, ch_fai_icount_motif, ch_fai_paraclu_motif, ch_fai_pureclip_motif, ch_fai_piranha_motif)\n\n    \tscript:\n    \t\"\"\"\n    \tsamtools faidx $ref\n    \t\"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess samtools_faidx {\n    label 'process_low'\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n    input:\n    path(fasta_gz) from bgzip_fasta_ch\n\n    output:\n    tuple(path(fasta_gz), path(\"${fasta_gz}.fai\")) into (fasta_gz_ch, asm_fasta_ch)\n\n    when:\n    params.fasta\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta_gz}\n    \"\"\"\n}", " process generate_fai {\n        tag \"$fasta\"\n        label 'process_low'\n\n        input:\n        path(fasta) from ch_fasta_fai\n\n        output:\n        path(\"*.fai\") into (ch_fai_crosslinks, ch_fai_icount, ch_fai_icount_motif, ch_fai_paraclu_motif, ch_fai_pureclip_motif, ch_fai_piranha_motif)\n\n        script:\n        \"\"\"\n        samtools faidx $fasta\n        \"\"\"\n    }", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.fai\") into fastaFaiBuilt\n\n  when: !(params.fastaFai) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", " process Indexing_custom_genome {\n    tag \"Indexes supplied reference FASTA file (Samtools)\"\n    label 'big_mem'\n    label 'generic'\n\n    publishDir \"$params.indir/custom_reference\"\n\n\n    input:\n    file reference_file from ch_reference\n\n    output:\n    file(\"${reference_file[0]}\")\n    file '*.fai'\n    \n    \"\"\"\n    samtools faidx ${reference_file[0]}\n    \"\"\"\n    \n  }", "\nprocess BuildSAMToolsIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: params.publishDirMode\n\n  input:\n    file(f_reference) from ch_fastaForSAMTools\n\n  output:\n    file(\"*.fai\") into ch_samtoolsIndex\n\n  script:\n  \"\"\"\n  samtools faidx ${f_reference}\n  \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/sickle-in-africa__saw.snv-indel/buildSamtoolsIndex", "nf-core/methylseq/nf-core__methylseq/makeFastaIndex", "genomic-medicine-sweden/RareDisease_RNA_workflow/genomic-medicine-sweden__RareDisease_RNA_workflow/index_fasta", "baiyuanxiang/ngs_nextflow/baiyuanxiang__ngs_nextflow/samtools_faidx", "liameabbott/reference-database/liameabbott__reference-database/index_fasta", "cgpu/sarek-mirror/cgpu__sarek-mirror/BuildFastaFai", "bcgsc/TMBur/bcgsc__TMBur/samtools_index_reference", "elyadlezmi/RNA2CM/elyadlezmi__RNA2CM/indexFasta", "clairecoleman1/clipseq1/clairecoleman1__clipseq1/generate_fai", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/BuildFastaFai", "h3abionet/HPCBio-Refgraph_pipeline/h3abionet__HPCBio-Refgraph_pipeline/prepare_genome", "lifebit-ai/GangSTR/lifebit-ai__GangSTR/preprocess_genome", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BuildFastaFai", "nf-core/eager/nf-core__eager/makeFastaIndex", "cgpu/pgp-chronek/cgpu__pgp-chronek/BuildFastaFai", "CENMIG/snpplet/CENMIG__snpplet/PREPARE_GENOME_SAMTOOLS", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BuildFastaFai", "bioinfo-pf-curie/annotationMaker/bioinfo-pf-curie__annotationMaker/indexFasta", "melnel000/Sarek_CBIO/melnel000__Sarek_CBIO/BuildSAMToolsIndex", "FAANG/GSM-pipeline/FAANG__GSM-pipeline/makeFastaIndex", "nf-core/sarek/nf-core__sarek/BuildFastaFai", "lifebit-ai/HipSTR/lifebit-ai__HipSTR/preprocess_genome", "isugifNF/freebayes/isugifNF__freebayes/createFastaIndex", "CRG-CNAG/CalliNGS-NF/CRG-CNAG__CalliNGS-NF/PREPARE_GENOME_SAMTOOLS", "DLBPointon/annotation-pipeline-nextflow/DLBPointon__annotation-pipeline-nextflow/samtools_faidx", "ampatchlab/nf-dnaseq/ampatchlab__nf-dnaseq/samtools_faidx", "csiro-crop-informatics/repset/csiro-crop-informatics__repset/faidxGenomeFASTA", "csiro-crop-informatics/repset/csiro-crop-informatics__repset/faidxTranscriptomeFASTA", "mvanins/stress_granule_RNA_manuscript/mvanins__stress_granule_RNA_manuscript/index_genome", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/BuildFastaFai", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BuildFastaFai", "erikwaskiewicz/ctdna-project/erikwaskiewicz__ctdna-project/preprocess_fasta_index", "jambler24/bac_pangenome/jambler24__bac_pangenome/prepare_genome_samtools", "nf-modules/samtools/nf-modules__samtools/faidx", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/BuildFastaFai", "doudoufeishangtian/WGSpipe-nf/doudoufeishangtian__WGSpipe-nf/SAMTOOLS_FAI_INDEX_FOR_INPUT_GENOME_FASTA", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/BuildFastaFai", "rmoran7/custom_sarek/rmoran7__custom_sarek/BuildFastaFai", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/BuildFastaFai", "gates-mri-bioinformatics/reference-database/gates-mri-bioinformatics__reference-database/index_fasta", "AgBC-UoP/mapNclean-nf/AgBC-UoP__mapNclean-nf/index_samtools", "nf-core/exoseq/nf-core__exoseq/makeFastaIndex", "b600a/tuber/b600a__tuber/PREPARE_GENOME_SAMTOOLS", "WarrenLab/shortread-polish-nf/WarrenLab__shortread-polish-nf/faidx", "ampatchlab/nf-pdx/ampatchlab__nf-pdx/samtools_faidx", "lifebit-ai/long-reads/lifebit-ai__long-reads/preprocess_fai", "ampatchlab/nf-rnasnv/ampatchlab__nf-rnasnv/samtools_faidx", "GMS6804-master/Sarek/GMS6804-master__Sarek/BuildSAMToolsIndex", "StaPH-B/mycosnp/StaPH-B__mycosnp/indexReference_2", "esrice/workflows/esrice__workflows/samtools_faidx", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/BuildFastaFai", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/generate_fai", "rmoran7/dx_sarek/rmoran7__dx_sarek/BuildFastaFai", "samlhao/nextflow-spid/samlhao__nextflow-spid/samtools_faidx", "nf-core/clipseq/nf-core__clipseq/generate_fai", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/BuildFastaFai", "cgpu/haplosarek/cgpu__haplosarek/BuildFastaFai", "SergFern/Variant_Calling/SergFern__Variant_Calling/Indexing_custom_genome", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/BuildSAMToolsIndex"], "list_wf_names": ["lifebit-ai/HipSTR", "Genomic-Medicine-Linkoping/nf-core-sarek", "doudoufeishangtian/WGSpipe-nf", "elyadlezmi/RNA2CM", "nf-core/clipseq", "GMS6804-master/Sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "samlhao/nextflow-spid", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "nf-core/methylseq", "genomic-medicine-sweden/RareDisease_RNA_workflow", "sickle-in-africa/saw.snv-indel", "cgpu/sarek-mirror", "bcgsc/TMBur", "WarrenLab/shortread-polish-nf", "baiyuanxiang/ngs_nextflow", "gates-mri-bioinformatics/reference-database", "bioinfo-pf-curie/annotationMaker", "mvanins/stress_granule_RNA_manuscript", "sickle-in-africa/saw.sarek", "CENMIG/snpplet", "cgpu/haplosarek", "AgBC-UoP/mapNclean-nf", "lifebit-ai/GangSTR", "ampatchlab/nf-pdx", "liameabbott/reference-database", "b600a/tuber", "csiro-crop-informatics/repset", "sripaladugu/germline_somatic", "clairecoleman1/clipseq1", "chelauk/test_nextflow_sarek", "nf-core/sarek", "ampatchlab/nf-dnaseq", "esrice/workflows", "jambler24/bac_pangenome", "SergFern/Variant_Calling", "cgpu/sarek-mirror-cache", "FAANG/GSM-pipeline", "isugifNF/freebayes", "nf-core/eager", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "CRG-CNAG/CalliNGS-NF", "rmoran7/dx_sarek", "StaPH-B/mycosnp", "cgpu/pgp-chronek", "UCL-BLIC/Sarek_v2.3.FIX1", "DLBPointon/annotation-pipeline-nextflow", "lifebit-ai/long-reads", "nf-modules/samtools", "oisinmccaffrey/clipseq.nextflow", "melnel000/Sarek_CBIO", "erikwaskiewicz/ctdna-project", "ampatchlab/nf-rnasnv", "h3abionet/HPCBio-Refgraph_pipeline", "nf-core/exoseq", "nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["TRAP", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess pmdtools {\n    label 'mc_medium'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/pmdtools\", mode: params.publish_dir_mode\n\n    when: params.run_pmdtools\n\n    input: \n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_rmdup_for_pmdtools\n    file fasta from ch_fasta_for_pmdtools.collect()\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.pmd.bam\"), path(\"*.pmd.bam.{bai,csi}\") into ch_output_from_pmdtools\n    file \"*.cpg.range*.txt\"\n\n    script:\n                                                      \n    def treatment = udg ? (udg == 'half' ? '--UDGhalf' : '--CpG') : '--UDGminus'\n    def size = params.large_ref ? '-c' : ''\n    def platypus = params.pmdtools_platypus ? '--platypus' : ''\n    \"\"\"\n    #Run Filtering step \n    samtools calmd ${bam} ${fasta} | pmdtools --threshold ${params.pmdtools_threshold} ${treatment} --header | samtools view -Sb - > \"${libraryid}\".pmd.bam\n    \n    #Run Calc Range step\n    ## To allow early shut off of pipe: https://github.com/nextflow-io/nextflow/issues/1564\n    trap 'if [[ \\$? == 141 ]]; then echo \"Shutting samtools early due to -n parameter\" && samtools index ${libraryid}.pmd.bam ${size}; exit 0; fi' EXIT\n    samtools calmd ${bam} ${fasta} | pmdtools --deamination ${platypus} --range ${params.pmdtools_range} ${treatment} -n ${params.pmdtools_max_reads} > \"${libraryid}\".cpg.range.\"${params.pmdtools_range}\".txt\n    \n    samtools index ${libraryid}.pmd.bam ${size}\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/pmdtools"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie"], "nb_own": 2, "list_own": ["heinzlab", "nf-core"], "nb_wf": 2, "list_wf": ["smrnaseq", "smrna-seq-pipeline"], "list_contrib": ["c-guzman", "ewels", "maxulysse", "lpantano", "chuan-wang", "sirselim", "lcabus-flomics", "nf-core-bot", "ErikDanielsson", "pericsson", "sdjebali", "pditommaso", "mjsteinbaugh", "Hammarn", "jemten", "KevinMenden", "apeltzer", "drpatelh", "kstawiski"], "nb_contrib": 19, "codes": ["\nprocess bowtie_miRBase_hairpin {\n    tag \"$reads\"\n    publishDir \"${params.outdir}/bowtie/miRBase_hairpin\", mode: 'copy', pattern: '*.hairpin_unmapped.fq.gz'\n\n    input:\n    file reads from mature_unmapped_reads\n    file index from hairpin_index\n\n    output:\n    file '*.hairpin.bam' into miRBase_hairpin_bam\n    file '*.hairpin_unmapped.fq.gz' into hairpin_unmapped_reads\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.toString() - '.mature_unmapped.fq.gz'\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -p 2 \\\\\n        -t \\\\\n        -k 1 \\\\\n        -m 1 \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        -q <(zcat $reads) \\\\\n        --un ${prefix}.hairpin_unmapped.fq \\\\\n        -S \\\\\n        | samtools view -bS - > ${prefix}.hairpin.bam\n    gzip ${prefix}.hairpin_unmapped.fq\n    \"\"\"\n}", "\nprocess bowtie_miRBase_hairpin {\n    label 'process_medium'\n    tag \"$reads\"\n    publishDir \"${params.outdir}/bowtie/miRBase_hairpin\", mode: params.publish_dir_mode, pattern: '*.hairpin_unmapped.fq.gz'\n\n    input:\n    file reads from mature_unmapped_reads\n    file index from hairpin_index_bowtie\n\n    output:\n    file '*.hairpin.bam' into miRBase_hairpin_bam, miRBase_hairpin_bam_mirtop\n    file '*.hairpin_unmapped.fq.gz' into hairpin_unmapped_reads\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.toString() - '.mature_unmapped.fq.gz'\n    seq_center = params.seq_center ? \"--sam-RG ID:${prefix} --sam-RG 'CN:${params.seq_center}'\" : ''\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -p ${task.cpus} \\\\\n        -t \\\\\n        -a \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        -q <(zcat $reads) \\\\\n        --un ${prefix}.hairpin_unmapped.fq \\\\\n        -S $seq_center \\\\\n        | samtools view -bS - > ${prefix}.hairpin.bam\n\n    gzip ${prefix}.hairpin_unmapped.fq\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/heinzlab__smrna-seq-pipeline/bowtie_miRBase_hairpin", "nf-core/smrnaseq/nf-core__smrnaseq/bowtie_miRBase_hairpin"], "list_wf_names": ["nf-core/smrnaseq", "heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["annotate", "BCFtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BCFTOOLS_ANNOTATE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.15--haf5b3da_0':\n        'quay.io/biocontainers/bcftools:1.15--haf5b3da_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*_annotated.vcf.gz\"), optional:true , emit: vcf\n    tuple val(meta), path(\"*_annotated.bcf\")   , optional:true , emit: bcf\n    path \"versions.yml\"                                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def matcher = input ==~ /\\S+\\.*vcf\\.\\S*/\n    def output_suffix = matcher ? \"vcf.gz\" : \"bcf\"\n    def output_type_compressed = matcher ? \"z\" : \"b\"\n    \"\"\"\n    bcftools \\\\\n        annotate \\\\\n        $args \\\\\n        --output ${prefix}_annotated.${output_suffix} \\\\\n        --output-type $output_type_compressed \\\\\n        --threads $task.cpus \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$( bcftools --version |& sed '1!d; s/^.*bcftools //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BCFTOOLS_ANNOTATE"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["FastQC", "fastPHASE", "BEDTools", "QualiMap", "MultiQC", "SAMtools", "Picard", "preseq", "Bowtie", "BWA", "kraken2", "FreeBayes", "MapDamage", "BCFtools", "ANGSD", "GATK", "BaMM"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess get_software_versions {\n  label 'mc_small'\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version &> v_pipeline.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    \n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    AdapterRemoval --version  &> v_adapterremoval.txt 2>&1 || true\n    fastp --version &> v_fastp.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    circulargenerator --help | head -n 1 &> v_circulargenerator.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    dedup -v &> v_dedup.txt 2>&1 || true\n    ## bioconda recipe of picard is incorrectly set up and extra warning made with stderr, this ugly command ensures only version exported\n    ( exec 7>&1; picard MarkDuplicates --version 2>&1 >&7 | grep -v '/' >&2 ) 2> v_markduplicates.txt || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    preseq &> v_preseq.txt 2>&1 || true\n    gatk --version 2>&1 | grep '(GATK)' > v_gatk.txt 2>&1 || true\n    gatk3 --version 2>&1 | head -n 1 > v_gatk3.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    bedtools --version &> v_bedtools.txt 2>&1 || true\n    damageprofiler --version &> v_damageprofiler.txt 2>&1 || true\n    bam --version &> v_bamutil.txt 2>&1 || true\n    pmdtools --version &> v_pmdtools.txt 2>&1 || true\n    angsd -h |& head -n 1 | cut -d ' ' -f3-4 &> v_angsd.txt 2>&1 || true \n    multivcfanalyzer --help | head -n 1 &> v_multivcfanalyzer.txt 2>&1 || true\n    malt-run --help |& tail -n 3 | head -n 1 | cut -f 2 -d'(' | cut -f 1 -d ',' &> v_malt.txt 2>&1 || true\n    MaltExtract --help | head -n 2 | tail -n 1 &> v_maltextract.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    vcf2genome -h |& head -n 1 &> v_vcf2genome.txt || true\n    mtnucratio --help &> v_mtnucratiocalculator.txt || true\n    sexdeterrmine --version &> v_sexdeterrmine.txt || true\n    kraken2 --version | head -n 1 &> v_kraken.txt || true\n    endorS.py --version &> v_endorSpy.txt || true\n    pileupCaller --version &> v_sequencetools.txt 2>&1 || true\n    bowtie2 --version | grep -a 'bowtie2-.* -fdebug' > v_bowtie2.txt || true\n    eigenstrat_snp_coverage --version | cut -d ' ' -f2 >v_eigenstrat_snp_coverage.txt || true\n    mapDamage --version > v_mapdamage.txt || true\n    bbduk.sh | grep 'Last modified' | cut -d ' ' -f 3-99 > v_bbduk.txt || true\n    bcftools --version | grep 'bcftools' | cut -d ' ' -f 2 > v_bcftools.txt || true\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/get_software_versions"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess fastqc_trimmed {\n    validExitStatus 0,1\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/qc/fastqc/\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(prefix), file(trimmed_reads) from trimmed_reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html,txt}\" into trimmed_fastqc_results\n\n    script:\n    prefix = trimmed_reads.baseName\n    \"\"\"\n    echo ${prefix}\n\n    fastqc $trimmed_reads\n\t\textract_fastqc_stats.sh --srr=${prefix} > ${prefix}_stats_fastqc.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/nascent/nf-core__nascent/fastqc_trimmed"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 2, "list_own": ["chelauk", "nf-core"], "nb_wf": 2, "list_wf": ["nf-core-mutectplatypus", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "chelauk", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process GATK4_GETPILEUPSUMMARIES {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), val(id_intervals), path(bams), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n    path  variants\n    path  variants_tbi\n\n    output:\n    tuple val(meta), val(id_intervals), path('*.pileups.table'), emit: table\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${id_intervals}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n    def reference_command = fasta ? \"--reference $fasta\" : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GetPileupSummaries] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GetPileupSummaries \\\\\n        --input ${bams[0]} \\\\\n        --variant $variants \\\\\n        --output ${prefix}.pileups.table \\\\\n        $reference_command \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n    stub:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${id_intervals}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n    def reference_command = fasta ? \"--reference $fasta\" : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GetPileupSummaries] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    echo -e \"gatk --java-options \"-Xmx${avail_mem}g\" GetPileupSummaries \\\\\n        --input ${bams[0]} \\\\\n        --variant $variants \\\\\n        --output ${prefix}.pileups.table \\\\\n        $reference_command \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\"\n    \n    touch ${prefix}.pileups.table\n    touch versions.yml\n    \"\"\"\n}", "process GATK4_GETPILEUPSUMMARIES {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(index), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n    path  variants\n    path  variants_tbi\n\n    output:\n    tuple val(meta), path('*.pileups.table'), emit: table\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n    def reference_command = fasta ? \"--reference $fasta\" : ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GetPileupSummaries] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GetPileupSummaries \\\\\n        --input $input \\\\\n        --variant $variants \\\\\n        --output ${prefix}.pileups.table \\\\\n        $reference_command \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["chelauk/nf-core-mutectplatypus/chelauk__nf-core-mutectplatypus/GATK4_GETPILEUPSUMMARIES", "nf-core/modules/nf-core__modules/GATK4_GETPILEUPSUMMARIES"], "list_wf_names": ["chelauk/nf-core-mutectplatypus", "nf-core/modules"]}, {"nb_reuse": 23, "tools": ["MultiQC"], "nb_own": 18, "list_own": ["rnharmening", "Genomic-Medicine-Linkoping", "wslh-bio", "WarrenLabFH", "rmoran7", "Eugloh", "chelauk", "sripaladugu", "UMCUGenetics", "sickle-in-africa", "rikenbit", "nf-core", "fargenfo", "cschu", "zellerlab", "hoelzer", "alemenze", "biocorecrg"], "nb_wf": 21, "list_wf": ["viral_genomics", "Illumina-ShotgunQuant", "sarek_ubec", "ramdaq", "germline_somatic", "dx_sarek", "nf-multipleReferenceMapper", "vortex_light", "linkseq", "test_nextflow_sarek", "AmpliconPCR-nf", "saw.sarek", "BioNextflow", "custom_sarek", "sarek", "virify", "vortex_knight", "gatk-joint-genotyping", "dryad", "spriggan", "nf-core-sarek"], "list_contrib": ["Eugloh", "alneberg", "yuifu", "arontommi", "FriederikeHanssen", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "rnharmening", "pcantalupo", "szilvajuhos", "nf-core-bot", "ElisabetThomsen", "jfnavarro", "alemenze", "shashidhar22", "pditommaso", "olavurmortensen", "jackmo375", "chelauk", "lucacozzuto", "adrlar", "lconde-ucl", "hoelzer", "cschu", "malinlarsson", "k-florek", "ffmmulder", "rmoran7", "toniher", "lescai", "apeltzer", "AbigailShockey", "myoshimura080822", "olgabot", "davidmasp"], "nb_contrib": 38, "codes": ["\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess multiqc {\n  tag \"MultiQC\"\n\n  echo =false\n  publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n  input:\n    file fastqc from fastqc_results_ch.collect().ifEmpty([])\n    file qualimap from qualimap_results_ch.collect().ifEmpty([])\n  \n  output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*.html\"\n  \n  script:\n  \"\"\"\n    multiqc -f ./\n  \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "process multiqc {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    path(reports)\n\tpath(multiqc_config)\n\n    output:\n    path(\"multiqc_report.html\")\n\n    script:\n    def send_report = (params.email) ? \"echo . | mailx -s 'multiqc_report' -a multiqc_report.html ${params.email}\" : \"\"\n    \"\"\"\n    multiqc -c ${multiqc_config} .\n    ${send_report}\n    \"\"\"\n}", "\nprocess multiqc {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  input:\n  file(a) from multiqc_clean_reads.collect()\n  file(b) from fastqc_multiqc.collect()\n                                        \n  file(d) from kraken_multiqc.collect()\n  file(e) from quast_multiqc.collect()\n  file(f) from prokka_multiqc.collect()\n  file(g) from logo\n  file(config) from multiqc_config\n\n\n\n  output:\n  file(\"*.html\") into multiqc_output\n\n  script:\n  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"\n}", "\nprocess multiqc {\n\n  publishDir 'out/multiqc/', mode: 'copy', overwrite: false\n  cpus = 1\n\n  input:\n    file report from fastqc_report.collect()\n\n  output:\n    file \"*multiqc_*\" into multiqc_report\n\n  script:\n  \"\"\"\n  multiqc -f .\n  \"\"\"\n}", "\nprocess multiqc_trim {\n\n  publishDir 'out/trimgalore/multiqc/', mode: 'copy' , overwrite: false\n  cpus = 1\n\n  input:\n    file report from trimgalore_fastqc_reports.collect()\n\n  output:\n    file \"*multiqc_*\" into multiqc_trimgalore_report\n\n  script:\n  \"\"\"\n  multiqc -f .\n  \"\"\"\n}", "\nprocess runMultiQC {\n  container \"$params.multiqc\"\n  errorStrategy 'retry'\n  label 'low_mem'\n  publishDir \"$params.out_path/mutli_qc\", mode : \"copy\"\n  input:\n    path raw_fastqc_out \n    path trimmed_fastqc_out\n    path bbduk_stats\n    path host_bowtie_stats\n    path ebv_bowtie_stats\n    path ebv_fastqc_out\n    path prokka_spadeess\n    path prokka_unicycler \n    path config\n  output:\n    path \"multiqc_report.html\", emit: mutli_qc_out\n  script:\n    \"\"\"\n    multiqc -c ${config} .\n    \"\"\"\n    \n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess MULTIQC {\n\n    label 'process_medium'\n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy', overwrite: true\n    container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path ('fastqc/*')\n    path ('fastqc/*')\n    path ('hisat2_genome/*')\n    path ('hisat2_rrna/*')\n    path ('rseqc/*')\n    path ('featureCounts/biotype_counts/*')\n    path ('rsem_bowtie2_allgenes/*')\n    path ('plot_sample_correlation/*')\n    path ('plot_ercc_correlation/*')\n    path ('plot_ercc_correlation/*')\n    path ('featurecounts_all_gtf/*')\n    path ('featurecounts_mt_gtf/*')\n    path ('featurecounts_histone_gtf/*')\n    path ('plot_assignedgenome/*')\n    path ('plot_assignedgenome/*')\n    path ('plot_fcounts_maprate_allgene/*')\n    path ('plot_fcounts_maprate_allgene/*')\n    path ('plot_fcounts_maprate_mt/*')\n    path ('plot_fcounts_maprate_mt/*')\n    path ('plot_fcounts_maprate_histone/*')\n    path ('plot_fcounts_maprate_histone/*')\n    path ('plot_detectedgenes_dr/*')\n    path ('plot_detectedgenes_dr/*')\n    path ('plot_detectedgenes_dr/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_entropy_sirv/*')\n    path ('plots_entropy_sirv/*')\n    path ('software_versions/*')\n    path workflow_summary\n    \n    output:\n    path \"*.html\", emit: multiqc_report\n    path \"*_data\", emit: data\n    path \"*_plots\", optional:true, emit: plots\n    \n    script:\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $custom_config .\n    \"\"\"\n}", "\nprocess multiqc {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  input:\n  file(a) from multiqc_clean_reads.collect()\n  file(b) from fastqc_multiqc.collect()\n  file(c) from stats_multiqc.collect()\n  file(d) from kraken_multiqc.collect()\n  file(e) from quast_multiqc.collect()\n  file(config) from multiqc_config\n\n  output:\n  file(\"*.html\") into multiqc_output\n\n  script:\n  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"\n}", "\nprocess multiqc {\n    memory = \"4GB\"\n    cpus = 1\n\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy', overwrite: true\n\n    input:\n    file table from variant_eval_table_ch\n\n    output:\n    file \"multiqc_report.html\" into multiqc_report_ch\n    file \"multiqc_data\" into multiqc_data_ch\n\n    script:\n    \"\"\"\n    multiqc -f $outdir\n    \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess SaveCohortRawReadsQualityReport {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig)\n        file (versions)\n        file ('FastQC/*')\n\n    script:\n        \"\"\"\n        multiqc -f .\n        \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "process multiqc {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    path(reports)\n\tpath(multiqc_config)\n\n    output:\n    path(\"multiqc_report.html\")\n\n    script:\n    def send_report = (params.email) ? \"echo . | mailx -s 'multiqc_report' -a multiqc_report.html ${params.email}\" : \"\"\n    \"\"\"\n    multiqc -c ${multiqc_config} .\n    ${send_report}\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"$outdir/multiqc\", mode: 'copy', overwrite: true\n\n    input:\n                                                    \n    val temp from multiqc_ch.collect()\n\n    output:\n    file \"multiqc_report.html\" into multiqc_report_ch\n    file \"multiqc_data\" into multiqc_data_ch\n\n    script:\n    \"\"\"\n    multiqc -f $outdir/multiqc_logs\n    \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess makeReportID {\n    label (params.LABEL)\n    tag { id }\n    \n    container params.CONTAINER\n    if (params.OUTPUT != \"\") { publishDir(params.OUTPUT, mode:'copy') }\n\n    input:\n    tuple val(id), path(input)\n\t\n    output:\n\ttuple val(id), path(id)\n\t\n    script:\n    \"\"\"\n\t\tmultiqc ${params.EXTRAPARS} -o ${id} .\n    \"\"\"\n}", "process multiqc {\n    publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}_multiqc_report.html\"\n    label 'multiqc'  \n  input:\n    tuple val(name), file(fastqc)\n  output:\n    tuple val(name), file(\"${name}_multiqc_report.html\")\n  script:\n    \"\"\"\n    multiqc -i ${name} .\n    \"\"\"\n  }", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"${meta}\"\n    label 'process_low'\n\n    publishDir \"${params.outdir}/multiqc\",\n        mode: \"copy\",\n        overwrite: true,\n        saveAs: { filename -> filename }\n\n    container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n\n    input:\n        path ('fastqc/*')\n        path ('trimgalore/fastqc/*')\n        path ('trimgalore/*')\n        path ('kraken/*')\n    \n    output:\n        path \"*multiqc_report.html\", emit: report\n        path \"*_data\"              , emit: data\n\n    script:\n        \"\"\"\n        multiqc -f .\n        \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/rmoran7__dx_sarek/MultiQC", "rnharmening/nf-multipleReferenceMapper/rnharmening__nf-multipleReferenceMapper/multiqc", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MultiQC", "cschu/vortex_knight/cschu__vortex_knight/multiqc", "wslh-bio/dryad/wslh-bio__dryad/multiqc", "Eugloh/AmpliconPCR-nf/Eugloh__AmpliconPCR-nf/multiqc", "Eugloh/AmpliconPCR-nf/Eugloh__AmpliconPCR-nf/multiqc_trim", "WarrenLabFH/viral_genomics/WarrenLabFH__viral_genomics/runMultiQC", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/MultiQC", "rikenbit/ramdaq/rikenbit__ramdaq/MULTIQC", "wslh-bio/spriggan/wslh-bio__spriggan/multiqc", "fargenfo/gatk-joint-genotyping/fargenfo__gatk-joint-genotyping/multiqc", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MultiQC", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/SaveCohortRawReadsQualityReport", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/MultiQC", "zellerlab/vortex_light/zellerlab__vortex_light/multiqc", "fargenfo/linkseq/fargenfo__linkseq/multiqc", "nf-core/sarek/nf-core__sarek/MultiQC", "rmoran7/custom_sarek/rmoran7__custom_sarek/MultiQC", "biocorecrg/BioNextflow/biocorecrg__BioNextflow/makeReportID", "hoelzer/virify/hoelzer__virify/multiqc", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MultiQC", "alemenze/Illumina-ShotgunQuant/alemenze__Illumina-ShotgunQuant/multiqc"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "Eugloh/AmpliconPCR-nf", "zellerlab/vortex_light", "biocorecrg/BioNextflow", "alemenze/Illumina-ShotgunQuant", "rnharmening/nf-multipleReferenceMapper", "UMCUGenetics/sarek_ubec", "hoelzer/virify", "sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rikenbit/ramdaq", "chelauk/test_nextflow_sarek", "cschu/vortex_knight", "nf-core/sarek", "fargenfo/gatk-joint-genotyping", "rmoran7/custom_sarek", "wslh-bio/spriggan", "fargenfo/linkseq", "WarrenLabFH/viral_genomics", "rmoran7/dx_sarek", "wslh-bio/dryad"]}, {"nb_reuse": 16, "tools": ["snpEff"], "nb_own": 10, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "UCL-BLIC", "lifebit-ai"], "nb_wf": 16, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "Sarek_v2.3.FIX1", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BuildCache_snpEff", "nf-core/sarek/nf-core__sarek/BuildCache_snpEff", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/BuildCache_snpEff", "cgpu/pgp-chronek/cgpu__pgp-chronek/BuildCache_snpEff", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/BuildCache_snpEff", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BuildCache_snpEff", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/BuildCache_snpEff", "cgpu/sarek-mirror/cgpu__sarek-mirror/BuildCache_snpEff", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BuildCache_snpEff", "rmoran7/custom_sarek/rmoran7__custom_sarek/BuildCache_snpEff", "rmoran7/dx_sarek/rmoran7__dx_sarek/BuildCache_snpEff", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/BuildCache_snpEff", "cgpu/haplosarek/cgpu__haplosarek/BuildCache_snpEff", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/BuildCache_snpEff", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/BuildCache_snpEff", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/BuildCache_snpEff"], "list_wf_names": ["cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "UCL-BLIC/Sarek_v2.3.FIX1", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-mirror-cache", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler"]}, {"nb_reuse": 2, "tools": ["kraken2"], "nb_own": 2, "list_own": ["ABMicroBioinf", "nf-core"], "nb_wf": 2, "list_wf": ["magph", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n    val save_output_fastqs\n    val save_reads_assignment\n\n    output:\n    tuple val(meta), path('*classified*')     , optional:true, emit: classified_reads_fastq\n    tuple val(meta), path('*unclassified*')   , optional:true, emit: unclassified_reads_fastq\n    tuple val(meta), path('*classifiedreads*'), optional:true, emit: classified_reads_assignment\n    tuple val(meta), path('*report.txt')                     , emit: report\n    path \"versions.yml\"                                      , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    def classified_command = save_output_fastqs ? \"--classified-out ${classified}\" : \"\"\n    def unclassified_command = save_output_fastqs ? \"--unclassified-out ${unclassified}\" : \"\"\n    def readclassification_command = save_reads_assignment ? \"--output ${prefix}.kraken2.classifiedreads.txt\" : \"\"\n    def compress_reads_command = save_output_fastqs ? \"pigz -p $task.cpus *.fastq\" : \"\"\n\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $unclassified_command \\\\\n        $classified_command \\\\\n        $readclassification_command \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    $compress_reads_command\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n    val save_output_fastqs\n    val save_reads_assignment\n\n    output:\n    tuple val(meta), path('*classified*')     , optional:true, emit: classified_reads_fastq\n    tuple val(meta), path('*unclassified*')   , optional:true, emit: unclassified_reads_fastq\n    tuple val(meta), path('*classifiedreads*'), optional:true, emit: classified_reads_assignment\n    tuple val(meta), path('*report.txt')                     , emit: report\n    path \"versions.yml\"                                      , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    def classified_command = save_output_fastqs ? \"--classified-out ${classified}\" : \"\"\n    def unclassified_command = save_output_fastqs ? \"--unclassified-out ${unclassified}\" : \"\"\n    def readclassification_command = save_reads_assignment ? \"--output ${prefix}.kraken2.classifiedreads.txt\" : \"\"\n    def compress_reads_command = save_output_fastqs ? \"pigz -p $task.cpus *.fastq\" : \"\"\n\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $unclassified_command \\\\\n        $classified_command \\\\\n        $readclassification_command \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    $compress_reads_command\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/KRAKEN2_KRAKEN2", "ABMicroBioinf/magph/ABMicroBioinf__magph/KRAKEN2_KRAKEN2"], "list_wf_names": ["ABMicroBioinf/magph", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["Roary"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process ROARY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::roary=3.13.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/roary:3.13.0--pl526h516909a_0' :\n        'quay.io/biocontainers/roary:3.13.0--pl526h516909a_0' }\"\n\n    input:\n    tuple val(meta), path(gff)\n\n    output:\n    tuple val(meta), path(\"results/*\")                    , emit: results\n    tuple val(meta), path(\"results/*.aln\"), optional: true, emit: aln\n    path \"versions.yml\"                                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    roary \\\\\n        $args \\\\\n        -p $task.cpus \\\\\n        -f results/ \\\\\n        $gff\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        roary: \\$( roary --version )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/ROARY"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 11, "tools": ["kraken2"], "nb_own": 8, "list_own": ["ksumngs", "ABMicroBioinf", "nf-core", "mahesh-panchal", "bactopia", "cidgoh", "xiaoli-dong", "jianhong"], "nb_wf": 10, "list_wf": ["pathogen", "v-met", "magph", "cidgoh_qc", "shotgun", "viralrecon", "nf-modules", "test_nfcore_workflow_chain", "bactopia", "taxprofiler"], "list_contrib": ["heuermh", "ewels", "Mxrcon", "maxulysse", "antunderwood", "ggabernet", "duanjunhyq", "ktrns", "rpetit3", "saramonzon", "svarona", "jfy133", "anwarMZ", "stevekm", "nf-core-bot", "fmaguire", "Accio", "stevin-wilson", "xiaoli-dong", "jcurado-flomics", "ljmesi", "ErikaKvalem", "jianhong", "MillironX", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "MiguelJulia", "maxibor", "TGotwig", "drpatelh"], "nb_contrib": 31, "codes": ["\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_high_memory'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*classified*\")  , emit: classified\n    tuple val(meta), path(\"*unclassified*\"), emit: unclassified\n    tuple val(meta), path(\"*.kraken.gz\")   , emit: kraken\n    tuple val(meta), path(\"*.kreport\")     , emit: kreport\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def pairedFlag = meta.single_end ? '' : '--paired'\n    def classifiedFlag = meta.single_end ? \"${prefix}_classified.fastq\" : \"${prefix}_classified#.fastq\"\n    def unclassifiedFlag = meta.single_end ? \"${prefix}_unclassified.fastq\" : \"${prefix}_unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n            --db ${db} \\\\\n            --threads ${task.cpus} \\\\\n            --classified-out ${classifiedFlag} \\\\\n            --unclassified-out ${unclassifiedFlag} \\\\\n            --report ${prefix}.kreport \\\\\n            ${pairedFlag} \\\\\n            ${args} \\\\\n            ${reads} \\\\\n        | gzip \\\\\n        > ${prefix}.kraken.gz\n\n    pigz -p${task.cpus} *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_high_memory'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*classified*\")  , emit: classified\n    tuple val(meta), path(\"*unclassified*\"), emit: unclassified\n    tuple val(meta), path(\"*.kraken.gz\")   , emit: kraken\n    tuple val(meta), path(\"*.kreport\")     , emit: kreport\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def pairedFlag = meta.single_end ? '' : '--paired'\n    def classifiedFlag = meta.single_end ? \"${prefix}_classified.fastq\" : \"${prefix}_classified#.fastq\"\n    def unclassifiedFlag = meta.single_end ? \"${prefix}_unclassified.fastq\" : \"${prefix}_unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n            --db ${db} \\\\\n            --threads ${task.cpus} \\\\\n            --classified-out ${classifiedFlag} \\\\\n            --unclassified-out ${unclassifiedFlag} \\\\\n            --report ${prefix}.kreport \\\\\n            ${pairedFlag} \\\\\n            ${args} \\\\\n            ${reads} \\\\\n        | gzip \\\\\n        > ${prefix}.kraken.gz\n\n    pigz -p${task.cpus} *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: report\n    path \"*.{log,err}\" , emit: logs, optional: true\n    path \".command.*\"  , emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads > /dev/null\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/pathogen/xiaoli-dong__pathogen/KRAKEN2_KRAKEN2", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/KRAKEN2_KRAKEN2", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/KRAKEN2_KRAKEN2", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/KRAKEN2_KRAKEN2", "ksumngs/nf-modules/ksumngs__nf-modules/KRAKEN2", "ksumngs/v-met/ksumngs__v-met/KRAKEN2", "nf-core/taxprofiler/nf-core__taxprofiler/KRAKEN2_KRAKEN2", "xiaoli-dong/magph/xiaoli-dong__magph/KRAKEN2_KRAKEN2", "bactopia/bactopia/bactopia__bactopia/KRAKEN2", "nf-core/viralrecon/nf-core__viralrecon/KRAKEN2_KRAKEN2", "jianhong/shotgun/jianhong__shotgun/KRAKEN2_KRAKEN2"], "list_wf_names": ["jianhong/shotgun", "xiaoli-dong/pathogen", "cidgoh/cidgoh_qc", "xiaoli-dong/magph", "ABMicroBioinf/pathogen", "nf-core/taxprofiler", "ksumngs/nf-modules", "bactopia/bactopia", "nf-core/viralrecon", "ksumngs/v-met", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 6, "tools": ["Cutadapt"], "nb_own": 5, "list_own": ["hukai916", "nf-core", "laclac102", "csf-ngs", "jianhong"], "nb_wf": 5, "list_wf": ["nf-core-hicar", "demo_nfcore_obsolete", "modules", "ampliseq", "controldna"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "xingaulaglag", "kmurat1", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "colindaven", "jfy133", "santiagorevale", "idot", "kevbrick", "nebfield", "ntoda03", "emnilsson", "PhilPalmer", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "DiegoBrambilla", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "asafpr", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "jtangrot", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 114, "codes": ["process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py37h73a75cf_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    \n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val(trimstring)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: trimstring\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n                                                                                  \n    def link_input = link_input(meta.single_end, prefix, reads)\n    def lreads = linked_reads(meta.single_end, prefix, reads)\n    \"\"\"\n    $link_input\n\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $lreads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1'\n    } else {\n        container 'quay.io/biocontainers/cutadapt:3.4--py37h73a75cf_1'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $options.args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/CUTADAPT", "nf-core/ampliseq/nf-core__ampliseq/CUTADAPT", "laclac102/ampliseq/laclac102__ampliseq/CUTADAPT", "csf-ngs/controldna/csf-ngs__controldna/CUTADAPT", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/CUTADAPT", "hukai916/demo_nfcore_obsolete/hukai916__demo_nfcore_obsolete/CUTADAPT"], "list_wf_names": ["jianhong/nf-core-hicar", "hukai916/demo_nfcore_obsolete", "laclac102/ampliseq", "csf-ngs/controldna", "nf-core/ampliseq", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["getnumber", "M-TRACK"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process DADA2_STATS {\n    tag \"$meta.run\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(\"filter_and_trim_files/*\"), path(denoised), path(mergers), path(seqtab_nochim)\n\n    output:\n    tuple val(meta), path(\"*.stats.tsv\"), emit: stats\n    path \"versions.yml\"                 , emit: versions\n\n    script:\n    if (!meta.single_end) {\n        \"\"\"\n        #!/usr/bin/env Rscript\n        suppressPackageStartupMessages(library(dada2))\n\n        #combine filter_and_trim files\n        for (data in list.files(\"./filter_and_trim_files\", full.names=TRUE)){\n            if (!exists(\"filter_and_trim\")){ filter_and_trim <- read.csv(data, header=TRUE, sep=\"\\\\t\") }\n            if (exists(\"filter_and_trim\")){\n                tempory <-read.csv(data, header=TRUE, sep=\"\\\\t\")\n                filter_and_trim <-unique(rbind(filter_and_trim, tempory))\n                rm(tempory)\n            }\n        }\n        rownames(filter_and_trim) <- filter_and_trim\\$ID\n        filter_and_trim[\"ID\"] <- NULL\n        #write.table( filter_and_trim, file = \"${meta.run}.filter_and_trim.tsv\", sep = \"\\\\t\", row.names = TRUE, quote = FALSE, na = '')\n\n        #read data\n        dadaFs = readRDS(\"${denoised[0]}\")\n        dadaRs = readRDS(\"${denoised[1]}\")\n        mergers = readRDS(\"$mergers\")\n        seqtab.nochim = readRDS(\"$seqtab_nochim\")\n\n        #track reads through pipeline\n        getN <- function(x) sum(getUniques(x))\n        if ( nrow(filter_and_trim) == 1 ) {\n            track <- cbind(filter_and_trim, getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))\n        } else {\n            track <- cbind(filter_and_trim, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\n        }\n        colnames(track) <- c(\"DADA2_input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\n        track <- cbind(sample = sub(pattern = \"(.*?)\\\\\\\\..*\\$\", replacement = \"\\\\\\\\1\", rownames(track)), track)\n        write.table( track, file = \"${meta.run}.stats.tsv\", sep = \"\\\\t\", row.names = FALSE, quote = FALSE, na = '')\n\n        writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n        \"\"\"\n    } else {\n        \"\"\"\n        #!/usr/bin/env Rscript\n        suppressPackageStartupMessages(library(dada2))\n\n        #combine filter_and_trim files\n        for (data in list.files(\"./filter_and_trim_files\", full.names=TRUE)){\n            if (!exists(\"filter_and_trim\")){ filter_and_trim <- read.csv(data, header=TRUE, sep=\"\\\\t\") }\n            if (exists(\"filter_and_trim\")){\n                tempory <-read.csv(data, header=TRUE, sep=\"\\\\t\")\n                filter_and_trim <-unique(rbind(filter_and_trim, tempory))\n                rm(tempory)\n            }\n        }\n        rownames(filter_and_trim) <- filter_and_trim\\$ID\n        filter_and_trim[\"ID\"] <- NULL\n        #write.table( filter_and_trim, file = \"${meta.run}.filter_and_trim.tsv\", sep = \"\\\\t\", row.names = TRUE, quote = FALSE, na = '')\n\n        #read data\n        dadaFs = readRDS(\"${denoised[0]}\")\n        seqtab.nochim = readRDS(\"$seqtab_nochim\")\n\n        #track reads through pipeline\n        getN <- function(x) sum(getUniques(x))\n        if ( nrow(filter_and_trim) == 1 ) {\n            track <- cbind(filter_and_trim, getN(dadaFs), rowSums(seqtab.nochim))\n        } else {\n            track <- cbind(filter_and_trim, sapply(dadaFs, getN), rowSums(seqtab.nochim))\n        }\n        colnames(track) <- c(\"input\", \"filtered\", \"denoised\", \"nonchim\")\n        track <- cbind(sample = sub(pattern = \"(.*?)\\\\\\\\..*\\$\", replacement = \"\\\\\\\\1\", rownames(track)), track)\n        write.table( track, file = \"${meta.run}.stats.tsv\", sep = \"\\\\t\", row.names = FALSE, quote = FALSE, na = '')\n\n        writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n        \"\"\"\n    }\n}", "process DADA2_STATS {\n    tag \"$meta.run\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(\"filter_and_trim_files/*\"), path(denoised), path(mergers), path(seqtab_nochim)\n\n    output:\n    tuple val(meta), path(\"*.stats.tsv\"), emit: stats\n    path \"versions.yml\"                 , emit: versions\n\n    script:\n    if (!meta.single_end) {\n        \"\"\"\n        #!/usr/bin/env Rscript\n        suppressPackageStartupMessages(library(dada2))\n\n        #combine filter_and_trim files\n        for (data in list.files(\"./filter_and_trim_files\", full.names=TRUE)){\n            if (!exists(\"filter_and_trim\")){ filter_and_trim <- read.csv(data, header=TRUE, sep=\"\\\\t\") }\n            if (exists(\"filter_and_trim\")){\n                tempory <-read.csv(data, header=TRUE, sep=\"\\\\t\")\n                filter_and_trim <-unique(rbind(filter_and_trim, tempory))\n                rm(tempory)\n            }\n        }\n        rownames(filter_and_trim) <- filter_and_trim\\$ID\n        filter_and_trim[\"ID\"] <- NULL\n        #write.table( filter_and_trim, file = \"${meta.run}.filter_and_trim.tsv\", sep = \"\\\\t\", row.names = TRUE, quote = FALSE, na = '')\n\n        #read data\n        dadaFs = readRDS(\"${denoised[0]}\")\n        dadaRs = readRDS(\"${denoised[1]}\")\n        mergers = readRDS(\"$mergers\")\n        seqtab.nochim = readRDS(\"$seqtab_nochim\")\n\n        #track reads through pipeline\n        getN <- function(x) sum(getUniques(x))\n        if ( nrow(filter_and_trim) == 1 ) {\n            track <- cbind(filter_and_trim, getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))\n        } else {\n            track <- cbind(filter_and_trim, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\n        }\n        colnames(track) <- c(\"DADA2_input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\n        rownames(track) <- sub(pattern = \"_1.fastq.gz\\$\", replacement = \"\", rownames(track)) #this is when cutadapt is skipped!\n        track <- cbind(sample = sub(pattern = \"(.*?)\\\\\\\\..*\\$\", replacement = \"\\\\\\\\1\", rownames(track)), track)\n        write.table( track, file = \"${meta.run}.stats.tsv\", sep = \"\\\\t\", row.names = FALSE, quote = FALSE, na = '')\n\n        writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n        \"\"\"\n    } else {\n        \"\"\"\n        #!/usr/bin/env Rscript\n        suppressPackageStartupMessages(library(dada2))\n\n        #combine filter_and_trim files\n        for (data in list.files(\"./filter_and_trim_files\", full.names=TRUE)){\n            if (!exists(\"filter_and_trim\")){ filter_and_trim <- read.csv(data, header=TRUE, sep=\"\\\\t\") }\n            if (exists(\"filter_and_trim\")){\n                tempory <-read.csv(data, header=TRUE, sep=\"\\\\t\")\n                filter_and_trim <-unique(rbind(filter_and_trim, tempory))\n                rm(tempory)\n            }\n        }\n        rownames(filter_and_trim) <- filter_and_trim\\$ID\n        filter_and_trim[\"ID\"] <- NULL\n        #write.table( filter_and_trim, file = \"${meta.run}.filter_and_trim.tsv\", sep = \"\\\\t\", row.names = TRUE, quote = FALSE, na = '')\n\n        #read data\n        dadaFs = readRDS(\"${denoised[0]}\")\n        seqtab.nochim = readRDS(\"$seqtab_nochim\")\n\n        #track reads through pipeline\n        getN <- function(x) sum(getUniques(x))\n        if ( nrow(filter_and_trim) == 1 ) {\n            track <- cbind(filter_and_trim, getN(dadaFs), rowSums(seqtab.nochim))\n        } else {\n            track <- cbind(filter_and_trim, sapply(dadaFs, getN), rowSums(seqtab.nochim))\n        }\n        colnames(track) <- c(\"input\", \"filtered\", \"denoised\", \"nonchim\")\n        track <- cbind(sample = sub(pattern = \"(.*?)\\\\\\\\..*\\$\", replacement = \"\\\\\\\\1\", rownames(track)), track)\n        write.table( track, file = \"${meta.run}.stats.tsv\", sep = \"\\\\t\", row.names = FALSE, quote = FALSE, na = '')\n\n        writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n        \"\"\"\n    }\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/DADA2_STATS", "nf-core/ampliseq/nf-core__ampliseq/DADA2_STATS"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["MultiQC"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path 'multiqc_config.yaml'\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_barcodes_no_sample\n    path fail_no_barcode_samples\n    path fail_barcode_count_samples\n    path fail_guppyplex_count_samples\n    path 'amplicon_heatmap_mqc.tsv'\n    path ('pycoqc/*')\n    path ('artic_minion/*')\n    path ('samtools_stats/*')\n    path ('bcftools_stats/*')\n    path ('mosdepth/*')\n    path ('quast/*')\n    path ('snpeff/*')\n    path pangolin_lineage\n    path nextclade_clade\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*.csv\"               , optional:true, emit: csv\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def custom_config = multiqc_custom_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    ## Run MultiQC once to parse tool logs\n    multiqc -f $args $custom_config .\n\n    ## Parse YAML files dumped by MultiQC to obtain metrics\n    multiqc_to_custom_csv.py --platform nanopore\n\n    ## Manually remove files that we don't want in the report\n    rm -rf quast\n\n    ## Run MultiQC a second time\n    multiqc -f $args -e general_stats --ignore *nextclade_clade_mqc.tsv $custom_config .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path 'multiqc_config.yaml'\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_barcodes_no_sample\n    path fail_no_barcode_samples\n    path fail_barcode_count_samples\n    path fail_guppyplex_count_samples\n    path 'amplicon_heatmap_mqc.tsv'\n    path ('pycoqc/*')\n    path ('artic_minion/*')\n    path ('samtools_stats/*')\n    path ('bcftools_stats/*')\n    path ('mosdepth/*')\n    path ('quast/*')\n    path ('snpeff/*')\n    path pangolin_lineage\n    path nextclade_clade\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*.csv\"               , optional:true, emit: csv\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def custom_config = multiqc_custom_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    ## Run MultiQC once to parse tool logs\n    multiqc -f $args $custom_config .\n\n    ## Parse YAML files dumped by MultiQC to obtain metrics\n    multiqc_to_custom_csv.py --platform nanopore\n\n    ## Manually remove files that we don't want in the report\n    rm -rf quast\n\n    ## Run MultiQC a second time\n    multiqc -f $args -e general_stats --ignore *nextclade_clade_mqc.tsv $custom_config .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/MULTIQC", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/MULTIQC"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess additional_library_merge {\n  label 'sc_tiny'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/merged_bams/additional\", mode: params.publish_dir_mode\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_trimmed_formerge.merge_me\n\n  output:\n  tuple samplename, val(\"${samplename}_libmerged\"), lane, seqtype, organism, strandedness, udg, path(\"*_libmerged_add.bam\"), path(\"*_libmerged_add.bam.{bai,csi}\") into ch_output_from_trimmerge\n\n  script:\n  def size = params.large_ref ? '-c' : ''\n  \"\"\"\n  samtools merge ${samplename}_libmerged_add.bam ${bam}\n  samtools index ${samplename}_libmerged_add.bam ${size}\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/additional_library_merge"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process DRAGMAP_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::dragmap=1.2.1 bioconda::samtools=1.15.1 conda-forge::pigz=2.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-580d344d9d4a496cd403932da8765f9e0187774d:5ebebbc128cd624282eaa37d2c7fe01505a91a69-0':\n        'quay.io/biocontainers/mulled-v2-580d344d9d4a496cd403932da8765f9e0187774d:5ebebbc128cd624282eaa37d2c7fe01505a91a69-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  hashmap\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path('*.log'), emit: log\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reads_command = meta.single_end ? \"-1 $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n\n    \"\"\"\n    dragen-os \\\\\n        -r $hashmap \\\\\n        $args \\\\\n        --num-threads $task.cpus \\\\\n        $reads_command \\\\\n        2> ${prefix}.dragmap.log \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        dragmap: \\$(echo \\$(dragen-os --version 2>&1))\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/DRAGMAP_ALIGN"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nascent"], "list_contrib": ["ignaciot", "apeltzer"], "nb_contrib": 2, "codes": ["\nprocess samtools {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/mapped/bams\", mode: 'copy', pattern: \"${prefix}.sorted.bam\"\n    publishDir \"${params.outdir}/mapped/bams\", mode: 'copy', pattern: \"${prefix}.sorted.bam.bai\"\n    publishDir \"${params.outdir}/qc/mapstats\", mode: 'copy', pattern: \"${prefix}.sorted.bam.flagstat\"\n    publishDir \"${params.outdir}/qc/mapstats\", mode: 'copy', pattern: \"${prefix}.sorted.bam.millionsmapped\"\n\n    input:\n    set val(name), file(mapped_sam) from hisat2_sam\n\n    output:\n    set val(name), file(\"${prefix}.sorted.bam\") into sorted_bam_ch\n    set val(name), file(\"${prefix}.sorted.bam.bai\") into sorted_bam_indices_ch\n    set val(name), file(\"${prefix}.sorted.bam.flagstat\") into bam_flagstat\n    set val(name), file(\"${prefix}.sorted.bam.millionsmapped\") into bam_milmapped_bedgraph\n\n    script:\n    prefix = mapped_sam.baseName\n                                                                                                                               \n                                                                                                 \n    if (!params.singleEnd) {\n    \"\"\"\n\n    samtools view -@ ${task.cpus} -bS -o ${prefix}.bam ${mapped_sam}\n    samtools sort -@ ${task.cpus} ${prefix}.bam > ${prefix}.sorted.bam\n    samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n    samtools view -@ ${task.cpus} -F 0x40 ${prefix}.sorted.bam | cut -f1 | sort | uniq | wc -l > ${prefix}.sorted.bam.millionsmapped\n    samtools index ${prefix}.sorted.bam ${prefix}.sorted.bam.bai\n    \"\"\"\n    } else {\n    \"\"\"\n\n    samtools view -@ ${task.cpus} -bS -o ${prefix}.bam ${mapped_sam}\n    samtools sort -@ ${task.cpus} ${prefix}.bam > ${prefix}.sorted.bam\n    samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n    samtools view -@ ${task.cpus} -F 0x904 -c ${prefix}.sorted.bam > ${prefix}.sorted.bam.millionsmapped\n    samtools index ${prefix}.sorted.bam ${prefix}.sorted.bam.bai\n    \"\"\"\n    }\n}"], "list_proc": ["nf-core/nascent/nf-core__nascent/samtools"], "list_wf_names": ["nf-core/nascent"]}, {"nb_reuse": 2, "tools": ["Picard"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "leebrian", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process PICARD_CLEANSAM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.9--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.9--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CleanSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CleanSam  \\\\\n        ${args} \\\\\n        -I ${bam} \\\\\n        -O ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CleanSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_CLEANSAM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CleanSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CleanSam  \\\\\n        ${args} \\\\\n        --INPUT ${bam} \\\\\n        --OUTPUT ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CleanSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/PICARD_CLEANSAM", "nf-core/modules/nf-core__modules/PICARD_CLEANSAM"], "list_wf_names": ["nf-core/modules", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 16, "tools": ["FastQC"], "nb_own": 9, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 15, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "olgabot", "davidmasp"], "nb_contrib": 24, "codes": ["\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_1'\n    disk '50 GB'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 4 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "process GetFastqcQualityReport {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: 'copy'\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), path(\"${idSample}_${idRun}_R1.fastq.gz\"), path(\"${idSample}_${idRun}_R2.fastq.gz\")\n\n    output:\n        path(\"*.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_4'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 4 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: step == 'mapping' && !('fastqc' in skipQC)\n    \n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["nf-core/sarek/nf-core__sarek/FastQCFQ", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/FastQCFQ", "rmoran7/custom_sarek/rmoran7__custom_sarek/FastQCFQ", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GetFastqcQualityReport", "cgpu/pgp-chronek/cgpu__pgp-chronek/FastQCFQ", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/FastQCFQ", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/FastQCFQ", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/FastQCFQ", "cgpu/sarek-mirror/cgpu__sarek-mirror/FastQCFQ", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/FastQCFQ", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/FastQCFQ", "rmoran7/dx_sarek/rmoran7__dx_sarek/FastQCFQ", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/FastQCFQ", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/FastQCFQ", "cgpu/haplosarek/cgpu__haplosarek/FastQCFQ", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/FastQCFQ"], "list_wf_names": ["cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror-cache", "cgpu/sarek-mirror", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 4, "tools": ["MetaPhlAn", "Bowtie"], "nb_own": 4, "list_own": ["xiaoli-dong", "ABMicroBioinf", "nf-core", "jianhong"], "nb_wf": 3, "list_wf": ["magph", "shotgun", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process METAPHLAN3 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::metaphlan=3.0.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.12--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.12--pyhb7b1952_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path metaphlan_db\n\n    output:\n    tuple val(meta), path(\"*_profile.txt\")   ,                emit: profile\n    tuple val(meta), path(\"*.biom\")          ,                emit: biom\n    tuple val(meta), path('*.bowtie2out.txt'), optional:true, emit: bt2out\n    path \"versions.yml\"                      ,                emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_type  = (\"$input\".endsWith(\".fastq.gz\")) ? \"--input_type fastq\" :  (\"$input\".contains(\".fasta\")) ? \"--input_type fasta\" : (\"$input\".endsWith(\".bowtie2out.txt\")) ? \"--input_type bowtie2out\" : \"--input_type sam\"\n    def input_data  = (\"$input_type\".contains(\"fastq\")) && !meta.single_end ? \"${input[0]},${input[1]}\" : \"$input\"\n    def bowtie2_out = \"$input_type\" == \"--input_type bowtie2out\" || \"$input_type\" == \"--input_type sam\" ? '' : \"--bowtie2out ${prefix}.bowtie2out.txt\"\n\n    \"\"\"\n    metaphlan \\\\\n        --nproc $task.cpus \\\\\n        $input_type \\\\\n        $input_data \\\\\n        $args \\\\\n        $bowtie2_out \\\\\n        --bowtie2db ${metaphlan_db} \\\\\n        --biom ${prefix}.biom \\\\\n        --output_file ${prefix}_profile.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan3: \\$(metaphlan --version 2>&1 | awk '{print \\$3}')\n    END_VERSIONS\n    \"\"\"\n}", "process METAPHLAN3 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::metaphlan=3.0.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.12--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.12--pyhb7b1952_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path metaphlan_db\n\n    output:\n    tuple val(meta), path(\"*_profile.txt\")   ,                emit: profile\n    tuple val(meta), path(\"*.biom\")          ,                emit: biom\n    tuple val(meta), path('*.bowtie2out.txt'), optional:true, emit: bt2out\n    path \"versions.yml\"                      ,                emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_type  = (\"$input\".endsWith(\".fastq.gz\") || \"$input\".endsWith(\".fq.gz\")) ? \"--input_type fastq\" :  (\"$input\".contains(\".fasta\")) ? \"--input_type fasta\" : (\"$input\".endsWith(\".bowtie2out.txt\")) ? \"--input_type bowtie2out\" : \"--input_type sam\"\n    def input_data  = (\"$input_type\".contains(\"fastq\")) && !meta.single_end ? \"${input[0]},${input[1]}\" : \"$input\"\n    def bowtie2_out = \"$input_type\" == \"--input_type bowtie2out\" || \"$input_type\" == \"--input_type sam\" ? '' : \"--bowtie2out ${prefix}.bowtie2out.txt\"\n\n    \"\"\"\n    metaphlan \\\\\n        --nproc $task.cpus \\\\\n        $input_type \\\\\n        $input_data \\\\\n        $args \\\\\n        $bowtie2_out \\\\\n        --bowtie2db ${metaphlan_db} \\\\\n        --biom ${prefix}.biom \\\\\n        --output_file ${prefix}_profile.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan3: \\$(metaphlan --version 2>&1 | awk '{print \\$3}')\n    END_VERSIONS\n    \"\"\"\n}", "process METAPHLAN3 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::metaphlan=3.0.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.12--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.12--pyhb7b1952_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path metaphlan_db\n\n    output:\n    tuple val(meta), path(\"*_profile.txt\")   ,                emit: profile\n    tuple val(meta), path(\"*.biom\")          ,                emit: biom\n    tuple val(meta), path('*.bowtie2out.txt'), optional:true, emit: bt2out\n    path \"versions.yml\"                      ,                emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def input_type  = (\"$input\".endsWith(\".fastq.gz\")) ? \"--input_type fastq\" :  (\"$input\".contains(\".fasta\")) ? \"--input_type fasta\" : (\"$input\".endsWith(\".bowtie2out.txt\")) ? \"--input_type bowtie2out\" : \"--input_type sam\"\n    def input_data  = (\"$input_type\".contains(\"fastq\")) && !meta.single_end ? \"${input[0]},${input[1]}\" : \"$input\"\n    def bowtie2_out = \"$input_type\" == \"--input_type bowtie2out\" || \"$input_type\" == \"--input_type sam\" ? '' : \"--bowtie2out ${prefix}.bowtie2out.txt\"\n\n    \"\"\"\n    metaphlan \\\\\n        --nproc $task.cpus \\\\\n        $input_type \\\\\n        $input_data \\\\\n        $args \\\\\n        $bowtie2_out \\\\\n        --bowtie2db ${metaphlan_db} \\\\\n        --biom ${prefix}.biom \\\\\n        --output_file ${prefix}_profile.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan3: \\$(metaphlan --version 2>&1 | awk '{print \\$3}')\n    END_VERSIONS\n    \"\"\"\n}", "process METAPHLAN3 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::metaphlan=3.0.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.12--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.12--pyhb7b1952_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path metaphlan_db\n\n    output:\n    tuple val(meta), path(\"*_profile.txt\")   ,                emit: profile\n    tuple val(meta), path(\"*.biom\")          ,                emit: biom\n    tuple val(meta), path('*.bowtie2out.txt'), optional:true, emit: bt2out\n    path \"versions.yml\"                      ,                emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_type  = (\"$input\".endsWith(\".fastq.gz\") || \"$input\".endsWith(\".fq.gz\")) ? \"--input_type fastq\" :  (\"$input\".contains(\".fasta\")) ? \"--input_type fasta\" : (\"$input\".endsWith(\".bowtie2out.txt\")) ? \"--input_type bowtie2out\" : \"--input_type sam\"\n    def input_data  = (\"$input_type\".contains(\"fastq\")) && !meta.single_end ? \"${input[0]},${input[1]}\" : \"$input\"\n    def bowtie2_out = \"$input_type\" == \"--input_type bowtie2out\" || \"$input_type\" == \"--input_type sam\" ? '' : \"--bowtie2out ${prefix}.bowtie2out.txt\"\n\n    \"\"\"\n    metaphlan \\\\\n        --nproc $task.cpus \\\\\n        $input_type \\\\\n        $input_data \\\\\n        $args \\\\\n        $bowtie2_out \\\\\n        --bowtie2db ${metaphlan_db} \\\\\n        --biom ${prefix}.biom \\\\\n        --output_file ${prefix}_profile.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan3: \\$(metaphlan --version 2>&1 | awk '{print \\$3}')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/jianhong__shotgun/METAPHLAN3", "ABMicroBioinf/magph/ABMicroBioinf__magph/METAPHLAN3", "xiaoli-dong/magph/xiaoli-dong__magph/METAPHLAN3", "nf-core/modules/nf-core__modules/METAPHLAN3"], "list_wf_names": ["ABMicroBioinf/magph", "jianhong/shotgun", "xiaoli-dong/magph", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Filter", "KAnalyze"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process PYDAMAGE_FILTER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::pydamage=0.70\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pydamage:0.70--pyhdfd78af_0' :\n        'quay.io/biocontainers/pydamage:0.70--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(csv)\n\n    output:\n    tuple val(meta), path(\"pydamage_results/pydamage_filtered_results.csv\"), emit: csv\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n\n    pydamage \\\\\n        filter \\\\\n        $args \\\\\n        $csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pydamage: \\$(echo \\$(pydamage --version 2>&1) | sed -e 's/pydamage, version //g')\n    END_VERSIONS\n    \"\"\"\n}", "process PYDAMAGE_ANALYZE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::pydamage=0.70\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pydamage:0.70--pyhdfd78af_0' :\n        'quay.io/biocontainers/pydamage:0.70--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"pydamage_results/pydamage_results.csv\"), emit: csv\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pydamage \\\\\n        analyze \\\\\n        $args \\\\\n        -p $task.cpus \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pydamage: \\$(echo \\$(pydamage --version 2>&1) | sed -e 's/pydamage, version //g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PYDAMAGE_FILTER", "nf-core/modules/nf-core__modules/PYDAMAGE_ANALYZE"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["VDJdb"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["airrflow"], "list_contrib": ["tbugfinder", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "subwaystation"], "nb_contrib": 8, "codes": ["process CHANGEO_MAKEDB {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::changeo=1.2.0 bioconda::igblast=1.17.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-2665a8a48fa054ad1fcccf53e711669939b3eac1:f479475bceae84156e57e303cfe804ab5629d62b-0' :\n        'quay.io/biocontainers/mulled-v2-2665a8a48fa054ad1fcccf53e711669939b3eac1:f479475bceae84156e57e303cfe804ab5629d62b-0' }\"\n\n\n    input:\n    tuple val(meta), path(reads)                         \n    path(igblast)                                                                                                             \n    path(imgt_base)\n\n    output:\n    tuple val(meta), path(\"*db-pass.tsv\"), emit: tab                                \n    path(\"*_command_log.txt\"), emit: logs               \n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    MakeDb.py igblast -i $igblast -s $reads -r \\\\\n    ${imgt_base}/${meta.species.toLowerCase()}/vdj/ \\\\\n    $args \\\\\n    --outname \"${meta.id}\" > \"${meta.id}_command_log.txt\"\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        igblastn: \\$( igblastn -version | grep -o \"igblast[0-9\\\\. ]\\\\+\" | grep -o \"[0-9\\\\. ]\\\\+\" )\n        changeo: \\$( MakeDb.py --version | awk -F' '  '{print \\$2}' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/airrflow/nf-core__airrflow/CHANGEO_MAKEDB"], "list_wf_names": ["nf-core/airrflow"]}, {"nb_reuse": 2, "tools": ["StringTie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "nanoseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "cying111", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "lwratten", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "csawye01", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 110, "codes": ["\nprocess STRINGTIE_MERGE {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n                                                         \n    conda     (params.enable_conda ? \"bioconda::stringtie=2.1.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/stringtie:2.1.7--h978d192_0\"\n    } else {\n        container \"quay.io/biocontainers/stringtie:2.1.7--h978d192_0\"\n    }\n\n    input:\n    path stringtie_gtf\n    path annotation_gtf\n\n    output:\n    path \"stringtie.merged.gtf\", emit: gtf\n    path  \"versions.yml\"       , emit: versions\n\n    script:\n    \"\"\"\n    stringtie \\\\\n        --merge $stringtie_gtf \\\\\n        -G $annotation_gtf \\\\\n        -o stringtie.merged.gtf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(stringtie --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process STRINGTIE_MERGE {\n    label 'process_medium'\n\n                                                         \n    conda     (params.enable_conda ? \"bioconda::stringtie=2.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/stringtie:2.2.1--hecb563c_2' :\n        'quay.io/biocontainers/stringtie:2.2.1--hecb563c_2' }\"\n\n    input:\n    path stringtie_gtf\n    path annotation_gtf\n\n    output:\n    path \"stringtie.merged.gtf\", emit: gtf\n    path  \"versions.yml\"       , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    stringtie \\\\\n        --merge $stringtie_gtf \\\\\n        -G $annotation_gtf \\\\\n        -o stringtie.merged.gtf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        stringtie: \\$(stringtie --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/STRINGTIE_MERGE", "nf-core/modules/nf-core__modules/STRINGTIE_MERGE"], "list_wf_names": ["nf-core/modules", "nf-core/nanoseq"]}, {"nb_reuse": 7, "tools": ["preseq"], "nb_own": 5, "list_own": ["raygozag", "nf-core", "mahesh-panchal", "harleenduggal", "csf-ngs"], "nb_wf": 6, "list_wf": ["RNASEQ", "test_nfcore_workflow_chain", "modules", "nfcore-rnaseq", "rnaseq", "controldna"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "idot", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 147, "codes": ["process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h445547b_2':\n        'quay.io/biocontainers/preseq:3.1.2--h445547b_2' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.lc_extrap.txt\"), emit: lc_extrap\n    tuple val(meta), path(\"*.log\")          , emit: log\n    path  \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.lc_extrap.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1' :\n        'quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"versions.yml\"                 , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h445547b_2':\n        'quay.io/biocontainers/preseq:3.1.2--h445547b_2' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.lc_extrap.txt\"), emit: lc_extrap\n    tuple val(meta), path(\"*.log\")          , emit: log\n    path  \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.lc_extrap.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1' :\n        'quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"versions.yml\"                 , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1' :\n        'quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"versions.yml\"                 , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1' :\n        'quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1' :\n        'quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        preseq: \\$(echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PRESEQ_LCEXTRAP", "raygozag/rnaseq/raygozag__rnaseq/PRESEQ_LCEXTRAP", "nf-core/rnaseq/nf-core__rnaseq/PRESEQ_LCEXTRAP", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/PRESEQ_LCEXTRAP", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/PRESEQ_LCEXTRAP", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/PRESEQ_LCEXTRAP", "csf-ngs/controldna/csf-ngs__controldna/PRESEQ_LCEXTRAP"], "list_wf_names": ["raygozag/rnaseq", "csf-ngs/controldna", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 4, "tools": ["mosdepth"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 4, "list_wf": ["test_nfcore_workflow_chain", "modules", "raredisease", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process MOSDEPTH {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::mosdepth=0.3.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mosdepth:0.3.3--hdfd78af_1' :\n        'quay.io/biocontainers/mosdepth:0.3.3--hdfd78af_1'}\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path  bed\n    val   window_size\n\n    output:\n    tuple val(meta), path('*.global.dist.txt')    , emit: global_txt\n    tuple val(meta), path('*.region.dist.txt')    , emit: regions_txt , optional:true\n    tuple val(meta), path('*.summary.txt')        , emit: summary_txt\n    tuple val(meta), path('*.per-base.d4')        , emit: d4          , optional:true\n    tuple val(meta), path('*.per-base.bed.gz')    , emit: per_base_bed, optional:true\n    tuple val(meta), path('*.per-base.bed.gz.csi'), emit: per_base_csi, optional:true\n    tuple val(meta), path('*.regions.bed.gz')     , emit: regions_bed , optional:true\n    tuple val(meta), path('*.regions.bed.gz.csi') , emit: regions_csi , optional:true\n    path  \"versions.yml\"                          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (window_size) {\n        interval = \"--by ${window_size}\"\n    } else if ( bed ) {\n        interval = \"--by ${bed}\"\n    } else {\n        interval = \"\"\n    }\n    \"\"\"\n    mosdepth \\\\\n        $interval \\\\\n        $args \\\\\n        $prefix \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.global.dist.txt\n    touch ${prefix}.region.dist.txt\n    touch ${prefix}.summary.txt\n    touch ${prefix}.per-base.d4\n    touch ${prefix}.per-base.bed.gz\n    touch ${prefix}.per-base.bed.gz.csi\n    touch ${prefix}.regions.bed.gz\n    touch ${prefix}.regions.bed.gz.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process MOSDEPTH {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::mosdepth=0.3.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mosdepth:0.3.3--hdfd78af_1' :\n        'quay.io/biocontainers/mosdepth:0.3.3--hdfd78af_1'}\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path  bed\n    val   window_size\n\n    output:\n    tuple val(meta), path('*.global.dist.txt')    , emit: global_txt\n    tuple val(meta), path('*.region.dist.txt')    , emit: regions_txt , optional:true\n    tuple val(meta), path('*.summary.txt')        , emit: summary_txt\n    tuple val(meta), path('*.per-base.d4')        , emit: d4          , optional:true\n    tuple val(meta), path('*.per-base.bed.gz')    , emit: per_base_bed, optional:true\n    tuple val(meta), path('*.per-base.bed.gz.csi'), emit: per_base_csi, optional:true\n    tuple val(meta), path('*.regions.bed.gz')     , emit: regions_bed , optional:true\n    tuple val(meta), path('*.regions.bed.gz.csi') , emit: regions_csi , optional:true\n    path  \"versions.yml\"                          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (window_size) {\n        interval = \"--by ${window_size}\"\n    } else if ( bed ) {\n        interval = \"--by ${bed}\"\n    } else {\n        interval = \"\"\n    }\n    \"\"\"\n    mosdepth \\\\\n        $interval \\\\\n        $args \\\\\n        $prefix \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.global.dist.txt\n    touch ${prefix}.region.dist.txt\n    touch ${prefix}.summary.txt\n    touch ${prefix}.per-base.d4\n    touch ${prefix}.per-base.bed.gz\n    touch ${prefix}.per-base.bed.gz.csi\n    touch ${prefix}.regions.bed.gz\n    touch ${prefix}.regions.bed.gz.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process MOSDEPTH {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::mosdepth=0.3.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mosdepth:0.3.3--hdfd78af_1' :\n        'quay.io/biocontainers/mosdepth:0.3.3--hdfd78af_1'}\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path  bed\n    val   window_size\n\n    output:\n    tuple val(meta), path('*.global.dist.txt')    , emit: global_txt\n    tuple val(meta), path('*.region.dist.txt')    , emit: regions_txt , optional:true\n    tuple val(meta), path('*.summary.txt')        , emit: summary_txt\n    tuple val(meta), path('*.per-base.d4')        , emit: d4          , optional:true\n    tuple val(meta), path('*.per-base.bed.gz')    , emit: per_base_bed, optional:true\n    tuple val(meta), path('*.per-base.bed.gz.csi'), emit: per_base_csi, optional:true\n    tuple val(meta), path('*.regions.bed.gz')     , emit: regions_bed , optional:true\n    tuple val(meta), path('*.regions.bed.gz.csi') , emit: regions_csi , optional:true\n    path  \"versions.yml\"                          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (window_size) {\n        interval = \"--by ${window_size}\"\n    } else if ( bed ) {\n        interval = \"--by ${bed}\"\n    } else {\n        interval = \"\"\n    }\n    \"\"\"\n    mosdepth \\\\\n        $interval \\\\\n        $args \\\\\n        $prefix \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.global.dist.txt\n    touch ${prefix}.region.dist.txt\n    touch ${prefix}.summary.txt\n    touch ${prefix}.per-base.d4\n    touch ${prefix}.per-base.bed.gz\n    touch ${prefix}.per-base.bed.gz.csi\n    touch ${prefix}.regions.bed.gz\n    touch ${prefix}.regions.bed.gz.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process MOSDEPTH {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::mosdepth=0.3.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mosdepth:0.3.3--hdfd78af_1' :\n        'quay.io/biocontainers/mosdepth:0.3.3--hdfd78af_1'}\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path  bed\n    val   window_size\n\n    output:\n    tuple val(meta), path('*.global.dist.txt')    , emit: global_txt\n    tuple val(meta), path('*.region.dist.txt')    , emit: regions_txt , optional:true\n    tuple val(meta), path('*.summary.txt')        , emit: summary_txt\n    tuple val(meta), path('*.per-base.d4')        , emit: d4          , optional:true\n    tuple val(meta), path('*.per-base.bed.gz')    , emit: per_base_bed, optional:true\n    tuple val(meta), path('*.per-base.bed.gz.csi'), emit: per_base_csi, optional:true\n    tuple val(meta), path('*.regions.bed.gz')     , emit: regions_bed , optional:true\n    tuple val(meta), path('*.regions.bed.gz.csi') , emit: regions_csi , optional:true\n    path  \"versions.yml\"                          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (window_size) {\n        interval = \"--by ${window_size}\"\n    } else if ( bed ) {\n        interval = \"--by ${bed}\"\n    } else {\n        interval = \"\"\n    }\n    \"\"\"\n    mosdepth \\\\\n        $interval \\\\\n        $args \\\\\n        $prefix \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.global.dist.txt\n    touch ${prefix}.region.dist.txt\n    touch ${prefix}.summary.txt\n    touch ${prefix}.per-base.d4\n    touch ${prefix}.per-base.bed.gz\n    touch ${prefix}.per-base.bed.gz.csi\n    touch ${prefix}.regions.bed.gz\n    touch ${prefix}.regions.bed.gz.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mosdepth: \\$(mosdepth --version 2>&1 | sed 's/^.*mosdepth //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/MOSDEPTH", "nf-core/viralrecon/nf-core__viralrecon/MOSDEPTH", "nf-core/modules/nf-core__modules/MOSDEPTH", "nf-core/raredisease/nf-core__raredisease/MOSDEPTH"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess samtools_filter {\n    label 'mc_medium'\n    tag \"$libraryid\"\n    publishDir \"${params.outdir}/samtools/filter\", mode: params.publish_dir_mode,\n    saveAs: {filename ->\n            if (filename.indexOf(\".fq.gz\") > 0) \"$filename\"\n            else if (filename.indexOf(\".unmapped.bam\") > 0) \"$filename\"\n            else if (filename.indexOf(\".filtered.bam\")) \"$filename\"\n            else null\n    }\n\n    when: \n    params.run_bam_filtering\n\n    input: \n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_seqtypemerged_for_samtools_filter\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*filtered.bam\"), file(\"*.{bai,csi}\") into ch_output_from_filtering\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*.unmapped.fastq.gz\") optional true into ch_bam_filtering_for_metagenomic,ch_metagenomic_for_skipentropyfilter\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*.unmapped.bam\") optional true\n\n    script:\n    \n    def size = params.large_ref ? '-c' : ''\n    \n                                                           \n    if ( \"${params.bam_unmapped_type}\" == \"keep\"  && params.bam_filter_minreadlength == 0 ) {\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -q ${params.bam_mapping_quality_threshold} -b > ${libraryid}.filtered.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"discard\" && params.bam_filter_minreadlength == 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > ${libraryid}.filtered.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"bam\" && params.bam_filter_minreadlength == 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -f4 -b > ${libraryid}.unmapped.bam\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > ${libraryid}.filtered.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"fastq\" && params.bam_filter_minreadlength == 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -f4 -b > ${libraryid}.unmapped.bam\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > ${libraryid}.filtered.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n\n        ## FASTQ\n        samtools fastq -tN ${libraryid}.unmapped.bam | pigz -p ${task.cpus - 1} > ${libraryid}.unmapped.fastq.gz\n        rm ${libraryid}.unmapped.bam\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"both\" && params.bam_filter_minreadlength == 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -f4 -b > ${libraryid}.unmapped.bam\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > ${libraryid}.filtered.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \n        ## FASTQ\n        samtools fastq -tN ${libraryid}.unmapped.bam | pigz -p ${task.cpus -1} > ${libraryid}.unmapped.fastq.gz\n        \"\"\"\n                                                        \n    } else if ( \"${params.bam_unmapped_type}\" == \"keep\" && params.bam_filter_minreadlength != 0 ) {\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -q ${params.bam_mapping_quality_threshold} -b > tmp_mapped.bam\n        filter_bam_fragment_length.py -a -l ${params.bam_filter_minreadlength} -o ${libraryid} tmp_mapped.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"discard\" && params.bam_filter_minreadlength != 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > tmp_mapped.bam\n        filter_bam_fragment_length.py -a -l ${params.bam_filter_minreadlength} -o ${libraryid} tmp_mapped.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"bam\" && params.bam_filter_minreadlength != 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -f4 -b > ${libraryid}.unmapped.bam\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > tmp_mapped.bam\n        filter_bam_fragment_length.py -a -l ${params.bam_filter_minreadlength} -o ${libraryid} tmp_mapped.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"fastq\" && params.bam_filter_minreadlength != 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -f4 -b > ${libraryid}.unmapped.bam\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > tmp_mapped.bam\n        filter_bam_fragment_length.py -a -l ${params.bam_filter_minreadlength} -o ${libraryid} tmp_mapped.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n\n        ## FASTQ\n        samtools fastq -tN ${libraryid}.unmapped.bam | pigz -p ${task.cpus - 1} > ${libraryid}.unmapped.fastq.gz\n        rm ${libraryid}.unmapped.bam\n        \"\"\"\n    } else if ( \"${params.bam_unmapped_type}\" == \"both\" && params.bam_filter_minreadlength != 0 ){\n        \"\"\"\n        samtools view -h ${bam} -@ ${task.cpus} -f4 -b > ${libraryid}.unmapped.bam\n        samtools view -h ${bam} -@ ${task.cpus} -F4 -q ${params.bam_mapping_quality_threshold} -b > tmp_mapped.bam\n        filter_bam_fragment_length.py -a -l ${params.bam_filter_minreadlength} -o ${libraryid} tmp_mapped.bam\n        samtools index ${libraryid}.filtered.bam ${size}\n        \n        ## FASTQ\n        samtools fastq -tN ${libraryid}.unmapped.bam | pigz -p ${task.cpus} > ${libraryid}.unmapped.fastq.gz\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/eager/nf-core__eager/samtools_filter"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["MEGAHIT"], "nb_own": 2, "list_own": ["xiaoli-dong", "nf-core"], "nb_wf": 2, "list_wf": ["magph", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process MEGAHIT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::megahit=1.2.9 conda-forge::pigz=2.6\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-0f92c152b180c7cd39d9b0e6822f8c89ccb59c99:8ec213d21e5d03f9db54898a2baeaf8ec729b447-0' :\n        'quay.io/biocontainers/mulled-v2-0f92c152b180c7cd39d9b0e6822f8c89ccb59c99:8ec213d21e5d03f9db54898a2baeaf8ec729b447-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"megahit_out/*.contigs.fa.gz\")                            , emit: contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.contigs.fa.gz\")      , emit: k_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.addi.fa.gz\")         , emit: addi_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.local.fa.gz\")        , emit: local_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.final.contigs.fa.gz\"), emit: kfinal_contigs\n    path \"versions.yml\"                                                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        megahit \\\\\n            -r ${reads} \\\\\n            -t $task.cpus \\\\\n            $args \\\\\n            --out-prefix $prefix\n\n        pigz \\\\\n            --no-name \\\\\n            -p $task.cpus \\\\\n            $args2 \\\\\n            megahit_out/*.fa \\\\\n            megahit_out/intermediate_contigs/*.fa\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            megahit: \\$(echo \\$(megahit -v 2>&1) | sed 's/MEGAHIT v//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        megahit \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            -t $task.cpus \\\\\n            $args \\\\\n            --out-prefix $prefix\n\n        pigz \\\\\n            --no-name \\\\\n            -p $task.cpus \\\\\n            $args2 \\\\\n            megahit_out/*.fa \\\\\n            megahit_out/intermediate_contigs/*.fa\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            megahit: \\$(echo \\$(megahit -v 2>&1) | sed 's/MEGAHIT v//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess MEGAHIT {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::megahit=1.2.9 conda-forge::pigz=2.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-0f92c152b180c7cd39d9b0e6822f8c89ccb59c99:8ec213d21e5d03f9db54898a2baeaf8ec729b447-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-0f92c152b180c7cd39d9b0e6822f8c89ccb59c99:8ec213d21e5d03f9db54898a2baeaf8ec729b447-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n                            \"megahit_out/*.contigs.fa.gz\"                                            \n    tuple val(meta), path(\"megahit_out/*.contigs.fa\")                            , emit: contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.contigs.fa.gz\")      , emit: k_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.addi.fa.gz\")         , emit: addi_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.local.fa.gz\")        , emit: local_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.final.contigs.fa.gz\"), emit: kfinal_contigs\n    path \"versions.yml\"                                                             , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        megahit \\\\\n            -r ${reads} \\\\\n            -t $task.cpus \\\\\n            $options.args \\\\\n            --out-prefix $prefix\n\n        pigz \\\\\n            --no-name \\\\\n            -p $task.cpus \\\\\n            $options.args2 \\\\\n            #megahit_out/*.fa \\\\\n            megahit_out/intermediate_contigs/*.fa\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(echo \\$(megahit -v 2>&1) | sed 's/MEGAHIT v//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        megahit \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            -t $task.cpus \\\\\n            $options.args \\\\\n            --out-prefix $prefix\n\n        pigz \\\\\n            --no-name \\\\\n            -p $task.cpus \\\\\n            megahit_out/intermediate_contigs/*.fa\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(echo \\$(megahit -v 2>&1) | sed 's/MEGAHIT v//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MEGAHIT", "xiaoli-dong/magph/xiaoli-dong__magph/MEGAHIT"], "list_wf_names": ["xiaoli-dong/magph", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess genotyping_ug {\n  label 'mc_small'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/genotyping\", mode: params.publish_dir_mode, pattern: '*{.vcf.gz,.realign.bam,realign.bai}'\n\n  when:\n  params.run_genotyping && params.genotyping_tool == 'ug'\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_damagemanipulation_for_genotyping_ug\n  file fasta from ch_fasta_for_genotyping_ug.collect()\n  file fai from ch_fai_for_ug.collect()\n  file dict from ch_dict_for_ug.collect()\n\n  output: \n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*vcf.gz\") into ch_ug_for_multivcfanalyzer,ch_ug_for_vcf2genome,ch_ug_for_bcftools_stats\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*.realign.{bam,bai}\") optional true\n\n  script:\n  def defaultbasequalities = !params.gatk_ug_defaultbasequalities ? '' : \" --defaultBaseQualities ${params.gatk_ug_defaultbasequalities}\" \n  def keep_realign = params.gatk_ug_keep_realign_bam ? \"samtools index ${samplename}.realign.bam\" : \"rm ${samplename}.realign.{bam,bai}\"\n  if (!params.gatk_dbsnp)\n    \"\"\"\n    samtools index -b ${bam}\n    gatk3 -Xmx${task.memory.toGiga()}g -T RealignerTargetCreator -R ${fasta} -I ${bam} -nt ${task.cpus} -o ${samplename}.intervals ${defaultbasequalities}\n    gatk3 -Xmx${task.memory.toGiga()}g -T IndelRealigner -R ${fasta} -I ${bam} -targetIntervals ${samplename}.intervals -o ${samplename}.realign.bam ${defaultbasequalities}\n    gatk3 -Xmx${task.memory.toGiga()}g -T UnifiedGenotyper -R ${fasta} -I ${samplename}.realign.bam -o ${samplename}.unifiedgenotyper.vcf -nt ${task.cpus} --genotype_likelihoods_model ${params.gatk_ug_genotype_model} -stand_call_conf ${params.gatk_call_conf} --sample_ploidy ${params.gatk_ploidy} -dcov ${params.gatk_downsample} --output_mode ${params.gatk_ug_out_mode} ${defaultbasequalities}\n    \n    $keep_realign\n    \n    bgzip -@ ${task.cpus} ${samplename}.unifiedgenotyper.vcf\n    \"\"\"\n  else if (params.gatk_dbsnp)\n    \"\"\"\n    samtools index ${bam}\n    gatk3 -Xmx${task.memory.toGiga()}g -T RealignerTargetCreator -R ${fasta} -I ${bam} -nt ${task.cpus} -o ${samplename}.intervals ${defaultbasequalities}\n    gatk3 -Xmx${task.memory.toGiga()}g -T IndelRealigner -R ${fasta} -I ${bam} -targetIntervals ${samplenane}.intervals -o ${samplename}.realign.bam ${defaultbasequalities}\n    gatk3 -Xmx${task.memory.toGiga()}g -T UnifiedGenotyper -R ${fasta} -I ${samplename}.realign.bam -o ${samplename}.unifiedgenotyper.vcf -nt ${task.cpus} --dbsnp ${params.gatk_dbsnp} --genotype_likelihoods_model ${params.gatk_ug_genotype_model} -stand_call_conf ${params.gatk_call_conf} --sample_ploidy ${params.gatk_ploidy} -dcov ${params.gatk_downsample} --output_mode ${params.gatk_ug_out_mode} ${defaultbasequalities}\n    \n    $keep_realign\n    \n    bgzip -@  ${task.cpus} ${samplename}.unifiedgenotyper.vcf\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/genotyping_ug"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "rnavar"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor"], "nb_contrib": 107, "codes": ["process GATK4_APPLYBQSR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(bqsr_table), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam,  optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK ApplyBQSR] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" ApplyBQSR \\\\\n        --input $input \\\\\n        --output ${prefix}.${input.getExtension()} \\\\\n        --reference $fasta \\\\\n        --bqsr-recal-file $bqsr_table \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_APPLYBQSR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(bqsr_table), path(intervals)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam,  optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK ApplyBQSR] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" ApplyBQSR \\\\\n        --input $input \\\\\n        --output ${prefix}.${input.getExtension()} \\\\\n        --reference $fasta \\\\\n        --bqsr-recal-file $bqsr_table \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_APPLYBQSR", "nf-core/rnavar/nf-core__rnavar/GATK4_APPLYBQSR"], "list_wf_names": ["nf-core/rnavar", "nf-core/modules"]}, {"nb_reuse": 4, "tools": ["Consensus", "BCFtools"], "nb_own": 3, "list_own": ["mahesh-panchal", "nf-core", "CDCgov"], "nb_wf": 4, "list_wf": ["modules", "test_nfcore_workflow_chain", "mycosnp-nf", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "leebrian", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 116, "codes": ["process BCFTOOLS_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(fasta)\n\n    output:\n    tuple val(meta), path('*.fa'), emit: fasta\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    cat $fasta \\\\\n        | bcftools \\\\\n            consensus \\\\\n            $vcf \\\\\n            $args \\\\\n            > ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(fasta)\n\n    output:\n    tuple val(meta), path('*.fa'), emit: fasta\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    cat $fasta \\\\\n        | bcftools \\\\\n            consensus \\\\\n            $vcf \\\\\n            $args \\\\\n            > ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(fasta)\n\n    output:\n    tuple val(meta), path('*.fa'), emit: fasta\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    cat $fasta \\\\\n        | bcftools \\\\\n            consensus \\\\\n            $vcf \\\\\n            $args \\\\\n            > ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_CONSENSUS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(fasta)\n\n    output:\n    tuple val(meta), path('*.fa'), emit: fasta\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    cat $fasta \\\\\n        | bcftools \\\\\n            consensus \\\\\n            $vcf \\\\\n            $args \\\\\n            > ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BCFTOOLS_CONSENSUS", "nf-core/viralrecon/nf-core__viralrecon/BCFTOOLS_CONSENSUS", "nf-core/modules/nf-core__modules/BCFTOOLS_CONSENSUS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BCFTOOLS_CONSENSUS"], "list_wf_names": ["nf-core/viralrecon", "nf-core/modules", "mahesh-panchal/test_nfcore_workflow_chain", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 53, "tools": ["FastQC"], "nb_own": 40, "list_own": ["ajodeh-juma", "SpikyClip", "ggabernet", "ablab", "luiskuhn", "wtsi-hgi", "kevbrick", "sguizard", "hukai916", "tamara-hodgetts", "avantonder", "lskatz", "Akazhiel", "junyu-boston", "Jojanneke-S", "lauramble", "NCBI-Codeathons", "chelauk", "happykhan", "MeghanaKB-Rheos", "vibbits", "nibscbioinformatics", "nf-core", "luslab", "jianhong", "peterk87", "alam1988", "seandavi", "MGordon09", "gongyh", "BonaBeavis", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "ray1919", "christopher-hakkaart", "letovesnoi", "cguyomar", "qbic-pipelines", "remiolsen"], "nb_wf": 51, "list_wf": ["bench", "PRECODE", "nf-core-platypus", "viclara", "nf-klebtest", "nf-core_modules_demo", "nanoseq", "lung-rna-seq", "graphamr", "nf-core-egatransfer", "lRNA-Seq", "associations", "nf-core-partial-umi", "nf-iav-illumina", "nf-core-scp", "nf-core-bagobugs", "bactmap", "hicscaff", "eqtl", "dicerna-rnaseq", "nf-core-mlst", "rnaseqpca", "rnaseq-editing", "NeoPred-NF", "nf-core-benchmark", "nf-core-cmgd", "vcreport", "nf-core-umi_preprocessing", "bacass", "nf-core-denovohybrid", "llrnaseq", "nf-core-buggybarcodes", "mag", "rnaseq-vizfada", "testpipe", "nf-core-tamanmd", "assembleBAC", "nf-core-viralevo", "nf-ase", "demo_nfcore_obsolete", "nf-core-conva", "nf-core-alttemplate", "universalModule", "nf-core-prototype", "nf-core-testpipeline", "ssds_nfcore", "clusterassembly", "dsltwotest", "liverctanalysis", "nf-atac-seq", "Nextflow"], "list_contrib": ["ajodeh-juma", "drejom", "asl", "HadrienG", "SpikyClip", "alex-botzki", "jordwil", "chuan-wang", "AntoniaSchuster", "Galithil", "avantonder", "lskatz", "na399", "cying111", "lauramble", "LaurenceKuhl", "thanhleviet", "peterk87", "seandavi", "MGordon09", "pranathivemuri", "aanil", "silviamorins", "d4straub", "yuukiiwa", "zxl124", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "antunderwood", "colindaven", "lpantano", "jfy133", "mjmansfi", "ppericard", "kevbrick", "rivera10", "bewt85", "mvanins", "rfenouil", "happykhan", "MeghanaKB-Rheos", "dshafranskaya", "jburos", "abotzki", "gongyh", "Hammarn", "alexa-salsbury", "sven1103", "jemten", "YanFangBio", "kaurravneet4123", "amayer21", "alexandregilardet", "heuermh", "ewels", "evanfloden", "pcantalupo", "jun-wan", "BABS-STP1", "senthil10", "kviljoen", "angelovangel", "xlinxlin", "alam1988", "drpatelh", "cguyomar", "Emiller88", "alneberg", "arontommi", "ggabernet", "vezzi", "skrakau", "luiskuhn", "ksaunders73", "grst", "lwratten", "sguizard", "tamara-hodgetts", "nf-core-bot", "csawye01", "drice-codeathons", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "Akazhiel", "remiolsen", "robsyme", "chelauk", "DSchreyer", "mashehu", "jianhong", "BonaBeavis", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ray1919", "maxibor", "Sanger-ad7", "letovesnoi", "olgabot", "paulklemm"], "nb_contrib": 107, "codes": ["\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n                            \n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", " process FASTQC {\n     tag \"$meta.id\"\n     label 'process_medium'\n     publishDir \"${params.outdir}/${options.publish_dir}\",\n         mode: options.publish_mode,\n         enabled: options.publish_enabled\n\n     conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n     if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n         container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n     } else {\n         container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n     }\n\n     input:\n     tuple val(meta), path(reads)\n     tuple path(index), path(fasta)\n\n     output:\n     tuple val(meta), path(\"*.html\"), emit: html\n     tuple val(meta), path(\"*.zip\"), emit: zip\n     path \"fastqc.version.txt\", emit: version\n\n     script:\n     params.options.forEach{key, value -> options[key]=value}\n      if (meta.single_end) {\n          \"\"\"\n          [ ! -f  ${meta.id}.fastq.gz ] && ln -s ${reads[0]} ${meta.id}.fastq.gz\n          fastqc $options.args --threads $task.cpus ${meta.id}.fastq.gz\n          fastqc --version | sed -e \"s/FastQC v//g\" > fastqc.version.txt\n          \"\"\"\n      } else {\n          \"\"\"\n          [ ! -f  ${meta.id}_1.fastq.gz ] && ln -s ${reads[0]} ${meta.id}_1.fastq.gz\n          [ ! -f  ${meta.id}_2.fastq.gz ] && ln -s ${reads[1]} ${meta.id}_2.fastq.gz\n          fastqc $options.args --threads $task.cpus ${meta.id}_1.fastq.gz ${meta.id}_2.fastq.gz\n          fastqc --version | sed -e \"s/FastQC v//g\" > fastqc.version.txt\n          \"\"\"\n      }\n }", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda     (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[2]} ${prefix}_3.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz ${prefix}_3.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" s> ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda     (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[2]} ${prefix}_3.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz ${prefix}_3.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n    def software = 'fastqc'                                           \n    def args     = task.ext.args?: ''\n    def prefix   = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n                                     \n                                         \n                                                                                                                                                                     \n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    \n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["luiskuhn/liverctanalysis/luiskuhn__liverctanalysis/FASTQC", "ablab/graphamr/ablab__graphamr/FASTQC", "nf-core/mag/nf-core__mag/FASTQC", "chelauk/nf-core-egatransfer/chelauk__nf-core-egatransfer/FASTQC", "nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/FASTQC", "jianhong/universalModule/jianhong__universalModule/FASTQC", "happykhan/nf-klebtest/happykhan__nf-klebtest/FASTQC", "KevinMenden/nf-core-testpipeline/KevinMenden__nf-core-testpipeline/FASTQC", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/FASTQC", "KevinMenden/testpipe/KevinMenden__testpipe/FASTQC", "remiolsen/hicscaff/remiolsen__hicscaff/FASTQC", "chelauk/nf-core-partial-umi/chelauk__nf-core-partial-umi/FASTQC", "chelauk/nf-core-platypus/chelauk__nf-core-platypus/FASTQC", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/FASTQC", "luslab/nf-core-denovohybrid/luslab__nf-core-denovohybrid/FASTQC", "seandavi/nf-core-cmgd/seandavi__nf-core-cmgd/FASTQC", "letovesnoi/clusterassembly/letovesnoi__clusterassembly/FASTQC", "MeghanaKB-Rheos/rnaseqpca/MeghanaKB-Rheos__rnaseqpca/FASTQC", "ggabernet/dsltwotest/ggabernet__dsltwotest/FASTQC", "chelauk/nf-core-umi_preprocessing/chelauk__nf-core-umi_preprocessing/FASTQC", "sguizard/nf-core-tamanmd/sguizard__nf-core-tamanmd/FASTQC", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/FASTQC", "kevbrick/ssds_nfcore/kevbrick__ssds_nfcore/FASTQC", "mahesh-panchal/nf-core-alttemplate/mahesh-panchal__nf-core-alttemplate/FASTQC", "mahesh-panchal/nf-core_modules_demo/mahesh-panchal__nf-core_modules_demo/FASTQC", "peterk87/nf-iav-illumina/peterk87__nf-iav-illumina/FASTQC", "qbic-pipelines/vcreport/qbic-pipelines__vcreport/FASTQC", "nibscbioinformatics/nf-core-buggybarcodes/nibscbioinformatics__nf-core-buggybarcodes/FASTQC", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/FASTQC", "wtsi-hgi/associations/wtsi-hgi__associations/FASTQC", "avantonder/assembleBAC/avantonder__assembleBAC/FASTQC", "wtsi-hgi/eqtl/wtsi-hgi__eqtl/FASTQC", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/FASTQC", "ajodeh-juma/viclara/ajodeh-juma__viclara/FASTQC", "nf-core/bacass/nf-core__bacass/FASTQC", "NCBI-Codeathons/lung-rna-seq/NCBI-Codeathons__lung-rna-seq/FASTQC", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/FASTQC", "MGordon09/nf-core-bagobugs/MGordon09__nf-core-bagobugs/FASTQC", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/FASTQC", "nf-core/bactmap/nf-core__bactmap/FASTQC", "MGordon09/nf-core-buggybarcodes/MGordon09__nf-core-buggybarcodes/FASTQC", "cguyomar/nf-ase/cguyomar__nf-ase/FASTQC", "Akazhiel/NeoPred-NF/Akazhiel__NeoPred-NF/FASTQC", "nf-core/nanoseq/nf-core__nanoseq/FASTQC", "BonaBeavis/nf-core-prototype/BonaBeavis__nf-core-prototype/FASTQC", "alam1988/Nextflow/alam1988__Nextflow/FASTQC", "gongyh/nf-core-scp/gongyh__nf-core-scp/FASTQC", "christopher-hakkaart/bench/christopher-hakkaart__bench/FASTQC", "nibscbioinformatics/nf-core-bagobugs/nibscbioinformatics__nf-core-bagobugs/FASTQC", "lskatz/nf-core-mlst/lskatz__nf-core-mlst/FASTQC", "hukai916/demo_nfcore_obsolete/hukai916__demo_nfcore_obsolete/FASTQC", "Jojanneke-S/PRECODE/Jojanneke-S__PRECODE/FASTQC", "JoseEspinosa/nf-core-benchmark/JoseEspinosa__nf-core-benchmark/FASTQC"], "list_wf_names": ["KevinMenden/testpipe", "nf-core/bacass", "mahesh-panchal/nf-core-alttemplate", "alam1988/Nextflow", "lauramble/rnaseq-vizfada", "jianhong/universalModule", "nibscbioinformatics/nf-core-conva", "JoseEspinosa/nf-core-benchmark", "wtsi-hgi/associations", "chelauk/nf-core-egatransfer", "luiskuhn/liverctanalysis", "chelauk/nf-core-partial-umi", "wtsi-hgi/eqtl", "ajodeh-juma/viclara", "nf-core/bactmap", "christopher-hakkaart/bench", "hukai916/demo_nfcore_obsolete", "ablab/graphamr", "ray1919/lRNA-Seq", "cguyomar/nf-ase", "mahesh-panchal/nf-core_modules_demo", "peterk87/nf-iav-illumina", "junyu-boston/dicerna-rnaseq", "qbic-pipelines/vcreport", "lskatz/nf-core-mlst", "MGordon09/nf-core-buggybarcodes", "chelauk/nf-core-platypus", "MeghanaKB-Rheos/rnaseqpca", "SpikyClip/llrnaseq", "letovesnoi/clusterassembly", "MGordon09/nf-core-bagobugs", "nf-core/mag", "tamara-hodgetts/nf-atac-seq", "vibbits/rnaseq-editing", "avantonder/assembleBAC", "gongyh/nf-core-scp", "ggabernet/dsltwotest", "kevbrick/ssds_nfcore", "nibscbioinformatics/nf-core-buggybarcodes", "BonaBeavis/nf-core-prototype", "nibscbioinformatics/nf-core-bagobugs", "remiolsen/hicscaff", "KevinMenden/nf-core-testpipeline", "NCBI-Codeathons/lung-rna-seq", "Akazhiel/NeoPred-NF", "happykhan/nf-klebtest", "luslab/nf-core-denovohybrid", "chelauk/nf-core-umi_preprocessing", "seandavi/nf-core-cmgd", "nf-core/nanoseq", "Jojanneke-S/PRECODE", "sguizard/nf-core-tamanmd", "nibscbioinformatics/nf-core-viralevo"]}, {"nb_reuse": 14, "tools": ["MarkDuplicates (IP)", "Picard"], "nb_own": 8, "list_own": ["raygozag", "vincenthhu", "nf-core", "mahesh-panchal", "CDCgov", "harleenduggal", "csf-ngs", "cguyomar"], "nb_wf": 13, "list_wf": ["raredisease", "mycosnp-nf", "RNASEQ", "viralrecon", "ssds", "modules", "nf-ase", "test_nfcore_workflow_chain", "nfcore-rnaseq", "rnaseq", "nf-core-westest", "controldna", "cutandrun"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "ktrns", "FelixKrueger", "jordwil", "dladd", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "mciprianoCDC", "santiagorevale", "ppericard", "idot", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "saramonzon", "cjjossart", "pcantalupo", "cjfields", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "charlotte-west", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "cguyomar", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "vincenthhu", "leebrian", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 163, "codes": ["process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::picard=2.25.7' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.25.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.25.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --METRICS_FILE ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    touch ${prefix}.bam.bai\n    touch ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    touch ${prefix}.bam.bai\n    touch ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    touch ${prefix}.bam.bai\n    touch ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.md.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::picard=2.25.7' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.25.7--hdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.25.7--hdfd78af_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        -I $bam \\\\\n        -O ${prefix}.bam \\\\\n        -M ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::picard=2.25.7' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.25.7--hdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.25.7--hdfd78af_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        -I $bam \\\\\n        -O ${prefix}.bam \\\\\n        -M ${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ssds/nf-core__ssds/PICARD_MARKDUPLICATES", "nf-core/modules/nf-core__modules/PICARD_MARKDUPLICATES", "nf-core/raredisease/nf-core__raredisease/PICARD_MARKDUPLICATES", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/PICARD_MARKDUPLICATES", "nf-core/rnaseq/nf-core__rnaseq/PICARD_MARKDUPLICATES", "raygozag/rnaseq/raygozag__rnaseq/PICARD_MARKDUPLICATES", "nf-core/viralrecon/nf-core__viralrecon/PICARD_MARKDUPLICATES", "csf-ngs/controldna/csf-ngs__controldna/PICARD_MARKDUPLICATES", "cguyomar/nf-ase/cguyomar__nf-ase/PICARD_MARKDUPLICATES", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/PICARD_MARKDUPLICATES", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/PICARD_MARKDUPLICATES", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/PICARD_MARKDUPLICATES", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/PICARD_MARKDUPLICATES", "nf-core/cutandrun/nf-core__cutandrun/PICARD_MARKDUPLICATES"], "list_wf_names": ["raygozag/rnaseq", "csf-ngs/controldna", "cguyomar/nf-ase", "nf-core/raredisease", "nf-core/ssds", "harleenduggal/RNASEQ", "vincenthhu/nf-core-westest", "nf-core/cutandrun", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/viralrecon", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 6, "tools": ["MultiQC"], "nb_own": 6, "list_own": ["alam1988", "ajodeh-juma", "vibbits", "nf-core", "ray1919", "junyu-boston"], "nb_wf": 6, "list_wf": ["viclara", "dicerna-rnaseq", "rnaseq-editing", "cutandrun", "lRNA-Seq", "Nextflow"], "list_contrib": ["jordeu", "ajodeh-juma", "alneberg", "FriederikeHanssen", "ewels", "drejom", "arontommi", "maxulysse", "rsuchecki", "matrulda", "ggabernet", "veeravalli", "jordwil", "vezzi", "alex-botzki", "colindaven", "lpantano", "dladd", "skrakau", "chuan-wang", "ppericard", "grst", "pcantalupo", "nf-core-bot", "mvanins", "Galithil", "cjfields", "jun-wan", "na399", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "BABS-STP1", "senthil10", "kviljoen", "rfenouil", "charlotte-west", "jburos", "chris-cheshire", "mashehu", "abotzki", "alam1988", "Hammarn", "sven1103", "jemten", "paulklemm", "pranathivemuri", "marchoeppner", "KevinMenden", "apeltzer", "ray1919", "aanil", "silviamorins", "d4straub", "olgabot", "drpatelh", "amayer21", "zxl124"], "nb_contrib": 60, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    \n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path software_versions_unique\n    path workflow_summary\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('bowtie2/*')\n    path ('bowtie2/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('reports/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    \n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--pyhdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--pyhdfd78af_1\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n             'fastqc/*' \n             'trimgalore/fastqc/*' \n             'trimgalore/*' \n    path ('data/fastp/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--pyhdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--pyhdfd78af_1\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc_trim/*')\n    path ('fastqc/*')\n    path ('star/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path ('fastqc/*')\n    path ('fastp/log/*')\n    path ('trimmomatic/log/*')\n            'bowtie2/*' \n    \n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}"], "list_proc": ["alam1988/Nextflow/alam1988__Nextflow/MULTIQC", "nf-core/cutandrun/nf-core__cutandrun/MULTIQC", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/MULTIQC", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/MULTIQC", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/MULTIQC", "ajodeh-juma/viclara/ajodeh-juma__viclara/MULTIQC"], "list_wf_names": ["ajodeh-juma/viclara", "ray1919/lRNA-Seq", "junyu-boston/dicerna-rnaseq", "alam1988/Nextflow", "nf-core/cutandrun", "vibbits/rnaseq-editing"]}, {"nb_reuse": 3, "tools": ["Filtlong"], "nb_own": 2, "list_own": ["ABMicroBioinf", "nf-core"], "nb_wf": 3, "list_wf": ["pathogen", "modules", "magph"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["\nprocess FILTLONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n     publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::filtlong=0.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/filtlong:0.2.1--h9a82719_0' :'quay.io/biocontainers/filtlong:0.2.1--h9a82719_0' }\"\n\n    input:\n    tuple val(meta),path(longreads)\n\n    output:\n    tuple val(meta), path(\"*_filtlong.fastq.gz\"), emit: reads\n    path \"versions.yml\"                                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n   \n    \"\"\"\n    filtlong \\\\\n        $options.args \\\\\n        $longreads \\\\\n        | gzip -n > ${prefix}_filtlong.fastq.gz\n        \n    cat <<-END_VERSIONS > versions.yml\n\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( filtlong --version | sed -e \"s/Filtlong v//g\" )\n\n    END_VERSIONS\n    \n    \"\"\"\n}", "\nprocess FILTLONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n     publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::filtlong=0.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/filtlong:0.2.1--h9a82719_0' :'quay.io/biocontainers/filtlong:0.2.1--h9a82719_0' }\"\n\n    input:\n    tuple val(meta),path(longreads)\n\n    output:\n    tuple val(meta), path(\"*_filtlong.fastq.gz\"), emit: reads\n    path \"versions.yml\"                                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n   \n    \"\"\"\n    filtlong \\\\\n        $options.args \\\\\n        $longreads \\\\\n        | gzip -n > ${prefix}_filtlong.fastq.gz\n        \n    cat <<-END_VERSIONS > versions.yml\n\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( filtlong --version | sed -e \"s/Filtlong v//g\" )\n\n    END_VERSIONS\n    \n    \"\"\"\n}", "process FILTLONG {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::filtlong=0.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/filtlong:0.2.1--h9a82719_0' :\n        'quay.io/biocontainers/filtlong:0.2.1--h9a82719_0' }\"\n\n    input:\n    tuple val(meta), path(shortreads), path(longreads)\n\n    output:\n    tuple val(meta), path(\"${meta.id}_lr_filtlong.fastq.gz\"), emit: reads\n    path \"versions.yml\"                                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def short_reads = meta.single_end ? \"-1 $shortreads\" : \"-1 ${shortreads[0]} -2 ${shortreads[1]}\"\n    \"\"\"\n    filtlong \\\\\n        $short_reads \\\\\n        $args \\\\\n        $longreads \\\\\n        | gzip -n > ${prefix}_lr_filtlong.fastq.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        filtlong: \\$( filtlong --version | sed -e \"s/Filtlong v//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/FILTLONG", "ABMicroBioinf/magph/ABMicroBioinf__magph/FILTLONG", "nf-core/modules/nf-core__modules/FILTLONG"], "list_wf_names": ["ABMicroBioinf/magph", "ABMicroBioinf/pathogen", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "QualiMap"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process QUALIMAP_BAMQCCRAM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-d3934ca6bb4e61334891ffa2e9a4c87a530e3188:61f6d4658ac88635fc37623af50bba77561988ab-0' :\n        'quay.io/biocontainers/mulled-v2-d3934ca6bb4e61334891ffa2e9a4c87a530e3188:61f6d4658ac88635fc37623af50bba77561988ab-0' }\"\n\n    input:\n    tuple val(meta), path(cram), path(crai)\n    path  gff\n    path  fasta\n    path  fasta_fai\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n\n    samtools view -hb -T ${fasta} ${cram} |\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $args \\\\\n        -bam /dev/stdin \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/QUALIMAP_BAMQCCRAM"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Arriba"], "nb_own": 2, "list_own": ["xiaoli-dong", "nf-core"], "nb_wf": 2, "list_wf": ["magph", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process ARRIBA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::arriba=2.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/arriba:2.2.1--hecb563c_2' :\n        'quay.io/biocontainers/arriba:2.2.1--hecb563c_2' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    path gtf\n    path blacklist\n    path known_fusions\n    path structural_variants\n    path tags\n    path protein_domains\n\n    output:\n    tuple val(meta), path(\"*.fusions.tsv\")          , emit: fusions\n    tuple val(meta), path(\"*.fusions.discarded.tsv\"), emit: fusions_fail\n    path \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def blacklist = blacklist ? \"-b $blacklist\" : \"-f blacklist\"\n    def known_fusions = known_fusions ? \"-k $known_fusions\" : \"\"\n    def structural_variants = structural_variants ? \"-d $structual_variants\" : \"\"\n    def tags = tags ? \"-t $tags\" : \"\"\n    def protein_domains = protein_domains ? \"-p $protein_domains\" : \"\"\n\n    \"\"\"\n    arriba \\\\\n        -x $bam \\\\\n        -a $fasta \\\\\n        -g $gtf \\\\\n        -o ${prefix}.fusions.tsv \\\\\n        -O ${prefix}.fusions.discarded.tsv \\\\\n        $blacklist \\\\\n        $known_fusions \\\\\n        $structural_variants \\\\\n        $tags \\\\\n        $protein_domains \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        arriba: \\$(arriba -h | grep 'Version:' 2>&1 |  sed 's/Version:\\s//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    echo stub > ${prefix}.fusions.tsv\n    echo stub > ${prefix}.fusions.discarded.tsv\n\n    echo \"${task.process}:\" > versions.yml\n    echo ' arriba: 2.2.1' >> versions.yml\n    \"\"\"\n}", "process ARRIBA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::arriba=2.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/arriba:2.1.0--h3198e80_1' :\n        'quay.io/biocontainers/arriba:2.1.0--h3198e80_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    path gtf\n\n    output:\n    tuple val(meta), path(\"*.fusions.tsv\")          , emit: fusions\n    tuple val(meta), path(\"*.fusions.discarded.tsv\"), emit: fusions_fail\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def blacklist = (args.contains('-b')) ? '' : '-f blacklist'\n    \"\"\"\n    arriba \\\\\n        -x $bam \\\\\n        -a $fasta \\\\\n        -g $gtf \\\\\n        -o ${prefix}.fusions.tsv \\\\\n        -O ${prefix}.fusions.discarded.tsv \\\\\n        $blacklist \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        arriba: \\$(arriba -h | grep 'Version:' 2>&1 |  sed 's/Version:\\s//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/ARRIBA", "xiaoli-dong/magph/xiaoli-dong__magph/ARRIBA"], "list_wf_names": ["xiaoli-dong/magph", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess dedup{\n    label 'mc_small'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/deduplication/\", mode: params.publish_dir_mode,\n        saveAs: {filename -> \"${libraryid}/$filename\"}\n\n    when:\n    !params.skip_deduplication && params.dedupper == 'dedup'\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_filtering_for_dedup\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.hist\") into ch_hist_for_preseq\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.json\") into ch_dedup_results_for_multiqc\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"${libraryid}_rmdup.bam\"), path(\"*.{bai,csi}\") into ch_output_from_dedup, ch_dedup_for_libeval\n\n    script:\n    def treat_merged = params.dedup_all_merged ? '-m' : ''\n    def size = params.large_ref ? '-c' : ''\n    \n    if ( bam.baseName != libraryid ) {\n                                                 \n    \"\"\"\n    mv ${bam} ${libraryid}.bam\n    dedup -Xmx${task.memory.toGiga()}g -i ${libraryid}.bam $treat_merged -o . -u \n    mv *.log dedup.log\n    samtools sort -@ ${task.cpus} \"${libraryid}\"_rmdup.bam -o \"${libraryid}\"_rmdup.bam\n    samtools index \"${libraryid}\"_rmdup.bam ${size}\n    \"\"\"\n    } else {\n    \"\"\"\n    dedup -Xmx${task.memory.toGiga()}g -i ${libraryid}.bam $treat_merged -o . -u \n    mv *.log dedup.log\n    samtools sort -@ ${task.cpus} \"${libraryid}\"_rmdup.bam -o \"${libraryid}\"_rmdup.bam\n    samtools index \"${libraryid}\"_rmdup.bam ${size}\n    \"\"\"\n    }\n}"], "list_proc": ["nf-core/eager/nf-core__eager/dedup"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["Diamond", "BLASTP-ACC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process DIAMOND_BLASTX {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::diamond=2.0.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/diamond:2.0.15--hb97b32f_0' :\n        'quay.io/biocontainers/diamond:2.0.15--hb97b32f_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path db\n    val out_ext\n    val blast_columns\n\n    output:\n    tuple val(meta), path('*.blast'), optional: true, emit: blast\n    tuple val(meta), path('*.xml')  , optional: true, emit: xml\n    tuple val(meta), path('*.txt')  , optional: true, emit: txt\n    tuple val(meta), path('*.daa')  , optional: true, emit: daa\n    tuple val(meta), path('*.sam')  , optional: true, emit: sam\n    tuple val(meta), path('*.tsv')  , optional: true, emit: tsv\n    tuple val(meta), path('*.paf')  , optional: true, emit: paf\n    path \"versions.yml\"                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def columns = blast_columns ? \"${blast_columns}\" : ''\n    switch ( out_ext ) {\n        case \"blast\": outfmt = 0; break\n        case \"xml\": outfmt = 5; break\n        case \"txt\": outfmt = 6; break\n        case \"daa\": outfmt = 100; break\n        case \"sam\": outfmt = 101; break\n        case \"tsv\": outfmt = 102; break\n        case \"paf\": outfmt = 103; break\n        default:\n            outfmt = '6';\n            out_ext = 'txt';\n            log.warn(\"Unknown output file format provided (${out_ext}): selecting DIAMOND default of tabular BLAST output (txt)\");\n            break\n    }\n    \"\"\"\n    DB=`find -L ./ -name \"*.dmnd\" | sed 's/.dmnd//'`\n\n    diamond \\\\\n        blastx \\\\\n        --threads $task.cpus \\\\\n        --db \\$DB \\\\\n        --query $fasta \\\\\n        --outfmt ${outfmt} ${columns} \\\\\n        $args \\\\\n        --out ${prefix}.${out_ext}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        diamond: \\$(diamond --version 2>&1 | tail -n 1 | sed 's/^diamond version //')\n    END_VERSIONS\n    \"\"\"\n}", "process DIAMOND_BLASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::diamond=2.0.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/diamond:2.0.15--hb97b32f_0' :\n        'quay.io/biocontainers/diamond:2.0.15--hb97b32f_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path db\n    val out_ext\n    val blast_columns\n\n    output:\n    tuple val(meta), path('*.blast'), optional: true, emit: blast\n    tuple val(meta), path('*.xml')  , optional: true, emit: xml\n    tuple val(meta), path('*.txt')  , optional: true, emit: txt\n    tuple val(meta), path('*.daa')  , optional: true, emit: daa\n    tuple val(meta), path('*.sam')  , optional: true, emit: sam\n    tuple val(meta), path('*.tsv')  , optional: true, emit: tsv\n    tuple val(meta), path('*.paf')  , optional: true, emit: paf\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def columns = blast_columns ? \"${blast_columns}\" : ''\n    switch ( out_ext ) {\n        case \"blast\": outfmt = 0; break\n        case \"xml\": outfmt = 5; break\n        case \"txt\": outfmt = 6; break\n        case \"daa\": outfmt = 100; break\n        case \"sam\": outfmt = 101; break\n        case \"tsv\": outfmt = 102; break\n        case \"paf\": outfmt = 103; break\n        default:\n            outfmt = '6';\n            out_ext = 'txt';\n            log.warn(\"Unknown output file format provided (${out_ext}): selecting DIAMOND default of tabular BLAST output (txt)\");\n            break\n    }\n    \"\"\"\n    DB=`find -L ./ -name \"*.dmnd\" | sed 's/.dmnd//'`\n\n    diamond \\\\\n        blastp \\\\\n        --threads $task.cpus \\\\\n        --db \\$DB \\\\\n        --query $fasta \\\\\n        --outfmt ${outfmt} ${columns} \\\\\n        $args \\\\\n        --out ${prefix}.${out_ext}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        diamond: \\$(diamond --version 2>&1 | tail -n 1 | sed 's/^diamond version //')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/DIAMOND_BLASTX", "nf-core/modules/nf-core__modules/DIAMOND_BLASTP"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["FreeBayes"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess genotyping_freebayes {\n  label 'mc_small'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/genotyping\", mode: params.publish_dir_mode\n\n  when:\n  params.run_genotyping && params.genotyping_tool == 'freebayes'\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_damagemanipulation_for_genotyping_freebayes\n  file fasta from ch_fasta_for_genotyping_freebayes.collect()\n  file fai from ch_fai_for_freebayes.collect()\n  file dict from ch_dict_for_freebayes.collect()\n\n  output: \n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*vcf.gz\") into ch_fb_for_bcftools_stats\n  \n  script:\n  def skip_coverage = \"${params.freebayes_g}\" == 0 ? \"\" : \"-g ${params.freebayes_g}\"\n  \"\"\"\n  freebayes -f ${fasta} -p ${params.freebayes_p} -C ${params.freebayes_C} ${skip_coverage} ${bam} > ${samplename}.freebayes.vcf\n  bgzip -@  ${task.cpus} ${samplename}.freebayes.vcf\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/genotyping_freebayes"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["SAMtools", "yara"], "nb_own": 2, "list_own": ["heuermh", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "nf-core-hlatyping2"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process YARA_MAPPER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::yara=1.0.2 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-f13549097a0d1ca36f9d4f017636fb3609f6c083:d6c969c1e20cc02a9234961c07a24bb0887f05ea-0' :\n        'quay.io/biocontainers/mulled-v2-f13549097a0d1ca36f9d4f017636fb3609f6c083:d6c969c1e20cc02a9234961c07a24bb0887f05ea-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.mapped.bam\"), emit: bam\n    tuple val(meta), path(\"*.mapped.bam.bai\"), emit: bai\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        yara_mapper \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f bam \\\\\n            ${index}/yara \\\\\n            $reads | samtools view -@ $task.cpus -hb -F4 | samtools sort -@ $task.cpus > ${prefix}.mapped.bam\n\n        samtools index -@ $task.cpus ${prefix}.mapped.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            yara: \\$(echo \\$(yara_mapper --version 2>&1) | sed 's/^.*yara_mapper version: //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        yara_mapper \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f bam \\\\\n            ${index}/yara \\\\\n            ${reads[0]} \\\\\n            ${reads[1]} > output.bam\n\n        samtools view -@ $task.cpus -hF 4 -f 0x40 -b output.bam | samtools sort -@ $task.cpus > ${prefix}_1.mapped.bam\n        samtools view -@ $task.cpus -hF 4 -f 0x80 -b output.bam | samtools sort -@ $task.cpus > ${prefix}_2.mapped.bam\n\n        samtools index -@ $task.cpus ${prefix}_1.mapped.bam\n        samtools index -@ $task.cpus ${prefix}_2.mapped.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            yara: \\$(echo \\$(yara_mapper --version 2>&1) | sed 's/^.*yara_mapper version: //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process YARA_MAPPER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::yara=1.0.2 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-f13549097a0d1ca36f9d4f017636fb3609f6c083:f794a548b8692f29264c8984ff116c2141b90d9e-0' :\n        'quay.io/biocontainers/mulled-v2-f13549097a0d1ca36f9d4f017636fb3609f6c083:f794a548b8692f29264c8984ff116c2141b90d9e-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.mapped.bam\"), emit: bam\n    tuple val(meta), path(\"*.mapped.bam.bai\"), emit: bai\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        yara_mapper \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f bam \\\\\n            ${index}/yara \\\\\n            $reads | samtools view -@ $task.cpus -hb -F4 | samtools sort -@ $task.cpus > ${prefix}.mapped.bam\n\n        samtools index -@ $task.cpus ${prefix}.mapped.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            yara: \\$(echo \\$(yara_mapper --version 2>&1) | sed 's/^.*yara_mapper version: //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        yara_mapper \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f bam \\\\\n            ${index}/yara \\\\\n            ${reads[0]} \\\\\n            ${reads[1]} > output.bam\n\n        samtools view -@ $task.cpus -hF 4 -f 0x40 -b output.bam | samtools sort -@ $task.cpus > ${prefix}_1.mapped.bam\n        samtools view -@ $task.cpus -hF 4 -f 0x80 -b output.bam | samtools sort -@ $task.cpus > ${prefix}_2.mapped.bam\n\n        samtools index -@ $task.cpus ${prefix}_1.mapped.bam\n        samtools index -@ $task.cpus ${prefix}_2.mapped.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            yara: \\$(echo \\$(yara_mapper --version 2>&1) | sed 's/^.*yara_mapper version: //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/YARA_MAPPER", "heuermh/nf-core-hlatyping2/heuermh__nf-core-hlatyping2/YARA_MAPPER"], "list_wf_names": ["heuermh/nf-core-hlatyping2", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["Graphmap2", "ALIGN"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["nanoseq"], "list_contrib": ["lwratten", "alneberg", "nf-core-bot", "ewels", "csawye01", "maxulysse", "KevinMenden", "cying111", "drpatelh", "yuukiiwa"], "nb_contrib": 10, "codes": ["\nprocess GRAPHMAP2_INDEX {\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda     (params.enable_conda ? \"bioconda::graphmap=0.6.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/graphmap:0.6.3--he513fc3_0\"\n    } else {\n        container \"quay.io/biocontainers/graphmap:0.6.3--he513fc3_0\"\n    }\n\n    input:\n    tuple path(fasta), path(sizes), val(gtf), val(bed), val(is_transcripts), val(annotation_str)\n\n    output:\n    tuple path(fasta), path(sizes), path(gtf), val(bed), val(is_transcripts), path(\"*.gmidx\"), val(annotation_str), emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def preset = (params.protocol == 'DNA' || is_transcripts) ? \"\" : \"-x rnaseq\"\n    def junctions = (params.protocol != 'DNA' && !is_transcripts && gtf) ? \"--gtf $gtf\" : \"\"\n    \"\"\"\n    graphmap2 \\\\\n        align \\\\\n        $preset \\\\\n        $junctions \\\\\n        -t $task.cpus \\\\\n        -I \\\\\n        -r $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(graphmap2 align 2>&1) | sed 's/^.*Version: v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/nanoseq/nf-core__nanoseq/GRAPHMAP2_INDEX"], "list_wf_names": ["nf-core/nanoseq"]}, {"nb_reuse": 9, "tools": ["seqtk", "subSeq"], "nb_own": 4, "list_own": ["jianhong", "ksumngs", "nf-core", "CDCgov"], "nb_wf": 4, "list_wf": ["modules", "shotgun", "v-met", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "leebrian", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process SEQTK_SEQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(fastx)\n\n    output:\n    tuple val(meta), path(\"*.gz\")     , emit: fastx\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def extension = \"fastq\"\n    if (\"$fastx\" ==~ /.+\\.fasta|.+\\.fasta.gz|.+\\.fa|.+\\.fa.gz|.+\\.fas|.+\\.fas.gz|.+\\.fna|.+\\.fna.gz/ || \"$args\" ==~ /\\-[aA]/ ) {\n        extension = \"fasta\"\n    }\n    \"\"\"\n    seqtk \\\\\n        seq \\\\\n        $args \\\\\n        $fastx | \\\\\n        gzip -c > ${prefix}.seqtk-seq.${extension}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SEQTK_SUBSEQ {\n    tag '$sequences'\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    path sequences\n    path filter_list\n\n    output:\n    path \"*.gz\"         , emit: sequences\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: ''\n    def ext = \"fa\"\n    if (\"$sequences\" ==~ /.+\\.fq|.+\\.fq.gz|.+\\.fastq|.+\\.fastq.gz/) {\n        ext = \"fq\"\n    }\n    \"\"\"\n    seqtk \\\\\n        subseq \\\\\n        $args \\\\\n        $sequences \\\\\n        $filter_list | \\\\\n        gzip --no-name > ${sequences}${prefix}.${ext}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SEQTK_SAMPLE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val sample_size\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        seqtk \\\\\n            sample \\\\\n            $args \\\\\n            $reads \\\\\n            $sample_size \\\\\n            | gzip --no-name > ${prefix}.fastq.gz \\\\\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        if (!(args ==~ /.*-s[0-9]+.*/)) {\n            args += \" -s100\"\n        }\n        \"\"\"\n        seqtk \\\\\n            sample \\\\\n            $args \\\\\n            ${reads[0]} \\\\\n            $sample_size \\\\\n            | gzip --no-name > ${prefix}_1.fastq.gz \\\\\n\n        seqtk \\\\\n            sample \\\\\n            $args \\\\\n            ${reads[1]} \\\\\n            $sample_size \\\\\n            | gzip --no-name > ${prefix}_2.fastq.gz \\\\\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SEQTK_RENAME {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(sequences)\n\n    output:\n    tuple val(meta), path(\"*.gz\")     , emit: sequences\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def extension = \"fasta\"\n    if (\"$sequences\" ==~ /.+\\.fq|.+\\.fq.gz|.+\\.fastq|.+\\.fastq.gz/) {\n        extension = \"fastq\"\n    }\n    \"\"\"\n    seqtk \\\\\n        rename \\\\\n        $args \\\\\n        $sequences \\\\\n        $prefix | \\\\\n        gzip -c --no-name > ${prefix}.renamed.${extension}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SEQTK_RENAME {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(sequences)\n\n    output:\n    tuple val(meta), path(\"*.gz\")     , emit: sequences\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def extension = \"fasta\"\n    if (\"$sequences\" ==~ /.+\\.fq|.+\\.fq.gz|.+\\.fastq|.+\\.fastq.gz/) {\n        extension = \"fastq\"\n    }\n    \"\"\"\n    seqtk \\\\\n        rename \\\\\n        $args \\\\\n        $sequences \\\\\n        $prefix | \\\\\n        gzip -c --no-name > ${prefix}.renamed.${extension}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SEQTK_SEQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(fastx)\n\n    output:\n    tuple val(meta), path(\"*.gz\")     , emit: fastx\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def extension = \"fastq\"\n    if (\"$fastx\" ==~ /.+\\.fasta|.+\\.fasta.gz|.+\\.fa|.+\\.fa.gz|.+\\.fas|.+\\.fas.gz|.+\\.fna|.+\\.fna.gz/ || \"$args\" ==~ /\\-[aA]/ ) {\n        extension = \"fasta\"\n    }\n    \"\"\"\n    seqtk \\\\\n        seq \\\\\n        $args \\\\\n        $fastx | \\\\\n        gzip -c > ${prefix}.seqtk-seq.${extension}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SEQTK_SUBSEQ {\n    tag '$sequences'\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(sequences), path(filter_list)\n\n    output:\n    tuple val(meta), path(\"*.gz\")         , emit: sequences\n    path \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: ''\n    def ext = \"fa\"\n    if (\"$sequences\" ==~ /.+\\.fq|.+\\.fq.gz|.+\\.fastq|.+\\.fastq.gz/) {\n        ext = \"fq\"\n    }\n    \"\"\"\n    seqtk \\\\\n        subseq \\\\\n        $args \\\\\n        $sequences \\\\\n        $filter_list | \\\\\n        gzip --no-name > ${sequences}${prefix}.${ext}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SEQTK_SAMPLE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val sample_size\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        seqtk \\\\\n            sample \\\\\n            $args \\\\\n            $reads \\\\\n            $sample_size \\\\\n            | gzip --no-name > ${prefix}.fastq.gz \\\\\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        if (!(args ==~ /.*-s[0-9]+.*/)) {\n            args += \" -s100\"\n        }\n        \"\"\"\n        seqtk \\\\\n            sample \\\\\n            $args \\\\\n            ${reads[0]} \\\\\n            $sample_size \\\\\n            | gzip --no-name > ${prefix}_1.fastq.gz \\\\\n\n        seqtk \\\\\n            sample \\\\\n            $args \\\\\n            ${reads[1]} \\\\\n            $sample_size \\\\\n            | gzip --no-name > ${prefix}_2.fastq.gz \\\\\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SEQTK_SEQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(fastx)\n\n    output:\n    tuple val(meta), path(\"*.gz\")     , emit: fastx\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def extension = \"fastq\"\n    if (\"$fastx\" ==~ /.+\\.fasta|.+\\.fasta.gz|.+\\.fa|.+\\.fa.gz|.+\\.fas|.+\\.fas.gz|.+\\.fna|.+\\.fna.gz/ || \"$args\" ==~ /\\-[aA]/ ) {\n        extension = \"fasta\"\n    }\n    \"\"\"\n    seqtk \\\\\n        seq \\\\\n        $args \\\\\n        $fastx | \\\\\n        gzip -c > ${prefix}.seqtk-seq.${extension}.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SEQTK_SEQ", "nf-core/modules/nf-core__modules/SEQTK_SUBSEQ", "nf-core/modules/nf-core__modules/SEQTK_SAMPLE", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SEQTK_RENAME", "nf-core/modules/nf-core__modules/SEQTK_RENAME", "ksumngs/v-met/ksumngs__v-met/SEQTK_SEQ", "jianhong/shotgun/jianhong__shotgun/SEQTK_SUBSEQ", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SEQTK_SAMPLE", "nf-core/modules/nf-core__modules/SEQTK_SEQ"], "list_wf_names": ["nf-core/modules", "jianhong/shotgun", "ksumngs/v-met", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 2, "tools": ["Bismark", "FastQC", "MultiQC", "QualiMap", "Cutadapt", "SAMtools", "Picard", "preseq", "BWA", "HISAT2"], "nb_own": 2, "list_own": ["FAANG", "nf-core"], "nb_wf": 2, "list_wf": ["methylseq", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "drpatelh", "Jani-94"], "nb_contrib": 19, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml_for_multiqc\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo \"$workflow.manifest.version\" &> v_pipeline.txt\n    echo \"$workflow.nextflow.version\" &> v_nextflow.txt\n    bismark_genome_preparation --version &> v_bismark_genome_preparation.txt\n    fastqc --version &> v_fastqc.txt\n    cutadapt --version &> v_cutadapt.txt\n    trim_galore --version &> v_trim_galore.txt\n    bismark --version &> v_bismark.txt\n    deduplicate_bismark --version &> v_deduplicate_bismark.txt\n    bismark_methylation_extractor --version &> v_bismark_methylation_extractor.txt\n    bismark2report --version &> v_bismark2report.txt\n    bismark2summary --version &> v_bismark2summary.txt\n    samtools --version &> v_samtools.txt\n    hisat2 --version &> v_hisat2.txt\n    bwa &> v_bwa.txt 2>&1 || true\n    bwameth.py --version &> v_bwameth.txt\n    picard MarkDuplicates --version &> v_picard_markdups.txt 2>&1 || true\n    MethylDackel --version &> v_methyldackel.txt\n    qualimap --version &> v_qualimap.txt || true\n    preseq &> v_preseq.txt\n    multiqc --version &> v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml_for_multiqc\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo \"$workflow.manifest.version\" &> v_pipeline.txt\n    echo \"$workflow.nextflow.version\" &> v_nextflow.txt\n    bismark_genome_preparation --version &> v_bismark_genome_preparation.txt\n    fastqc --version &> v_fastqc.txt\n    cutadapt --version &> v_cutadapt.txt\n    trim_galore --version &> v_trim_galore.txt\n    bismark --version &> v_bismark.txt\n    deduplicate_bismark --version &> v_deduplicate_bismark.txt\n    bismark_methylation_extractor --version &> v_bismark_methylation_extractor.txt\n    bismark2report --version &> v_bismark2report.txt\n    bismark2summary --version &> v_bismark2summary.txt\n    samtools --version &> v_samtools.txt\n    hisat2 --version &> v_hisat2.txt\n    bwa &> v_bwa.txt 2>&1 || true\n    bwameth.py --version &> v_bwameth.txt\n    picard MarkDuplicates --version &> v_picard_markdups.txt 2>&1 || true\n    MethylDackel --version &> v_methyldackel.txt\n    qualimap --version &> v_qualimap.txt || true\n    preseq &> v_preseq.txt\n    multiqc --version &> v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["FAANG/GSM-pipeline/FAANG__GSM-pipeline/get_software_versions", "nf-core/methylseq/nf-core__methylseq/get_software_versions"], "list_wf_names": ["nf-core/methylseq", "FAANG/GSM-pipeline"]}, {"nb_reuse": 18, "tools": ["FreeBayes"], "nb_own": 12, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "melnel000", "cgpu", "UCL-BLIC", "lifebit-ai", "GMS6804-master"], "nb_wf": 18, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "sarek-mirror", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "Sarek_CBIO", "germline_somatic", "Sarek", "custom_sarek", "pgp-chronek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["FriederikeHanssen", "alneberg", "ewels", "arontommi", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "pallolason", "szilvajuhos", "nf-core-bot", "Sebastian-D", "jfnavarro", "pditommaso", "jackmo375", "olgabot", "marcelm", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "J35P312", "ffmmulder", "rmoran7", "jongtaek-kim", "lescai", "apeltzer", "cgpu", "waffle-iron", "jtk622", "davidmasp"], "nb_contrib": 32, "codes": ["\nprocess RunFreeBayes {\n  tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFFB\n    file(genomeFile) from Channel.value(referenceMap.genomeFile)\n    file(genomeIndex) from Channel.value(referenceMap.genomeIndex)\n\n  output:\n    set val(\"freebayes\"), idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into freebayesOutput\n\n  when: 'freebayes' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n  freebayes \\\n    -f ${genomeFile} \\\n    --pooled-continuous \\\n    --pooled-discrete \\\n    --genotype-qualities \\\n    --report-genotype-likelihood-max \\\n    --allele-balance-priors-off \\\n    --min-alternate-fraction 0.03 \\\n    --min-repeat-entropy 1 \\\n    --min-alternate-count 2 \\\n    -t ${intervalBed} \\\n    ${bamTumor} \\\n    ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n  \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess RunFreeBayes {\n  tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFFB\n    file(genomeFile) from Channel.value(referenceMap.genomeFile)\n    file(genomeIndex) from Channel.value(referenceMap.genomeIndex)\n\n  output:\n    set val(\"freebayes\"), idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into freebayesOutput\n\n  when: 'freebayes' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n  freebayes \\\n    -f ${genomeFile} \\\n    --pooled-continuous \\\n    --pooled-discrete \\\n    --genotype-qualities \\\n    --report-genotype-likelihood-max \\\n    --allele-balance-priors-off \\\n    --min-alternate-fraction 0.03 \\\n    --min-repeat-entropy 1 \\\n    --min-alternate-count 2 \\\n    -t ${intervalBed} \\\n    ${bamTumor} \\\n    ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n  \"\"\"\n}", "\nprocess FreeBayes {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess RunFreeBayes {\n  tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFFB\n    file(genomeFile) from Channel.value(referenceMap.genomeFile)\n    file(genomeIndex) from Channel.value(referenceMap.genomeIndex)\n\n  output:\n    set val(\"FreeBayes\"), idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into freebayesOutput\n\n  when: 'freebayes' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n  freebayes \\\n    -f ${genomeFile} \\\n    --pooled-continuous \\\n    --pooled-discrete \\\n    --genotype-qualities \\\n    --report-genotype-likelihood-max \\\n    --allele-balance-priors-off \\\n    --min-alternate-fraction 0.03 \\\n    --min-repeat-entropy 1 \\\n    --min-alternate-count 2 \\\n    -t ${intervalBed} \\\n    ${bamTumor} \\\n    ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n  \"\"\"\n}", "\nprocess FreeBayes {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        -t ${intervalBed} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/melnel000__Sarek_CBIO/RunFreeBayes", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/FreeBayes", "rmoran7/dx_sarek/rmoran7__dx_sarek/FreeBayes", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/FreeBayes", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/FreeBayes", "cgpu/haplosarek/cgpu__haplosarek/FreeBayes", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/FreeBayes", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/FreeBayes", "GMS6804-master/Sarek/GMS6804-master__Sarek/RunFreeBayes", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/FreeBayes", "cgpu/sarek-mirror/cgpu__sarek-mirror/FreeBayes", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/FreeBayes", "nf-core/sarek/nf-core__sarek/FreeBayes", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/RunFreeBayes", "cgpu/pgp-chronek/cgpu__pgp-chronek/FreeBayes", "rmoran7/custom_sarek/rmoran7__custom_sarek/FreeBayes", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/FreeBayes", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/FreeBayes"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "cgpu/pgp-chronek", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "UCL-BLIC/Sarek_v2.3.FIX1", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-mirror", "GMS6804-master/Sarek", "cgpu/sarek-mirror-cache", "cgpu/sarek-genomechronicler", "melnel000/Sarek_CBIO", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess BUSCO {\n    tag \"${bin}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> filename.indexOf(\"busco_downloads\") == -1 ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[]) : null }\n\n    conda (params.enable_conda ? \"bioconda::busco=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/busco:5.1.0--py_1\"\n    } else {\n        container \"quay.io/biocontainers/busco:5.1.0--py_1\"\n    }\n\n    input:\n    tuple val(meta), path(bin)\n    path(db)\n    path(download_folder)\n\n    output:\n    tuple val(meta), path(\"short_summary.domain.*.${bin}.txt\")          , optional:true , emit: summary_domain\n    tuple val(meta), path(\"short_summary.specific_lineage.*.${bin}.txt\"), optional:true , emit: summary_specific\n    tuple env(most_spec_db), path('busco_downloads/')                   , optional:true , emit: busco_downloads\n    path(\"${bin}_busco.log\")\n    path(\"${bin}_busco.err\")\n    path(\"${bin}_buscos.*.faa.gz\")                                      , optional:true\n    path(\"${bin}_buscos.*.fna.gz\")                                      , optional:true\n    path(\"${bin}_prodigal.gff\")                                         , optional:true , emit: prodigal_genes\n    tuple val(meta), path(\"${bin}_busco.failed_bin.txt\")                , optional:true , emit: failed_bin\n    path '*.version.txt'                                                                , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def cp_augustus_config = \"Y\"\n    if( workflow.profile.toString().indexOf(\"conda\") != -1)\n        cp_augustus_config = \"N\"\n\n    def lineage_dataset_provided = \"N\"\n    if (params.busco_reference)\n        lineage_dataset_provided = \"Y\"\n\n    def p = \"--auto-lineage\"\n    if (params.busco_reference){\n        p = \"--lineage_dataset dataset/${db}\"\n    } else {\n        if (params.busco_auto_lineage_prok)\n            p = \"--auto-lineage-prok\"\n        if (params.busco_download_path)\n            p += \" --offline --download_path ${download_folder}\"\n    }\n    \"\"\"\n    # ensure augustus has write access to config directory\n    if [ ${cp_augustus_config} = \"Y\" ] ; then\n        cp -r /usr/local/config/ augustus_config/\n        export AUGUSTUS_CONFIG_PATH=augustus_config\n    fi\n\n    # place db in extra folder to ensure BUSCO recognizes it as path (instead of downloading it)\n    if [ ${lineage_dataset_provided} = \"Y\" ] ; then\n        mkdir dataset\n        mv ${db} dataset/\n    fi\n\n    # set nullgob: if pattern matches no files, expand to a null string rather than to itself\n    shopt -s nullglob\n\n    # only used for saving busco downloads\n    most_spec_db=\"NA\"\n\n    if busco ${p} \\\n        --mode genome \\\n        --in ${bin} \\\n        --cpu \"${task.cpus}\" \\\n        --out \"BUSCO\" > ${bin}_busco.log 2> ${bin}_busco.err; then\n\n        # get name of used specific lineage dataset\n        summaries=(BUSCO/short_summary.specific.*.BUSCO.txt)\n        if [ \\${#summaries[@]} -ne 1 ]; then\n            echo \"ERROR: none or multiple 'BUSCO/short_summary.specific.*.BUSCO.txt' files found. Expected one.\"\n            exit 1\n        fi\n        [[ \\$summaries =~ BUSCO/short_summary.specific.(.*).BUSCO.txt ]];\n        db_name_spec=\"\\${BASH_REMATCH[1]}\"\n        most_spec_db=\\${db_name_spec}\n        echo \"Used specific lineage dataset: \\${db_name_spec}\"\n\n        if [ ${lineage_dataset_provided} = \"Y\" ]; then\n            cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n\n            # if lineage dataset is provided, BUSCO analysis does not fail in case no genes can be found as when using the auto selection setting\n            # report bin as failed to allow consistent warnings within the pipeline for both settings\n            if egrep -q \\$'WARNING:\\tBUSCO did not find any match.' ${bin}_busco.log ; then\n                echo \"WARNING: BUSCO could not find any genes for the provided lineage dataset! See also ${bin}_busco.log.\"\n                echo -e \"${bin}\\tNo genes\" > \"${bin}_busco.failed_bin.txt\"\n            fi\n        else\n            # auto lineage selection\n            if { egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tLineage \\\\S+ is selected, supported by ' ${bin}_busco.log ; } || \\\n                { egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tThe results from the Prodigal gene predictor indicate that your data belongs to the mollicutes clade. Testing subclades...' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tUsing local lineages directory ' ${bin}_busco.log ; }; then\n                # the second statement is necessary, because certain mollicute clades use a different genetic code, are not part of the BUSCO placement tree, are tested separately\n                # and cause different log messages\n                echo \"Domain and specific lineage could be selected by BUSCO.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n\n                db_name_gen=\"\"\n                summaries_gen=(BUSCO/short_summary.generic.*.BUSCO.txt)\n                if [ \\${#summaries_gen[@]} -lt 1 ]; then\n                    echo \"No 'BUSCO/short_summary.generic.*.BUSCO.txt' file found. Assuming selected domain and specific lineages are the same.\"\n                    cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.domain.\\${db_name_spec}.${bin}.txt\n                    db_name_gen=\\${db_name_spec}\n                else\n                    [[ \\$summaries_gen =~ BUSCO/short_summary.generic.(.*).BUSCO.txt ]];\n                    db_name_gen=\"\\${BASH_REMATCH[1]}\"\n                    echo \"Used generic lineage dataset: \\${db_name_gen}\"\n                    cp BUSCO/short_summary.generic.\\${db_name_gen}.BUSCO.txt short_summary.domain.\\${db_name_gen}.${bin}.txt\n                fi\n\n                for f in BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa; do\n                    cat BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_gen}.faa.gz\n                    break\n                done\n                for f in BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna; do\n                    cat BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_gen}.fna.gz\n                    break\n                done\n\n            elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'INFO:\\tNot enough markers were placed on the tree \\\\([0-9]*\\\\). Root lineage \\\\S+ is kept' ${bin}_busco.log ; then\n                echo \"Domain could be selected by BUSCO, but no more specific lineage.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.domain.\\${db_name_spec}.${bin}.txt\n\n            elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'INFO:\\tRunning virus detection pipeline' ${bin}_busco.log ; then\n                # TODO double-check if selected dataset is not one of bacteria_*, archaea_*, eukaryota_*?\n                echo \"Domain could not be selected by BUSCO, but virus dataset was selected.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n            else\n                echo \"ERROR: Some not expected case occurred! See ${bin}_busco.log.\" >&2\n                exit 1\n            fi\n        fi\n\n        for f in BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*faa; do\n            cat BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_spec}.faa.gz\n            break\n        done\n        for f in BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*fna; do\n            cat BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_spec}.fna.gz\n            break\n        done\n\n    elif egrep -q \\$'ERROR:\\tNo genes were recognized by BUSCO' ${bin}_busco.err ; then\n        echo \"WARNING: BUSCO analysis failed due to no recognized genes! See also ${bin}_busco.err.\"\n        echo -e \"${bin}\\tNo genes\" > \"${bin}_busco.failed_bin.txt\"\n\n    elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'ERROR:\\tPlacements failed' ${bin}_busco.err ; then\n        echo \"WARNING: BUSCO analysis failed due to failed placements! See also ${bin}_busco.err. Still using results for selected generic lineage dataset.\"\n        echo -e \"${bin}\\tPlacements failed\" > \"${bin}_busco.failed_bin.txt\"\n\n        message=\\$(egrep \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log)\n        [[ \\$message =~ INFO:[[:space:]]([_[:alnum:]]+)[[:space:]]selected ]];\n        db_name_gen=\"\\${BASH_REMATCH[1]}\"\n        most_spec_db=\\${db_name_gen}\n        echo \"Used generic lineage dataset: \\${db_name_gen}\"\n        cp BUSCO/auto_lineage/run_\\${db_name_gen}/short_summary.txt short_summary.domain.\\${db_name_gen}.${bin}.txt\n\n        for f in BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa; do\n            cat BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_gen}.faa.gz\n            break\n        done\n        for f in BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna; do\n            cat BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_gen}.fna.gz\n            break\n        done\n\n    else\n        echo \"ERROR: BUSCO analysis failed for some unknown reason! See also ${bin}_busco.err.\" >&2\n        exit 1\n    fi\n\n    # additionally output genes predicted with Prodigal (GFF3)\n    if [ -f BUSCO/logs/prodigal_out.log ]; then\n        mv BUSCO/logs/prodigal_out.log \"${bin}_prodigal.gff\"\n    fi\n\n    busco --version | sed \"s/BUSCO //\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/BUSCO"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 49, "tools": ["MultiQC"], "nb_own": 28, "list_own": ["heuermh", "CDCgov", "andriesfeder", "ggabernet", "harleenduggal", "sguizard", "ABMicroBioinf", "erikrikarddaniel", "vincenthhu", "xiaoli-dong", "LNUc-EEMiS", "chelauk", "ksumngs", "nf-core", "mashehu", "jianhong", "bigbio", "sanger-tol", "raygozag", "gwright99", "c3g", "marchoeppner", "laclac102", "priyanka-surana", "asthara10", "MrMarkW", "csf-ngs", "drpatelh"], "nb_wf": 44, "list_wf": ["pathogen", "readmapping", "isoseq", "genflow-dnaseq", "gwas", "metatdenovo", "nf-core-trimmomatic", "rnaseq", "nanostring", "nf-core-fetchdata", "RNASEQ", "nf-core-umipreprocessing", "epitopeprediction", "funcscan", "nf-core-mutectplatypus", "nf-core-dragen", "modules", "nfcore-rnaseq", "taxprofiler", "controldna", "hic", "rnavar", "v-met", "raredisease", "mycosnp-nf", "magmap", "shotgun", "nf-core-hlatyping2", "esga2", "ampliseq", "nf-core-readtwoalign", "quantms", "16S_pipeline", "cladebreaker", "nf-core_test", "insertseq", "elixir", "magph", "airrflow", "nf-core-hicar", "nf-core-blasr", "nf-core-westest", "spatialtranscriptomics", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "tbugfinder", "drejom", "SpikyClip", "jordwil", "FelixKrueger", "xingaulaglag", "kmurat1", "chuan-wang", "yuxuth", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "jpfeuffer", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "christopher-mohr", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "priyanka-surana", "piotr-faba-ardigen", "rannick", "silviamorins", "d4straub", "aanil", "Midnighter", "SPPearce", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "mciprianoCDC", "marissaDubbelaar", "santiagorevale", "ppericard", "idot", "kevbrick", "mvanins", "nebfield", "Alexey-ebi", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "PhilPalmer", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "MrMarkW", "fullama", "kaurravneet4123", "amayer21", "DiegoBrambilla", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "asafpr", "adomingues", "cjjossart", "jonasscheid", "pcantalupo", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "peterwharrison", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "asthara10", "jtangrot", "spficklin", "subwaystation", "jasmezz", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "ypriverol", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "lkuchenb", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "leebrian", "vincenthhu", "lassefolkersen", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "chelauk", "projectoriented", "praveenraj2018", "tamuanand", "sdomanskyi", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "m3hdad", "SusiJo", "maxibor", "olgabot", "daichengxin", "paulklemm"], "nb_contrib": 175, "codes": ["process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch multiqc_data\n    touch multiqc_plots\n    touch multiqc_report.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch multiqc_data\n    touch multiqc_plots\n    touch multiqc_report.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    path ('rseqc/tin/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc \\\\\n        -f \\\\\n        $args \\\\\n        $custom_config \\\\\n        .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    path ('rseqc/tin/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc \\\\\n        -f \\\\\n        $args \\\\\n        $custom_config \\\\\n        .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    path ('rseqc/tin/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc \\\\\n        -f \\\\\n        $args \\\\\n        $custom_config \\\\\n        .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n    file report\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch multiqc_data\n    touch multiqc_plots\n    touch multiqc_report.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch multiqc_data\n    touch multiqc_plots\n    touch multiqc_report.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    path ('rseqc/tin/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc \\\\\n        -f \\\\\n        $args \\\\\n        $custom_config \\\\\n        .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    multiqc -f $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch multiqc_data\n    touch multiqc_plots\n    touch multiqc_report.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch multiqc_data\n    touch multiqc_plots\n    touch multiqc_report.html\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: version\n\n    script:\n    \"\"\"\n    multiqc -f  $options.args .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n    stub:\n    \"\"\"\n    touch multiqc_report.html\n    touch test_data\n    touch test_plots\n\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/MULTIQC", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/MULTIQC", "marchoeppner/esga2/marchoeppner__esga2/MULTIQC", "nf-core/ssds/nf-core__ssds/MULTIQC", "ABMicroBioinf/magph/ABMicroBioinf__magph/MULTIQC", "chelauk/nf-core-mutectplatypus/chelauk__nf-core-mutectplatypus/MULTIQC", "nf-core/taxprofiler/nf-core__taxprofiler/MULTIQC", "chelauk/nf-core-trimmomatic/chelauk__nf-core-trimmomatic/MULTIQC", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/MULTIQC", "gwright99/nf-core-dragen/gwright99__nf-core-dragen/MULTIQC", "drpatelh/nf-core-fetchdata/drpatelh__nf-core-fetchdata/MULTIQC", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/MULTIQC", "nf-core/rnaseq/nf-core__rnaseq/MULTIQC", "sguizard/isoseq/sguizard__isoseq/MULTIQC", "ggabernet/elixir/ggabernet__elixir/MULTIQC", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/MULTIQC", "chelauk/nf-core-umipreprocessing/chelauk__nf-core-umipreprocessing/MULTIQC", "laclac102/ampliseq/laclac102__ampliseq/MULTIQC", "priyanka-surana/hic/priyanka-surana__hic/MULTIQC", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/MULTIQC", "LNUc-EEMiS/magmap/LNUc-EEMiS__magmap/MULTIQC", "heuermh/nf-core-hlatyping2/heuermh__nf-core-hlatyping2/MULTIQC", "csf-ngs/controldna/csf-ngs__controldna/MULTIQC", "jianhong/16S_pipeline/jianhong__16S_pipeline/MULTIQC", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/MULTIQC", "asthara10/insertseq/asthara10__insertseq/MULTIQC", "nf-core/airrflow/nf-core__airrflow/MULTIQC", "LNUc-EEMiS/metatdenovo/LNUc-EEMiS__metatdenovo/MULTIQC", "mashehu/nf-core_test/mashehu__nf-core_test/MULTIQC", "nf-core/ampliseq/nf-core__ampliseq/MULTIQC", "nf-core/rnavar/nf-core__rnavar/MULTIQC", "MrMarkW/nf-core-readtwoalign/MrMarkW__nf-core-readtwoalign/MULTIQC", "bigbio/quantms/bigbio__quantms/MULTIQC", "nf-core/epitopeprediction/nf-core__epitopeprediction/MULTIQC", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/MULTIQC", "c3g/genflow-dnaseq/c3g__genflow-dnaseq/MULTIQC", "nf-core/spatialtranscriptomics/nf-core__spatialtranscriptomics/MULTIQC", "nf-core/funcscan/nf-core__funcscan/MULTIQC", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/MULTIQC", "nf-core/gwas/nf-core__gwas/MULTIQC", "raygozag/rnaseq/raygozag__rnaseq/MULTIQC", "jianhong/shotgun/jianhong__shotgun/MULTIQC", "xiaoli-dong/magph/xiaoli-dong__magph/MULTIQC", "nf-core/nanostring/nf-core__nanostring/MULTIQC", "ksumngs/v-met/ksumngs__v-met/MULTIQC", "sanger-tol/readmapping/sanger-tol__readmapping/MULTIQC", "nf-core/modules/nf-core__modules/MULTIQC", "andriesfeder/cladebreaker/andriesfeder__cladebreaker/MULTIQC", "chelauk/nf-core-blasr/chelauk__nf-core-blasr/MULTIQC"], "list_wf_names": ["nf-core/airrflow", "MrMarkW/nf-core-readtwoalign", "ggabernet/elixir", "heuermh/nf-core-hlatyping2", "harleenduggal/RNASEQ", "sguizard/isoseq", "nf-core/ampliseq", "harleenduggal/nfcore-rnaseq", "ABMicroBioinf/magph", "c3g/genflow-dnaseq", "nf-core/rnavar", "chelauk/nf-core-mutectplatypus", "LNUc-EEMiS/magmap", "csf-ngs/controldna", "ABMicroBioinf/pathogen", "chelauk/nf-core-blasr", "nf-core/raredisease", "nf-core/ssds", "nf-core/epitopeprediction", "vincenthhu/nf-core-westest", "nf-core/modules", "nf-core/nanostring", "nf-core/rnaseq", "asthara10/insertseq", "CDCgov/mycosnp-nf", "sanger-tol/readmapping", "gwright99/nf-core-dragen", "jianhong/nf-core-hicar", "jianhong/shotgun", "jianhong/16S_pipeline", "erikrikarddaniel/magmap", "nf-core/spatialtranscriptomics", "nf-core/gwas", "andriesfeder/cladebreaker", "nf-core/taxprofiler", "marchoeppner/esga2", "mashehu/nf-core_test", "priyanka-surana/hic", "xiaoli-dong/magph", "ksumngs/v-met", "LNUc-EEMiS/metatdenovo", "bigbio/quantms", "laclac102/ampliseq", "nf-core/funcscan", "xiaoli-dong/pathogen", "raygozag/rnaseq", "chelauk/nf-core-trimmomatic", "drpatelh/nf-core-fetchdata", "chelauk/nf-core-umipreprocessing"]}, {"nb_reuse": 14, "tools": ["GATK"], "nb_own": 11, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 13, "list_wf": ["saw.sarek", "sarek_ubec", "layer_lab_caw", "layer_lab_chco", "PGP-UK-sarek", "germline_somatic", "layer_lab_vc", "custom_sarek", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "apeltzer", "cgpu", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'container_llab'\n                     \n    label 'cpus_8'\n                  \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                       \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta) \n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex) \n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.table\")\n                                                          \n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                            \n                                         \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fastaFai\n        file(knownIndels) from ch_knownIndels\n        file(knownIndelsIndex) from ch_knownIndelsIndex\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.knownIndels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.knownIndels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir /tmp \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'container_llab'\n                     \n    label 'cpus_8'\n                  \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                       \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta) \n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex) \n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.table\")\n                                                          \n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                            \n                                         \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n    disk '65 GB'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fastaFai\n        file(knownIndels) from ch_knownIndels\n        file(knownIndelsIndex) from ch_knownIndelsIndex\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.knownIndels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.knownIndels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir /tmp \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_4'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n                     \n    label 'cpus_8'\n                  \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                       \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta) \n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex) \n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.table\")\n                                                          \n\n    when: params.known_indels  && step != 'variantcalling' &&\n        ('haplotypecaller' in tools || \n        'mutect2' in tools ||\n        'mutect2_single' in tools ||\n        'gen_somatic_pon' in tools)\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                            \n                                         \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess GetBaseRecalibrationReport {\n    label 'cpus_1'\n    label 'withGatkContainer'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        tuple val(idPatient), val(idSample), file(bam), file(bai), file(intervalBed)\n        file(dbsnp)\n        file(dbsnpIndex)\n        file(fasta)\n        file(dict)\n        file(fastaFai)\n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${prefix}${idSample}.recal.table\")\n\n    when: (params.skip_baserecalibration == false)\n\n    script:\n    dbsnpOptions = isChannelActive(dbsnp) ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = isChannelActive(knownIndels) ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/BaseRecalibrator", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/BaseRecalibrator", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/BaseRecalibrator", "nf-core/sarek/nf-core__sarek/BaseRecalibrator", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BaseRecalibrator", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/BaseRecalibrator", "rmoran7/custom_sarek/rmoran7__custom_sarek/BaseRecalibrator", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BaseRecalibrator", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/BaseRecalibrator", "rmoran7/dx_sarek/rmoran7__dx_sarek/BaseRecalibrator", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BaseRecalibrator", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/BaseRecalibrator", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/GetBaseRecalibrationReport", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/BaseRecalibrator"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "sripaladugu/germline_somatic", "Genomic-Medicine-Linkoping/nf-core-sarek", "ryanlayerlab/layer_lab_caw", "nf-core/sarek", "chelauk/test_nextflow_sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek", "javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BISCUIT_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::biscuit=1.0.2.20220113 bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-db16f1c237a26ea9245cf9924f858974ff321d6e:17fa66297f088a1bc7560b7b90dc273bf23f2d8c-0':\n        'quay.io/biocontainers/mulled-v2-db16f1c237a26ea9245cf9924f858974ff321d6e:17fa66297f088a1bc7560b7b90dc273bf23f2d8c-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def biscuit_cpus = (int) Math.max(Math.floor(task.cpus*0.9),1)\n    def samtools_cpus = task.cpus-biscuit_cpus\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.bis.amb\" | sed 's/.bis.amb//'`\n\n    biscuit align \\\\\n        $args \\\\\n        -@ $biscuit_cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools sort $args2 --threads $samtools_cpus -o ${prefix}.bam -\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        biscuit: \\$( biscuit version |& sed '1!d; s/^.*BISCUIT Version: //' )\n        samtools: \\$( samtools --version |& sed '1!d; s/^.*samtools //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BISCUIT_ALIGN"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["kraken2"], "nb_own": 2, "list_own": ["cidgoh", "nf-core"], "nb_wf": 2, "list_wf": ["cidgoh_qc", "mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "duanjunhyq", "maxibor", "skrakau", "jfy133", "anwarMZ"], "nb_contrib": 16, "codes": ["\nprocess KRAKEN2 {\n    tag \"${meta.id}-${db_name}\"\n    label 'process_low'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::kraken2=2.1.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/kraken2:2.0.8_beta--pl526hc9558a2_2\"\n    } else {\n        container \"quay.io/biocontainers/kraken2:2.0.8_beta--pl526hc9558a2_2\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(\"database/*\")\n\n    output:\n    tuple val(\"kraken2\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path  \"kraken2_report.txt\"                            , emit: report\n    path  '*.version.yml'                                 , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"\\\"${reads}\\\"\" :  \"--paired \\\"${reads[0]}\\\" \\\"${reads[1]}\\\"\"\n    \"\"\"\n    kraken2 \\\n        --report-zero-counts \\\n        --threads ${task.cpus} \\\n        --db database \\\n        --report kraken2_report.txt \\\n        $input \\\n        > kraken2.kraken\n    cat kraken2.kraken | cut -f 2,3 > results.krona\n    echo \\$(kraken2 --version 2>&1) | sed 's/Kraken version //; s/ Copyright.*//' > ${software}.version.yml\n    \"\"\"\n}", "\nprocess KRAKEN2 {\n    tag \"${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::kraken2=2.0.8_beta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/kraken2:2.0.8_beta--pl526hc9558a2_2\"\n    } else {\n        container \"quay.io/biocontainers/kraken2:2.0.8_beta--pl526hc9558a2_2\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(\"database/*\")\n\n    output:\n    tuple val(\"kraken2\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path  \"kraken2_report.txt\"                            , emit: report\n    path  '*.version.txt'                                 , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"\\\"${reads}\\\"\" :  \"--paired \\\"${reads[0]}\\\" \\\"${reads[1]}\\\"\"\n    \"\"\"\n    kraken2 \\\n        --report-zero-counts \\\n        --threads ${task.cpus} \\\n        --db database \\\n        --report kraken2_report.txt \\\n        $input \\\n        > kraken2.kraken\n    cat kraken2.kraken | cut -f 2,3 > results.krona\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/Kraken version //; s/ Copyright.*//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/KRAKEN2", "nf-core/mag/nf-core__mag/KRAKEN2"], "list_wf_names": ["cidgoh/cidgoh_qc", "nf-core/mag"]}, {"nb_reuse": 4, "tools": ["G-BLASTN"], "nb_own": 3, "list_own": ["ksumngs", "nf-core", "mahesh-panchal"], "nb_wf": 4, "list_wf": ["test_nfcore_workflow_chain", "modules", "v-met", "viralrecon"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 113, "codes": ["process BLAST_BLASTN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::blast=2.12.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/blast:2.12.0--pl5262h3289130_0' :\n        'quay.io/biocontainers/blast:2.12.0--pl5262h3289130_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path  db\n\n    output:\n    tuple val(meta), path('*.blastn.txt'), emit: txt\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    DB=`find -L ./ -name \"*.ndb\" | sed 's/.ndb//'`\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db \\$DB \\\\\n        -query $fasta \\\\\n        $args \\\\\n        -out ${prefix}.blastn.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        blast: \\$(blastn -version 2>&1 | sed 's/^.*blastn: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BLAST_BLASTN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::blast=2.12.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/blast:2.12.0--pl5262h3289130_0' :\n        'quay.io/biocontainers/blast:2.12.0--pl5262h3289130_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path  db\n\n    output:\n    tuple val(meta), path('*.blastn.txt'), emit: txt\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    DB=`find -L ./ -name \"*.ndb\" | sed 's/.ndb//'`\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db \\$DB \\\\\n        -query $fasta \\\\\n        $args \\\\\n        -out ${prefix}.blastn.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        blast: \\$(blastn -version 2>&1 | sed 's/^.*blastn: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BLAST_BLASTN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::blast=2.12.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/blast:2.12.0--pl5262h3289130_0' :\n        'quay.io/biocontainers/blast:2.12.0--pl5262h3289130_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path  db\n\n    output:\n    tuple val(meta), path('*.blastn.txt'), emit: txt\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    DB=`find -L ./ -name \"*.ndb\" | sed 's/.ndb//'`\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db \\$DB \\\\\n        -query $fasta \\\\\n        $args \\\\\n        -out ${prefix}.blastn.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        blast: \\$(blastn -version 2>&1 | sed 's/^.*blastn: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BLAST_BLASTN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::blast=2.12.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/blast:2.12.0--pl5262h3289130_0' :\n        'quay.io/biocontainers/blast:2.12.0--pl5262h3289130_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path  db\n\n    output:\n    tuple val(meta), path('*.blastn.txt'), emit: txt\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    DB=`find -L ./ -name \"*.ndb\" | sed 's/.ndb//'`\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db \\$DB \\\\\n        -query $fasta \\\\\n        $args \\\\\n        -out ${prefix}.blastn.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        blast: \\$(blastn -version 2>&1 | sed 's/^.*blastn: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BLAST_BLASTN", "nf-core/modules/nf-core__modules/BLAST_BLASTN", "ksumngs/v-met/ksumngs__v-met/BLAST_BLASTN", "nf-core/viralrecon/nf-core__viralrecon/BLAST_BLASTN"], "list_wf_names": ["nf-core/viralrecon", "ksumngs/v-met", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess library_merge {\n  label 'sc_tiny'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/merged_bams/initial\", mode: params.publish_dir_mode\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_fixedinput_for_librarymerging.dump(tag: \"library_merge_input\")\n\n  output:\n  tuple samplename, val(\"${samplename}_libmerged\"), lane, seqtype, organism, strandedness, udg, path(\"*_libmerged_rmdup.bam\"), path(\"*_libmerged_rmdup.bam.{bai,csi}\") into ch_output_from_librarymerging\n\n  script:\n  def size = params.large_ref ? '-c' : ''\n  \"\"\"\n  samtools merge ${samplename}_udg${udg}_libmerged_rmdup.bam ${bam}\n  samtools index ${samplename}_udg${udg}_libmerged_rmdup.bam ${size}\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/library_merge"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["shovill"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process SHOVILL {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::shovill=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/shovill:1.1.0--0' :\n        'quay.io/biocontainers/shovill:1.1.0--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"contigs.fa\")                         , emit: contigs\n    tuple val(meta), path(\"shovill.corrections\")                , emit: corrections\n    tuple val(meta), path(\"shovill.log\")                        , emit: log\n    tuple val(meta), path(\"{skesa,spades,megahit,velvet}.fasta\"), emit: raw_contigs\n    tuple val(meta), path(\"contigs.{fastg,gfa,LastGraph}\")      , optional:true, emit: gfa\n    path \"versions.yml\"                                         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def memory = task.memory.toGiga()\n    \"\"\"\n    shovill \\\\\n        --R1 ${reads[0]} \\\\\n        --R2 ${reads[1]} \\\\\n        $args \\\\\n        --cpus $task.cpus \\\\\n        --ram $memory \\\\\n        --outdir ./ \\\\\n        --force\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        shovill: \\$(echo \\$(shovill --version 2>&1) | sed 's/^.*shovill //')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SHOVILL"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["ANGSD"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess genotyping_angsd {\n  label 'mc_small'\n  tag \"${samplename}\"\n  publishDir \"${params.outdir}/genotyping\", mode: params.publish_dir_mode\n\n  when:\n  params.run_genotyping && params.genotyping_tool == 'angsd'\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_damagemanipulation_for_genotyping_angsd\n  file fasta from ch_fasta_for_genotyping_angsd.collect()\n  file fai from ch_fai_for_angsd.collect()\n  file dict from ch_dict_for_angsd.collect()\n\n  output: \n  path(\"${samplename}*\")\n  \n  script:\n  switch ( \"${params.angsd_glmodel}\" ) {\n    case \"samtools\":\n    angsd_glmodel = \"1\"; break\n    case \"gatk\":\n    angsd_glmodel = \"2\"; break\n    case \"soapsnp\":\n    angsd_glmodel = \"3\"; break\n    case \"syk\":\n    angsd_glmodel = \"4\"; break\n  }\n\n  switch ( \"${params.angsd_glformat}\" ) {\n    case \"text\":\n    angsd_glformat = \"4\"; break\n    case \"binary\":\n    angsd_glformat = \"1\"; break\n    case \"beagle\":\n    angsd_glformat = \"2\"; break\n    case \"binary_three\":\n    angsd_glformat = \"3\"; break\n  }\n  \n  def angsd_fasta = !params.angsd_createfasta ? '' : params.angsd_fastamethod == 'random' ? '-doFasta 1 -doCounts 1' : '-doFasta 2 -doCounts 1' \n  def angsd_majorminor = params.angsd_glformat != \"beagle\" ? '' : '-doMajorMinor 1'\n  \"\"\"\n  echo ${bam} > bam.filelist\n  mkdir angsd\n  angsd -bam bam.filelist -nThreads ${task.cpus} -GL ${angsd_glmodel} -doGlF ${angsd_glformat} ${angsd_majorminor} ${angsd_fasta} -out ${samplename}.angsd\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/genotyping_angsd"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 20, "tools": ["SAMtools"], "nb_own": 13, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "vladsaveliev", "sripaladugu", "sickle-in-africa", "nf-core", "melnel000", "cgpu", "UCL-BLIC", "lifebit-ai", "GMS6804-master"], "nb_wf": 19, "list_wf": ["sarek_ubec", "Sarek_CBIO", "germline_somatic", "dx_sarek", "haplosarek", "sarek-mirror-cache", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "Sarek", "sarek-mirror", "pgp-chronek", "test_nextflow_sarek", "sarek-genomechronicler", "saw.sarek", "custom_sarek", "sarek", "GenomeChronicler-Sarek-nf", "cawdor", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "pallolason", "szilvajuhos", "nf-core-bot", "Sebastian-D", "jfnavarro", "pditommaso", "jackmo375", "olgabot", "chelauk", "marcelm", "adrlar", "lconde-ucl", "malinlarsson", "J35P312", "ffmmulder", "rmoran7", "jongtaek-kim", "vladsaveliev", "lescai", "cgpu", "apeltzer", "waffle-iron", "jtk622", "davidmasp"], "nb_contrib": 33, "codes": ["\nprocess MergeBams {\n  tag {idPatient + \"-\" + idSample}\n\n  input:\n  set idPatient, status, idSample, idLane, file(bam) from groupedBam\n\n  output:\n  set idPatient, status, idSample, file(\"${idSample}.bam\") into mergedBam\n\n  when: !params.onlyQC\n\n  script:\n  \"\"\"\n  samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n  \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    when: step == 'mapping'\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    when: step == 'mapping'\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBams {\n  tag {idPatient + \"-\" + idSample}\n\n  input:\n    set idPatient, status, idSample, idRun, file(bam) from groupedBam\n\n  output:\n    set idPatient, status, idSample, file(\"${idSample}.bam\") into mergedBam\n\n  when: step == 'mapping' && !params.onlyQC\n\n  script:\n  \"\"\"\n  samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n  \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeReadGroupsForSample {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), file(bam)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.bam\")\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_4'\n    disk '120 GB'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    when: step == 'mapping'\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBams {\n  tag {idPatient + \"-\" + idSample}\n\n  input:\n    set idPatient, status, idSample, idRun, file(bam) from groupedBam\n\n  output:\n    set idPatient, status, idSample, file(\"${idSample}.bam\") into mergedBam\n\n  when: step == 'mapping' && !params.onlyQC\n\n  script:\n  \"\"\"\n  samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n  \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    when: step == 'mapping'\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBams {\n  tag {idPatient + \"-\" + idSample}\n\n  input:\n    set idPatient, status, idSample, idRun, file(bam) from groupedBam\n\n  output:\n    set idPatient, status, idSample, file(\"${idSample}.bam\") into mergedBam\n\n  when: step == 'mapping' && !params.onlyQC\n\n  script:\n  \"\"\"\n  samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n  \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into mergedBam\n\n    when: step == 'mapping'\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}"], "list_proc": ["vladsaveliev/cawdor/vladsaveliev__cawdor/MergeBams", "cgpu/sarek-mirror/cgpu__sarek-mirror/MergeBamMapped", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/MergeBamMapped", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/MergeBamMapped", "cgpu/pgp-chronek/cgpu__pgp-chronek/MergeBamMapped", "melnel000/Sarek_CBIO/melnel000__Sarek_CBIO/MergeBams", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/MergeBamMapped", "nf-core/sarek/nf-core__sarek/MergeBamMapped", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergeReadGroupsForSample", "rmoran7/custom_sarek/rmoran7__custom_sarek/MergeBamMapped", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/MergeBamMapped", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/MergeBamMapped", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergeBamMapped", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MergeBamMapped", "GMS6804-master/Sarek/GMS6804-master__Sarek/MergeBams", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/MergeBamMapped", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/MergeBams", "rmoran7/dx_sarek/rmoran7__dx_sarek/MergeBamMapped", "cgpu/haplosarek/cgpu__haplosarek/MergeBamMapped", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MergeBamMapped"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "GMS6804-master/Sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "UMCUGenetics/sarek_ubec", "vladsaveliev/cawdor", "cgpu/PGP-UK-sarek", "cgpu/sarek-mirror", "cgpu/haplosarek", "sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "cgpu/pgp-chronek", "UCL-BLIC/Sarek_v2.3.FIX1", "melnel000/Sarek_CBIO", "rmoran7/dx_sarek", "cgpu/sarek-mirror-cache"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["FAANG", "nf-core"], "nb_wf": 2, "list_wf": ["methylseq", "GSM-pipeline"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "drpatelh", "Jani-94"], "nb_contrib": 19, "codes": [" process bwamem_align {\n        tag \"$name\"\n        publishDir \"${params.outdir}/bwa-mem_alignments\", mode: params.publish_dir_mode,\n            saveAs: {filename ->\n                if( !params.save_align_intermeds && filename == \"where_are_my_files.txt\" ) filename\n                else if( params.save_align_intermeds && filename != \"where_are_my_files.txt\" ) filename\n                else null\n            }\n\n        input:\n        set val(name), file(reads) from ch_trimmed_reads_for_alignment\n        file bwa_meth_indices from ch_bwa_meth_indices_for_bwamem_align.collect()\n        file wherearemyfiles from ch_wherearemyfiles_for_bwamem_align.collect()\n\n        output:\n        set val(name), file('*.bam') into ch_bam_for_samtools_sort_index_flagstat, ch_bam_for_preseq, ch_bam_cgmaptools\n        file \"where_are_my_files.txt\"\n\n        script:\n        fasta = bwa_meth_indices[0].toString() - '.bwameth' - '.c2t' - '.amb' - '.ann' - '.bwt' - '.pac' - '.sa'\n        prefix = reads[0].toString() - ~/(_R1)?(_trimmed)?(_val_1)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        \"\"\"\n        bwameth.py \\\\\n            --threads ${task.cpus} \\\\\n            --reference $fasta \\\\\n            $reads | samtools view -bS - > ${prefix}.bam\n        \"\"\"\n    }", " process bwamem_align {\n        tag \"$name\"\n        publishDir \"${params.outdir}/bwa-mem_alignments\", mode: params.publish_dir_mode,\n            saveAs: {filename ->\n                if( !params.save_align_intermeds && filename == \"where_are_my_files.txt\" ) filename\n                else if( params.save_align_intermeds && filename != \"where_are_my_files.txt\" ) filename\n                else null\n            }\n\n        input:\n        set val(name), file(reads) from ch_trimmed_reads_for_alignment\n        file bwa_meth_indices from ch_bwa_meth_indices_for_bwamem_align.collect()\n        file wherearemyfiles from ch_wherearemyfiles_for_bwamem_align.collect()\n\n        output:\n        set val(name), file('*.bam') into ch_bam_for_samtools_sort_index_flagstat, ch_bam_for_preseq\n        file \"where_are_my_files.txt\"\n\n        script:\n        fasta = bwa_meth_indices[0].toString() - '.bwameth' - '.c2t' - '.amb' - '.ann' - '.bwt' - '.pac' - '.sa'\n        prefix = reads[0].toString() - ~/(_R1)?(_trimmed)?(_val_1)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        \"\"\"\n        bwameth.py \\\\\n            --threads ${task.cpus} \\\\\n            --reference $fasta \\\\\n            $reads | samtools view -bS - > ${prefix}.bam\n        \"\"\"\n    }"], "list_proc": ["FAANG/GSM-pipeline/FAANG__GSM-pipeline/bwamem_align", "nf-core/methylseq/nf-core__methylseq/bwamem_align"], "list_wf_names": ["nf-core/methylseq", "FAANG/GSM-pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n    path mqc_custom_config\n    path 'fastqc_raw/*'\n    path 'fastp/*'\n    path 'fastqc_trimmed/*'\n    path host_removal\n    path 'quast*/*'\n    path 'bowtie2log/*'\n    path short_summary\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    read_type = params.single_end ? \"--single_end\" : ''\n    if ( params.host_fasta || params.host_genome ) {\n        \"\"\"\n        # get multiqc parsed data for bowtie2\n        multiqc -f $custom_config_file *.bowtie2.log\n        multiqc_to_custom_tsv.py ${read_type}\n        # run multiqc using custom content file instead of original bowtie2 log files\n        multiqc -f $custom_config_file --ignore \"*.bowtie2.log\" .\n        multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        multiqc -f $options.args .\n        multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/mag/nf-core__mag/MULTIQC"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 5, "tools": ["fastPHASE"], "nb_own": 3, "list_own": ["cidgoh", "nf-core", "mahesh-panchal"], "nb_wf": 5, "list_wf": ["cidgoh_qc", "viralrecon", "test_nfcore_workflow_chain", "modules", "taxprofiler"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "anwarMZ", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "duanjunhyq", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 115, "codes": ["process FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.23.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastp:0.23.2--h79da9fb_0' :\n        'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val   save_trimmed_fail\n    val   save_merged\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz')  , emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n    tuple val(meta), path('*.html')           , emit: html\n    tuple val(meta), path('*.log')            , emit: log\n    path \"versions.yml\"                       , emit: versions\n    tuple val(meta), path('*.fail.fastq.gz')  , optional:true, emit: reads_fail\n    tuple val(meta), path('*.merged.fastq.gz'), optional:true, emit: reads_merged\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                           \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def fail_fastq  = save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        def merge_fastq = save_merged ? \"-m --merged_out ${prefix}.merged.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $merge_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTP {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.23.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastp:0.23.2--h79da9fb_0' :\n        'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val   save_trimmed_fail\n    val   save_merged\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz')  , emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n    tuple val(meta), path('*.html')           , emit: html\n    tuple val(meta), path('*.log')            , emit: log\n    path \"versions.yml\"                       , emit: versions\n    tuple val(meta), path('*.fail.fastq.gz')  , optional:true, emit: reads_fail\n    tuple val(meta), path('*.merged.fastq.gz'), optional:true, emit: reads_merged\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                           \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def fail_fastq  = save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        def merge_fastq = save_merged ? \"-m --merged_out ${prefix}.merged.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $merge_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.23.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastp:0.23.2--h79da9fb_0' :\n        'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val   save_trimmed_fail\n    val   save_merged\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz')  , optional:true, emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n    tuple val(meta), path('*.html')           , emit: html\n    tuple val(meta), path('*.log')            , emit: log\n    path \"versions.yml\"                       , emit: versions\n    tuple val(meta), path('*.fail.fastq.gz')  , optional:true, emit: reads_fail\n    tuple val(meta), path('*.merged.fastq.gz'), optional:true, emit: reads_merged\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                           \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def fail_fastq  = save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        def merge_fastq = save_merged ? \"-m --merged_out ${prefix}.merged.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $merge_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.23.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastp:0.23.2--h79da9fb_0' :\n        'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val   save_trimmed_fail\n    val   save_merged\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz')  , emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n    tuple val(meta), path('*.html')           , emit: html\n    tuple val(meta), path('*.log')            , emit: log\n    path \"versions.yml\"                       , emit: versions\n    tuple val(meta), path('*.fail.fastq.gz')  , optional:true, emit: reads_fail\n    tuple val(meta), path('*.merged.fastq.gz'), optional:true, emit: reads_merged\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                           \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def fail_fastq  = save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        def merge_fastq = save_merged ? \"-m --merged_out ${prefix}.merged.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $merge_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.23.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastp:0.23.2--h79da9fb_0' :\n        'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val   save_trimmed_fail\n    val   save_merged\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz')  , optional:true, emit: reads\n    tuple val(meta), path('*.json')           , emit: json\n    tuple val(meta), path('*.html')           , emit: html\n    tuple val(meta), path('*.log')            , emit: log\n    path \"versions.yml\"                       , emit: versions\n    tuple val(meta), path('*.fail.fastq.gz')  , optional:true, emit: reads_fail\n    tuple val(meta), path('*.merged.fastq.gz'), optional:true, emit: reads_merged\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                           \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def fail_fastq  = save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        def merge_fastq = save_merged ? \"-m --merged_out ${prefix}.merged.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $merge_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $args \\\\\n            2> ${prefix}.fastp.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastp: \\$(fastp --version 2>&1 | sed -e \"s/fastp //g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/FASTP", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/FASTP", "nf-core/taxprofiler/nf-core__taxprofiler/FASTP", "nf-core/viralrecon/nf-core__viralrecon/FASTP", "nf-core/modules/nf-core__modules/FASTP"], "list_wf_names": ["cidgoh/cidgoh_qc", "nf-core/taxprofiler", "nf-core/modules", "nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 2, "tools": ["SnpSift"], "nb_own": 2, "list_own": ["nf-core", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "viralrecon"], "list_contrib": ["stevekm", "heuermh", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "antunderwood", "ggabernet", "MiguelJulia", "ktrns", "saramonzon", "jcurado-flomics", "stevin-wilson", "svarona", "drpatelh", "ErikaKvalem"], "nb_contrib": 18, "codes": ["process SNPSIFT_EXTRACTFIELDS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpsift=4.3.1t\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpsift:4.3.1t--hdfd78af_3' :\n        'quay.io/biocontainers/snpsift:4.3.1t--hdfd78af_3' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.snpsift.txt\"), emit: txt\n    path \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[SnpSift] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    SnpSift \\\\\n        -Xmx${avail_mem}g \\\\\n        extractFields \\\\\n        -s \",\" \\\\\n        -e \".\" \\\\\n        $args \\\\\n        $vcf \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${prefix}.snpsift.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpsift: \\$( echo \\$(SnpSift split -h 2>&1) | sed 's/^.*version //' | sed 's/(.*//' | sed 's/t//g' )\n    END_VERSIONS\n    \"\"\"\n}", "process SNPSIFT_EXTRACTFIELDS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpsift=4.3.1t\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpsift:4.3.1t--hdfd78af_3' :\n        'quay.io/biocontainers/snpsift:4.3.1t--hdfd78af_3' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.snpsift.txt\"), emit: txt\n    path \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[SnpSift] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    SnpSift \\\\\n        -Xmx${avail_mem}g \\\\\n        extractFields \\\\\n        -s \",\" \\\\\n        -e \".\" \\\\\n        $args \\\\\n        $vcf \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${prefix}.snpsift.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpsift: \\$( echo \\$(SnpSift split -h 2>&1) | sed 's/^.*version //' | sed 's/(.*//' | sed 's/t//g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/viralrecon/nf-core__viralrecon/SNPSIFT_EXTRACTFIELDS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SNPSIFT_EXTRACTFIELDS"], "list_wf_names": ["nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["BWA", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess bwamem {\n    label 'mc_medium'\n    tag \"$libraryid\"\n    publishDir \"${params.outdir}/mapping/bwamem\", mode: params.publish_dir_mode\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(r1), file(r2) from ch_lanemerge_for_bwamem\n    path index from bwa_index_bwamem.collect()\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.mapped.bam\"), path(\"*.{bai,csi}\") into ch_output_from_bwamem\n\n    when: \n    params.mapper == 'bwamem'\n\n    script:\n    def split_cpus = Math.floor(task.cpus/2)\n    def fasta = \"${index}/${fasta_base}\"\n    def size = params.large_ref ? '-c' : ''\n\n    if (!params.single_end && params.skip_collapse){\n    \"\"\"\n    bwa mem -t ${split_cpus} $fasta $r1 $r2 -R \"@RG\\\\tID:ILLUMINA-${libraryid}\\\\tSM:${samplename}\\\\tPL:illumina\\\\tPU:ILLUMINA-${libraryid}-${seqtype}\" | samtools sort -@ ${split_cpus} -O bam - > \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    samtools index ${size} -@ ${task.cpus} \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    \"\"\"\n    } else {\n    \"\"\"\n    bwa mem -t ${split_cpus} $fasta $r1 -R \"@RG\\\\tID:ILLUMINA-${libraryid}\\\\tSM:${samplename}\\\\tPL:illumina\\\\tPU:ILLUMINA-${libraryid}-${seqtype}\" | samtools sort -@ ${split_cpus} -O bam - > \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    samtools index -@ ${task.cpus} \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size} \n    \"\"\"\n    }\n    \n}"], "list_proc": ["nf-core/eager/nf-core__eager/bwamem"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process GATK4_ESTIMATELIBRARYCOMPLEXITY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path('*.metrics'), emit: metrics\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = input.collect(){\"--INPUT $it\"}.join(\" \")\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK EstimateLibraryComplexity] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" EstimateLibraryComplexity \\\\\n        $input_list \\\\\n        --OUTPUT ${prefix}.metrics \\\\\n        --REFERENCE_SEQUENCE ${fasta} \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_ESTIMATELIBRARYCOMPLEXITY"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["BEDTools", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess bedtools {\n  label 'mc_small'\n  tag \"${libraryid}\"\n  publishDir \"${params.outdir}/bedtools\", mode: params.publish_dir_mode\n\n  when:\n  params.run_bedtools_coverage\n\n  input:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_rmdup_for_bedtools\n  file anno_file from ch_anno_for_bedtools.collect()\n\n  output:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*\")\n\n  script:\n  \"\"\"\n  ## Create genome file from bam header\n  samtools view -H ${bam} | grep '@SQ' | sed 's#@SQ\\tSN:\\\\|LN:##g' > genome.txt\n  \n  ##  Run bedtools\n  bedtools coverage -nonamecheck -g genome.txt -sorted -a ${anno_file} -b ${bam} | pigz -p ${task.cpus - 1} > \"${bam.baseName}\".breadth.gz\n  bedtools coverage -nonamecheck -g genome.txt -sorted -a ${anno_file} -b ${bam} -mean | pigz -p ${task.cpus - 1} > \"${bam.baseName}\".depth.gz\n  \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/bedtools"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["ExpansionHunter"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "raredisease"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process EXPANSIONHUNTER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::expansionhunter=4.0.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/expansionhunter:4.0.2--he785bd8_0' :\n        'quay.io/biocontainers/expansionhunter:4.0.2--he785bd8_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path fasta\n    path variant_catalog\n\n    output:\n    tuple val(meta), path(\"*.vcf\"), emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def gender = (meta.gender == 'male' || meta.gender == 1 || meta.gender == 'XY') ? \"male\" : \"female\"\n    \"\"\"\n    ExpansionHunter \\\\\n        $args \\\\\n        --reads $bam \\\\\n        --output-prefix $prefix \\\\\n        --reference $fasta \\\\\n        --variant-catalog $variant_catalog \\\\\n        --sex $gender\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        expansionhunter: \\$( echo \\$(ExpansionHunter --version 2>&1) | sed 's/^.*ExpansionHunter v//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        expansionhunter: \\$( echo \\$(ExpansionHunter --version 2>&1) | sed 's/^.*ExpansionHunter v//')\n    END_VERSIONS\n    \"\"\"\n}", "process EXPANSIONHUNTER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::expansionhunter=4.0.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/expansionhunter:4.0.2--he785bd8_0' :\n        'quay.io/biocontainers/expansionhunter:4.0.2--he785bd8_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path fasta\n    path variant_catalog\n\n    output:\n    tuple val(meta), path(\"*.vcf\"), emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def gender = (meta.gender == 'male' || meta.gender == 1 || meta.gender == 'XY') ? \"male\" : \"female\"\n    \"\"\"\n    ExpansionHunter \\\\\n        $args \\\\\n        --reads $bam \\\\\n        --output-prefix $prefix \\\\\n        --reference $fasta \\\\\n        --variant-catalog $variant_catalog \\\\\n        --sex $gender\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        expansionhunter: \\$( echo \\$(ExpansionHunter --version 2>&1) | sed 's/^.*ExpansionHunter v//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        expansionhunter: \\$( echo \\$(ExpansionHunter --version 2>&1) | sed 's/^.*ExpansionHunter v//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/EXPANSIONHUNTER", "nf-core/modules/nf-core__modules/EXPANSIONHUNTER"], "list_wf_names": ["nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 1, "tools": ["MEGAHIT"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess MEGAHIT {\n    tag \"$meta.id\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::megahit=1.2.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/megahit:1.2.9--h2e03b76_1\"\n    } else {\n        container \"quay.io/biocontainers/megahit:1.2.9--h2e03b76_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads1), path(reads2)\n\n    output:\n    tuple val(meta), path(\"MEGAHIT/${meta.id}.contigs.fa\"), emit: assembly\n    path \"MEGAHIT/*.log\"                                  , emit: log\n    path \"MEGAHIT/${meta.id}.contigs.fa.gz\"               , emit: assembly_gz\n    path '*.version.txt'                                  , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = params.single_end ? \"-r \\\"\" + reads1.join(\",\") + \"\\\"\" : \"-1 \\\"\" + reads1.join(\",\") + \"\\\" -2 \\\"\" + reads2.join(\",\") + \"\\\"\"\n    mem = task.memory.toBytes()\n    if ( !params.megahit_fix_cpu_1 || task.cpus == 1 )\n        \"\"\"\n        megahit ${params.megahit_options} -t \"${task.cpus}\" -m $mem $input -o MEGAHIT --out-prefix \"${meta.id}\"\n        gzip -c \"MEGAHIT/${meta.id}.contigs.fa\" > \"MEGAHIT/${meta.id}.contigs.fa.gz\"\n\n        megahit --version | sed \"s/MEGAHIT v//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--megahit_fix_cpu_1' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/MEGAHIT"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie"], "nb_own": 2, "list_own": ["ABMicroBioinf", "nf-core"], "nb_wf": 2, "list_wf": ["magph", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.4 bioconda::samtools=1.15.1 conda-forge::pigz=2.6\" : null)\n    container \"${ workflow.containerEngine == \"singularity\" && !task.ext.singularity_pull_docker_container ?\n        \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:1744f68fe955578c63054b55309e05b41c37a80d-0\" :\n        \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:1744f68fe955578c63054b55309e05b41c37a80d-0\" }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   save_unaligned\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\")    , emit: bam\n    tuple val(meta), path(\"*.log\")    , emit: log\n    tuple val(meta), path(\"*fastq.gz\"), emit: fastq, optional:true\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: \"\"\n    def args2 = task.ext.args2 ?: \"\"\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def unaligned = \"\"\n    def reads_args = \"\"\n    if (meta.single_end) {\n        unaligned = save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : \"\"\n        reads_args = \"-U ${reads}\"\n    } else {\n        unaligned = save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : \"\"\n        reads_args = \"-1 ${reads[0]} -2 ${reads[1]}\"\n    }\n\n    def samtools_command = sort_bam ? 'sort' : 'view'\n\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed \"s/.rev.1.bt2//\"`\n    [ -z \"\\$INDEX\" ] && INDEX=`find -L ./ -name \"*.rev.1.bt2l\" | sed \"s/.rev.1.bt2l//\"`\n    [ -z \"\\$INDEX\" ] && echo \"Bowtie2 index files not found\" 1>&2 && exit 1\n\n    bowtie2 \\\\\n        -x \\$INDEX \\\\\n        $reads_args \\\\\n        --threads $task.cpus \\\\\n        $unaligned \\\\\n        $args \\\\\n        2> ${prefix}.bowtie2.log \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n        mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n    fi\n\n    if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n        mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.4 bioconda::samtools=1.15.1 conda-forge::pigz=2.6\" : null)\n    container \"${ workflow.containerEngine == \"singularity\" && !task.ext.singularity_pull_docker_container ?\n        \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:1744f68fe955578c63054b55309e05b41c37a80d-0\" :\n        \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:1744f68fe955578c63054b55309e05b41c37a80d-0\" }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   save_unaligned\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\")    , emit: bam\n    tuple val(meta), path(\"*.log\")    , emit: log\n    tuple val(meta), path(\"*fastq.gz\"), emit: fastq, optional:true\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: \"\"\n    def args2 = task.ext.args2 ?: \"\"\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def unaligned = \"\"\n    def reads_args = \"\"\n    if (meta.single_end) {\n        unaligned = save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : \"\"\n        reads_args = \"-U ${reads}\"\n    } else {\n        unaligned = save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : \"\"\n        reads_args = \"-1 ${reads[0]} -2 ${reads[1]}\"\n    }\n\n    def samtools_command = sort_bam ? 'sort' : 'view'\n\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed \"s/.rev.1.bt2//\"`\n    [ -z \"\\$INDEX\" ] && INDEX=`find -L ./ -name \"*.rev.1.bt2l\" | sed \"s/.rev.1.bt2l//\"`\n    [ -z \"\\$INDEX\" ] && echo \"Bowtie2 index files not found\" 1>&2 && exit 1\n\n    bowtie2 \\\\\n        -x \\$INDEX \\\\\n        $reads_args \\\\\n        --threads $task.cpus \\\\\n        $unaligned \\\\\n        $args \\\\\n        2> ${prefix}.bowtie2.log \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n        mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n    fi\n\n    if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n        mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ABMicroBioinf/magph/ABMicroBioinf__magph/BOWTIE2_ALIGN", "nf-core/modules/nf-core__modules/BOWTIE2_ALIGN"], "list_wf_names": ["ABMicroBioinf/magph", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Picard"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "raredisease"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process PICARD_COLLECTHSMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    path fai\n    path bait_intervals\n    path target_intervals\n\n    output:\n    tuple val(meta), path(\"*_metrics\")  , emit: metrics\n    path \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"-R $fasta\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectHsMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectHsMetrics \\\\\n        $args \\\\\n        $reference \\\\\n        -BAIT_INTERVALS $bait_intervals \\\\\n        -TARGET_INTERVALS $target_intervals \\\\\n        -INPUT $bam \\\\\n        -OUTPUT ${prefix}.CollectHsMetrics.coverage_metrics\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard CollectHsMetrics --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.CollectHsMetrics.coverage_metrics\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard CollectHsMetrics --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_COLLECTHSMETRICS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    path fai\n    path bait_intervals\n    path target_intervals\n\n    output:\n    tuple val(meta), path(\"*_metrics\")  , emit: metrics\n    path \"versions.yml\"                 , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"-R $fasta\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CollectHsMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CollectHsMetrics \\\\\n        $args \\\\\n        $reference \\\\\n        --BAIT_INTERVALS $bait_intervals \\\\\n        --TARGET_INTERVALS $target_intervals \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.CollectHsMetrics.coverage_metrics\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard CollectHsMetrics --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.CollectHsMetrics.coverage_metrics\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard CollectHsMetrics --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/PICARD_COLLECTHSMETRICS", "nf-core/modules/nf-core__modules/PICARD_COLLECTHSMETRICS"], "list_wf_names": ["nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 1, "tools": ["BWA", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess circularmapper{\n    label 'mc_medium'\n    tag \"$libraryid\"\n    publishDir \"${params.outdir}/mapping/circularmapper\", mode: params.publish_dir_mode\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(r1), file(r2) from ch_lanemerge_for_cm\n    file index from ch_circularmapper_indices.collect()\n    file fasta from ch_fasta_for_circularmapper.collect()\n    file elongated from ch_circularmapper_elongatedfasta.collect()\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*.mapped.bam\"), file(\"*.{bai,csi}\") into ch_output_from_cm\n\n    when: \n    params.mapper == 'circularmapper'\n\n    script:\n    def filter = params.circularfilter ? '-f true -x true' : ''\n    def elongated_root = \"${fasta.baseName}_${params.circularextension}.fasta\"\n    def size = params.large_ref ? '-c' : ''\n\n    if (!params.single_end && params.skip_collapse ){\n    \"\"\"\n    bwa aln -t ${task.cpus} $elongated_root $r1 -n ${params.bwaalnn} -l ${params.bwaalnl} -k ${params.bwaalnk} -f ${libraryid}.r1.sai\n    bwa aln -t ${task.cpus} $elongated_root $r2 -n ${params.bwaalnn} -l ${params.bwaalnl} -k ${params.bwaalnk} -f ${libraryid}.r2.sai\n    bwa sampe -r \"@RG\\\\tID:ILLUMINA-${libraryid}\\\\tSM:${samplename}\\\\tPL:illumina\\\\tPU:ILLUMINA-${libraryid}-${seqtype}\" $elongated_root ${libraryid}.r1.sai ${libraryid}.r2.sai $r1 $r2 > tmp.out\n    realignsamfile -Xmx${task.memory.toGiga()}g -e ${params.circularextension} -i tmp.out -r $fasta $filter \n    samtools sort -@ ${task.cpus} -O bam tmp_realigned.bam > ${libraryid}_\"${seqtype}\".mapped.bam\n    samtools index \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size} \n    \"\"\"\n    } else {\n    \"\"\" \n    bwa aln -t ${task.cpus} $elongated_root $r1 -n ${params.bwaalnn} -l ${params.bwaalnl} -k ${params.bwaalnk} -f ${libraryid}.sai\n    bwa samse -r \"@RG\\\\tID:ILLUMINA-${libraryid}\\\\tSM:${samplename}\\\\tPL:illumina\\\\tPU:ILLUMINA-${libraryid}-${seqtype}\" $elongated_root ${libraryid}.sai $r1 > tmp.out\n    realignsamfile -Xmx${task.memory.toGiga()}g -e ${params.circularextension} -i tmp.out -r $fasta $filter \n    samtools sort -@ ${task.cpus} -O bam tmp_realigned.bam > \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    samtools index \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size}\n    \"\"\"\n    }\n    \n}"], "list_proc": ["nf-core/eager/nf-core__eager/circularmapper"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["Racon"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process RACON {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::racon=1.4.20\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/racon:1.4.20--h9a82719_1' :\n        'quay.io/biocontainers/racon:1.4.20--h9a82719_1' }\"\n\n    input:\n    tuple val(meta), path(reads), path(assembly), path(paf)\n\n    output:\n    tuple val(meta), path('*_assembly_consensus.fasta.gz') , emit: improved_assembly\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    racon -t \"$task.cpus\" \\\\\n        \"${reads}\" \\\\\n        \"${paf}\" \\\\\n        $args \\\\\n        \"${assembly}\" > \\\\\n        ${prefix}_assembly_consensus.fasta\n\n    gzip -n ${prefix}_assembly_consensus.fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        racon: \\$( racon --version 2>&1 | sed 's/^.*v//' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/RACON"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess mask_reference_for_pmdtools {\n    label 'sc_tiny'\n    tag \"${fasta}\"\n    publishDir \"${params.outdir}/reference_genome/masked_reference\", mode: params.publish_dir_mode\n\n    when: (params.pmdtools_reference_mask && params.run_pmdtools)\n\n    input:\n    file fasta from ch_unmasked_fasta_for_masking\n    file bedfile from ch_bedfile_for_reference_masking\n\n    output:\n    file \"${fasta.baseName}_masked.fa\" into ch_masked_fasta_for_pmdtools\n\n    script:\n    log.info \"[nf-core/eager]: Masking reference \\'${fasta}\\' at positions found in \\'${bedfile}\\'. Masked reference will be used for pmdtools.\"\n    \"\"\"\n    bedtools maskfasta -fi ${fasta} -bed ${bedfile} -fo ${fasta.baseName}_masked.fa\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/mask_reference_for_pmdtools"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie"], "nb_own": 2, "list_own": ["heinzlab", "nf-core"], "nb_wf": 2, "list_wf": ["smrnaseq", "smrna-seq-pipeline"], "list_contrib": ["c-guzman", "ewels", "maxulysse", "lpantano", "chuan-wang", "sirselim", "lcabus-flomics", "nf-core-bot", "ErikDanielsson", "pericsson", "sdjebali", "pditommaso", "mjsteinbaugh", "Hammarn", "jemten", "KevinMenden", "apeltzer", "drpatelh", "kstawiski"], "nb_contrib": 19, "codes": ["\nprocess bowtie_miRBase_mature {\n    tag \"$reads\"\n    publishDir \"${params.outdir}/bowtie/miRBase_mature\", mode: 'copy', pattern: '*.mature_unmapped.fq.gz'\n\n    input:\n    file reads from trimmed_reads_bowtie\n    file index from mature_index\n\n    output:\n    file '*.mature.bam' into miRBase_mature_bam\n    file '*.mature_unmapped.fq.gz' into mature_unmapped_reads\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.toString() - ~/(.R1)?(_R1)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -q <(zcat $reads) \\\\\n        -p 2 \\\\\n        -t \\\\\n        -k 1 \\\\\n        -m 1 \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        --un ${prefix}.mature_unmapped.fq \\\\\n        -S \\\\\n        | samtools view -bS - > ${prefix}.mature.bam\n    gzip ${prefix}.mature_unmapped.fq\n    \"\"\"\n}", "\nprocess bowtie_miRBase_mature {\n    label 'process_medium'\n    tag \"$reads\"\n    publishDir \"${params.outdir}/bowtie/miRBase_mature\", mode: params.publish_dir_mode, pattern: '*.mature_unmapped.fq.gz'\n\n    input:\n    file reads from trimmed_reads_bowtie\n    file index from mature_index_bowtie\n\n    output:\n    file '*.mature.bam' into miRBase_mature_bam\n    file '*.mature_unmapped.fq.gz' into mature_unmapped_reads\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.toString() - ~/(.R1)?(_R1)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    seq_center = params.seq_center ? \"--sam-RG ID:${prefix} --sam-RG 'CN:${params.seq_center}'\" : ''\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -q <(zcat $reads) \\\\\n        -p ${task.cpus} \\\\\n        -t \\\\\n        -k 50 \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        --un ${prefix}.mature_unmapped.fq \\\\\n        -S $seq_center \\\\\n        | samtools view -bS - > ${prefix}.mature.bam\n\n    gzip ${prefix}.mature_unmapped.fq\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/heinzlab__smrna-seq-pipeline/bowtie_miRBase_mature", "nf-core/smrnaseq/nf-core__smrnaseq/bowtie_miRBase_mature"], "list_wf_names": ["nf-core/smrnaseq", "heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process CHROMAP_CHROMAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::chromap=0.2.1 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1f09f39f20b1c4ee36581dc81cc323c70e661633:963e4fe6a85c548a4018585660aed79780a175d3-0' :\n        'quay.io/biocontainers/mulled-v2-1f09f39f20b1c4ee36581dc81cc323c70e661633:963e4fe6a85c548a4018585660aed79780a175d3-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path fasta\n    path index\n    path barcodes\n    path whitelist\n    path chr_order\n    path pairs_chr_order\n\n    output:\n    tuple val(meta), path(\"*.bed.gz\")     , optional:true, emit: bed\n    tuple val(meta), path(\"*.bam\")        , optional:true, emit: bam\n    tuple val(meta), path(\"*.tagAlign.gz\"), optional:true, emit: tagAlign\n    tuple val(meta), path(\"*.pairs.gz\")   , optional:true, emit: pairs\n    path \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def args_list = args.tokenize()\n\n    def file_extension = args.contains(\"--SAM\") ? 'sam' : args.contains(\"--TagAlign\")? 'tagAlign' : args.contains(\"--pairs\")? 'pairs' : 'bed'\n    if (barcodes) {\n        args_list << \"-b ${barcodes.join(',')}\"\n        if (whitelist) {\n            args_list << \"--barcode-whitelist $whitelist\"\n        }\n    }\n    if (chr_order) {\n        args_list << \"--chr-order $chr_order\"\n    }\n    if (pairs_chr_order){\n        args_list << \"--pairs-natural-chr-order $pairs_chr_order\"\n    }\n    def final_args = args_list.join(' ')\n    def compression_cmds = \"gzip -n ${prefix}.${file_extension}\"\n    if (args.contains(\"--SAM\")) {\n        compression_cmds = \"\"\"\n        samtools view $args2 -@ $task.cpus -bh \\\\\n            -o ${prefix}.bam ${prefix}.${file_extension}\n        rm ${prefix}.${file_extension}\n        \"\"\"\n    }\n    if (meta.single_end) {\n        \"\"\"\n        chromap \\\\\n            $final_args \\\\\n            -t $task.cpus \\\\\n            -x $index \\\\\n            -r $fasta \\\\\n            -1 ${reads.join(',')} \\\\\n            -o ${prefix}.${file_extension}\n\n        $compression_cmds\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            chromap: \\$(echo \\$(chromap --version 2>&1))\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        chromap \\\\\n            $final_args \\\\\n            -t $task.cpus \\\\\n            -x $index \\\\\n            -r $fasta \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            -o ${prefix}.${file_extension}\n\n        $compression_cmds\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            chromap: \\$(echo \\$(chromap --version 2>&1))\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/CHROMAP_CHROMAP"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 17, "tools": ["GATK"], "nb_own": 3, "list_own": ["vincenthhu", "nf-core", "CDCgov"], "nb_wf": 4, "list_wf": ["modules", "rnavar", "mycosnp-nf", "nf-core-westest"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "leebrian", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor"], "nb_contrib": 111, "codes": ["process GATK4_VARIANTFILTRATION {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_tbi)\n    path  fasta\n    path  fai\n    path  dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"\t\t         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK VariantFiltration] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}G\" VariantFiltration \\\\\n        -R $fasta \\\\\n        -V $vcf \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GENOTYPEGVCFS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(gvcf), path(gvcf_index), path(intervals)\n    path  fasta\n    path  fasta_index\n    path  fasta_dict\n    path  dbsnp\n    path  dbsnp_index\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path  \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def dbsnp_options    = dbsnp ? \"-D ${dbsnp}\" : \"\"\n    def interval_options = intervals ? \"-L ${intervals}\" : \"\"\n    def gvcf_options     = gvcf.name.endsWith(\".vcf\") || gvcf.name.endsWith(\".vcf.gz\") ? \"$gvcf\" : \"gendb://$gvcf\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenotypeGVCFs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        GenotypeGVCFs \\\\\n        $args \\\\\n        $interval_options \\\\\n        $dbsnp_options \\\\\n        -R $fasta \\\\\n        -V $gvcf_options \\\\\n        -O ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_INDEXFEATUREFILE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(feature_file)\n\n    output:\n    tuple val(meta), path(\"*.{tbi,idx}\"), emit: index\n    path  \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IndexFeatureFile] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" IndexFeatureFile \\\\\n        --input $feature_file \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_MERGEMUTECTSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(stats)\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz.stats\"), emit: stats\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = stats.collect{ \"--stats ${it}\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK MergeMutectStats] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" MergeMutectStats \\\\\n        $input_list \\\\\n        --output ${prefix}.vcf.gz.stats \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_LEARNREADORIENTATIONMODEL {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(f1r2)\n\n    output:\n    tuple val(meta), path(\"*.tar.gz\"), emit: artifactprior\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = f1r2.collect{\"--input $it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK LearnReadOrientationModel] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" LearnReadOrientationModel \\\\\n        $input_list \\\\\n        --output ${prefix}.tar.gz \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_HAPLOTYPECALLER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n    path dbsnp\n    path dbsnp_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_option = intervals ? \"-L ${intervals}\" : \"\"\n    def dbsnp_option    = dbsnp ? \"-D ${dbsnp}\" : \"\"\n    def avail_mem       = 3\n    if (!task.memory) {\n        log.info '[GATK HaplotypeCaller] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk \\\\\n        --java-options \"-Xmx${avail_mem}g\" \\\\\n        HaplotypeCaller \\\\\n        -R $fasta \\\\\n        -I $input \\\\\n        ${dbsnp_option} \\\\\n        ${interval_option} \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --tmp-dir .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_FILTERMUTECTCALLS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(stats), path(orientationbias), path(segmentation), path(contaminationfile), val(contaminationest)\n    path fasta\n    path fai\n    path dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\")            , emit: vcf\n    tuple val(meta), path(\"*.vcf.gz.tbi\")        , emit: tbi\n    tuple val(meta), path(\"*.filteringStats.tsv\"), emit: stats\n    path \"versions.yml\"                          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def orientationbias_options = ''\n    if (orientationbias) {\n        orientationbias_options = '--orientation-bias-artifact-priors ' + orientationbias.join(' --orientation-bias-artifact-priors ')\n    }\n\n    def segmentation_options = ''\n    if (segmentation) {\n        segmentation_options = '--tumor-segmentation ' + segmentation.join(' --tumor-segmentation ')\n    }\n\n    def contamination_options = contaminationest ? \" --contamination-estimate ${contaminationest} \" : ''\n    if (contaminationfile) {\n        contamination_options = '--contamination-table ' + contaminationfile.join(' --contamination-table ')\n    }\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK FilterMutectCalls] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" FilterMutectCalls \\\\\n        -R $fasta \\\\\n        -V $vcf \\\\\n        $orientationbias_options \\\\\n        $segmentation_options \\\\\n        $contamination_options \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_MUTECT2 {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n    path germline_resource\n    path germline_resource_tbi\n    path panel_of_normals\n    path panel_of_normals_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\")     , emit: vcf\n    tuple val(meta), path(\"*.tbi\")        , emit: tbi\n    tuple val(meta), path(\"*.stats\")      , emit: stats\n    tuple val(meta), path(\"*.f1r2.tar.gz\"), optional:true, emit: f1r2\n    path \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def inputs = input.collect{ \"--input $it\"}.join(\" \")\n    def interval_command = intervals ? \"--intervals $intervals\" : \"\"\n    def pon_command = panel_of_normals ? \"--panel-of-normals $panel_of_normals\" : \"\"\n    def gr_command = germline_resource ? \"--germline-resource $germline_resource\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK Mutect2] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" Mutect2 \\\\\n        $inputs \\\\\n        --output ${prefix}.vcf.gz \\\\\n        --reference $fasta \\\\\n        $pon_command \\\\\n        $gr_command \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf.gz\n    touch ${prefix}.vcf.gz.tbi\n    touch ${prefix}.vcf.gz.stats\n    touch ${prefix}.f1r2.tar.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GENOMICSDBIMPORT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(interval_file), val(interval_value), path(wspace)\n    val   run_intlist\n    val   run_updatewspace\n    val   input_map\n\n    output:\n    tuple val(meta), path(\"$prefix\")        , optional:true, emit: genomicsdb\n    tuple val(meta), path(\"$updated_db\")    , optional:true, emit: updatedb\n    tuple val(meta), path(\"*.interval_list\"), optional:true, emit: intervallist\n    path \"versions.yml\"                                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n                                                     \n    input_command = input_map ? \"--sample-name-map ${vcf[0]}\" : vcf.collect(){\"--variant $it\"}.join(' ')\n\n    genomicsdb_command = \"--genomicsdb-workspace-path ${prefix}\"\n    interval_command = interval_file ? \"--intervals ${interval_file}\" : \"--intervals ${interval_value}\"\n\n                                                                                  \n    if (run_intlist) {\n        genomicsdb_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        interval_command = \"--output-interval-list-to-file ${prefix}.interval_list\"\n    }\n\n                                                                                                                                       \n    if (run_updatewspace) {\n        genomicsdb_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        interval_command = ''\n        updated_db = \"${wspace}\"\n    }\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenomicsDBImport] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GenomicsDBImport \\\\\n        $input_command \\\\\n        $genomicsdb_command \\\\\n        $interval_command \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_COMBINEGVCFS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(vcf_idx)\n    path (fasta)\n    path (fasta_fai)\n    path (fasta_dict)\n\n    output:\n    tuple val(meta), path(\"*.combined.g.vcf.gz\"), emit: combined_gvcf\n    path \"versions.yml\"                         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem       = 3\n    if (!task.memory) {\n        log.info '[GATK COMBINEGVCFS] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    def input_files = vcf.collect{\"-V ${it}\"}.join(' ') // add '-V' to each vcf file\n    \"\"\"\n    gatk \\\\\n        --java-options \"-Xmx${avail_mem}g\" \\\\\n        CombineGVCFs \\\\\n        -R ${fasta} \\\\\n        -O ${prefix}.combined.g.vcf.gz \\\\\n        ${args} \\\\\n        ${input_files}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_INDEXFEATUREFILE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(feature_file)\n\n    output:\n    tuple val(meta), path(\"*.{tbi,idx}\"), emit: index\n    path  \"versions.yml\"                , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IndexFeatureFile] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        IndexFeatureFile \\\\\n        $args \\\\\n        -I $feature_file\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_HAPLOTYPECALLER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n    path dbsnp\n    path dbsnp_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_option = intervals ? \"-L ${intervals}\" : \"\"\n    def dbsnp_option    = dbsnp ? \"-D ${dbsnp}\" : \"\"\n    def avail_mem       = 3\n    if (!task.memory) {\n        log.info '[GATK HaplotypeCaller] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk \\\\\n        --java-options \"-Xmx${avail_mem}g\" \\\\\n        HaplotypeCaller \\\\\n        -R $fasta \\\\\n        -I $input \\\\\n        ${dbsnp_option} \\\\\n        ${interval_option} \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --tmp-dir .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GATHERBQSRREPORTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(table)\n\n    output:\n    tuple val(meta), path(\"*.table\"), emit: table\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = table.collect{\"--input $it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GatherBQSRReports] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GatherBQSRReports \\\\\n        $input_list \\\\\n        --output ${prefix}.table \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_LEARNREADORIENTATIONMODEL {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(f1r2)\n\n    output:\n    tuple val(meta), path(\"*.tar.gz\"), emit: artifactprior\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def inputs_list = []\n    f1r2.each() { a -> inputs_list.add(\" -I \" + a) }\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK LearnReadOrientationModel] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        LearnReadOrientationModel \\\\\n        ${inputs_list.join(' ')} \\\\\n        -O ${prefix}.tar.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GENOMICSDBIMPORT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(intervalfile), val(intervalval), path(wspace)\n    val run_intlist\n    val run_updatewspace\n    val input_map\n\n    output:\n    tuple val(meta), path(\"${prefix}\")      , optional:true, emit: genomicsdb\n    tuple val(meta), path(\"$updated_db\")    , optional:true, emit: updatedb\n    tuple val(meta), path(\"*.interval_list\"), optional:true, emit: intervallist\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n                                                     \n    inputs_command = input_map ? \"--sample-name-map ${vcf[0]}\" : \"${'-V ' + vcf.join(' -V ')}\"\n    dir_command = \"--genomicsdb-workspace-path ${prefix}\"\n    intervals_command = intervalfile ? \" -L ${intervalfile} \" : \" -L ${intervalval} \"\n\n                                                                                  \n    if (run_intlist) {\n        inputs_command = ''\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = \"--output-interval-list-to-file ${prefix}.interval_list\"\n    }\n\n                                                                                                                                        \n    if (run_updatewspace) {\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = ''\n        updated_db = wspace.toString()\n    }\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenomicsDBImport] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GenomicsDBImport \\\\\n        $inputs_command \\\\\n        $dir_command \\\\\n        $intervals_command \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_INDEXFEATUREFILE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(feature_file)\n\n    output:\n    tuple val(meta), path(\"*.{tbi,idx}\"), emit: index\n    path  \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IndexFeatureFile] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" IndexFeatureFile \\\\\n        --input $feature_file \\\\\n        --tmp-dir . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GENOTYPEGVCFS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(gvcf), path(gvcf_index), path(intervals)\n    path  fasta\n    path  fasta_index\n    path  fasta_dict\n    path  dbsnp\n    path  dbsnp_index\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path  \"versions.yml\"             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def dbsnp_options    = dbsnp ? \"-D ${dbsnp}\" : \"\"\n    def interval_options = intervals ? \"-L ${intervals}\" : \"\"\n    def gvcf_options     = gvcf.name.endsWith(\".vcf\") || gvcf.name.endsWith(\".vcf.gz\") ? \"$gvcf\" : \"gendb://$gvcf\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenotypeGVCFs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        GenotypeGVCFs \\\\\n        $args \\\\\n        $interval_options \\\\\n        $dbsnp_options \\\\\n        -R $fasta \\\\\n        -V $gvcf_options \\\\\n        -O ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/GATK4_VARIANTFILTRATION", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/GATK4_GENOTYPEGVCFS", "nf-core/modules/nf-core__modules/GATK4_INDEXFEATUREFILE", "nf-core/modules/nf-core__modules/GATK4_MERGEMUTECTSTATS", "nf-core/modules/nf-core__modules/GATK4_LEARNREADORIENTATIONMODEL", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/GATK4_HAPLOTYPECALLER", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/GATK4_FILTERMUTECTCALLS", "nf-core/modules/nf-core__modules/GATK4_MUTECT2", "nf-core/modules/nf-core__modules/GATK4_GENOMICSDBIMPORT", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/GATK4_COMBINEGVCFS", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/GATK4_INDEXFEATUREFILE", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/GATK4_HAPLOTYPECALLER", "nf-core/modules/nf-core__modules/GATK4_GATHERBQSRREPORTS", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/GATK4_LEARNREADORIENTATIONMODEL", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/GATK4_GENOMICSDBIMPORT", "nf-core/rnavar/nf-core__rnavar/GATK4_INDEXFEATUREFILE", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/GATK4_GENOTYPEGVCFS"], "list_wf_names": ["nf-core/modules", "nf-core/rnavar", "CDCgov/mycosnp-nf", "vincenthhu/nf-core-westest"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie"], "nb_own": 2, "list_own": ["goodwright", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "imaps-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "rfara", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "samirelanduk", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "alexharston", "Gwennid", "Jeremy1805", "marc-jones", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "CharlotteAnne", "jianhong", "mashehu", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 110, "codes": ["process BOWTIE_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie=1.3.0 bioconda::samtools=1.15.1' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-ffbf83a6b0ab6ec567a336cf349b80637135bca3:676c5bcfe34af6097728fea60fb7ea83f94a4a5f-0' :\n        'quay.io/biocontainers/mulled-v2-ffbf83a6b0ab6ec567a336cf349b80637135bca3:676c5bcfe34af6097728fea60fb7ea83f94a4a5f-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    tuple val(meta), path('*.out'), emit: log\n    path  \"versions.yml\"          , emit: versions\n    tuple val(meta), path('*fastq.gz'), optional:true, emit: fastq\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def unaligned = params.save_unaligned ? \"--un ${prefix}.unmapped.fastq\" : ''\n    def endedness = meta.single_end ? \"$reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.3.ebwt\" | sed 's/.3.ebwt//'`\n    bowtie \\\\\n        --threads $task.cpus \\\\\n        --sam \\\\\n        -x \\$INDEX \\\\\n        -q \\\\\n        $unaligned \\\\\n        $args \\\\\n        $endedness \\\\\n        2> ${prefix}.out \\\\\n        | samtools view $args2 -@ $task.cpus -bS -o ${prefix}.bam -\n\n    if [ -f ${prefix}.unmapped.fastq ]; then\n        gzip ${prefix}.unmapped.fastq\n    fi\n    if [ -f ${prefix}.unmapped_1.fastq ]; then\n        gzip ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie: \\$(echo \\$(bowtie --version 2>&1) | sed 's/^.*bowtie-align-s version //; s/ .*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BOWTIE_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie=1.3.0 bioconda::samtools=1.15.1' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-ffbf83a6b0ab6ec567a336cf349b80637135bca3:676c5bcfe34af6097728fea60fb7ea83f94a4a5f-0' :\n        'quay.io/biocontainers/mulled-v2-ffbf83a6b0ab6ec567a336cf349b80637135bca3:676c5bcfe34af6097728fea60fb7ea83f94a4a5f-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    tuple val(meta), path('*.out'), emit: log\n    path  \"versions.yml\"          , emit: versions\n    tuple val(meta), path('*fastq.gz'), optional:true, emit: fastq\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def unaligned = params.save_unaligned ? \"--un ${prefix}.unmapped.fastq\" : ''\n    def endedness = meta.single_end ? \"$reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.3.ebwt\" | sed 's/.3.ebwt//'`\n    bowtie \\\\\n        --threads $task.cpus \\\\\n        --sam \\\\\n        -x \\$INDEX \\\\\n        -q \\\\\n        $unaligned \\\\\n        $args \\\\\n        $endedness \\\\\n        2> ${prefix}.out \\\\\n        | samtools view $args2 -@ $task.cpus -bS -o ${prefix}.bam -\n\n    if [ -f ${prefix}.unmapped.fastq ]; then\n        gzip ${prefix}.unmapped.fastq\n    fi\n    if [ -f ${prefix}.unmapped_1.fastq ]; then\n        gzip ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie: \\$(echo \\$(bowtie --version 2>&1) | sed 's/^.*bowtie-align-s version //; s/ .*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["goodwright/imaps-nf/goodwright__imaps-nf/BOWTIE_ALIGN", "nf-core/modules/nf-core__modules/BOWTIE_ALIGN"], "list_wf_names": ["nf-core/modules", "goodwright/imaps-nf"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process sourmash_sig_merge {\n    tag \"${sig_id}\"\n    label \"low_memory\"\n    publishDir \"${params.outdir}/sketches_merged/${sketch_id}\", mode: \"${params.publish_dir_mode}\",\n        saveAs: {filename ->\n            if (filename.indexOf(\".csv\") > 0) \"description/$filename\"\n            else if (filename.indexOf(\".sig\") > 0) \"sigs/$filename\"\n            else null\n        }\n\n    input:\n    set val(fasta_ids), val(cell_id), val(is_aligned), val(sketch_id), val(moltypes), val(ksizes), file(sigs) from ch_sourmash_sketches_to_merge\n\n    output:\n    file(csv) into ch_sourmash_sig_describe_merged\n    set val(cell_id), val(sketch_id), val(moltypes), val(ksizes), file(output_sig) into ch_sourmash_sketches_merged, ch_sourmash_sketches_merged_to_view, ch_sourmash_sketches_merged_for_moltypes_ksizes\n\n    script:\n                                                                                               \n    sig_id = \"${cell_id}---${sketch_id}\"\n    csv = \"${sig_id}.csv\"\n    output_sig = \"${sig_id}.sig\"\n    \"\"\"\n    merge_rename_sigs.py \\\\\n        --ksizes ${ksizes} \\\\\n        --moltypes ${moltypes} \\\\\n        --name '${cell_id}' \\\\\n        --outsig ${output_sig} \\\\\n        ${sigs}\n\n    # Add csv showing number of hashes at each ksize\n    sourmash sig describe --csv ${csv} ${output_sig}\n    \"\"\"\n\n  }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/sourmash_sig_merge"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 2, "tools": ["SAMtools", "preseq"], "nb_own": 2, "list_own": ["nf-core", "robinfchan"], "nb_wf": 2, "list_wf": ["methylseq", "bisulfite_align_nf"], "list_contrib": ["alesssia", "phue", "alneberg", "ewels", "maxulysse", "FelixKrueger", "colindaven", "nf-core-bot", "pditommaso", "robsyme", "noirot", "nvk747", "mashehu", "Hammarn", "gdevailly", "sven1103", "apeltzer", "robinfchan", "drpatelh"], "nb_contrib": 19, "codes": [" process preseq {\n        if (params.custom_container) container \"${params.custom_container}\"\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/preseq\", mode: 'copy', overwrite: true\n\n        input:\n        set val(name), file(bam) from ch_bam_for_preseq\n\n        output:\n        file \"${bam.baseName}.ccurve.txt\" into preseq_results\n\n        script:\n        def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n        def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n        \"\"\"\n        samtools sort $bam \\\\\n            -@ ${task.cpus} $sort_mem \\\\\n            -o ${bam.baseName}.sorted.bam\n        preseq lc_extrap -v -B ${bam.baseName}.sorted.bam -o ${bam.baseName}.ccurve.txt\n        \"\"\"\n    }", "\nprocess preseq {\n    tag \"$name\"\n    publishDir \"${params.outdir}/preseq\", mode: params.publish_dir_mode\n\n    input:\n    set val(name), file(bam) from ch_bam_for_preseq\n\n    output:\n    file \"${bam.baseName}.ccurve.txt\" into preseq_results\n\n    script:\n    def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n    def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n    \"\"\"\n    samtools sort $bam \\\\\n        -@ ${task.cpus} $sort_mem \\\\\n        -o ${bam.baseName}.sorted.bam\n    preseq lc_extrap -v -B ${bam.baseName}.sorted.bam -o ${bam.baseName}.ccurve.txt\n    \"\"\"\n}"], "list_proc": ["robinfchan/bisulfite_align_nf/robinfchan__bisulfite_align_nf/preseq", "nf-core/methylseq/nf-core__methylseq/preseq"], "list_wf_names": ["robinfchan/bisulfite_align_nf", "nf-core/methylseq"]}, {"nb_reuse": 7, "tools": ["FeatureCounts"], "nb_own": 5, "list_own": ["erikrikarddaniel", "raygozag", "nf-core", "mahesh-panchal", "harleenduggal"], "nb_wf": 6, "list_wf": ["RNASEQ", "magmap", "test_nfcore_workflow_chain", "modules", "nfcore-rnaseq", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 146, "codes": ["process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0\"\n    } else {\n        container \"quay.io/biocontainers/subread:2.0.1--hed695b0_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    script:\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SUBREAD_FEATURECOUNTS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SUBREAD_FEATURECOUNTS", "nf-core/rnaseq/nf-core__rnaseq/SUBREAD_FEATURECOUNTS", "raygozag/rnaseq/raygozag__rnaseq/SUBREAD_FEATURECOUNTS", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SUBREAD_FEATURECOUNTS", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SUBREAD_FEATURECOUNTS", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/SUBREAD_FEATURECOUNTS"], "list_wf_names": ["erikrikarddaniel/magmap", "raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 6, "tools": ["RAxML-NG"], "nb_own": 3, "list_own": ["ksumngs", "nf-core", "CDCgov"], "nb_wf": 3, "list_wf": ["nf-modules", "modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process RAXMLNG_SUPPORT {\n    tag \"$tree\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path tree\n    path bootstraps\n\n    output:\n    path \"*.raxml.support\", emit: support\n    path \"versions.yml\"   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        --support \\\\\n        --threads auto{${task.cpus}} \\\\\n        --tree ${tree} \\\\\n        --bs-trees ${bootstraps} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process RAXMLNG_PARSE {\n    tag \"$msa\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path(msa)\n\n    output:\n    path \"*.raxml.rba\" , emit: rba\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        --parse \\\\\n        --threads auto{${task.cpus}} \\\\\n        --msa ${msa} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process RAXMLNG_SEARCH {\n    tag \"$msa\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path msa\n\n    output:\n    path \"*.raxml.bestTree\", emit: best_tree\n    path \"versions.yml\"    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    \"\"\"\n    raxml-ng \\\\\n        --threads auto{${task.cpus}} \\\\\n        --workers auto \\\\\n        --msa ${msa} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process RAXMLNG_BOOTSTRAP {\n    tag \"$msa\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path msa\n\n    output:\n    path \"*.raxml.bootstraps\", emit: bootstraps\n    path \"versions.yml\"      , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        --bootstrap \\\\\n        --threads auto{${task.cpus}} \\\\\n        --workers auto \\\\\n        --msa ${msa} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process RAXMLNG {\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::raxml-ng=1.0.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.0.3--h32fcf60_0' :\n        'quay.io/biocontainers/raxml-ng:1.0.3--h32fcf60_0' }\"\n\n    input:\n    path alignment\n\n    output:\n    path \"*.raxml.bestTree\", emit: phylogeny\n    path \"*.raxml.support\" , optional:true, emit: phylogeny_bootstrapped\n    path \"versions.yml\"    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        $args \\\\\n        --msa $alignment \\\\\n        --threads $task.cpus \\\\\n        --prefix output\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process RAXMLNG {\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::raxml-ng=1.0.3' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.0.3--h32fcf60_0' :\n        'quay.io/biocontainers/raxml-ng:1.0.3--h32fcf60_0' }\"\n\n    input:\n    path alignment\n\n    output:\n    path \"*.raxml.bestTree\", emit: phylogeny\n    path \"*.raxml.support\" , optional:true, emit: phylogeny_bootstrapped\n    path \"versions.yml\"    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        $args \\\\\n        --msa $alignment \\\\\n        --threads $task.cpus \\\\\n        --prefix output\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ksumngs/nf-modules/ksumngs__nf-modules/RAXMLNG_SUPPORT", "ksumngs/nf-modules/ksumngs__nf-modules/RAXMLNG_PARSE", "ksumngs/nf-modules/ksumngs__nf-modules/RAXMLNG_SEARCH", "ksumngs/nf-modules/ksumngs__nf-modules/RAXMLNG_BOOTSTRAP", "nf-core/modules/nf-core__modules/RAXMLNG", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/RAXMLNG"], "list_wf_names": ["CDCgov/mycosnp-nf", "ksumngs/nf-modules", "nf-core/modules"]}, {"nb_reuse": 22, "tools": ["BCFtools"], "nb_own": 4, "list_own": ["vincenthhu", "mahesh-panchal", "nf-core", "CDCgov"], "nb_wf": 7, "list_wf": ["raredisease", "mycosnp-nf", "viralrecon", "gwas", "test_nfcore_workflow_chain", "modules", "nf-core-westest"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "leebrian", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 117, "codes": ["process BCFTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    tuple val(meta), path(\"*.tbi\"), optional:true, emit: tbi\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    \"\"\"\n    bcftools \\\\\n        index \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcfs), path(tbis)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools merge -Oz \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        *.vcf.gz\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(index)\n    path(regions)\n    path(targets)\n    path(samples)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def regions_file  = regions ? \"--regions-file ${regions}\" : \"\"\n    def targets_file = targets ? \"--targets-file ${targets}\" : \"\"\n    def samples_file =  samples ? \"--samples-file ${samples}\" : \"\"\n    \"\"\"\n    bcftools view \\\\\n        --output ${prefix}.vcf.gz \\\\\n        ${regions_file} \\\\\n        ${targets_file} \\\\\n        ${samples_file} \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_QUERY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    path regions\n    path targets\n    path samples\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def regions_file = regions ? \"--regions-file ${regions}\" : \"\"\n    def targets_file = targets ? \"--targets-file ${targets}\" : \"\"\n    def samples_file =  samples ? \"--samples-file ${samples}\" : \"\"\n    \"\"\"\n    bcftools query \\\\\n        --output ${prefix}.txt \\\\\n        $regions_file \\\\\n        $targets_file \\\\\n        $samples_file \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_QUERY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    path regions\n    path targets\n    path samples\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def regions_file = regions ? \"--regions-file ${regions}\" : \"\"\n    def targets_file = targets ? \"--targets-file ${targets}\" : \"\"\n    def samples_file =  samples ? \"--samples-file ${samples}\" : \"\"\n    \"\"\"\n    bcftools query \\\\\n        --output ${prefix}.txt \\\\\n        $regions_file \\\\\n        $targets_file \\\\\n        $samples_file \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_NORM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools norm \\\\\n        --fasta-ref ${fasta} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_NORM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools norm \\\\\n        --fasta-ref ${fasta} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_FILTER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools filter \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_FILTER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools filter \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_QUERY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    path regions\n    path targets\n    path samples\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def regions_file = regions ? \"--regions-file ${regions}\" : \"\"\n    def targets_file = targets ? \"--targets-file ${targets}\" : \"\"\n    def samples_file =  samples ? \"--samples-file ${samples}\" : \"\"\n    \"\"\"\n    bcftools query \\\\\n        --output ${prefix}.txt \\\\\n        $regions_file \\\\\n        $targets_file \\\\\n        $samples_file \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_CONCAT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcfs)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools concat \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcfs}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_REHEADER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path fai\n    path header\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def update_sequences = fai ? \"-f $fai\" : \"\"\n    def new_header       = header ? \"-h $header\" : \"\"\n    \"\"\"\n    bcftools \\\\\n        reheader \\\\\n        $update_sequences \\\\\n        $new_header \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        -o ${prefix}.vcf.gz \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0':\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools \\\\\n        sort \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_QUERY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi)\n    path regions\n    path targets\n    path samples\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def regions_file = regions ? \"--regions-file ${regions}\" : \"\"\n    def targets_file = targets ? \"--targets-file ${targets}\" : \"\"\n    def samples_file =  samples ? \"--samples-file ${samples}\" : \"\"\n    \"\"\"\n    bcftools query \\\\\n        --output ${prefix}.txt \\\\\n        $regions_file \\\\\n        $targets_file \\\\\n        $samples_file \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(index)\n    path(regions)\n    path(targets)\n    path(samples)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def regions_file  = regions ? \"--regions-file ${regions}\" : \"\"\n    def targets_file = targets ? \"--targets-file ${targets}\" : \"\"\n    def samples_file =  samples ? \"--samples-file ${samples}\" : \"\"\n    \"\"\"\n    bcftools view \\\\\n        --output ${prefix}.vcf.gz \\\\\n        ${regions_file} \\\\\n        ${targets_file} \\\\\n        ${samples_file} \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_FILTER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools filter \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcfs), path(tbis)\n\n    output:\n    tuple val(meta), path(\"*.gz\"), emit: vcf\n    path  \"versions.yml\"         , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools merge -Oz \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        *.vcf.gz\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_NORM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools norm \\\\\n        --fasta-ref ${fasta} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    tuple val(meta), path(\"*.tbi\"), optional:true, emit: tbi\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    \"\"\"\n    bcftools \\\\\n        index \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        $vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_ISEC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcfs), path(tbis)\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools isec  \\\\\n        $args \\\\\n        -p $prefix \\\\\n        *.vcf.gz\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_NORM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools norm \\\\\n        --fasta-ref ${fasta} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process BCFTOOLS_NORM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools norm \\\\\n        --fasta-ref ${fasta} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BCFTOOLS_INDEX", "nf-core/modules/nf-core__modules/BCFTOOLS_MERGE", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BCFTOOLS_VIEW", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BCFTOOLS_QUERY", "nf-core/viralrecon/nf-core__viralrecon/BCFTOOLS_QUERY", "nf-core/modules/nf-core__modules/BCFTOOLS_NORM", "nf-core/viralrecon/nf-core__viralrecon/BCFTOOLS_NORM", "nf-core/viralrecon/nf-core__viralrecon/BCFTOOLS_FILTER", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BCFTOOLS_FILTER", "nf-core/modules/nf-core__modules/BCFTOOLS_QUERY", "nf-core/modules/nf-core__modules/BCFTOOLS_CONCAT", "nf-core/modules/nf-core__modules/BCFTOOLS_REHEADER", "nf-core/modules/nf-core__modules/BCFTOOLS_SORT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BCFTOOLS_QUERY", "nf-core/modules/nf-core__modules/BCFTOOLS_VIEW", "nf-core/modules/nf-core__modules/BCFTOOLS_FILTER", "nf-core/gwas/nf-core__gwas/BCFTOOLS_MERGE", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BCFTOOLS_NORM", "nf-core/modules/nf-core__modules/BCFTOOLS_INDEX", "nf-core/modules/nf-core__modules/BCFTOOLS_ISEC", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/BCFTOOLS_NORM", "nf-core/raredisease/nf-core__raredisease/BCFTOOLS_NORM"], "list_wf_names": ["nf-core/gwas", "vincenthhu/nf-core-westest", "nf-core/raredisease", "nf-core/modules", "nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess hostremoval_input_fastq {\n    label 'mc_medium'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/hostremoved_fastq\", mode: params.publish_dir_mode\n\n    when: \n    params.hostremoval_input_fastq\n\n    input: \n    tuple samplename, libraryid, seqtype, organism, strandedness, udg, file(r1), file(r2), file(bam), file(bai) from ch_synced_for_hostremovalfastq\n\n    output:\n    tuple samplename, libraryid, seqtype, organism, strandedness, udg, file(\"*.fq.gz\") into ch_output_from_hostremovalfastq\n\n    script:\n    if ( seqtype == 'SE' ) {\n        out_fwd = bam.baseName+'.hostremoved.fq.gz'\n        \"\"\"\n        samtools index $bam\n        extract_map_reads.py $bam ${r1} -m ${params.hostremoval_mode} -of $out_fwd -p ${task.cpus}\n        \"\"\"\n    } else {\n        out_fwd = bam.baseName+'.hostremoved.fwd.fq.gz'\n        out_rev = bam.baseName+'.hostremoved.rev.fq.gz'\n        \"\"\"\n        samtools index $bam\n        extract_map_reads.py $bam ${r1} -rev ${r2} -m  ${params.hostremoval_mode} -of $out_fwd -or $out_rev -p ${task.cpus}\n        \"\"\" \n    }\n    \n}"], "list_proc": ["nf-core/eager/nf-core__eager/hostremoval_input_fastq"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["MSI"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MSISENSORPRO_MSI_SOMATIC {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::msisensor-pro=1.2.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/msisensor-pro:1.2.0--hfc31af2_0' :\n        'quay.io/biocontainers/msisensor-pro:1.2.0--hfc31af2_0' }\"\n\n    input:\n    tuple val(meta), path(normal), path(normal_index), path(tumor), path(tumor_index), path(intervals)\n    path (fasta)\n    path (msisensor_scan)\n\n    output:\n    tuple val(meta), path(\"${prefix}\")         , emit: output_report\n    tuple val(meta), path(\"${prefix}_dis\")     , emit: output_dis\n    tuple val(meta), path(\"${prefix}_germline\"), emit: output_germline\n    tuple val(meta), path(\"${prefix}_somatic\") , emit: output_somatic\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def fasta = fasta ? \"-g ${fasta}\" : \"\"\n    def intervals = intervals ? \" -e ${intervals} \" : \"\"\n    \"\"\"\n    msisensor-pro \\\\\n        msi \\\\\n        -d ${msisensor_scan} \\\\\n        -n ${normal} \\\\\n        -t ${tumor} \\\\\n        ${fasta} \\\\\n        -o $prefix \\\\\n        -b ${task.cpus} \\\\\n        ${intervals} \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        msisensor-pro: \\$(msisensor-pro 2>&1 | sed -nE 's/Version:\\\\sv([0-9]\\\\.[0-9])/\\\\1/ p')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MSISENSORPRO_MSI_SOMATIC"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BaMM"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess bam_trim {\n    label 'mc_small'\n    tag \"${libraryid}\" \n    publishDir \"${params.outdir}/trimmed_bam\", mode: params.publish_dir_mode\n\n    when: params.run_trim_bam\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_bamutils_decision.totrim\n\n    output: \n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*.trimmed.bam\"), file(\"*.trimmed.bam.{bai,csi}\") into ch_trimmed_from_bamutils\n\n    script:\n    def softclip = params.bamutils_softclip ? '-c' : '' \n    def size = params.large_ref ? '-c' : ''\n    def left_clipping = strandedness == \"double\" ? (udg == \"half\" ? \"${params.bamutils_clip_double_stranded_half_udg_left}\" : \"${params.bamutils_clip_double_stranded_none_udg_left}\") : (udg == \"half\" ? \"${params.bamutils_clip_single_stranded_half_udg_left}\" : \"${params.bamutils_clip_single_stranded_none_udg_left}\")\n    def right_clipping = strandedness == \"double\" ? (udg == \"half\" ? \"${params.bamutils_clip_double_stranded_half_udg_right}\" : \"${params.bamutils_clip_double_stranded_none_udg_right}\") : (udg == \"half\" ? \"${params.bamutils_clip_single_stranded_half_udg_right}\" : \"${params.bamutils_clip_single_stranded_none_udg_right}\")\n\n                                                                                                                            \n                                                                                                                               \n    \"\"\"\n    bam trimBam $bam tmp.bam -L ${left_clipping} -R ${right_clipping} ${softclip}\n    samtools sort -@ ${task.cpus} tmp.bam -o ${libraryid}.trimmed.bam \n    samtools index ${libraryid}.trimmed.bam ${size}\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/bam_trim"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 5, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "rnavar"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor"], "nb_contrib": 107, "codes": ["process GATK4_INTERVALLISTTOBED {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IntervalListToBed] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" IntervalListToBed \\\\\n        --INPUT $intervals \\\\\n        --OUTPUT ${prefix}.bed \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_SAMTOFASTQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path('*.fastq.gz'), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def output = meta.single_end ? \"--FASTQ ${prefix}.fastq.gz\" : \"--FASTQ ${prefix}_1.fastq.gz --SECOND_END_FASTQ ${prefix}_2.fastq.gz\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK SamToFastq] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" SamToFastq \\\\\n        --INPUT $bam \\\\\n        $output \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.fastq.gz\n    touch ${prefix}_1.fastq.gz\n    touch ${prefix}_2.fastq.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_REVERTSAM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK RevertSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" RevertSam \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.reverted.bam \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.reverted.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_MERGEVCFS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path  dict\n\n    output:\n    tuple val(meta), path('*.vcf.gz'), emit: vcf\n    path  \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = vcf.collect{ \"--INPUT $it\"}.join(' ')\n    def reference_command = dict ? \"--SEQUENCE_DICTIONARY $dict\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK MergeVcfs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" MergeVcfs \\\\\n        $input_list \\\\\n        --OUTPUT ${prefix}.vcf.gz \\\\\n        $reference_command \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_MERGEVCFS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path  dict\n\n    output:\n    tuple val(meta), path('*.vcf.gz'), emit: vcf\n    path  \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = vcf.collect{ \"--INPUT $it\"}.join(' ')\n    def reference_command = dict ? \"--SEQUENCE_DICTIONARY $dict\" : \"\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK MergeVcfs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" MergeVcfs \\\\\n        $input_list \\\\\n        --OUTPUT ${prefix}.vcf.gz \\\\\n        $reference_command \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_INTERVALLISTTOBED", "nf-core/modules/nf-core__modules/GATK4_SAMTOFASTQ", "nf-core/modules/nf-core__modules/GATK4_REVERTSAM", "nf-core/modules/nf-core__modules/GATK4_MERGEVCFS", "nf-core/rnavar/nf-core__rnavar/GATK4_MERGEVCFS"], "list_wf_names": ["nf-core/rnavar", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["FastTree"], "nb_own": 2, "list_own": ["ajodeh-juma", "nf-core"], "nb_wf": 2, "list_wf": ["bactmap", "rvfvtyping"], "list_contrib": ["ajodeh-juma", "alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess FASTTREE {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::fasttree=2.1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fasttree:2.1.10--hb4d813b_5\"\n    } else {\n        container \"quay.io/biocontainers/fasttree:2.1.10--hb4d813b_5\"\n    }\n\n    input:\n    path alignment\n\n    output:\n    path \"*.tre\",         emit: phylogeny\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    fasttree \\\\\n        $options.args \\\\\n        -log fasttree_phylogeny.tre.log \\\\\n        -nt $alignment \\\\\n        > fasttree_phylogeny.tre\n\n    echo \\$(fasttree -help 2>&1) | head -1  | sed 's/^FastTree \\\\([0-9\\\\.]*\\\\) .*\\$/\\\\1/' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess FASTTREE {\n    label 'process_medium'\n    memory { 1.GB * task.attempt }\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::fasttree=2.1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fasttree:2.1.10--h516909a_4\"\n    } else {\n        container \"quay.io/biocontainers/fasttree:2.1.10--h516909a_4\"\n    }\n\n    input:\n    path alignment\n\n    output:\n    path \"*.tre\",         emit: phylogeny\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    fasttree \\\\\n        $options.args \\\\\n        -log fasttree_phylogeny.tre.log \\\\\n        -nt $alignment \\\\\n        > fasttree_phylogeny.tre\n\n    echo \\$(fasttree -help 2>&1) | head -1  | sed 's/^FastTree \\\\([0-9\\\\.]*\\\\) .*\\$/\\\\1/' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ajodeh-juma/rvfvtyping/ajodeh-juma__rvfvtyping/FASTTREE", "nf-core/bactmap/nf-core__bactmap/FASTTREE"], "list_wf_names": ["ajodeh-juma/rvfvtyping", "nf-core/bactmap"]}, {"nb_reuse": 2, "tools": ["BCFtools"], "nb_own": 2, "list_own": ["ajodeh-juma", "nf-core"], "nb_wf": 2, "list_wf": ["bactmap", "viclara"], "list_contrib": ["ajodeh-juma", "alexandregilardet", "thanhleviet", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess BCFTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.gz\")      , emit: vcf\n    tuple val(meta), path(\"*.tbi\")     , emit: tbi\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    echo \"${meta.id}\" > sample_name.list\n    bcftools mpileup \\\\\n        --fasta-ref $fasta \\\\\n        $options.args \\\\\n        $bam \\\\\n        | bcftools call --output-type v $options.args2 \\\\\n        | bcftools reheader --samples sample_name.list \\\\\n        | bcftools view --output-file ${prefix}.vcf.gz --output-type z $options.args3\n    tabix -p vcf -f ${prefix}.vcf.gz\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.gz\")      , emit: vcf\n    tuple val(meta), path(\"*.tbi\")     , emit: tbi\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    echo \"${meta.id}\" > sample_name.list\n    bcftools mpileup \\\\\n        --fasta-ref $fasta \\\\\n        $options.args \\\\\n        $bam \\\\\n        | bcftools call --output-type v $options.args2 \\\\\n        | bcftools reheader --samples sample_name.list \\\\\n        | bcftools view --output-file ${prefix}.vcf.gz --output-type z $options.args3\n    tabix -p vcf -f ${prefix}.vcf.gz\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ajodeh-juma/viclara/ajodeh-juma__viclara/BCFTOOLS_MPILEUP", "nf-core/bactmap/nf-core__bactmap/BCFTOOLS_MPILEUP"], "list_wf_names": ["ajodeh-juma/viclara", "nf-core/bactmap"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process sourmash_compare_sketches {\n                                              \n    tag \"${compare_id}\"\n    publishDir \"${params.outdir}/compare_sketches\", mode: 'copy'\n\n    input:\n                                                                   \n    set val(molecule), file(\"*.sig\"), val(ksize) from ch_sourmash_sketches_to_compare\n\n    output:\n    file(csv)\n\n    script:\n    compare_id = \"${molecule}__k-${ksize}\"\n    processes = \"--processes ${task.cpus}\"\n    csv = \"similarities__${compare_id}.csv\"\n    \"\"\"\n    sourmash compare \\\\\n          --ksize ${ksize} \\\\\n          --${molecule} \\\\\n          --csv ${csv} \\\\\n          ${processes} \\\\\n          --traverse-directory .\n    # Use --traverse-directory instead of all the files explicitly to avoid\n    # \"too many arguments\" error for bash when there are lots of samples\n    \"\"\"\n\n  }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/sourmash_compare_sketches"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["Kraken"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": [" process kraken {\n        tag { \"Running Kraken on \" + sample_id }\n        publishDir \"${params.outdir}/${sample_id}/\", mode: 'copy'\n\n        input:\n            set sample_id, file(fq1), file(fq2) from fastq_for_kraken_ch\n        output:\n            file(\"${sample_id}_kraken.report\")\n        script:\n            \"\"\"\n            kraken --threads ${task.cpus} --preload --db ${kraken_db} \\\n              -paired ${fq1} ${fq2} > kraken.out;\n            # do not gzip! otherwise kraken-report happily runs (with some warnings) and produces rubbish results\n            kraken-report --db ${kraken_db} kraken.out > ${sample_id}_kraken.report\n            \"\"\"\n    }"], "list_proc": ["nf-core/vipr/nf-core__vipr/kraken"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process SOURMASH_SKETCH {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::sourmash=4.2.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sourmash:4.2.4--hdfd78af_0':\n        'quay.io/biocontainers/sourmash:4.2.4--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(sequence)\n\n    output:\n    tuple val(meta), path(\"*.sig\"), emit: signatures\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: \"dna --param-string 'scaled=1000,k=31'\"\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    sourmash sketch \\\\\n        $args \\\\\n        --merge '${prefix}' \\\\\n        --output '${prefix}.sig' \\\\\n        $sequence\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sourmash: \\$(echo \\$(sourmash --version 2>&1) | sed 's/^sourmash //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SOURMASH_SKETCH"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 16, "tools": ["BCFtools"], "nb_own": 10, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "vladsaveliev", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai"], "nb_wf": 16, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek", "cawdor"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "vladsaveliev", "lescai", "cgpu", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 25, "codes": ["\nprocess BcftoolsStats {\n\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess RunBcftoolsStats {\n  tag {vcf}\n\n  publishDir \"${params.outDir}/Reports/BCFToolsStats\", mode: params.publishDirMode\n\n  input:\n    set variantCaller, file(vcf) from vcfForBCFtools\n\n  output:\n  file (\"*_stats.txt\") into bcfReport\n\n  script:\n  \"\"\"\n  bcftools stats ${vcf} > ${vcf.simpleName}.bcftools_stats.txt\n  \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BcftoolsStats", "nf-core/sarek/nf-core__sarek/BcftoolsStats", "cgpu/pgp-chronek/cgpu__pgp-chronek/BcftoolsStats", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/BcftoolsStats", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/BcftoolsStats", "cgpu/sarek-mirror/cgpu__sarek-mirror/BcftoolsStats", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BcftoolsStats", "rmoran7/custom_sarek/rmoran7__custom_sarek/BcftoolsStats", "vladsaveliev/cawdor/vladsaveliev__cawdor/RunBcftoolsStats", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/BcftoolsStats", "rmoran7/dx_sarek/rmoran7__dx_sarek/BcftoolsStats", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BcftoolsStats", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/BcftoolsStats", "cgpu/haplosarek/cgpu__haplosarek/BcftoolsStats", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/BcftoolsStats", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/BcftoolsStats"], "list_wf_names": ["cgpu/pgp-chronek", "vladsaveliev/cawdor", "cgpu/PGP-UK-sarek", "UMCUGenetics/sarek_ubec", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/haplosarek", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-mirror-cache"]}, {"nb_reuse": 2, "tools": ["Rgin"], "nb_own": 2, "list_own": ["bactopia", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "bactopia"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "Accio", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "fmaguire", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "TGotwig"], "nb_contrib": 108, "codes": ["process RGI_MAIN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::rgi=5.2.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/rgi:5.2.1--pyha8f3691_2':\n        'quay.io/biocontainers/rgi:5.2.1--pyha8f3691_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.json\"), emit: json\n    tuple val(meta), path(\"*.txt\") , emit: tsv\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    rgi \\\\\n        main \\\\\n        $args \\\\\n        --num_threads $task.cpus \\\\\n        --output_file $prefix \\\\\n        --input_sequence $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rgi: \\$(rgi main --version)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess RGI_MAIN {\n    tag \"$meta.id\"\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/rgi:5.2.1--pyha8f3691_2' :\n        'quay.io/biocontainers/rgi:5.2.1--pyha8f3691_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.json\"), emit: json\n    tuple val(meta), path(\"*.txt\") , emit: tsv\n    path \"*.{log,err}\"             , emit: logs, optional: true\n    path \".command.*\"              , emit: nf_logs\n    path \"versions.yml\"            , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    rgi \\\\\n        main \\\\\n        $options.args \\\\\n        --clean \\\\\n        --data wgs \\\\\n        --num_threads $task.cpus \\\\\n        --output_file $prefix \\\\\n        --input_sequence $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rgi: \\$(rgi main --version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/RGI_MAIN", "bactopia/bactopia/bactopia__bactopia/RGI_MAIN"], "list_wf_names": ["bactopia/bactopia", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": [" process makeBT2Index {\n    label 'mc_medium'\n    tag \"${fasta}\"\n    publishDir path: \"${params.outdir}/reference_genome/bt2_index\", mode: params.publish_dir_mode, saveAs: { filename -> \n            if (params.save_reference) filename \n            else if(!params.save_reference && filename == \"where_are_my_files.txt\") filename\n            else null\n    }\n\n    input:\n    path fasta from ch_fasta_for_bt2index\n    path where_are_my_files\n\n    output:\n    path \"BT2Index\" into (bt2_index)\n    path \"where_are_my_files.txt\"\n\n    script:\n    \"\"\"\n    bowtie2-build --threads ${task.cpus} $fasta $fasta\n    mkdir BT2Index && mv ${fasta}* BT2Index\n    \"\"\"\n    }"], "list_proc": ["nf-core/eager/nf-core__eager/makeBT2Index"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess samtools_flagstat {\n    label 'sc_tiny'\n    tag \"$libraryid\"\n    publishDir \"${params.outdir}/samtools/stats\", mode: params.publish_dir_mode\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_seqtypemerged_for_samtools_flagstat\n\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*stats\") into ch_flagstat_for_multiqc,ch_flagstat_for_endorspy\n\n    script:\n    \"\"\"\n    samtools flagstat $bam > ${libraryid}_flagstat.stats\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/samtools_flagstat"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process PMDTOOLS_FILTER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::pmdtools=0.60\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pmdtools:0.60--hdfd78af_5' :\n        'quay.io/biocontainers/pmdtools:0.60--hdfd78af_5' }\"\n\n    input:\n    tuple val(meta), path(bam), path (bai)\n    val(threshold)\n    path(reference)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args3 = task.ext.args3 ?: ''\n    def split_cpus = Math.floor(task.cpus/2)\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"[pmdtools/filter] Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n                                                                        \n    \"\"\"\n    samtools \\\\\n        calmd \\\\\n        $bam \\\\\n        $reference \\\\\n        $args \\\\\n        -@ ${split_cpus} \\\\\n    | pmdtools \\\\\n        --threshold $threshold \\\\\n        --header \\\\\n        $args2 \\\\\n    | samtools \\\\\n        view \\\\\n        $args3 \\\\\n        -Sb \\\\\n        - \\\\\n        -@ ${split_cpus} \\\\\n        -o ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pmdtools: \\$( pmdtools --version | cut -f2 -d ' ' | sed 's/v//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PMDTOOLS_FILTER"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["ClonalFrameML"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process CLONALFRAMEML {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::clonalframeml=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/clonalframeml:1.12--h7d875b9_1' :\n        'quay.io/biocontainers/clonalframeml:1.12--h7d875b9_1' }\"\n\n    input:\n    tuple val(meta), path(newick), path(msa)\n\n    output:\n    tuple val(meta), path(\"*.emsim.txt\")                   , emit: emsim, optional: true\n    tuple val(meta), path(\"*.em.txt\")                      , emit: em\n    tuple val(meta), path(\"*.importation_status.txt\")      , emit: status\n    tuple val(meta), path(\"*.labelled_tree.newick\")        , emit: newick\n    tuple val(meta), path(\"*.ML_sequence.fasta\")           , emit: fasta\n    tuple val(meta), path(\"*.position_cross_reference.txt\"), emit: pos_ref\n    path \"versions.yml\"                                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    ClonalFrameML \\\\\n        $newick \\\\\n        <(gzip -cdf $msa) \\\\\n        $prefix \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        clonalframeml: \\$( echo \\$(ClonalFrameML -version 2>&1) | sed 's/^.*ClonalFrameML v//' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/CLONALFRAMEML"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 22, "tools": ["GATK"], "nb_own": 15, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "nibscbioinformatics", "melnel000", "cgpu", "UCL-BLIC", "lifebit-ai", "GMS6804-master", "javaidm", "ryanlayerlab"], "nb_wf": 22, "list_wf": ["sarek_ubec", "layer_lab_chco", "Sarek_CBIO", "layer_lab_vc", "germline_somatic", "dx_sarek", "humgen", "haplosarek", "sarek-mirror-cache", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "Sarek", "sarek-mirror", "pgp-chronek", "test_nextflow_sarek", "sarek-genomechronicler", "saw.sarek", "custom_sarek", "sarek", "GenomeChronicler-Sarek-nf", "layer_lab_caw", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "pallolason", "szilvajuhos", "nf-core-bot", "Sebastian-D", "bleazard", "jfnavarro", "pditommaso", "jackmo375", "olgabot", "chelauk", "marcelm", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "J35P312", "ffmmulder", "rmoran7", "jongtaek-kim", "lescai", "cgpu", "apeltzer", "waffle-iron", "MSBradshaw", "jtk622", "davidmasp"], "nb_contrib": 35, "codes": ["\nprocess BuildDict {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta.baseName}.dict\") into dictBuilt\n\n  when: !(params.dict) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${fasta} \\\n  --OUTPUT ${fasta.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta.baseName}.dict\")\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    label 'cpus_1'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta.baseName}.dict\") into dictBuilt\n\n  when: !(params.dict) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${fasta} \\\n  --OUTPUT ${fasta.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildReferenceIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaReference\n\n  output:\n    file(\"*.dict\") into ch_referenceIndex\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${f_reference} \\\n  --OUTPUT ${f_reference.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta.baseName}.dict\") into dictBuilt\n\n  when: !(params.dict) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${fasta} \\\n  --OUTPUT ${fasta.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n    label 'cpus_1'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta.baseName}.dict\")\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildReferenceIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaReference\n\n  output:\n    file(\"*.dict\") into ch_referenceIndex\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${f_reference} \\\n  --OUTPUT ${f_reference.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta.baseName}.dict\") into dictBuilt\n\n  when: !(params.dict) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${fasta} \\\n  --OUTPUT ${fasta.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BuildDict {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta.baseName}.dict\") into dictBuilt\n\n  when: !(params.dict) && params.fasta && !('annotate' in step)\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${fasta} \\\n  --OUTPUT ${fasta.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildReferenceIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: params.publishDirMode\n\n  input:\n    file(f_reference) from ch_fastaReference\n\n  output:\n    file(\"*.dict\") into ch_referenceIndex\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${f_reference} \\\n  --OUTPUT ${f_reference.baseName}.dict\n  \"\"\"\n}", "\nprocess BuildDict {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta.baseName}.dict\")\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}"], "list_proc": ["cgpu/sarek-mirror/cgpu__sarek-mirror/BuildDict", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/BuildDict", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/BuildDict", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BuildDict", "cgpu/pgp-chronek/cgpu__pgp-chronek/BuildDict", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BuildDict", "melnel000/Sarek_CBIO/melnel000__Sarek_CBIO/BuildReferenceIndex", "nf-core/sarek/nf-core__sarek/BuildDict", "rmoran7/custom_sarek/rmoran7__custom_sarek/BuildDict", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/BuildDict", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BuildDict", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/BuildDict", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/BuildDict", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/BuildDict", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/BuildDict", "GMS6804-master/Sarek/GMS6804-master__Sarek/BuildReferenceIndex", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/BuildDict", "rmoran7/dx_sarek/rmoran7__dx_sarek/BuildDict", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/BuildDict", "cgpu/haplosarek/cgpu__haplosarek/BuildDict", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/BuildReferenceIndex", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/BuildDict"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "GMS6804-master/Sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "ryanlayerlab/layer_lab_chco", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "cgpu/sarek-mirror", "sickle-in-africa/saw.sarek", "cgpu/haplosarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/sarek-mirror-cache", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "javaidm/layer_lab_vc", "cgpu/pgp-chronek", "UCL-BLIC/Sarek_v2.3.FIX1", "ryanlayerlab/layer_lab_caw", "melnel000/Sarek_CBIO", "rmoran7/dx_sarek", "nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess BUSCO_PLOT {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::busco=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/busco:5.1.0--py_1\"\n    } else {\n        container \"quay.io/biocontainers/busco:5.1.0--py_1\"\n    }\n\n    input:\n    tuple val(meta), path(summaries)\n\n    output:\n    path(\"${meta.assembler}-${meta.id}.*.busco_figure.png\") , optional:true, emit: png\n    path(\"${meta.assembler}-${meta.id}.*.busco_figure.R\")   , optional:true, emit: rscript\n    path '*.version.txt'                                                   , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    if [ -n \"${summaries}\" ]\n    then\n        # replace dots in bin names within summary file names by underscores\n        # currently (BUSCO v5.1.0) generate_plot.py does not allow further dots\n        for sum in ${summaries}; do\n            [[ \\${sum} =~ short_summary.([_[:alnum:]]+).([_[:alnum:]]+).${meta.assembler}-${meta.id}.(.+).txt ]];\n            mode=\\${BASH_REMATCH[1]}\n            db_name=\\${BASH_REMATCH[2]}\n            bin=\"${meta.assembler}-${meta.id}.\\${BASH_REMATCH[3]}\"\n            bin_new=\"\\${bin//./_}\"\n            mv \\${sum} short_summary.\\${mode}.\\${db_name}.\\${bin_new}.txt\n        done\n        generate_plot.py --working_directory .\n\n        mv busco_figure.png \"${meta.assembler}-${meta.id}.\\${mode}.\\${db_name}.busco_figure.png\"\n        mv busco_figure.R \"${meta.assembler}-${meta.id}.\\${mode}.\\${db_name}.busco_figure.R\"\n    fi\n\n    busco --version | sed \"s/BUSCO //\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/BUSCO_PLOT"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 1, "tools": ["BWA", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": ["\nprocess final_mapping {\n    tag { \"Mapping to polished assembly for \" + sample_id }\n    publishDir \"${params.outdir}/${sample_id}/\", mode: 'copy'\n\n    input:\n        set sample_id, file(ref_fa), file(fq1), file(fq2) \\\n            from polished_assembly_ch.join(fastq_for_mapping_ch)\n    output:\n        set sample_id, file(ref_fa), file(\"${sample_id}.bam\"), file(\"${sample_id}.bam.bai\") into \\\n            final_mapping_for_vcf_ch, final_mapping_for_cov_ch\n        set sample_id, \"${sample_id}.bam.stats\" into final_mapping_bamstats_ch\n    script:\n        \"\"\"\n        bwa index ${ref_fa};\n        samtools faidx ${ref_fa};\n        bwa mem -t ${task.cpus} ${ref_fa} ${fq1} ${fq2} | \\\n            lofreq viterbi -f ${ref_fa} - | \\\n            lofreq alnqual -u - ${ref_fa} | \\\n            lofreq indelqual --dindel -f ${ref_fa} - | \\\n            samtools sort -o ${sample_id}.bam -T ${sample_id}.final.tmp -;\n        samtools index ${sample_id}.bam;\n        samtools stats ${sample_id}.bam > ${sample_id}.bam.stats\n        \"\"\"\n}"], "list_proc": ["nf-core/vipr/nf-core__vipr/final_mapping"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process GATK4_FASTQTOSAM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reads_command = meta.single_end ? \"--FASTQ $reads\" : \"--FASTQ ${reads[0]} --FASTQ2 ${reads[1]}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK FastqToSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" FastqToSam \\\\\n        $reads_command \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SAMPLE_NAME $prefix \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_FASTQTOSAM"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["SISTR"], "nb_own": 2, "list_own": ["bactopia", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "bactopia"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "Accio", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "fmaguire", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "TGotwig"], "nb_contrib": 108, "codes": ["\nprocess SISTR {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sistr_cmd:1.1.1--pyh864c0ab_2' :\n        'quay.io/biocontainers/sistr_cmd:1.1.1--pyh864c0ab_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\")            , emit: tsv\n    tuple val(meta), path(\"*-allele.fasta.gz\"), emit: allele_fasta\n    tuple val(meta), path(\"*-allele.json.gz\") , emit: allele_json\n    tuple val(meta), path(\"*-cgmlst.csv\")     , emit: cgmlst_csv\n    path \"*.{log,err}\"                        , emit: logs, optional: true\n    path \".command.*\"                         , emit: nf_logs\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    sistr \\\\\n        --qc \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --alleles-output ${prefix}-allele.json \\\\\n        --novel-alleles ${prefix}-allele.fasta \\\\\n        --cgmlst-profiles ${prefix}-cgmlst.csv \\\\\n        --output-prediction ${prefix} \\\\\n        --output-format tab \\\\\n        $fasta_name\n\n    mv ${prefix}.tab ${prefix}.tsv\n    gzip ${prefix}-allele.json\n    gzip ${prefix}-allele.fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sistr: \\$(echo \\$(sistr --version 2>&1) | sed 's/^.*sistr_cmd //; s/ .*\\$//' )\n    END_VERSIONS\n    \"\"\"\n}", "process SISTR {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::sistr_cmd=1.1.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sistr_cmd:1.1.1--pyh864c0ab_2':\n        'quay.io/biocontainers/sistr_cmd:1.1.1--pyh864c0ab_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tab\")         , emit: tsv\n    tuple val(meta), path(\"*-allele.fasta\"), emit: allele_fasta\n    tuple val(meta), path(\"*-allele.json\") , emit: allele_json\n    tuple val(meta), path(\"*-cgmlst.csv\")  , emit: cgmlst_csv\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    sistr \\\\\n        --qc \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --alleles-output ${prefix}-allele.json \\\\\n        --novel-alleles ${prefix}-allele.fasta \\\\\n        --cgmlst-profiles ${prefix}-cgmlst.csv \\\\\n        --output-prediction ${prefix} \\\\\n        --output-format tab \\\\\n        $fasta_name\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sistr: \\$(echo \\$(sistr --version 2>&1) | sed 's/^.*sistr_cmd //; s/ .*\\$//' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["bactopia/bactopia/bactopia__bactopia/SISTR", "nf-core/modules/nf-core__modules/SISTR"], "list_wf_names": ["bactopia/bactopia", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess convertBam {\n    label 'mc_small'\n    tag \"$libraryid\"\n    \n    when: \n    params.run_convertinputbam\n\n    input: \n    tuple samplename, libraryid, lane, colour, seqtype, organism, strandedness, udg, path(bam) from ch_input_for_convertbam \n\n    output:\n    tuple samplename, libraryid, lane, colour, seqtype, organism, strandedness, udg, path(\"*fastq.gz\"), val('NA') into ch_output_from_convertbam\n\n    script:\n    base = \"${bam.baseName}\"\n    \"\"\"\n    samtools fastq -t ${bam} | pigz -p ${task.cpus} > ${base}.converted.fastq.gz\n    \"\"\" \n}"], "list_proc": ["nf-core/eager/nf-core__eager/convertBam"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["SAMtools", "SAMBLASTER"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process SAMBLASTER {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samblaster=0.1.26 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-19fa9f1a5c3966b63a24166365e81da35738c5ab:fff03944e664bbf9a139f7b174b9cb2d4163271a-0' :\n        'quay.io/biocontainers/mulled-v2-19fa9f1a5c3966b63a24166365e81da35738c5ab:fff03944e664bbf9a139f7b174b9cb2d4163271a-0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args3 = task.ext.args3 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if( \"$bam\" == \"${prefix}.bam\" ) error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools view -h $args2 $bam | \\\\\n    samblaster $args | \\\\\n    samtools view $args3 -Sb - >${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samblaster: \\$( samblaster -h 2>&1 | head -n 1 | sed 's/^samblaster: Version //' )\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SAMBLASTER"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess samtools_flagstat_after_filter {\n    label 'sc_tiny'\n    tag \"$libraryid\"\n    publishDir \"${params.outdir}/samtools/filtered_stats\", mode: params.publish_dir_mode\n\n    when:\n    params.run_bam_filtering\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_filtering_for_flagstat\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.stats\") into ch_bam_filtered_flagstat_for_multiqc, ch_bam_filtered_flagstat_for_endorspy\n\n    script:\n    \"\"\"\n    samtools flagstat $bam > ${libraryid}_postfilterflagstat.stats\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/samtools_flagstat_after_filter"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["Delly2"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process DELLY_CALL {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::delly=0.8.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/delly:0.8.7--he03298f_1' :\n        'quay.io/biocontainers/delly:0.8.7--he03298f_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path fasta\n    path fai\n\n    output:\n    tuple val(meta), path(\"*.bcf\"), emit: bcf\n    tuple val(meta), path(\"*.csi\"), emit: csi\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    delly \\\\\n        call \\\\\n        $args \\\\\n        -o ${prefix}.bcf \\\\\n        -g  $fasta \\\\\n        $bam \\\\\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        delly: \\$( echo \\$(delly --version 2>&1) | sed 's/^.*Delly version: v//; s/ using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/DELLY_CALL"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Picard"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "leebrian", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process PICARD_CREATESEQUENCEDICTIONARY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.9--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.9--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.dict\"), emit: reference_dict\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CreateSequenceDictionary] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CreateSequenceDictionary  \\\\\n        $args \\\\\n        R=$fasta \\\\\n        O=${prefix}.dict\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CreateSequenceDictionary --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_CREATESEQUENCEDICTIONARY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.dict\"), emit: reference_dict\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard CreateSequenceDictionary] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        CreateSequenceDictionary  \\\\\n        $args \\\\\n        --REFERENCE $fasta \\\\\n        --OUTPUT ${prefix}.dict\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard CreateSequenceDictionary --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/PICARD_CREATESEQUENCEDICTIONARY", "nf-core/modules/nf-core__modules/PICARD_CREATESEQUENCEDICTIONARY"], "list_wf_names": ["nf-core/modules", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 1, "tools": ["Filtlong"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess FILTLONG {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"bioconda::filtlong=0.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/filtlong:0.2.0--he513fc3_3\"\n    } else {\n        container \"quay.io/biocontainers/filtlong:0.2.0--he513fc3_3\"\n    }\n\n    input:\n    tuple val(meta), path(long_reads), path(short_reads_1), path(short_reads_2)\n\n    output:\n    tuple val(meta), path(\"${meta.id}_lr_filtlong.fastq.gz\"), emit: reads\n    path '*.version.txt'                                    , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    filtlong \\\n        -1 ${short_reads_1} \\\n        -2 ${short_reads_2} \\\n        --min_length ${params.longreads_min_length} \\\n        --keep_percent ${params.longreads_keep_percent} \\\n        --trim \\\n        --length_weight ${params.longreads_length_weight} \\\n        ${long_reads} | gzip > ${meta.id}_lr_filtlong.fastq.gz\n\n    filtlong --version | sed -e \"s/Filtlong v//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/FILTLONG"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 153, "tools": ["DEPTH", "FastQC", "SAMtools", "rgpicfixmate"], "nb_own": 19, "list_own": ["CDCgov", "harleenduggal", "salzmanlab", "cidgoh", "sguizard", "ABMicroBioinf", "erikrikarddaniel", "vincenthhu", "xiaoli-dong", "chelauk", "nf-core", "jianhong", "sanger-tol", "raygozag", "mahesh-panchal", "NBISweden", "csf-ngs", "cguyomar", "goodwright"], "nb_wf": 27, "list_wf": ["pathogen", "readmapping", "isoseq", "cidgoh_qc", "nanoseq", "rnaseq", "ReadZS", "RNASEQ", "viralrecon", "Earth-Biogenome-Project-pilot", "modules", "test_nfcore_workflow_chain", "nfcore-rnaseq", "controldna", "rnavar", "raredisease", "mycosnp-nf", "magmap", "shotgun", "ssds", "imaps-nf", "nf-ase", "magph", "nf-core-hicar", "nf-core-blasr", "nf-core-westest", "cutandrun"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "ktrns", "FelixKrueger", "jordwil", "salzmanlab", "rfara", "dladd", "kmurat1", "chuan-wang", "yuxuth", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "cying111", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "priyanka-surana", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "aersoares81", "zxl124", "samirelanduk", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "mciprianoCDC", "santiagorevale", "ppericard", "anwarMZ", "idot", "kevbrick", "nebfield", "mvanins", "eameyer", "Alexey-ebi", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jcurado-flomics", "jburos", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "saramonzon", "cjjossart", "pcantalupo", "cjfields", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "xiaoli-dong", "BABS-STP1", "senthil10", "kviljoen", "alexharston", "Gwennid", "peterwharrison", "charlotte-west", "Jeremy1805", "marc-jones", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "cguyomar", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "kaitlinchaung", "ggabernet", "vezzi", "duanjunhyq", "mjcipriano", "skrakau", "svarona", "Erkison", "bjohnnyd", "grst", "lwratten", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "csawye01", "leebrian", "vincenthhu", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "chelauk", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "CharlotteAnne", "jianhong", "mashehu", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 184, "codes": ["process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MARKDUP {\n    tag \"$meta.id\"\n    label 'process_samtools'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        markdup  \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        ${prefix}.bam \\\\\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools index -@ ${task.cpus-1} $args $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    touch ${prefix}.cram\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    \"\"\"\n    samtools view --threads ${task.cpus-1} ${reference} $args $input > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\"), emit: fai\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${fasta}.fai\n    cat <<-END_VERSIONS > versions.yml\n\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n    \"\"\"\n    samtools \\\\\n        fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_CUSTOMVIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.14--hb421002_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.txt\") , emit: tsv\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args -@ $task.cpus $bam | $options.args2 > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , optional: true, emit: bam\n    tuple val(meta), path(\"*.cram\"), optional: true, emit: cram\n    path  \"versions.yml\"                           , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    \"\"\"\n    samtools view ${reference} $options.args $input > ${prefix}.${file_type}\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.14--hb421002_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $options.args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.14--hb421002_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORTNAME {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*namesorted.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.namesorted.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort -n $args -@ $task.cpus -o ${prefix}.namesorted.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools index -@ ${task.cpus-1} $args $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools flagstat --threads ${task.cpus-1} $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools flagstat --threads ${task.cpus-1} $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\"), emit: fai\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${fasta}.fai\n    cat <<-END_VERSIONS > versions.yml\n\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_AMPLICONCLIP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path bed\n    val save_cliprejects\n    val save_clipstats\n\n    output:\n    tuple val(meta), path(\"*.bam\")            , emit: bam\n    tuple val(meta), path(\"*.clipstats.txt\")  , optional:true, emit: stats\n    tuple val(meta), path(\"*.cliprejects.bam\"), optional:true, emit: rejects_bam\n    path \"versions.yml\"                       , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def rejects = save_cliprejects ? \"--rejects-file ${prefix}.cliprejects.bam\" : \"\"\n    def stats   = save_clipstats   ? \"-f ${prefix}.clipstats.txt\"               : \"\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        ampliconclip \\\\\n        $args \\\\\n        $rejects \\\\\n        $stats \\\\\n        -b $bed \\\\\n        -o ${prefix}.bam \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n    \"\"\"\n    samtools \\\\\n        fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools index -@ ${task.cpus-1} $args $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools flagstat --threads ${task.cpus-1} $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.14--hb421002_0\"\n    }\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    \"\"\"\n    samtools view --threads ${task.cpus-1} ${reference} $options.args $input > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams)\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\"), emit: bam\n    path  \"versions.yml\"                  , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools merge ${prefix}.bam $bams\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.sorted.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.sorted.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n    stub:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.sorted.bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${bam}.bai\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n    touch ${prefix}.cram\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    \"\"\"\n    touch ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\"), emit: fai\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${fasta}.fai\n    cat <<-END_VERSIONS > versions.yml\n\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FIXMATE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        fixmate  \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        ${prefix}.bam \\\\\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_DEPTH {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools \\\\\n        depth \\\\\n        $args \\\\\n        -o ${prefix}.tsv \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW_BAM {\n    tag \"$meta.id\"\n    label 'process_medium'\n                                    \n                                        \n                                                                                                                                                       \n\n    conda     (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.11--h6270b1f_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.11--h6270b1f_0\"\n    }\n\n    input:\n    tuple val(meta), path(sizes), val(is_transcripts), path(sam)\n\n    output:\n    tuple val(meta), path(\"*.bam\") ,emit: bam\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    \"\"\"\n    samtools view -b -h -O BAM -@ $task.cpus -o ${meta.id}.bam $sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n    \"\"\"\n    samtools \\\\\n        fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\"), emit: fai\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${fasta}.fai\n    cat <<-END_VERSIONS > versions.yml\n\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${prefix}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    \"\"\"\n    touch ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${prefix}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${prefix}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\"), emit: fai\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\"), emit: fai\n    path \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${fasta}.fai\n    cat <<-END_VERSIONS > versions.yml\n\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    \"\"\"\n    touch ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools flagstat --threads ${task.cpus-1} $bam > ${bam}.flagstat\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"${bam}\"\n    label 'process_low'\n    publishDir \"${params.outdir}/bams\",\n        mode: params.publish_dir_mode\n\n    conda 'bioconda::samtools=1.13'\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(inputChannel), val(bamFileID), path(bam)\n\n    output:\n    tuple val(inputChannel), val(bamFileID), path(bam), path(\"*bai\")     , emit: bam_tuple\n    path  \"versions.yml\"                                            , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"${bamFileID}\"\n    label 'process_medium'\n\n    conda 'bioconda::samtools=1.13'\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(inputChannel), val(bamFileID), path(bam)\n\n    output:\n    tuple val(inputChannel), val(bamFileID), path(\"sorted*.bam\")   , emit: bam\n    path  \"versions.yml\"                                , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = \"sorted.${bamFileID}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_samtools'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.fastq\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools \\\\\n        fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $input \\\\\n        > ${prefix}.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path (\"*.fai\") , emit: fai\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_COLLATE {\n    tag \"$meta.id\"\n    label 'process_samtools'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1':\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        collate \\\\\n        $args \\\\\n        -@ ${task.cpus-1} \\\\\n        -o ${prefix}.bam \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' )\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    \"\"\"\n    touch ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_FIXMATE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        fixmate  \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        ${prefix}.bam \\\\\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    \"\"\"\n    touch ${input}.bai\n    touch ${input}.crai\n    touch ${input}.csi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_FLAGSTAT", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_MARKDUP", "nf-core/ssds/nf-core__ssds/SAMTOOLS_INDEX", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_FLAGSTAT", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_VIEW", "nf-core/ssds/nf-core__ssds/SAMTOOLS_VIEW", "nf-core/ssds/nf-core__ssds/SAMTOOLS_IDXSTATS", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_IDXSTATS", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_SORT", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_STATS", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_SORT", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_FAIDX", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_FASTQ", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_CUSTOMVIEW", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_FLAGSTAT", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_INDEX", "ABMicroBioinf/magph/ABMicroBioinf__magph/SAMTOOLS_INDEX", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_IDXSTATS", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_VIEW", "nf-core/cutandrun/nf-core__cutandrun/SAMTOOLS_STATS", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_IDXSTATS", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/SAMTOOLS_VIEW", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_INDEX", "sguizard/isoseq/sguizard__isoseq/SAMTOOLS_FASTQ", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/SAMTOOLS_FAIDX", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/SAMTOOLS_SORT", "sguizard/isoseq/sguizard__isoseq/SAMTOOLS_SORT", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_FLAGSTAT", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/SAMTOOLS_INDEX", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_VIEW", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_SORT", "csf-ngs/controldna/csf-ngs__controldna/SAMTOOLS_SORT", "csf-ngs/controldna/csf-ngs__controldna/SAMTOOLS_SORTNAME", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/SAMTOOLS_FLAGSTAT", "csf-ngs/controldna/csf-ngs__controldna/SAMTOOLS_MERGE", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/SAMTOOLS_IDXSTATS", "nf-core/viralrecon/nf-core__viralrecon/SAMTOOLS_STATS", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/SAMTOOLS_SORT", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/SAMTOOLS_INDEX", "erikrikarddaniel/magmap/erikrikarddaniel__magmap/SAMTOOLS_STATS", "csf-ngs/controldna/csf-ngs__controldna/SAMTOOLS_INDEX", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_VIEW", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_FAIDX", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_STATS", "raygozag/rnaseq/raygozag__rnaseq/SAMTOOLS_SORT", "raygozag/rnaseq/raygozag__rnaseq/SAMTOOLS_STATS", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_SORT", "raygozag/rnaseq/raygozag__rnaseq/SAMTOOLS_INDEX", "raygozag/rnaseq/raygozag__rnaseq/SAMTOOLS_FLAGSTAT", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_INDEX", "raygozag/rnaseq/raygozag__rnaseq/SAMTOOLS_IDXSTATS", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_IDXSTATS", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_FLAGSTAT", "xiaoli-dong/magph/xiaoli-dong__magph/SAMTOOLS_FASTQ", "goodwright/imaps-nf/goodwright__imaps-nf/SAMTOOLS_FAIDX", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/SAMTOOLS_SORT", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/SAMTOOLS_INDEX", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/SAMTOOLS_VIEW", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/SAMTOOLS_FAIDX", "nf-core/modules/nf-core__modules/SAMTOOLS_AMPLICONCLIP", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SAMTOOLS_INDEX", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SAMTOOLS_SORT", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SAMTOOLS_STATS", "nf-core/modules/nf-core__modules/SAMTOOLS_FASTQ", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SAMTOOLS_IDXSTATS", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/SAMTOOLS_FLAGSTAT", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SAMTOOLS_INDEX", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SAMTOOLS_SORT", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SAMTOOLS_STATS", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SAMTOOLS_IDXSTATS", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SAMTOOLS_FLAGSTAT", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_INDEX", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_MERGE", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_IDXSTATS", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_STATS", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_FLAGSTAT", "nf-core/modules/nf-core__modules/SAMTOOLS_SORT", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_VIEW", "nf-core/modules/nf-core__modules/SAMTOOLS_IDXSTATS", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/SAMTOOLS_SORT", "jianhong/shotgun/jianhong__shotgun/SAMTOOLS_INDEX", "cguyomar/nf-ase/cguyomar__nf-ase/SAMTOOLS_VIEW", "cguyomar/nf-ase/cguyomar__nf-ase/SAMTOOLS_FAIDX", "jianhong/shotgun/jianhong__shotgun/SAMTOOLS_SORT", "cguyomar/nf-ase/cguyomar__nf-ase/SAMTOOLS_MERGE", "jianhong/shotgun/jianhong__shotgun/SAMTOOLS_STATS", "jianhong/shotgun/jianhong__shotgun/SAMTOOLS_IDXSTATS", "chelauk/nf-core-blasr/chelauk__nf-core-blasr/SAMTOOLS_SORT", "jianhong/shotgun/jianhong__shotgun/SAMTOOLS_FLAGSTAT", "chelauk/nf-core-blasr/chelauk__nf-core-blasr/SAMTOOLS_INDEX", "jianhong/shotgun/jianhong__shotgun/SAMTOOLS_VIEW", "nf-core/modules/nf-core__modules/SAMTOOLS_INDEX", "nf-core/modules/nf-core__modules/SAMTOOLS_VIEW", "nf-core/modules/nf-core__modules/SAMTOOLS_MERGE", "nf-core/modules/nf-core__modules/SAMTOOLS_FAIDX", "nf-core/modules/nf-core__modules/SAMTOOLS_STATS", "NBISweden/Earth-Biogenome-Project-pilot/NBISweden__Earth-Biogenome-Project-pilot/SAMTOOLS_FASTQ", "nf-core/modules/nf-core__modules/SAMTOOLS_FIXMATE", "nf-core/modules/nf-core__modules/SAMTOOLS_FLAGSTAT", "nf-core/modules/nf-core__modules/SAMTOOLS_DEPTH", "nf-core/nanoseq/nf-core__nanoseq/SAMTOOLS_VIEW_BAM", "nf-core/nanoseq/nf-core__nanoseq/SAMTOOLS_IDXSTATS", "nf-core/nanoseq/nf-core__nanoseq/SAMTOOLS_SORT", "nf-core/nanoseq/nf-core__nanoseq/SAMTOOLS_FLAGSTAT", "nf-core/nanoseq/nf-core__nanoseq/SAMTOOLS_STATS", "nf-core/nanoseq/nf-core__nanoseq/SAMTOOLS_INDEX", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/SAMTOOLS_FLAGSTAT", "nf-core/raredisease/nf-core__raredisease/SAMTOOLS_STATS", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/SAMTOOLS_INDEX", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/SAMTOOLS_FASTQ", "nf-core/raredisease/nf-core__raredisease/SAMTOOLS_FAIDX", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_STATS", "nf-core/raredisease/nf-core__raredisease/SAMTOOLS_MERGE", "cidgoh/cidgoh_qc/cidgoh__cidgoh_qc/SAMTOOLS_SORT", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_FLAGSTAT", "nf-core/raredisease/nf-core__raredisease/SAMTOOLS_INDEX", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_IDXSTATS", "nf-core/rnaseq/nf-core__rnaseq/SAMTOOLS_INDEX", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_VIEW", "nf-core/rnaseq/nf-core__rnaseq/SAMTOOLS_SORT", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_FAIDX", "nf-core/rnaseq/nf-core__rnaseq/SAMTOOLS_STATS", "nf-core/rnaseq/nf-core__rnaseq/SAMTOOLS_IDXSTATS", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_SORT", "nf-core/rnaseq/nf-core__rnaseq/SAMTOOLS_FLAGSTAT", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/SAMTOOLS_INDEX", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_INDEX", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_FLAGSTAT", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_IDXSTATS", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_STATS", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_SORT", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_SORT", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_FAIDX", "nf-core/rnavar/nf-core__rnavar/SAMTOOLS_MERGE", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_FLAGSTAT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_IDXSTATS", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/SAMTOOLS_MERGE", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_INDEX", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/SAMTOOLS_SORT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_STATS", "salzmanlab/ReadZS/salzmanlab__ReadZS/SAMTOOLS_INDEX", "salzmanlab/ReadZS/salzmanlab__ReadZS/SAMTOOLS_SORT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SAMTOOLS_VIEW", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/SAMTOOLS_STATS", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/SAMTOOLS_INDEX", "goodwright/imaps-nf/goodwright__imaps-nf/SAMTOOLS_INDEX", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_IDXSTATS", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_FASTQ", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/SAMTOOLS_FAIDX", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_COLLATE", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_MERGE", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_FIXMATE", "sanger-tol/readmapping/sanger-tol__readmapping/SAMTOOLS_INDEX"], "list_wf_names": ["sguizard/isoseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "goodwright/imaps-nf", "ABMicroBioinf/magph", "nf-core/rnavar", "csf-ngs/controldna", "cguyomar/nf-ase", "cidgoh/cidgoh_qc", "ABMicroBioinf/pathogen", "chelauk/nf-core-blasr", "nf-core/raredisease", "nf-core/ssds", "vincenthhu/nf-core-westest", "nf-core/modules", "nf-core/rnaseq", "CDCgov/mycosnp-nf", "sanger-tol/readmapping", "jianhong/nf-core-hicar", "jianhong/shotgun", "erikrikarddaniel/magmap", "nf-core/cutandrun", "nf-core/viralrecon", "xiaoli-dong/magph", "mahesh-panchal/test_nfcore_workflow_chain", "raygozag/rnaseq", "xiaoli-dong/pathogen", "NBISweden/Earth-Biogenome-Project-pilot", "salzmanlab/ReadZS", "nf-core/nanoseq"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": ["\nprocess genomecov {\n    tag { \"Genome coverage for \" + sample_id }\n    publishDir \"${params.outdir}/${sample_id}/\", mode: 'copy'\n\n    input:\n        set sample_id, file(ref_fa), file(bam), file(bai) from final_mapping_for_cov_ch\n    output:\n        set sample_id, file(\"${sample_id}.cov.gz\") into cov_ch\n    script:\n        \"\"\"\n        # note: -d is one-based. -dz is zero-based but only non-zero values, so less explicit.\n        bedtools genomecov -d -ibam ${bam} | gzip > ${sample_id}.cov.gz;\n        \"\"\"\n}"], "list_proc": ["nf-core/vipr/nf-core__vipr/genomecov"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 44, "tools": ["SAMtools"], "nb_own": 39, "list_own": ["dthorburn", "Thomas-LeCoent", "torchij", "hugovaysset", "aduvermy", "BFSSI-Bioinformatics-Lab", "lifebit-ai", "h3abionet", "naryamanesh", "cmatKhan", "wtsi-hgi", "MDegener", "MarieGurke", "sickle-in-africa", "JingQiChong", "genomic-medicine-sweden", "cpang429", "nf-modules", "adrodrzywolski", "ampatchlab", "mblanche", "dakehero", "nf-core", "nibscbioinformatics", "jamez-eh", "DLBPointon", "SergFern", "luslab", "Gregor-Mendel-Institute", "R-Cardenas", "palfalvi", "fargenfo", "MaddalenaCella", "cgpu", "vib-singlecell-nf", "biocorecrg", "maxibor", "BenNolann", "crickbabs"], "nb_wf": 40, "list_wf": ["LGAflow", "directrna", "Agam_Methylation", "RNAseq-pipeline", "recalling", "nextflow", "ReproHackaton", "nanoporeseq", "nextflow_pipelines", "humgen", "brentlab_rnaseq_nf", "reprohackathon-bioinfo", "quantitative_nucleosome_analysis", "samtools", "PGP-UK-sarek", "eager", "admapipe", "RareDisease_RNA_workflow", "nfconsensus", "nf-methylpy", "vsn-pipelines", "Variant_Calling", "saw.structural-variants", "linkseq", "rnaseq_pipeline_rewrite", "nf-clip", "nf-rnasnv", "rnaSeq_byBABS", "nf-fh-pcp-wes-mutect2", "BioNextflow", "bfssi-assembly-pipeline", "GenomeChronicler-Sarek-nf", "rsi_analysis", "rnaseq_analysis_pipeline", "nf_CRACpipeline", "nextflow-pipelines", "nf-lcWGS-mapping-and-imputation", "nfvacal", "nf-bismarkAlignment", "annotation-pipeline-nextflow"], "list_contrib": ["torchij", "ghuls", "alexhbnr", "pallolason", "aidaanva", "mamanambiya", "olavurmortensen", "cflerin", "lucacozzuto", "amchakra", "ojziff", "candiceh08", "toniher", "samuell", "lescai", "viklund", "cgpu", "BeibeiDu", "kkuret", "MorganePhilipp", "dthorburn", "louisedem", "phue", "dependabot[bot]", "maxulysse", "jfy133", "MarieGurke", "TatsuyaDaniel", "adrodrzywolski", "jackmo375", "Landris18", "stekaz", "dweemx", "abeaude", "Hammarn", "sven1103", "jemten", "MaddalenaCella", "wtsi-mercury", "Thomas-LeCoent", "alexandregilardet", "ewels", "evanfloden", "hugovaysset", "Rahul1711arora", "MDegener", "JingQiChong", "trambau", "adrienlemeur", "TCLamnidis", "IdoBar", "jamez-eh", "SergFern", "chris-cheshire", "grbot", "ashildv", "sc13-bioinf", "gn5", "jhagberg", "drpatelh", "alneberg", "aduvermy", "ggabernet", "bfssi-forest-dussault", "naryamanesh", "cmatKhan", "ZandraFagernas", "nf-core-bot", "bleazard", "alexthiery", "ElisabetThomsen", "abhi18av", "pditommaso", "KrisDavie", "glormph", "marcelm", "projectoriented", "kusalananda", "charles-plessy", "DLBPointon", "rbpisupati", "CharlotteAnne", "J35P312", "R-Cardenas", "palfalvi", "apeltzer", "waffle-iron", "maxibor", "BenNolann", "olgabot", "scarlhoff"], "nb_contrib": 91, "codes": ["\nprocess sam_index {\n    label 'samtools'\n\n    input:\n        path(reads) from quality_filtered\n\n    output:\n        path \"*\" into sam_indexed\n\n    script:\n    \"\"\"\n    samtools index $reads \n    \"\"\"\n\n}", "\nprocess BAM_file_indexing{\n\n  tag \"Indexing BAM file\"\n  label 'big_mem'\n  label 'generic'\n\n  errorStrategy 'retry'\n  maxRetries 3\n\n  publishDir \"$params.outdir/alignment\", mode: 'copy', overwrite: true\n\n  input:\n  set sampleId, file(bam_file) from ch_index_bam\n\n  output:\n  set sampleId,  file(\"${bam_file[0]}\"), file('*.bai') into ch_bamFilesForBaseRecalibration\n\n  script:\n\n  \"\"\"\n  samtools index -@ ${params.threads} ${bam_file[0]}\n  \"\"\"\n\n  }", "\nprocess clip_overlap_idx {\n    label 'toolbox'\n    \n    publishDir params.merged, mode:'copy'\n\n    input:\n        path(input) from clipoverlap_ch\n\n    output:\n        path \"merged_dedup_overlapclipped.bam.bai\" into clipoverlap_idx\n\n    script:\n    \"\"\"\n    samtools index $input\n    \"\"\"\n}", "\nprocess IndexBAM {\n  input:\n  file(bam) from bam_ch\n\n  output:\n  tuple file(bam), file('*.bai') into bambai_ch\n\n  script:\n  \"\"\"\n  samtools index $bam\n  \"\"\"\n}", "\nprocess index_bam {\n    tag { \"${sample_id}\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n    input:\n    set val(sample_id), file(bam_file) from samples\n    output:\n    file(\"*.bai\")\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools index ${bam_file}\n    \"\"\"\n}", "\nprocess bam_index {\n  storeDir \"$baseDir/output/hg38_decoy/aligned_sorted\"\n  input:\n  file pair_read_8 from index66_ch\n  output:\n  file \"${pair_read_8}.bai\"\n\n  script:\n  \"\"\"\n  samtools index ${pair_read_8}\n\n  \"\"\"\n}", "\nprocess index_bam {\n    label 'samtools'\n    label 'smallTask'\n    \n    publishDir \"${params.output}/${params.hisat2_dir}\", mode: 'copy', pattern: \"*.bai\"\n\n    input:\n    tuple val(sample_name), path(bam_file)\n\n    output:\n    path(\"${bam_file}.bai\")\n\n    script:\n    \"\"\"\n    samtools index ${bam_file}\n    \"\"\"\n}", "\nprocess indexBamFile {\n\tcontainer params.samtoolsImage\n\n\tinput:\n\tpath bamFile\n\n\toutput:\n\ttuple path(\"${bamFile}\"), path(\"${bamFile}.bai\")\n\n\tscript:\n\t\"\"\"\n\tsamtools index ${bamFile}\n\t\"\"\"\n}", "\nprocess IndexBams {\n\n  input:\n  file(bam) from rgbam_ch\n\n  output:\n  tuple file(bam), file('*.bai') into ibam_ch\n  tuple file(bam), file('*.bai') into ibam2_ch\n\n  script:\n  \"\"\"\n  samtools index $bam\n  \"\"\"\n}", "\nprocess index_bam{\n\n    input:\n        tuple val(sample), path(bam)\n\n    output:\n        tuple val(sample), path('*.bai'), emit: bai\n\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}", "\nprocess bamIndex {\n    input:\n    file bam_file from bam_files\n\n    output:\n    file \"${prefix}.bai\" into bamIndex_files\n    \n    \"\"\"\n    samtools index ${bam_file}\n    \"\"\"    \n }", "\nprocess samtools {\n    tag \"${sample_id}\"\n\n    publishDir \"${params.internal_outdir}/${params.internal_process_name}\",\n        mode: \"copy\", overwrite: true\n\n    input:\n      tuple val(sample_id), path(bam)\n\n    output:\n      tuple val(sample_id), path(\"*.bam.bai\"), emit: baiFiles\n \n    shell:\n    \n                             \n    samtools_args = ''\n    samtools_args += \"$params.internal_custom_args \"\n    \n    \"\"\"\n    samtools index $samtools_args -@ ${task.cpus} $bam\n    \"\"\"\n}", "\nprocess index {\n    label 'index'\n    tag '_${id}'\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n    \n    publishDir \"${params.bamDir}\",\n\tmode: \"copy\"\n\n    input:\n    path(bam) from bam_ch\n\n    output:\n    tuple id, path(bam), path(\"*.bam.bai\") into bamNidx_ch\n    \n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools index -@${task.cpus} ${bam}\n    \"\"\"\n    \n\n}", "\nprocess index_bam {\n    cpus = 1\n    input:\n    file bam from fixed_sorted_bam\n\n    output:\n    file \"*_fixmate_sorted.bam.bai\" into indexof_fixed_sorted_bam\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}", "\nprocess index_bam {\n    label 'index'\n    tag \"_${id}\"\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n\n    input:\n    path(bam) from bam_ch\n    \n    output:\n    tuple id, path(bam), path(\"*.bai\") into bam_capStats_ch, bam_cleanBam_ch, bam_mapFile_ch\n\n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools index -@${task.cpus} ${bam}\n    \"\"\"\n}", "\nprocess index_picard {\n\n\t               \n\ttag { sample }\n\n\t                               \n\tbeforeScript \"module purge\"\n\n\t          \n\tmodule MODULE_SAMTOOLS\n\n\t      \n\tcpus 32\n\texecutor \"slurm\"\n\tmemory \"6000\"\n\n\t                   \n\tpublishDir picard_dir.toString(),\n\t\tmode: PUBLISHDIR_MODE,\n\t\toverwrite: PUBLISHDIR_OVERWRITE\n\n\tinput:\n\t\tset val(sample), file(bam) from picard_dupmarked_indexing\n\n\toutput:\n\t\tset val(sample), file(bam), file(\"*.bai\") into bai_picard\n\n\tshell:\n\t\t\"\"\"\n\t\tsamtools index ${bam}\n\t\t\"\"\"\n}", "\nprocess BWAMAPTOOLS__INDEX_BAM {\n\n    container toolParams.container\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId),\n              path(bam)\n\n    output:\n        tuple val(sampleId),\n              path(bam),\n              path(\"*.bai\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams)\n        processParams = sampleParams.local\n        \"\"\"\n        samtools index ${bam}\n        \"\"\"\n}", " process IndexBams {\n    errorStrategy { 'retry' }\n    maxRetries 3\n    maxForks params.BI_Forks\n  \n    executor = 'pbspro'\n    clusterOptions = \"-lselect=1:ncpus=${params.Index_threads}:mem=${params.Index_memory}gb -lwalltime=${params.Index_walltime}:00:00\"\n    \n    publishDir(\n      path: \"${params.GuppyDir}\",\n      mode: 'copy',\n    )\n    \n    beforeScript 'module load samtools/1.2'\n\n    input: \n    set path(queryBam) from bams_ch\n    \n    output:\n    path(\"*.bai\") into bais_ch\n      \n    script:\n    \"\"\"\n    samtools index ${queryBam}\n    \"\"\"\n  }", "\nprocess indexinputbam {\n  label 'sc_small'\n  tag \"$libraryid\"\n\n  when: \n  bam != 'NA' && !params.run_convertinputbam\n\n  input:\n  tuple samplename, libraryid, lane, colour, seqtype, organism, strandedness, udg, path(bam) from ch_input_for_indexbam \n\n  output:\n  tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), file(\"*.{bai,csi}\")  into ch_indexbam_for_filtering\n\n  script:\n  def size = params.large_ref ? '-c' : ''\n  \"\"\"\n  samtools index ${bam} ${size}\n  \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, file(bam) from mergedBamForSentieon\n\n    output:\n        set idPatient, idSample, file(bam), file(\"${idSample}.bam.bai\") into bamForSentieonDedup\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}", "\nprocess indexBAM {\n    container 'staphb/samtools:latest'\n    tag \"$pair_id\"\n    publishDir \"$params.outdir/$pair_id\", mode: 'symlink'\n\n\n    input:\n    tuple val(pair_id), path(assembly), path(sorted_bamfile) from sorted_bam_ch\n\n    output:\n    tuple val(pair_id), path(assembly), path(sorted_bamfile), path('*.bai') into indexed_bam_ch, bamqc_ch\n\n    script:\n    \"\"\"\n    samtools index $sorted_bamfile\n    \"\"\"\n}", " process index_bam_paired {\n    tag \"$file_id\"\n    publishDir \"results/training/bams/\", mode: 'copy'\n\n    input:\n      set file_id, file(bam) from sorted_bam_files_paired\n\n    output:\n      set file_id, \"*.bam*\" into indexed_bam_file_paired\n\n    script:\n  \"\"\"\n  samtools index ${bam}\n  \"\"\"\n  }", "process samtools_index {\n\n  label 'small_job'\n\n  conda \"$baseDir/conda-envs/samtools-env.yaml\"\n\n                                                                \n\n  input:\n    path bam\n    val options\n\n  output:\n    path \"*.bai\", emit: baidx\n\n  script:\n    \"\"\"\n    samtools index $options ${bam}\n    \"\"\"\n}", " process index_bam_single {\n    tag \"$file_id\"\n    publishDir \"results/training/bams/\", mode: 'copy'\n\n    input:\n      set file_id, file(bam) from sorted_bam_files_single\n\n    output:\n      set file_id, \"*.bam*\" into indexed_bam_file_single\n\n    script:\n  \"\"\"\n  samtools index ${bam}\n  \"\"\"\n  }", "\nprocess BAM_INDEX {\n\n\n    label 'index'\n    beforeScript \"ml novoalign/3.09.01 samtools\"\n\n    publishDir \"${params.output_dir}/rnaseq_pipeline_results/run_${params.run_number}_samples/align\", overwite: true, pattern: \"*.bam*\", mode: 'copy'\n\n    input:\n        tuple path(bam)\n    output:\n        path(bam)\n        path \"*.bai*\"\n\n    script:\n\n            \"\"\"\n            samtools index -@ 8 $bam\n            \"\"\"\n}", "\nprocess samtools_index {\n    tag \"SAMTOOLS_INDEX on $aligned_sorted\"\n    cpus 4\n    publishDir \"${params.outdir}/bwa_aligned\", mode: 'copy'\n\n    input:\n    path(aligned_sorted)\n\n    output:\n    path \"aligned_sorted.bam.bai\", emit: aligned_index                                                                        \n\n\n    script:\n    \"\"\"\n    samtools index -@${task.cpus} ${aligned_sorted}\n    \"\"\"\n}", "\nprocess IndexFinalBam {\n    executor \"slurm\"\n    memory \"10G\"\n    module \"rnaseq_pipeline\"\n                       \n    publishDir \"$params.align_count_results/$run_directory/align\", mode:\"copy\", overwite: true, pattern: \"*.bai\"\n\n    input:\n        tuple val(run_directory), file(sorted_annoted_bam) from sorted_bam_ch\n    output:\n        file('*.bai') into index_ch\n\n    script:\n    \"\"\"\n        samtools index $sorted_annoted_bam\n    \"\"\"\n}", "\nprocess bam_index {\n  tag { \"${prefix}\" }\n  publishDir \"${params.outdir}/alignedBams\", mode: 'copy'\n  label 'env_picard_small'\n\n  input:\n  set val(prefix), file(bam) from bam_aligned\n\n  output:\n  set val(prefix), file(\"*processed_reads_no_clonal.bam.bai\") into aligned_bam_index\n\n  script:\n  \"\"\"\n  samtools index $bam\n  \"\"\"\n}", "\nprocess index {\n    publishDir params.indexResultsDir, mode: params.saveMode\n    container 'quay.io/biocontainers/samtools:1.10--h2e538c0_3'\n\n    when:\n    params.index\n\n    input:\n    path refFasta from ch_refFasta\n    file(sortedBam) from ch_in_index\n\n    output:\n    file(\"*.bai\") into ch_out_index\n\n    script:\n\n    \"\"\"\n    samtools index ${sortedBam}\n    \"\"\"\n}", "\nprocess indexBams {\n\ttag \"${base} ${bam}\"\n\n\tpublishDir \"${params.outdir}/bams/\", pattern: \"*.bai\", mode:'copy'\n\n\tinput:\n\tset val(base), file(bam) from hisat_bams1\n\n\toutput:\n\tset val(base), file(bam), file(\"*.bai\") into bai_bam\n\n\tscript:\n\t\"\"\"\n\tsamtools index ${bam}\n\t\"\"\"\n\n}", "\nprocess index_cram {\n    memory '3G'\n    tag \"$cram_file\"\n    cpus 1\n                   \n    time '100m'\n    queue 'normal'\n    container \"graphtyper\"\n    containerOptions = \"--bind /lustre\"\n                                \n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    publishDir \"${params.outdir}/cram_index/\", mode: 'symlink', overwrite: true, pattern: \"${cram_file}.crai\"\n    maxRetries 3\n\n    when:\n    params.run\n     \n    input:\n    file(cram_file)\n\n    output: \n    tuple file(\"${cram_file}.crai\"), emit: indexes\n\n    script:\n\"\"\" \nsamtools index $cram_file\n\"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, file(bam) from mergedBamForSentieon\n\n    output:\n        set idPatient, idSample, file(bam), file(\"${idSample}.bam.bai\") into bamForSentieonDedup\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}", " process index{\n          label \"counting\"\n\t\t  conda \"$envDir\"\n          cpus 8\n          publishDir = [path: \"$outDir/sort_index\", mode:'copy']\n          tag \"bam: $sample_id\"\n          \n          input:\n            set val(sample_id), file(bam_s) from bam_sorted\n            \n          output:\n            set val(sample_id), file(\"${sample_id}.Aligned.sorted.out.bam.bai\") into bam_indexed\n            \n          script:\n              \"\"\"\n              samtools index -@ ${task.cpus} ${bam_s}\n              \"\"\"\n      }", "\nprocess indexrecalibrated {\n  publishDir \"$params.outdir/alignments\", mode: \"copy\"\n    tag \"$name\"\n    label 'process_medium'\n\n  input:\n  set ( sampleprefix, file(bqsrfile) ) from recalibratedforindex\n\n  output:\n  set ( sampleprefix, file(\"${bqsrfile}.bai\") ) into indexedbam\n\n  \"\"\"\n  samtools index $bqsrfile\n  \"\"\"\n}", "\nprocess bam_index {\n    tag \"$name\"\n\n    conda 'bioconda::samtools'\n\n    label 'normal'\n\n    cpus 1\n\n    publishDir \"${params.results}/alignment\", mode: 'copy'\n\n    input:\n        set val(name), file(bam) from alignment_to_index\n    output:\n        set val(name), file(\"*.bam.bai\")\n    script:\n        \"\"\"\n        samtools index $bam\n        \"\"\"\n}", "\nprocess samtools{\n\tpublishDir \"files/samtools/\"\n\t\n\tinput:\n\tfile(bam_to_index) from bam_chan\n\n\toutput:\n\tfile \"${bam_to_index}.bai\" into end\n\t\n\t\n\tscript:\n\t\"\"\"\n\tsamtools index $bam_to_index \n\t\"\"\"\n}", "\nprocess runSamtools_merge_index {\n    tag { filename + ' - samtools_index' }\n\n    publishDir \"${params.outDir}/${filename}/samtools\", mode: 'copy'\n\n    input:\n    set filename,\n        file(bam_sort) from ch_Samtools_merge_index\n\n    output:\n    file \"${filename}*.{bai}\" into results_Samtools_index\n\n    script:\n    \"\"\"\n    samtools index ${bam_sort}\n    \"\"\"\n}", "\nprocess samtools_index {\n    tag { sample }\n\n    label 'samtools'\n\n    publishDir \"${params.outdir}/STAR/${sample}\", mode: 'copy'\n\n    input:\n    set sample, file(bam) from star_csorted_bam_files\n\n    output:\n    file \"*.bai\"\n\n    \"\"\"\n    samtools index \"${bam}\"\n    \"\"\"\n}", "\nprocess index_phased_bam {\n    publishDir \"$outdir/$sample/bam\", mode: 'copy', pattern: '*.bam', overwrite: true\n    publishDir \"$outdir/$sample/bam\", mode: 'copy', pattern: '*.bam.bai', overwrite: true\n\n    input:\n    set sample, file(bam) from phased_bam_ch\n\n    output:\n    set sample, file(bam), file(\"${bam}.bai\") into indexed_phased_bam_qualimap_ch, indexed_phased_bam_bx_ch\n\n    script:\n    \"\"\"\n    samtools index $bam\n    \"\"\"\n}", "\nprocess samtoolsIndex {\n\tcontainer \"fredhutch/bwa:0.7.17\"\t\n\tlabel 'small'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patientID),  file(bam)\t\n\t\n\toutput:\n\ttuple val(\"${kitID}\"), val(\"${patientID}\"), val(\"${type}\"), val(\"${sampleID}\"), file(\"${bam}\"), file(\"${bam}.bai\")\n\n\n\t\"\"\"\n\tsamtools index ${bam}\n\t\"\"\"\n}", "\nprocess index {\n    label 'index'\n    tag '_${id}'\n    cpus 4\n    memory '16 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    path(bam) from bam_ch\n\n    output:\n    tuple id, path(bam), path(\"*.bam.bai\") into bamNidx_ch\n    \n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools index -@${task.cpus} ${bam}\n    \"\"\"\n    \n\n}", "\nprocess indexBAM {\n                                                                                        \n                                                             \n\n    publishDir \"results/BAM_files\", mode: 'symlink' \n                                                                     \n    \n    input:\n    file bam from bamfiles\n\n    output:\n    tuple file(\"${bam}.bai\"), file(\"${bam}\") into indexedBAM_1, indexedBAM_2\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}", "\nprocess indexBamfiles {\n  publishDir \"${params.output_dir}/aligned_bamsorted\", mode: \"copy\"\n  tag \"${bamsorted}\"\n  \n  input:\n  path bamsorted\n  \n  output:\n  path \"*.bam.bai\"\n  \n  script:\n  \"\"\"\n  samtools index ${bamsorted}\n  \"\"\"\n}", "\nprocess indexBam {\n    label (params.LABEL)\n    tag { pair_id }\n    container params.CONTAINER\n    if (params.OUTPUT != \"\") { publishDir(params.OUTPUT, mode:params.OUTPUTMODE) }\n\n    input:\n    tuple val(pair_id), path(reads)\n\n    output:\n    tuple val(pair_id), path(\"*.bai\") \n    \n\tscript:\n    \"\"\"    \n    samtools index ${params.EXTRAPARS} ${reads}\n    \"\"\"\n}"], "list_proc": ["MaddalenaCella/nf-lcWGS-mapping-and-imputation/MaddalenaCella__nf-lcWGS-mapping-and-imputation/sam_index", "SergFern/Variant_Calling/SergFern__Variant_Calling/BAM_file_indexing", "MaddalenaCella/nf-lcWGS-mapping-and-imputation/MaddalenaCella__nf-lcWGS-mapping-and-imputation/clip_overlap_idx", "MarieGurke/nfconsensus/MarieGurke__nfconsensus/IndexBAM", "h3abionet/recalling/h3abionet__recalling/index_bam", "R-Cardenas/nextflow_pipelines/R-Cardenas__nextflow_pipelines/bam_index", "dakehero/LGAflow/dakehero__LGAflow/index_bam", "sickle-in-africa/saw.structural-variants/sickle-in-africa__saw.structural-variants/indexBamFile", "MarieGurke/nfvacal/MarieGurke__nfvacal/IndexBams", "genomic-medicine-sweden/RareDisease_RNA_workflow/genomic-medicine-sweden__RareDisease_RNA_workflow/index_bam", "cpang429/directrna/cpang429__directrna/bamIndex", "luslab/nf-clip/luslab__nf-clip/samtools", "mblanche/nextflow/mblanche__nextflow/index", "adrodrzywolski/rnaseq_analysis_pipeline/adrodrzywolski__rnaseq_analysis_pipeline/index_bam", "mblanche/nextflow/mblanche__nextflow/index_bam", "crickbabs/rnaSeq_byBABS/crickbabs__rnaSeq_byBABS/index_picard", "vib-singlecell-nf/vsn-pipelines/vib-singlecell-nf__vsn-pipelines/BWAMAPTOOLS__INDEX_BAM", "dthorburn/Agam_Methylation/dthorburn__Agam_Methylation/IndexBams", "nf-core/eager/nf-core__eager/indexinputbam", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/IndexBamMergedForSentieon", "BFSSI-Bioinformatics-Lab/bfssi-assembly-pipeline/BFSSI-Bioinformatics-Lab__bfssi-assembly-pipeline/indexBAM", "aduvermy/quantitative_nucleosome_analysis/aduvermy__quantitative_nucleosome_analysis/index_bam_paired", "palfalvi/nanoporeseq/palfalvi__nanoporeseq/samtools_index", "aduvermy/quantitative_nucleosome_analysis/aduvermy__quantitative_nucleosome_analysis/index_bam_single", "cmatKhan/brentlab_rnaseq_nf/cmatKhan__brentlab_rnaseq_nf/BAM_INDEX", "DLBPointon/annotation-pipeline-nextflow/DLBPointon__annotation-pipeline-nextflow/samtools_index", "cmatKhan/rnaseq_pipeline_rewrite/cmatKhan__rnaseq_pipeline_rewrite/IndexFinalBam", "Gregor-Mendel-Institute/nf-methylpy/Gregor-Mendel-Institute__nf-methylpy/bam_index", "nf-modules/samtools/nf-modules__samtools/index", "BenNolann/rsi_analysis/BenNolann__rsi_analysis/indexBams", "wtsi-hgi/nextflow-pipelines/wtsi-hgi__nextflow-pipelines/index_cram", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/IndexBamMergedForSentieon", "MDegener/RNAseq-pipeline/MDegener__RNAseq-pipeline/index", "nibscbioinformatics/humgen/nibscbioinformatics__humgen/indexrecalibrated", "maxibor/admapipe/maxibor__admapipe/bam_index", "Thomas-LeCoent/reprohackathon-bioinfo/Thomas-LeCoent__reprohackathon-bioinfo/samtools", "naryamanesh/nf-bismarkAlignment/naryamanesh__nf-bismarkAlignment/runSamtools_merge_index", "ampatchlab/nf-rnasnv/ampatchlab__nf-rnasnv/samtools_index", "fargenfo/linkseq/fargenfo__linkseq/index_phased_bam", "jamez-eh/nf-fh-pcp-wes-mutect2/jamez-eh__nf-fh-pcp-wes-mutect2/samtoolsIndex", "torchij/nextflow/torchij__nextflow/index", "hugovaysset/ReproHackaton/hugovaysset__ReproHackaton/indexBAM", "JingQiChong/nf_CRACpipeline/JingQiChong__nf_CRACpipeline/indexBamfiles", "biocorecrg/BioNextflow/biocorecrg__BioNextflow/indexBam"], "list_wf_names": ["wtsi-hgi/nextflow-pipelines", "cmatKhan/brentlab_rnaseq_nf", "torchij/nextflow", "dakehero/LGAflow", "Gregor-Mendel-Institute/nf-methylpy", "biocorecrg/BioNextflow", "MarieGurke/nfvacal", "Thomas-LeCoent/reprohackathon-bioinfo", "R-Cardenas/nextflow_pipelines", "lifebit-ai/GenomeChronicler-Sarek-nf", "cpang429/directrna", "aduvermy/quantitative_nucleosome_analysis", "cgpu/PGP-UK-sarek", "hugovaysset/ReproHackaton", "MaddalenaCella/nf-lcWGS-mapping-and-imputation", "genomic-medicine-sweden/RareDisease_RNA_workflow", "dthorburn/Agam_Methylation", "cmatKhan/rnaseq_pipeline_rewrite", "MarieGurke/nfconsensus", "vib-singlecell-nf/vsn-pipelines", "SergFern/Variant_Calling", "adrodrzywolski/rnaseq_analysis_pipeline", "MDegener/RNAseq-pipeline", "BFSSI-Bioinformatics-Lab/bfssi-assembly-pipeline", "sickle-in-africa/saw.structural-variants", "jamez-eh/nf-fh-pcp-wes-mutect2", "JingQiChong/nf_CRACpipeline", "mblanche/nextflow", "nf-core/eager", "h3abionet/recalling", "BenNolann/rsi_analysis", "luslab/nf-clip", "fargenfo/linkseq", "DLBPointon/annotation-pipeline-nextflow", "nf-modules/samtools", "palfalvi/nanoporeseq", "crickbabs/rnaSeq_byBABS", "maxibor/admapipe", "ampatchlab/nf-rnasnv", "naryamanesh/nf-bismarkAlignment", "nibscbioinformatics/humgen"]}, {"nb_reuse": 6, "tools": ["kraken2"], "nb_own": 6, "list_own": ["ajodeh-juma", "MGordon09", "nibscbioinformatics", "nf-core", "jfy133", "cancerbioinformatics"], "nb_wf": 5, "list_wf": ["bacass", "viclara", "nf-core-bagobugs", "PATCH-pipeline", "archaeodiet"], "list_contrib": ["ajodeh-juma", "MGordon09", "rivera10", "bewt85", "nf-core-bot", "ewels", "jfy133", "maxulysse", "angelovangel", "KevinMenden", "xlinxlin", "apeltzer", "radhika-kataria", "d4straub", "drpatelh"], "nb_contrib": 15, "codes": ["\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        --use-mpa-style \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n    pigz -p $task.cpus *.fastq\n    echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//' | head -n 1 > ${software}.version.txt\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess KRAKEN2_KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n    label 'process_high_memory'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.1 conda-forge::pigz=2.6' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    } else {\n        container 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: txt\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n    def prefix       = options.suffix  ? \"${meta.id}${options.suffix}\"  : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads\n\n    pigz -p $task.cpus *.fastq\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["cancerbioinformatics/PATCH-pipeline/cancerbioinformatics__PATCH-pipeline/KRAKEN2_KRAKEN2", "nibscbioinformatics/nf-core-bagobugs/nibscbioinformatics__nf-core-bagobugs/KRAKEN2_KRAKEN2", "ajodeh-juma/viclara/ajodeh-juma__viclara/KRAKEN2_KRAKEN2", "jfy133/archaeodiet/jfy133__archaeodiet/KRAKEN2_KRAKEN2", "MGordon09/nf-core-bagobugs/MGordon09__nf-core-bagobugs/KRAKEN2_KRAKEN2", "nf-core/bacass/nf-core__bacass/KRAKEN2_KRAKEN2"], "list_wf_names": ["ajodeh-juma/viclara", "nf-core/bacass", "jfy133/archaeodiet", "nibscbioinformatics/nf-core-bagobugs", "MGordon09/nf-core-bagobugs", "cancerbioinformatics/PATCH-pipeline"]}, {"nb_reuse": 2, "tools": ["PLINK"], "nb_own": 2, "list_own": ["PGScatalog", "nf-core"], "nb_wf": 2, "list_wf": ["pgsc_calc", "modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "smlmbrt", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process PLINK_EXTRACT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::plink=1.90b6.21\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/plink:1.90b6.21--h779adbc_1' :\n        'quay.io/biocontainers/plink:1.90b6.21--h779adbc_1' }\"\n\n    input:\n    tuple val(meta), path(bed), path(bim), path(fam), path(variants)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    tuple val(meta), path(\"*.bim\"), emit: bim\n    tuple val(meta), path(\"*.fam\"), emit: fam\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if( \"$bed\" == \"${prefix}.bed\" ) error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    plink \\\\\n        --bfile ${meta.id} \\\\\n        $args \\\\\n        --extract $variants \\\\\n        --threads $task.cpus \\\\\n        --make-bed \\\\\n        --out $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        plink: \\$(echo \\$(plink --version) | sed 's/^PLINK v//;s/64.*//')\n    END_VERSIONS\n    \"\"\"\n}", "process PLINK_EXTRACT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::plink=1.90b6.21\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/plink:1.90b6.21--h779adbc_1' :\n        'quay.io/biocontainers/plink:1.90b6.21--h779adbc_1' }\"\n\n    input:\n    tuple val(meta), path(bed), path(bim), path(fam), path(variants)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    tuple val(meta), path(\"*.bim\"), emit: bim\n    tuple val(meta), path(\"*.fam\"), emit: fam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if( \"$bed\" == \"${prefix}.bed\" ) error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    plink \\\\\n        --bfile ${meta.id} \\\\\n        $args \\\\\n        --extract $variants \\\\\n        --threads $task.cpus \\\\\n        --make-bed \\\\\n        --out $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        plink: \\$(echo \\$(plink --version) | sed 's/^PLINK v//;s/64.*//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["PGScatalog/pgsc_calc/PGScatalog__pgsc_calc/PLINK_EXTRACT", "nf-core/modules/nf-core__modules/PLINK_EXTRACT"], "list_wf_names": ["PGScatalog/pgsc_calc", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess fastp {\n    label 'mc_small'\n    tag \"${libraryid}_L${lane}\"\n    publishDir \"${params.outdir}/FastP\", mode: params.publish_dir_mode\n\n    when: \n    params.complexity_filter_poly_g\n\n    input:\n    tuple samplename, libraryid, lane, colour, seqtype, organism, strandedness, udg, file(r1), file(r2) from ch_input_for_fastp.twocol\n\n    output:\n    tuple samplename, libraryid, lane, colour, seqtype, organism, strandedness, udg, path(\"*.pG.fq.gz\") into ch_output_from_fastp\n    path(\"*.json\") into ch_fastp_for_multiqc\n\n    script:\n    if( seqtype == 'SE' ){\n    \"\"\"\n    fastp --in1 ${r1} --out1 \"${r1.baseName}.pG.fq.gz\" -A -g --poly_g_min_len \"${params.complexity_filter_poly_g_min}\" -Q -L -w ${task.cpus} --json \"${r1.baseName}\"_L${lane}_fastp.json \n    \"\"\"\n    } else {\n    \"\"\"\n    fastp --in1 ${r1} --in2 ${r2} --out1 \"${r1.baseName}.pG.fq.gz\" --out2 \"${r2.baseName}.pG.fq.gz\" -A -g --poly_g_min_len \"${params.complexity_filter_poly_g_min}\" -Q -L -w ${task.cpus} --json \"${libraryid}\"_L${lane}_polyg_fastp.json \n    \"\"\"\n    }\n}"], "list_proc": ["nf-core/eager/nf-core__eager/fastp"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process GATK4_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")    , emit: bam\n    tuple val(meta), path(\"*.bai\")    , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics\"), emit: metrics\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_list = bam.collect{\"--INPUT $it\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" MarkDuplicates \\\\\n        $input_list \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --METRICS_FILE ${prefix}.metrics \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GATK4_MARKDUPLICATES"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["Picard", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess markduplicates{\n    label 'mc_small'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/deduplication/\", mode: params.publish_dir_mode,\n        saveAs: {filename -> \"${libraryid}/$filename\"}\n\n    when:\n    !params.skip_deduplication && params.dedupper == 'markduplicates'\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(bam), path(bai) from ch_filtering_for_markdup\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.metrics\") into ch_markdup_results_for_multiqc\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"${libraryid}_rmdup.bam\"), path(\"*.{bai,csi}\") into ch_output_from_markdup, ch_markdup_for_libeval\n\n    script:\n    def size = params.large_ref ? '-c' : ''\n\n    if ( bam.baseName != libraryid ) {\n                                                 \n    \"\"\"\n    mv ${bam} ${libraryid}.bam\n    picard -Xmx${task.memory.toMega()}M MarkDuplicates INPUT=${libraryid}.bam OUTPUT=${libraryid}_rmdup.bam REMOVE_DUPLICATES=TRUE AS=TRUE METRICS_FILE=\"${libraryid}_rmdup.metrics\" VALIDATION_STRINGENCY=SILENT\n    samtools index ${libraryid}_rmdup.bam ${size}\n    \"\"\"\n    } else {\n    \"\"\"\n    picard -Xmx${task.memory.toMega()}M MarkDuplicates INPUT=${libraryid}.bam OUTPUT=${libraryid}_rmdup.bam REMOVE_DUPLICATES=TRUE AS=TRUE METRICS_FILE=\"${libraryid}_rmdup.metrics\" VALIDATION_STRINGENCY=SILENT\n    samtools index ${libraryid}_rmdup.bam ${size}\n    \"\"\"\n    }\n\n}"], "list_proc": ["nf-core/eager/nf-core__eager/markduplicates"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 3, "tools": ["Salmon"], "nb_own": 3, "list_own": ["jiangfuqing", "nf-core", "robinfchan"], "nb_wf": 3, "list_wf": ["citeseq-nf", "scrnaseq", "scrna-seq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "robinfchan", "olgabot"], "nb_contrib": 8, "codes": ["\nprocess build_salmon_index {\n    if (params.custom_container) container \"${params.custom_container}\"\n    \n    tag \"$fasta\"\n    label 'mini_memory'\n    publishDir \"${params.outdir}/reference_genome/salmon_index\", mode: 'copy'\n\n    input:\n    file fasta from transcriptome_fasta_alevin.mix(transcriptome_fasta_alevin_extracted)\n\n    output:\n    file \"salmon_index\" into salmon_index_alevin\n\n    when:\n    !params.salmon_index && !params.skip_rna\n\n    script:\n    \"\"\"\n    salmon index -i salmon_index --gencode -k 31 -p 4 -t $fasta\n    \"\"\"\n}", "\nprocess build_salmon_index {\n    tag \"$fasta\"\n    label 'low_memory'\n    publishDir path: { params.save_reference ? \"${params.outdir}/reference_genome/salmon_index\" : params.outdir },\n                saveAs: { params.save_reference ? it : null }, mode: 'copy'\n\n    when:\n    params.aligner == 'alevin' && !params.salmon_index\n\n    input:\n    file fasta from transcriptome_fasta_alevin.mix(transcriptome_fasta_alevin_extracted)\n\n    output:\n    file \"salmon_index\" into salmon_index_alevin\n\n    when: params.aligner == 'alevin' && !params.salmon_index\n\n    script:\n    \"\"\"\n    salmon index -i salmon_index --gencode -k 31 -p 4 -t $fasta\n    \"\"\"\n}", " process build_salmon_index {\n         tag \"$fasta\"\n         publishDir \"${params.outdir}/salmon_index\", mode: 'copy'\n\n         input:\n         file fasta from fasta_alevin\n\n\n         output:\n         file \"salmon_index\" into salmon_index_alevin\n\n         script:\n\n         \"\"\"\n         salmon index -i salmon_index --gencode -k 31 -p 4 -t $fasta\n         \"\"\"\n     }"], "list_proc": ["robinfchan/citeseq-nf/robinfchan__citeseq-nf/build_salmon_index", "nf-core/scrnaseq/nf-core__scrnaseq/build_salmon_index", "jiangfuqing/scrna-seq/jiangfuqing__scrna-seq/build_salmon_index"], "list_wf_names": ["jiangfuqing/scrna-seq", "robinfchan/citeseq-nf", "nf-core/scrnaseq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process PICARD_FILTERSAMREADS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(readlist)\n    val filter\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard FilterSamReads] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    if ( filter == 'includeAligned' || filter == 'excludeAligned' ) {\n        \"\"\"\n        picard \\\\\n            FilterSamReads \\\\\n            -Xmx${avail_mem}g \\\\\n            --INPUT $bam \\\\\n            --OUTPUT ${prefix}.bam \\\\\n            --FILTER $filter \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            picard: \\$(picard FilterSamReads --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n        END_VERSIONS\n        \"\"\"\n    } else if ( filter == 'includeReadList' || filter == 'excludeReadList' ) {\n        \"\"\"\n        picard \\\\\n            FilterSamReads \\\\\n            -Xmx${avail_mem}g \\\\\n            --INPUT $bam \\\\\n            --OUTPUT ${prefix}.bam \\\\\n            --FILTER $filter \\\\\n            --READ_LIST_FILE $readlist \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            picard: \\$(picard FilterSamReads --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_FILTERSAMREADS"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["Picard"], "nb_own": 2, "list_own": ["nf-core", "CDCgov"], "nb_wf": 2, "list_wf": ["modules", "mycosnp-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "cjjossart", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "leebrian", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 108, "codes": ["process PICARD_ADDORREPLACEREADGROUPS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.27.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.27.1--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.27.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args        ?: ''\n    def prefix = task.ext.prefix    ?: \"${meta.id}\"\n    def ID = task.ext.id            ?: \"id\"\n    def LIBRARY= task.ext.library   ?: \"library\"\n    def PLATFORM= task.ext.platform ?: \"illumina\"\n    def BARCODE= task.ext.barcode   ?: \"barcode\"\n    def SAMPLE= task.ext.sample     ?: \"sample\"\n    def INDEX= task.ext.index       ?: \"index\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard AddOrReplaceReadGroups] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        AddOrReplaceReadGroups \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT ${bam} \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --RGID ${ID} \\\\\n        --RGLB ${LIBRARY} \\\\\n        --RGPL ${PLATFORM} \\\\\n        --RGPU ${BARCODE} \\\\\n        --RGSM ${SAMPLE} \\\\\n        --CREATE_INDEX true\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard AddOrReplaceReadGroups --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "process PICARD_ADDORREPLACEREADGROUPS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.9--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.9--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args        ?: ''\n    def prefix = task.ext.prefix    ?: \"${meta.id}\"\n    def ID = task.ext.id            ?: \"id\"\n    def LIBRARY= task.ext.library   ?: \"library\"\n    def PLATFORM= task.ext.platform ?: \"illumina\"\n    def BARCODE= task.ext.barcode   ?: \"barcode\"\n    def SAMPLE= task.ext.sample     ?: \"sample\"\n    def INDEX= task.ext.index       ?: \"index\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard AddOrReplaceReadGroups] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        AddOrReplaceReadGroups \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT ${bam} \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        -ID ${ID} \\\\\n        -LB ${LIBRARY} \\\\\n        -PL ${PLATFORM} \\\\\n        -PU ${BARCODE} \\\\\n        -SM ${SAMPLE} \\\\\n        -CREATE_INDEX true\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard AddOrReplaceReadGroups --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PICARD_ADDORREPLACEREADGROUPS", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/PICARD_ADDORREPLACEREADGROUPS"], "list_wf_names": ["CDCgov/mycosnp-nf", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process subsample_input {\n    tag \"${id}_subsample\"\n    publishDir \"${params.outdir}/seqtk/\", mode: params.publish_dir_mode\n\n    input:\n    set val(id), file(reads) from subsample_ch_reads_for_ribosomal_removal\n\n    output:\n    set val(id), file(\"*_${params.subsample}.fastq.gz\") into ch_reads_for_ribosomal_removal\n\n    script:\n    read1 = reads[0]\n    read2 = reads[1]\n    read1_prefix = read1.simpleName\n    read2_prefix = read2.simpleName\n\n    \"\"\"\n    seqtk sample -s100 ${read1} ${params.subsample} > ${read1_prefix}_${params.subsample}.fastq.gz\n    seqtk sample -s100 ${read2} ${params.subsample} > ${read2_prefix}_${params.subsample}.fastq.gz\n    \"\"\"\n    }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/subsample_input"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess bowtie_miRBase_hairpin_collapsed {\n    label 'process_medium'\n    tag \"$reads\"\n\n    input:\n    file reads from collapsed_fasta\n    file index from hairpin_index_bowtie_2\n\n    output:\n    file '*.bam' into miRBase_hairpin_collapse_bam\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.baseName\n    seq_center = params.seq_center ? \"--sam-RG ID:${prefix} --sam-RG 'CN:${params.seq_center}'\" : ''\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -p ${task.cpus} \\\\\n        -t \\\\\n        -k 50 \\\\\n        -a \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        -q <(cat $reads) \\\\\n        -S $seq_center \\\\\n        | samtools view -bS - > ${prefix}.bam\n    \"\"\"\n}"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/bowtie_miRBase_hairpin_collapsed"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess BOWTIE2_ASSEMBLY_BUILD {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1'\n    } else {\n        container 'quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1'\n    }\n\n    input:\n    tuple val(meta), path(assembly)\n\n    output:\n    tuple val(meta), path(assembly), path('bt2_index_base*'), emit: assembly_index\n    path '*.version.txt'                                    , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie\n    bowtie2-build --threads $task.cpus $assembly \"bt2_index_base\"\n    bowtie2 --version > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/BOWTIE2_ASSEMBLY_BUILD"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 2, "tools": ["MUSCLE"], "nb_own": 2, "list_own": ["nf-core", "jianhong"], "nb_wf": 2, "list_wf": ["modules", "shotgun"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MUSCLE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::muscle=3.8.1551\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/muscle:3.8.1551--h7d875b9_6' :\n        'quay.io/biocontainers/muscle:3.8.1551--h7d875b9_6' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.afa\") , optional: true, emit: aligned_fasta\n    tuple val(meta), path(\"*.phyi\"), optional: true, emit: phyi\n    tuple val(meta), path(\"*.phys\"), optional: true, emit: phys\n    tuple val(meta), path(\"*.clw\") , optional: true, emit: clustalw\n    tuple val(meta), path(\"*.html\"), optional: true, emit: html\n    tuple val(meta), path(\"*.msf\") , optional: true, emit: msf\n    tuple val(meta), path(\"*.tree\"), optional: true, emit: tree\n    path \"*.log\"                                   , emit: log\n    path \"versions.yml\"                            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def fasta_out = args.contains('-fasta') ? \"-fastaout ${prefix}_muscle_msa.afa\" : ''\n    def clw_out   = args.contains('-clw') ? \"-clwout ${prefix}_muscle_msa.clw\" : ''\n    def msf_out   = args.contains('-msf') ? \"-msfout ${prefix}_muscle_msa.msf\" : ''\n    def phys_out  = args.contains('-phys') ? \"-physout ${prefix}_muscle_msa.phys\" : ''\n    def phyi_out  = args.contains('-phyi') ? \"-phyiout ${prefix}_muscle_msa.phyi\" : ''\n    def html_out  = args.contains('-html') ? \"-htmlout ${prefix}_muscle_msa.html\" : ''\n    def tree_out  = args.contains('-maketree') ? \"-out ${prefix}_muscle_msa.tree\" : ''\n    \"\"\"\n    muscle \\\\\n        $args \\\\\n        -in $fasta \\\\\n        $fasta_out \\\\\n        $clw_out \\\\\n        $msf_out \\\\\n        $phys_out \\\\\n        $phyi_out \\\\\n        $html_out \\\\\n        $tree_out \\\\\n        -loga muscle_msa.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        muscle: \\$(muscle -version |  sed 's/^MUSCLE v//; s/by.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process MUSCLE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::muscle=3.8.1551\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/muscle:3.8.1551--h7d875b9_6' :\n        'quay.io/biocontainers/muscle:3.8.1551--h7d875b9_6' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.afa\") , optional: true, emit: aligned_fasta\n    tuple val(meta), path(\"*.phyi\"), optional: true, emit: phyi\n    tuple val(meta), path(\"*.phys\"), optional: true, emit: phys\n    tuple val(meta), path(\"*.clw\") , optional: true, emit: clustalw\n    tuple val(meta), path(\"*.html\"), optional: true, emit: html\n    tuple val(meta), path(\"*.msf\") , optional: true, emit: msf\n    tuple val(meta), path(\"*.tree\"), optional: true, emit: tree\n    path \"*.log\"                                   , emit: log\n    path \"versions.yml\"                            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def fasta_out = args.contains('-fasta') ? \"-fastaout ${prefix}_muscle_msa.afa\" : ''\n    def clw_out   = args.contains('-clw') ? \"-clwout ${prefix}_muscle_msa.clw\" : ''\n    def msf_out   = args.contains('-msf') ? \"-msfout ${prefix}_muscle_msa.msf\" : ''\n    def phys_out  = args.contains('-phys') ? \"-physout ${prefix}_muscle_msa.phys\" : ''\n    def phyi_out  = args.contains('-phyi') ? \"-phyiout ${prefix}_muscle_msa.phyi\" : ''\n    def html_out  = args.contains('-html') ? \"-htmlout ${prefix}_muscle_msa.html\" : ''\n    def tree_out  = args.contains('-maketree') ? \"-out ${prefix}_muscle_msa.tree\" : ''\n    \"\"\"\n    muscle \\\\\n        $args \\\\\n        -in $fasta \\\\\n        $fasta_out \\\\\n        $clw_out \\\\\n        $msf_out \\\\\n        $phys_out \\\\\n        $phyi_out \\\\\n        $html_out \\\\\n        $tree_out \\\\\n        -loga muscle_msa.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        muscle: \\$(muscle -version |  sed 's/^MUSCLE v//; s/by.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MUSCLE", "jianhong/shotgun/jianhong__shotgun/MUSCLE"], "list_wf_names": ["jianhong/shotgun", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": [" process dedup {\n        tag \"$name\"\n        label 'process_high'\n        publishDir \"${params.outdir}/dedup\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(name), path(bam), path(bai) from ch_aligned\n\n        output:\n        tuple val(name), path(\"${name}.dedup.bam\"), path(\"${name}.dedup.bam.bai\") into ch_dedup, ch_dedup_pureclip, ch_dedup_rseqc\n        path \"*.log\" into ch_dedup_mqc, ch_dedup_qc\n\n        script:\n        \"\"\"\n        umi_tools \\\\\n            dedup \\\\\n            --umi-separator=\"$params.umi_separator\" \\\\\n            -I $bam \\\\\n            -S ${name}.dedup.bam \\\\\n            --output-stats=${name} \\\\\n            --log=${name}.log\n        samtools index -@ $task.cpus ${name}.dedup.bam\n        \"\"\"\n    }"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/dedup"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["BWA", "SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess bwa {\n    label 'mc_medium'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/mapping/bwa\", mode: params.publish_dir_mode\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(r1), path(r2) from ch_lanemerge_for_bwa.dump(tag: \"bwa_input_reads\")\n    path index from bwa_index.collect().dump(tag: \"input_index\")\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.mapped.bam\"), path(\"*.{bai,csi}\") into ch_output_from_bwa   \n\n    when: \n    params.mapper == 'bwaaln'\n\n    script:\n    def size = params.large_ref ? '-c' : ''\n    def fasta = \"${index}/${fasta_base}\"\n\n                                                             \n    if ( seqtype == 'PE' && ( params.skip_collapse || params.skip_adapterremoval ) ){\n    \"\"\"\n    bwa aln -t ${task.cpus} $fasta ${r1} -n ${params.bwaalnn} -l ${params.bwaalnl} -k ${params.bwaalnk} -o ${params.bwaalno} -f ${libraryid}.r1.sai\n    bwa aln -t ${task.cpus} $fasta ${r2} -n ${params.bwaalnn} -l ${params.bwaalnl} -k ${params.bwaalnk} -o ${params.bwaalno} -f ${libraryid}.r2.sai\n    bwa sampe -r \"@RG\\\\tID:ILLUMINA-${libraryid}\\\\tSM:${samplename}\\\\tPL:illumina\\\\tPU:ILLUMINA-${libraryid}-${seqtype}\" $fasta ${libraryid}.r1.sai ${libraryid}.r2.sai ${r1} ${r2} | samtools sort -@ ${task.cpus - 1} -O bam - > ${libraryid}_\"${seqtype}\".mapped.bam\n    samtools index \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size}\n    \"\"\"\n    } else {\n                              \n    \"\"\"\n    bwa aln -t ${task.cpus} ${fasta} ${r1} -n ${params.bwaalnn} -l ${params.bwaalnl} -k ${params.bwaalnk} -o ${params.bwaalno} -f ${libraryid}.sai\n    bwa samse -r \"@RG\\\\tID:ILLUMINA-${libraryid}\\\\tSM:${samplename}\\\\tPL:illumina\\\\tPU:ILLUMINA-${libraryid}-${seqtype}\" $fasta ${libraryid}.sai $r1 | samtools sort -@ ${task.cpus - 1} -O bam - > \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    samtools index \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size}\n    \"\"\"\n    }\n    \n}"], "list_proc": ["nf-core/eager/nf-core__eager/bwa"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 2, "tools": ["snpEff"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "rnavar"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor"], "nb_contrib": 107, "codes": ["process SNPEFF {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    val   db\n    path  cache\n\n    output:\n    tuple val(meta), path(\"*.ann.vcf\"), emit: vcf\n    path \"*.csv\"                      , emit: report\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def avail_mem = 6\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 6GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def cache_command = cache ? \"-dataDir \\${PWD}/${cache}\" : \"\"\n    \"\"\"\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        $db \\\\\n        $args \\\\\n        -csvStats ${prefix}.csv \\\\\n        $cache_command \\\\\n        $vcf \\\\\n        > ${prefix}.ann.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}", "process SNPEFF {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    val   db\n    path  cache\n\n    output:\n    tuple val(meta), path(\"*.ann.vcf\"), emit: vcf\n    path \"*.csv\"                      , emit: report\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def avail_mem = 6\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 6GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def cache_command = cache ? \"-dataDir \\${PWD}/${cache}\" : \"\"\n    \"\"\"\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        $db \\\\\n        $args \\\\\n        -csvStats ${prefix}.csv \\\\\n        $cache_command \\\\\n        $vcf \\\\\n        > ${prefix}.ann.vcf\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SNPEFF", "nf-core/rnavar/nf-core__rnavar/SNPEFF"], "list_wf_names": ["nf-core/rnavar", "nf-core/modules"]}, {"nb_reuse": 42, "tools": ["MultiQC"], "nb_own": 34, "list_own": ["SpikyClip", "ggabernet", "salzmanlab", "luiskuhn", "jfy133", "wtsi-hgi", "kevbrick", "sguizard", "hukai916", "tamara-hodgetts", "avantonder", "lskatz", "Jojanneke-S", "lauramble", "NCBI-Codeathons", "chelauk", "happykhan", "MeghanaKB-Rheos", "nibscbioinformatics", "nf-core", "czbiohub", "luslab", "peterk87", "seandavi", "MGordon09", "BonaBeavis", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "christopher-hakkaart", "letovesnoi", "cguyomar", "qbic-pipelines", "remiolsen"], "nb_wf": 40, "list_wf": ["bench", "PRECODE", "nf-core-platypus", "nf-klebtest", "nanoseq", "lung-rna-seq", "nf-core-egatransfer", "associations", "ReadZS", "nf-iav-illumina", "nf-core-bagobugs", "hicscaff", "eqtl", "nf-core-mlst", "rnaseqpca", "nf-core-benchmark", "nf-core-cmgd", "archaeodiet", "vcreport", "bacass", "nf-core-denovohybrid", "llrnaseq", "nf-core-buggybarcodes", "rnaseq-vizfada", "testpipe", "nf-core-tamanmd", "assembleBAC", "nf-core-viralevo", "nf-ase", "demo_nfcore_obsolete", "nf-sicilian", "nf-core-conva", "nf-core-alttemplate", "nf-core-prototype", "nf-core-testpipeline", "ssds_nfcore", "clusterassembly", "dsltwotest", "liverctanalysis", "nf-atac-seq"], "list_contrib": ["yuukiiwa", "Emiller88", "alneberg", "ewels", "kaitlinchaung", "evanfloden", "maxulysse", "SpikyClip", "ggabernet", "salzmanlab", "luiskuhn", "jfy133", "ksaunders73", "mjmansfi", "lwratten", "kevbrick", "sguizard", "rivera10", "tamara-hodgetts", "nf-core-bot", "bewt85", "csawye01", "avantonder", "eameyer", "lskatz", "drice-codeathons", "kaurravneet4123", "pditommaso", "cying111", "lauramble", "LaurenceKuhl", "chelauk", "happykhan", "MeghanaKB-Rheos", "DSchreyer", "angelovangel", "xlinxlin", "peterk87", "seandavi", "MGordon09", "BonaBeavis", "alexa-salsbury", "KevinMenden", "mahesh-panchal", "YanFangBio", "apeltzer", "JoseEspinosa", "d4straub", "Sanger-ad7", "letovesnoi", "olgabot", "drpatelh", "cguyomar", "remiolsen"], "nb_contrib": 54, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    executor 'slurm'\n\tmemory   '8 GB'\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda '/data/scratch/DMP/UCEC/EVGENMOD/cjames/.conda/envs/nf-core'\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = 'multiqc'                                                                          \n    \"\"\"\n    multiqc -f $task.ext.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path 'multiqc_config.yaml'\n    path multiqc_custom_config\n    path software_versions\n    path ('fastqc/*')\n    path ('fastp/*')\n    path ('quast/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n    path 'fastqc_raw/*'\n    path 'fastqc_trimmed/*'\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_small'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n    path 'fastqc_raw/*'\n    path 'fastqc_trimmed/*'\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), publish_id: '') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path ('star/*')\n    \n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/MULTIQC", "chelauk/nf-core-egatransfer/chelauk__nf-core-egatransfer/MULTIQC", "remiolsen/hicscaff/remiolsen__hicscaff/MULTIQC", "KevinMenden/nf-core-testpipeline/KevinMenden__nf-core-testpipeline/MULTIQC", "happykhan/nf-klebtest/happykhan__nf-klebtest/MULTIQC", "KevinMenden/testpipe/KevinMenden__testpipe/MULTIQC", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/MULTIQC", "chelauk/nf-core-platypus/chelauk__nf-core-platypus/MULTIQC", "luslab/nf-core-denovohybrid/luslab__nf-core-denovohybrid/MULTIQC", "seandavi/nf-core-cmgd/seandavi__nf-core-cmgd/MULTIQC", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/MULTIQC", "MeghanaKB-Rheos/rnaseqpca/MeghanaKB-Rheos__rnaseqpca/MULTIQC", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/MULTIQC", "ggabernet/dsltwotest/ggabernet__dsltwotest/MULTIQC", "sguizard/nf-core-tamanmd/sguizard__nf-core-tamanmd/MULTIQC", "kevbrick/ssds_nfcore/kevbrick__ssds_nfcore/MULTIQC", "mahesh-panchal/nf-core-alttemplate/mahesh-panchal__nf-core-alttemplate/MULTIQC", "qbic-pipelines/vcreport/qbic-pipelines__vcreport/MULTIQC", "peterk87/nf-iav-illumina/peterk87__nf-iav-illumina/MULTIQC", "luiskuhn/liverctanalysis/luiskuhn__liverctanalysis/MULTIQC", "wtsi-hgi/associations/wtsi-hgi__associations/MULTIQC", "avantonder/assembleBAC/avantonder__assembleBAC/MULTIQC", "wtsi-hgi/eqtl/wtsi-hgi__eqtl/MULTIQC", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/MULTIQC", "NCBI-Codeathons/lung-rna-seq/NCBI-Codeathons__lung-rna-seq/MULTIQC", "MGordon09/nf-core-bagobugs/MGordon09__nf-core-bagobugs/MULTIQC", "jfy133/archaeodiet/jfy133__archaeodiet/MULTIQC", "nf-core/bacass/nf-core__bacass/MULTIQC", "salzmanlab/ReadZS/salzmanlab__ReadZS/MULTIQC", "nf-core/nanoseq/nf-core__nanoseq/MULTIQC", "MGordon09/nf-core-buggybarcodes/MGordon09__nf-core-buggybarcodes/MULTIQC", "cguyomar/nf-ase/cguyomar__nf-ase/MULTIQC", "BonaBeavis/nf-core-prototype/BonaBeavis__nf-core-prototype/MULTIQC", "nibscbioinformatics/nf-core-bagobugs/nibscbioinformatics__nf-core-bagobugs/MULTIQC", "christopher-hakkaart/bench/christopher-hakkaart__bench/MULTIQC", "lskatz/nf-core-mlst/lskatz__nf-core-mlst/MULTIQC", "nibscbioinformatics/nf-core-buggybarcodes/nibscbioinformatics__nf-core-buggybarcodes/MULTIQC", "letovesnoi/clusterassembly/letovesnoi__clusterassembly/MULTIQC", "Jojanneke-S/PRECODE/Jojanneke-S__PRECODE/MULTIQC", "hukai916/demo_nfcore_obsolete/hukai916__demo_nfcore_obsolete/MULTIQC", "JoseEspinosa/nf-core-benchmark/JoseEspinosa__nf-core-benchmark/MULTIQC", "czbiohub/nf-sicilian/czbiohub__nf-sicilian/MULTIQC"], "list_wf_names": ["KevinMenden/testpipe", "nf-core/bacass", "mahesh-panchal/nf-core-alttemplate", "lauramble/rnaseq-vizfada", "czbiohub/nf-sicilian", "nibscbioinformatics/nf-core-conva", "JoseEspinosa/nf-core-benchmark", "wtsi-hgi/associations", "chelauk/nf-core-egatransfer", "luiskuhn/liverctanalysis", "wtsi-hgi/eqtl", "christopher-hakkaart/bench", "hukai916/demo_nfcore_obsolete", "cguyomar/nf-ase", "peterk87/nf-iav-illumina", "qbic-pipelines/vcreport", "lskatz/nf-core-mlst", "MGordon09/nf-core-buggybarcodes", "chelauk/nf-core-platypus", "MeghanaKB-Rheos/rnaseqpca", "SpikyClip/llrnaseq", "MGordon09/nf-core-bagobugs", "letovesnoi/clusterassembly", "tamara-hodgetts/nf-atac-seq", "avantonder/assembleBAC", "ggabernet/dsltwotest", "kevbrick/ssds_nfcore", "nibscbioinformatics/nf-core-buggybarcodes", "jfy133/archaeodiet", "BonaBeavis/nf-core-prototype", "nibscbioinformatics/nf-core-bagobugs", "remiolsen/hicscaff", "KevinMenden/nf-core-testpipeline", "NCBI-Codeathons/lung-rna-seq", "happykhan/nf-klebtest", "luslab/nf-core-denovohybrid", "salzmanlab/ReadZS", "seandavi/nf-core-cmgd", "nf-core/nanoseq", "Jojanneke-S/PRECODE", "sguizard/nf-core-tamanmd", "nibscbioinformatics/nf-core-viralevo"]}, {"nb_reuse": 32, "tools": ["QualiMap"], "nb_own": 13, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "vladsaveliev", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "UCL-BLIC", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 27, "list_wf": ["sarek_ubec", "layer_lab_chco", "ion-somatic-variant-calling", "layer_lab_vc", "germline_somatic", "sparkling-preprocessing", "dx_sarek", "covbench", "haplosarek", "sparkling-gatk", "sarek-mirror-cache", "Sarek_v2.3.FIX1", "PGP-UK-sarek", "sarek-mirror", "pgp-chronek", "test_nextflow_sarek", "sarek-genomechronicler", "saw.sarek", "somatic-variant-caller", "custom_sarek", "sarek", "GenomeChronicler-Sarek-nf", "cawdor", "layer_lab_caw", "long-reads", "valibam", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "pprieto", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "PhilPalmer", "ffmmulder", "rmoran7", "vladsaveliev", "lescai", "cgpu", "apeltzer", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 29, "codes": ["\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess RunQualimap {\n  tag {idPatient + \"-\" + idSample}\n\n  publishDir \"${params.outDir}/Reports/Qualimap\", mode: params.publishDirMode\n\n  input:\n  set idPatient, idSample, file(bam), file(bai) from bamForQualimap\n\n  output:\n  file(\"${bam.baseName}\") into qualimapReport\n\n  when: !params.noQualimap\n\n  script:\n  \"\"\"\n  qualimap --java-mem-size=${task.memory.toGiga()}G \\\n  bamqc \\\n  -bam ${bam} \\\n  --paint-chromosome-limits \\\n  --genome-gc-distr HUMAN \\\n  -nt ${task.cpus} \\\n  -skip-duplicated \\\n  --skip-dup-mode 0 \\\n  -outdir ${bam.baseName} \\\n  -outformat HTML\n  \"\"\"\n}", "\nprocess RunBamQCmapped {\n    tag \"$bam\"\n\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(bam) from bam_sort_qc\n\n    output:\n    file(\"${name}\") into bamQCmappedReport\n\n    when: !params.skip_multiqc\n\n    script:\n                                                         \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCrecalibrated {\n    tag \"$bam\"\n\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(bam), file(bai) from bam_for_qc\n\n    output:\n    file(\"${name}_recalibrated\") into bamQCrecalibratedReport\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name}_recalibrated \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCmapped {\n    tag \"$bam\"\n\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(shared_matched_pair_id), val(unique_subject_id), val(case_control_status), val(name), file(bam) from bam_sort_qc\n\n    output:\n    file(\"${name}\") into bamQCmappedReport\n\n    when: !params.skip_multiqc\n\n    script:\n                                                         \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCrecalibrated {\n    tag \"$bam\"\n\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(shared_matched_pair_id), val(unique_subject_id), val(case_control_status), val(name), file(bam), file(bai) from bam_for_qc\n\n    output:\n    file(\"${name}_recalibrated\") into bamQCrecalibratedReport\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name}_recalibrated \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n                         \n    label 'cpus_16'\n                  \n\n    tag {idPatient + \"-\" + idSample}\n\n                                                                                             \n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam) \n        file(targetBED)\n\n    output:\n        file(\"${bam.baseName}\")\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    init.sh\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n                         \n    label 'container_llab'\n    label 'cpus_16'\n                  \n\n    tag {idPatient + \"-\" + idSample}\n\n                                                                                             \n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam) \n        file(targetBED)\n\n    output:\n        file(\"${bam.baseName}\")\n\n                                 \n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    init.sh\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCmapped {\n    tag \"$bam\"\n    container 'nfcore/sarek:2.5.1'\n    memory threadmem_more \n    cpus 4\n\n    input:\n    set val(name), file(bam) from bam_sort_qc\n\n    output:\n    file(\"${name}\") into bamQCmappedReport\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCrecalibrated {\n    tag \"$bam\"\n    container 'nfcore/sarek:2.5.1'\n    memory threadmem_more\n    cpus 4\n\n    input:\n    set val(name), file(bam), file(bai) from indexed_bam_qc\n\n    output:\n    file(\"${name}_recalibrated\") into bamQCrecalibratedReport\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    --java-mem-size=${task.memory.toGiga()}G \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name}_recalibrated \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCmapped {\n    tag \"$bam\"\n    container 'nfcore/sarek:2.5.1'\n    memory threadmem_more \n    cpus 4\n\n    input:\n    set val(name), file(bam) from bam_sort_qc\n\n    output:\n    file(\"${name}\") into bamQCmappedReport\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n                         \n    label 'container_llab'\n    label 'cpus_16'\n                  \n\n    tag {idPatient + \"-\" + idSample}\n\n                                                                                             \n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam) \n        file(targetBED)\n\n    output:\n        file(\"${bam.baseName}\")\n\n                                 \n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    init.sh\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCrecalibrated {\n    tag \"$bam\"\n    container 'nfcore/sarek:2.5.1'\n    memory threadmem_more\n    cpus 4\n\n    input:\n    set val(name), file(bam), file(bai) from indexed_bam_qc\n\n    output:\n    file(\"${name}_recalibrated\") into bamQCrecalibratedReport\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    --java-mem-size=${task.memory.toGiga()}G \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name}_recalibrated \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess qualimap_bamqc {\n  tag \"$bam\"\n\n  input:\n  file(bam) from qualimap_bamqc_channel\n  each file(ref) from ref_qualimap_bamqc_channel\n\n  output:\n  file(\"${bam.baseName}_folder\") into inliner_channel\n  file(\"*\") into multiqc_channel_qualimap_bamqc\n  \n  script:\n  \"\"\"\n  qualimap \\\n  bamqc \\\n  -bam ${bam} \\\n  --paint-chromosome-limits \\\n  --genome-gc-distr HUMAN \\\n  -skip-duplicated \\\n  --skip-dup-mode 0 \\\n  -outdir ${bam.baseName}_folder \\\n  -outformat HTML \\\n  -nt ${task.cpus}\n  \"\"\"\n}", "\nprocess BamQC {\n    machineType 'mem3_ssd1_x8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess bam_qc {\n    tag \"$bam\"\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(name), file(bam), file(bai) from marked_bam_qc\n\n    output:\n    file(\"${name}\") into bam_qc_report\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess RunBamQCmapped {\n  tag {idPatient + \"-\" + idSample}\n\n  publishDir \"${params.outDir}/Reports/bamQC\", mode: params.publishDirMode\n\n  input:\n    file(targetBED) from ch_targetBED\n    set idPatient, status, idSample, idRun, file(bam) from mappedBamForQC\n\n  output:\n    file(\"${bam.baseName}\") into bamQCmappedReport\n\n  when: !params.noReports && !params.noBAMQC\n\n  script:\n  use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n  \"\"\"\n  qualimap --java-mem-size=${task.memory.toGiga()}G \\\n  bamqc \\\n  -bam ${bam} \\\n  --paint-chromosome-limits \\\n  --genome-gc-distr HUMAN \\\n  $use_bed \\\n  -nt ${task.cpus} \\\n  -skip-duplicated \\\n  --skip-dup-mode 0 \\\n  -outdir ${bam.baseName} \\\n  -outformat HTML\n  \"\"\"\n}", "\nprocess qualimap_bamqc {\n  tag \"$bam\"\n\n  input:\n  file(bam) from qualimap_bamqc_channel\n  each file(ref) from ref_qualimap_bamqc_channel\n\n  output:\n  file(\"${bam.baseName}_folder\") into inliner_channel\n  file(\"*\") into multiqc_channel_qualimap_bamqc\n  \n  script:\n  \"\"\"\n  qualimap \\\n  bamqc \\\n  -bam ${bam} \\\n  --paint-chromosome-limits \\\n  --genome-gc-distr HUMAN \\\n  -skip-duplicated \\\n  --skip-dup-mode 0 \\\n  -outdir ${bam.baseName}_folder \\\n  -outformat HTML \\\n  -nt ${task.cpus}\n  \"\"\"\n}", "\nprocess RunBamQCrecalibrated {\n  tag {idPatient + \"-\" + idSample}\n\n  publishDir \"${params.outDir}/Reports/bamQC\", mode: params.publishDirMode\n\n  input:\n    file(targetBED) from ch_targetBED\n    set idPatient, status, idSample, file(bam), file(bai) from bamForBamQC\n\n  output:\n    file(\"${bam.baseName}\") into bamQCrecalibratedReport\n\n  when: !params.noReports && !params.noBAMQC\n\n  script:\n  use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n  \"\"\"\n  qualimap --java-mem-size=${task.memory.toGiga()}G \\\n  bamqc \\\n  -bam ${bam} \\\n  --paint-chromosome-limits \\\n  --genome-gc-distr HUMAN \\\n  $use_bed \\\n  -nt ${task.cpus} \\\n  -skip-duplicated \\\n  --skip-dup-mode 0 \\\n  -outdir ${bam.baseName} \\\n  -outformat HTML\n  \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_targetBED\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.targetBED ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}"], "list_proc": ["cgpu/haplosarek/cgpu__haplosarek/BamQC", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/BamQC", "vladsaveliev/cawdor/vladsaveliev__cawdor/RunQualimap", "lifebit-ai/somatic-variant-caller/lifebit-ai__somatic-variant-caller/RunBamQCmapped", "lifebit-ai/somatic-variant-caller/lifebit-ai__somatic-variant-caller/RunBamQCrecalibrated", "cgpu/ion-somatic-variant-calling/cgpu__ion-somatic-variant-calling/RunBamQCmapped", "cgpu/ion-somatic-variant-calling/cgpu__ion-somatic-variant-calling/RunBamQCrecalibrated", "cgpu/sarek-mirror/cgpu__sarek-mirror/BamQC", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/BamQC", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/BamQC", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/BamQC", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/BamQC", "cgpu/pgp-chronek/cgpu__pgp-chronek/BamQC", "cgpu/sparkling-gatk/cgpu__sparkling-gatk/RunBamQCmapped", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/BamQC", "cgpu/sparkling-gatk/cgpu__sparkling-gatk/RunBamQCrecalibrated", "cgpu/sparkling-preprocessing/cgpu__sparkling-preprocessing/RunBamQCmapped", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/BamQC", "cgpu/sparkling-preprocessing/cgpu__sparkling-preprocessing/RunBamQCrecalibrated", "nf-core/sarek/nf-core__sarek/BamQC", "cgpu/valibam/cgpu__valibam/qualimap_bamqc", "rmoran7/custom_sarek/rmoran7__custom_sarek/BamQC", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/BamQC", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/BamQC", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/BamQC", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/BamQC", "lifebit-ai/long-reads/lifebit-ai__long-reads/bam_qc", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/RunBamQCmapped", "cgpu/covbench/cgpu__covbench/qualimap_bamqc", "UCL-BLIC/Sarek_v2.3.FIX1/UCL-BLIC__Sarek_v2.3.FIX1/RunBamQCrecalibrated", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/BamQC", "rmoran7/dx_sarek/rmoran7__dx_sarek/BamQC"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "cgpu/valibam", "lifebit-ai/GenomeChronicler-Sarek-nf", "ryanlayerlab/layer_lab_chco", "UMCUGenetics/sarek_ubec", "vladsaveliev/cawdor", "cgpu/PGP-UK-sarek", "lifebit-ai/somatic-variant-caller", "cgpu/sarek-mirror", "cgpu/haplosarek", "sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "cgpu/sarek-mirror-cache", "cgpu/ion-somatic-variant-calling", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "rmoran7/dx_sarek", "javaidm/layer_lab_vc", "cgpu/sparkling-preprocessing", "cgpu/pgp-chronek", "ryanlayerlab/layer_lab_caw", "cgpu/covbench", "UCL-BLIC/Sarek_v2.3.FIX1", "lifebit-ai/long-reads", "cgpu/sparkling-gatk"]}, {"nb_reuse": 78, "tools": ["SAMtools"], "nb_own": 15, "list_own": ["alam1988", "ajodeh-juma", "tamara-hodgetts", "vibbits", "jianhong", "jfy133", "nibscbioinformatics", "SpikyClip", "nf-core", "ray1919", "Akazhiel", "junyu-boston", "lauramble", "cancerbioinformatics", "remiolsen"], "nb_wf": 17, "list_wf": ["archaeodiet", "viclara", "bactmap", "rnaseq-vizfada", "hicscaff", "dicerna-rnaseq", "PATCH-pipeline", "NeoPred-NF", "rnaseq-editing", "nf-core-viralevo", "bacass", "nf-core-conva", "nf-atac-seq", "universalModule", "lRNA-Seq", "llrnaseq", "Nextflow"], "list_contrib": ["ajodeh-juma", "alexandregilardet", "alneberg", "FriederikeHanssen", "ewels", "drejom", "arontommi", "evanfloden", "maxulysse", "rsuchecki", "SpikyClip", "matrulda", "ggabernet", "veeravalli", "jordwil", "vezzi", "alex-botzki", "colindaven", "antunderwood", "lpantano", "skrakau", "jfy133", "chuan-wang", "ppericard", "grst", "pcantalupo", "rivera10", "tamara-hodgetts", "nf-core-bot", "mvanins", "Galithil", "bewt85", "avantonder", "jun-wan", "na399", "sofiahaglund", "orionzhou", "Akazhiel", "pditommaso", "abhi18av", "lauramble", "robsyme", "remiolsen", "BABS-STP1", "senthil10", "kviljoen", "rfenouil", "thanhleviet", "angelovangel", "xlinxlin", "jburos", "mashehu", "jianhong", "alam1988", "abotzki", "Hammarn", "sven1103", "paulklemm", "jemten", "pranathivemuri", "marchoeppner", "KevinMenden", "apeltzer", "ray1919", "radhika-kataria", "aanil", "silviamorins", "d4straub", "kaurravneet4123", "olgabot", "drpatelh", "amayer21", "zxl124"], "nb_contrib": 73, "codes": ["\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $options.args \\\\\n        -@ $task.cpus \\\\\n        $endedness \\\\\n        $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams)\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\"), emit: bam\n    path  \"*.version.txt\"                 , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools merge ${prefix}.bam $bams\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess PROFILEDAMAGE {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n                                                    \n                                                                                                 \n                                                                                                                                      \n                                                                                                                                              \n    conda (params.enable_conda ? \"bioconda::damageprofiler=1.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/https://depot.galaxyproject.org/singularity/damageprofiler:1.1--hdfd78af_2\"\n    } else {\n        container \"quay.io/biocontainers/quay.io/biocontainers/damageprofiler:1.1--hdfd78af_2\"\n    }\n\n    input:\n                                                                                                           \n                                                                                 \n                                                                                                                 \n                                                                                              \n                                                                                             \n                                                                                      \n    tuple val(meta), path(bam)\n\n    output:\n                                                                                  \n    tuple val(meta), path(\"*.bam\"), emit: bam\n                                                                         \n    path \"*.version.txt\"          , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n                                                                                                                      \n                                                                                                                               \n                                                                                                             \n                                                                                                                                            \n                                                                                                         \n                                                                                   \n                                                                                                 \n                                                                                                     \n    \"\"\"\n    samtools \\\\\n        sort \\\\\n        $options.args \\\\\n        -@ $task.cpus \\\\\n        -o ${prefix}.bam \\\\\n        -T $prefix \\\\\n        $bam\n\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n                                     \n                                         \n                                                                                                                                                                     \n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"*.version.txt\"                         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n\n    \"\"\"\n    samtools index $options.args $input\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"        , emit: fai\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools faidx $fasta\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"        , emit: fai\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools faidx $fasta\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    \n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    \n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n    \n    input:\n    tuple val(meta), path(bam), path(bai)\n    \n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    \n    output:\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    \n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", " process SAMTOOLS_INDEX {\n     tag \"$meta.id\"\n     publishDir \"${params.outdir}/${options.publish_dir}\",\n         mode: options.publish_mode,\n         enabled: options.publish_enabled\n\n     conda (params.enable_conda ? \"bioconda::samtools=1.09\" : null)\n     if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n         container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n     } else {\n         container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n     }\n\n     input:\n     tuple val(meta), path(bam)\n\n     output:\n     tuple val(meta), path(bam), path(\"*.bai\"), emit: bai\n     path \"samtools.version.txt\", emit: version\n\n     script:\n     params.options.forEach{key, value -> options[key]=value}\n     \"\"\"\n     samtools index $bam\n     echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > samtools.version.txt\n     \"\"\"\n }", " process SAMTOOLS_SORT {\n     tag \"$meta.id\"\n     label 'process_medium'\n     publishDir \"${params.outdir}/${options.publish_dir}\",\n         mode: options.publish_mode,\n         enabled: options.publish_enabled\n\n     conda (params.enable_conda ? \"bioconda::samtools=1.09\" : null)\n     if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n         container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n     } else {\n         container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n     }\n\n     input:\n     tuple val(meta), path(bam)\n\n     output:\n     tuple val(meta), path(\"*.bam\"), emit: bam\n     path \"samtools.version.txt\", emit: version\n\n     script:\n     params.options.forEach{key, value -> options[key]=value}\n     def prefix   = ioptions.suffix ? \"${meta.id}${ioptions.suffix}\" : \"${meta.id}\"\n     \"\"\"\n     samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n     echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > samtools.version.txt\n     \"\"\"\n }", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n                                                    \n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    \n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    \n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n    \n    input:\n    tuple val(meta), path(bam), path(bai)\n    \n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    \n    output:\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    \n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $options.args \\\\\n        -@ $task.cpus \\\\\n        $endedness \\\\\n        $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }    \n\nif (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/SAMTOOLS_FLAGSTAT", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/SAMTOOLS_STATS", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/SAMTOOLS_IDXSTATS", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/SAMTOOLS_SORT", "lauramble/rnaseq-vizfada/lauramble__rnaseq-vizfada/SAMTOOLS_INDEX", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/SAMTOOLS_INDEX", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/SAMTOOLS_IDXSTATS", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/SAMTOOLS_SORT", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/SAMTOOLS_STATS", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_FASTQ", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/SAMTOOLS_FLAGSTAT", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_MERGE", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_IDXSTATS", "jfy133/archaeodiet/jfy133__archaeodiet/PROFILEDAMAGE", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_SORT", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_STATS", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_FLAGSTAT", "jfy133/archaeodiet/jfy133__archaeodiet/SAMTOOLS_INDEX", "Akazhiel/NeoPred-NF/Akazhiel__NeoPred-NF/SAMTOOLS_INDEX", "nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/SAMTOOLS_FLAGSTAT", "remiolsen/hicscaff/remiolsen__hicscaff/SAMTOOLS_FAIDX", "remiolsen/hicscaff/remiolsen__hicscaff/SAMTOOLS_SORT", "nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/SAMTOOLS_INDEX", "remiolsen/hicscaff/remiolsen__hicscaff/SAMTOOLS_VIEW", "nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/SAMTOOLS_STATS", "remiolsen/hicscaff/remiolsen__hicscaff/SAMTOOLS_INDEX", "nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/SAMTOOLS_IDXSTATS", "nibscbioinformatics/nf-core-conva/nibscbioinformatics__nf-core-conva/SAMTOOLS_SORT", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/SAMTOOLS_IDXSTATS", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/SAMTOOLS_FAIDX", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/SAMTOOLS_INDEX", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/SAMTOOLS_STATS", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/SAMTOOLS_SORT", "nibscbioinformatics/nf-core-viralevo/nibscbioinformatics__nf-core-viralevo/SAMTOOLS_FLAGSTAT", "ajodeh-juma/viclara/ajodeh-juma__viclara/SAMTOOLS_STATS", "ajodeh-juma/viclara/ajodeh-juma__viclara/SAMTOOLS_IDXSTATS", "ajodeh-juma/viclara/ajodeh-juma__viclara/SAMTOOLS_SORT", "ajodeh-juma/viclara/ajodeh-juma__viclara/SAMTOOLS_VIEW", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/SAMTOOLS_INDEX", "ajodeh-juma/viclara/ajodeh-juma__viclara/SAMTOOLS_INDEX", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/SAMTOOLS_FLAGSTAT", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/SAMTOOLS_SORT", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/SAMTOOLS_STATS", "ajodeh-juma/viclara/ajodeh-juma__viclara/SAMTOOLS_FLAGSTAT", "SpikyClip/llrnaseq/SpikyClip__llrnaseq/SAMTOOLS_IDXSTATS", "alam1988/Nextflow/alam1988__Nextflow/SAMTOOLS_IDXSTATS", "alam1988/Nextflow/alam1988__Nextflow/SAMTOOLS_FLAGSTAT", "alam1988/Nextflow/alam1988__Nextflow/SAMTOOLS_STATS", "alam1988/Nextflow/alam1988__Nextflow/SAMTOOLS_INDEX", "alam1988/Nextflow/alam1988__Nextflow/SAMTOOLS_SORT", "jianhong/universalModule/jianhong__universalModule/SAMTOOLS_INDEX", "jianhong/universalModule/jianhong__universalModule/SAMTOOLS_SORT", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/SAMTOOLS_INDEX", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/SAMTOOLS_STATS", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/SAMTOOLS_FLAGSTAT", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/SAMTOOLS_SORT", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/SAMTOOLS_IDXSTATS", "tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/SAMTOOLS_VIEW", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/SAMTOOLS_IDXSTATS", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/SAMTOOLS_FLAGSTAT", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/SAMTOOLS_STATS", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/SAMTOOLS_INDEX", "junyu-boston/dicerna-rnaseq/junyu-boston__dicerna-rnaseq/SAMTOOLS_SORT", "cancerbioinformatics/PATCH-pipeline/cancerbioinformatics__PATCH-pipeline/SAMTOOLS_FASTQ", "cancerbioinformatics/PATCH-pipeline/cancerbioinformatics__PATCH-pipeline/SAMTOOLS_VIEW", "cancerbioinformatics/PATCH-pipeline/cancerbioinformatics__PATCH-pipeline/SAMTOOLS_SORT", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/SAMTOOLS_STATS", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/SAMTOOLS_INDEX", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/SAMTOOLS_FLAGSTAT", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/SAMTOOLS_SORT", "vibbits/rnaseq-editing/vibbits__rnaseq-editing/SAMTOOLS_IDXSTATS", "nf-core/bacass/nf-core__bacass/SAMTOOLS_INDEX", "nf-core/bacass/nf-core__bacass/SAMTOOLS_SORT", "nf-core/bactmap/nf-core__bactmap/SAMTOOLS_INDEX", "nf-core/bactmap/nf-core__bactmap/SAMTOOLS_STATS", "nf-core/bactmap/nf-core__bactmap/SAMTOOLS_FLAGSTAT", "nf-core/bactmap/nf-core__bactmap/SAMTOOLS_IDXSTATS", "nf-core/bactmap/nf-core__bactmap/SAMTOOLS_SORT"], "list_wf_names": ["Akazhiel/NeoPred-NF", "ajodeh-juma/viclara", "nf-core/bacass", "ray1919/lRNA-Seq", "junyu-boston/dicerna-rnaseq", "jfy133/archaeodiet", "alam1988/Nextflow", "lauramble/rnaseq-vizfada", "jianhong/universalModule", "vibbits/rnaseq-editing", "nibscbioinformatics/nf-core-conva", "SpikyClip/llrnaseq", "remiolsen/hicscaff", "nf-core/bactmap", "tamara-hodgetts/nf-atac-seq", "cancerbioinformatics/PATCH-pipeline", "nibscbioinformatics/nf-core-viralevo"]}, {"nb_reuse": 1, "tools": ["SnpSift"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process SNPSIFT_SPLIT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::snpsift=4.3.1t\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpsift:4.3.1t--hdfd78af_3' :\n        'quay.io/biocontainers/snpsift:4.3.1t--hdfd78af_3' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.vcf\"), emit: out_vcfs\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.split) {\n        \"\"\"\n        SnpSift \\\\\n            split \\\\\n            $args \\\\\n            $vcf\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            snpsift: \\$( echo \\$(SnpSift split -h 2>&1) | sed 's/^.*version //' | sed 's/(.*//' | sed 's/t//g' )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        SnpSift \\\\\n            split \\\\\n            -j \\\\\n            $args \\\\\n            $vcf \\\\\n            > ${prefix}.joined.vcf\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            snpsift: \\$( echo \\$(SnpSift split -h 2>&1) | sed 's/^.*version //' | sed 's/(.*//' | sed 's/t//g' )\n        END_VERSIONS\n        \"\"\"\n    }\n\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SNPSIFT_SPLIT"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 20, "tools": ["Minimap2"], "nb_own": 6, "list_own": ["sanger-tol", "ksumngs", "sguizard", "ABMicroBioinf", "nf-core", "xiaoli-dong"], "nb_wf": 7, "list_wf": ["pathogen", "readmapping", "isoseq", "magph", "nf-modules", "modules", "nanoseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "cying111", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "priyanka-surana", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "Alexey-ebi", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "xiaoli-dong", "Gwennid", "Jeremy1805", "peterwharrison", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "lwratten", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "csawye01", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 114, "codes": ["process MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21 bioconda::samtools=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:1679e915ddb9d6b4abda91880c4b48857d471bd8-0' :\n        'quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:1679e915ddb9d6b4abda91880c4b48857d471bd8-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n    val bam_format\n    val cigar_paf_format\n    val cigar_bam\n\n    output:\n    tuple val(meta), path(\"*.paf\"), optional: true, emit: paf\n    tuple val(meta), path(\"*.bam\"), optional: true, emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    def bam_output = bam_format ? \"-a | samtools sort | samtools view -@ ${task.cpus} -b -h -o ${prefix}.bam\" : \"-o ${prefix}.paf\"\n    def cigar_paf = cigar_paf_format && !bam_format ? \"-c\" : ''\n    def set_cigar_bam = cigar_bam && bam_format ? \"-L\" : ''\n    \"\"\"\n    minimap2 \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $cigar_paf \\\\\n        $set_cigar_bam \\\\\n        $bam_output\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_SHORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_LONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    \n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_LONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    \n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process MINIMAP2_INDEX {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0' :\n        'quay.io/biocontainers/minimap2:2.21--h5bf99c6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.mmi\"        , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    minimap2 \\\\\n        -t $task.cpus \\\\\n        -d ${fasta.baseName}.mmi \\\\\n        $args \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21 bioconda::samtools=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:1679e915ddb9d6b4abda91880c4b48857d471bd8-0' :\n        'quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:1679e915ddb9d6b4abda91880c4b48857d471bd8-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n    val bam_format\n    val cigar_paf_format\n    val cigar_bam\n\n    output:\n    tuple val(meta), path(\"*.paf\"), optional: true, emit: paf\n    tuple val(meta), path(\"*.bam\"), optional: true, emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    def bam_output = bam_format ? \"-a | samtools sort | samtools view -@ ${task.cpus} -b -h -o ${prefix}.bam\" : \"-o ${prefix}.paf\"\n    def cigar_paf = cigar_paf_format && !bam_format ? \"-c\" : ''\n    def set_cigar_bam = cigar_bam && bam_format ? \"-L\" : ''\n    \"\"\"\n    minimap2 \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $cigar_paf \\\\\n        $set_cigar_bam \\\\\n        $bam_output\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_LONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    \n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_SHORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process MINIMAP2_INDEX {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0' :\n        'quay.io/biocontainers/minimap2:2.21--h5bf99c6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.mmi\"        , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    minimap2 \\\\\n        -t $task.cpus \\\\\n        -d ${fasta.baseName}.mmi \\\\\n        $args \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_LONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    \n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_INDEX {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda     (params.enable_conda ? \"bioconda::minimap2=2.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.17--hed695b0_3\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.17--hed695b0_3\"\n    }\n\n    input:\n    tuple path(fasta), path(sizes), val(gtf), val(bed), val(is_transcripts), val(annotation_str)\n\n    output:\n    tuple path(fasta), path(sizes), val(gtf), val(bed), val(is_transcripts), path(\"*.mmi\"), val(annotation_str), emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def preset    = (params.protocol == 'DNA' || is_transcripts) ? \"-ax map-ont\" : \"-ax splice\"\n    def kmer      = (params.protocol == 'directRNA') ? \"-k14\" : \"\"\n    def stranded  = (params.stranded || params.protocol == 'directRNA') ? \"-uf\" : \"\"\n    def junctions = (params.protocol != 'DNA' && bed) ? \"--junc-bed ${file(bed)}\" : \"\"\n    \"\"\"\n    minimap2 \\\\\n        $preset \\\\\n        $kmer \\\\\n        $stranded \\\\\n        $junctions \\\\\n        -t $task.cpus \\\\\n        -d ${fasta}.mmi \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21 bioconda::samtools=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:1679e915ddb9d6b4abda91880c4b48857d471bd8-0' :\n        'quay.io/biocontainers/mulled-v2-66534bcbb7031a148b13e2ad42583020b9cd25c4:1679e915ddb9d6b4abda91880c4b48857d471bd8-0' }\"\n\n    input:\n    tuple val(meta), path(reads), path(reference)\n    val bam_format\n    val cigar_paf_format\n    val cigar_bam\n\n    output:\n    tuple val(meta), path(\"*.paf\"), optional: true, emit: paf\n    tuple val(meta), path(\"*.bam\"), optional: true, emit: bam\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    def bam_output = bam_format ? \"-a | samtools sort | samtools view -@ ${task.cpus} -b -h -o ${prefix}.bam\" : \"-o ${prefix}.paf\"\n    def cigar_paf = cigar_paf_format && !bam_format ? \"-c\" : ''\n    def set_cigar_bam = cigar_bam && bam_format ? \"-L\" : ''\n    \"\"\"\n    minimap2 \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $cigar_paf \\\\\n        $set_cigar_bam \\\\\n        $bam_output\n\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'meta.id') }\n\n    conda     (params.enable_conda ? \"bioconda::minimap2=2.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.17--hed695b0_3\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.17--hed695b0_3\"\n    }\n\n    input:\n    tuple val(meta), path(fastq), path(fasta), path(sizes), val(gtf), val(bed), val(is_transcripts), path(index)\n\n    output:\n    tuple val(meta), path(sizes), val(is_transcripts), path(\"*.sam\"), emit: align_sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def preset    = (params.protocol == 'DNA' || is_transcripts) ? \"-ax map-ont\" : \"-ax splice\"\n    def kmer      = (params.protocol == 'directRNA') ? \"-k14\" : \"\"\n    def stranded  = (params.stranded || params.protocol == 'directRNA') ? \"-uf\" : \"\"\n    def junctions = (params.protocol != 'DNA' && bed) ? \"--junc-bed ${file(bed)}\" : \"\"\n    \"\"\"\n    minimap2 \\\\\n        $preset \\\\\n        $kmer \\\\\n        $stranded \\\\\n        $junctions \\\\\n        -t $task.cpus \\\\\n        $index \\\\\n        $fastq > ${meta.id}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_SHORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "process MINIMAP2_INDEX {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0' :\n        'quay.io/biocontainers/minimap2:2.21--h5bf99c6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.mmi\"        , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    minimap2 \\\\\n        -t $task.cpus \\\\\n        -d ${fasta.baseName}.mmi \\\\\n        $args \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_SHORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ABMicroBioinf/magph/ABMicroBioinf__magph/MINIMAP2_ALIGN", "ABMicroBioinf/magph/ABMicroBioinf__magph/MINIMAP2_ALIGN_SHORT", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/MINIMAP2_ALIGN", "ABMicroBioinf/magph/ABMicroBioinf__magph/MINIMAP2_ALIGN_LONG", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/MINIMAP2_ALIGN_LONG", "ABMicroBioinf/magph/ABMicroBioinf__magph/MINIMAP2_INDEX", "nf-core/modules/nf-core__modules/MINIMAP2_ALIGN", "sguizard/isoseq/sguizard__isoseq/MINIMAP2_ALIGN", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/MINIMAP2_ALIGN_LONG", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/MINIMAP2_ALIGN_SHORT", "ABMicroBioinf/pathogen/ABMicroBioinf__pathogen/MINIMAP2_ALIGN", "nf-core/modules/nf-core__modules/MINIMAP2_INDEX", "xiaoli-dong/magph/xiaoli-dong__magph/MINIMAP2_ALIGN_LONG", "nf-core/nanoseq/nf-core__nanoseq/MINIMAP2_INDEX", "ksumngs/nf-modules/ksumngs__nf-modules/MINIMAP2_ALIGN", "xiaoli-dong/magph/xiaoli-dong__magph/MINIMAP2_ALIGN", "nf-core/nanoseq/nf-core__nanoseq/MINIMAP2_ALIGN", "xiaoli-dong/magph/xiaoli-dong__magph/MINIMAP2_ALIGN_SHORT", "sanger-tol/readmapping/sanger-tol__readmapping/MINIMAP2_INDEX", "xiaoli-dong/pathogen/xiaoli-dong__pathogen/MINIMAP2_ALIGN_SHORT"], "list_wf_names": ["sanger-tol/readmapping", "xiaoli-dong/pathogen", "ABMicroBioinf/pathogen", "sguizard/isoseq", "ksumngs/nf-modules", "nf-core/modules", "nf-core/nanoseq", "ABMicroBioinf/magph", "xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["SKAT"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process ska_compute_sketch {\n        tag \"${sketch_id}\"\n        publishDir \"${params.outdir}/ska/sketches/\", mode: params.publish_dir_mode\n        errorStrategy 'retry'\n        maxRetries 3\n\n\n      input:\n      each ksize from ksizes\n      set id, file(reads) from ch_reads_to_sketch\n\n      output:\n      set val(ksize), file(\"${sketch_id}.skf\") into ska_sketches\n\n      script:\n      sketch_id = \"${id}_ksize_${ksize}\"\n\n        \"\"\"\n        ska fastq \\\\\n          -k $ksize \\\\\n          -o ${sketch_id} \\\\\n          ${reads}\n        \"\"\"\n\n      }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/ska_compute_sketch"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["miRTop"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess mirtop_bam_hairpin {\n    label 'process_medium'\n    tag \"$input\"\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n    when:\n    mirna_gtf\n\n    input:\n    file input from miRBase_hairpin_collapse_bam.collect()\n    file hairpin from hairpin_mirtop\n    file gtf from mirna_gtf\n\n    output:\n    file \"mirtop/mirtop.gff\" into mirtop_gff\n    file \"mirtop/mirtop.tsv\" into mirtop_tsv\n    file \"mirtop/mirna.tsv\" into mirna_tsv\n    file \"mirtop/mirtop_rawData.tsv\" into isomir_tsv\n\n    script:\n    \"\"\"\n    mirtop gff --hairpin $hairpin --gtf $gtf -o mirtop --sps $params.mirtrace_species $input\n    mirtop counts --hairpin $hairpin --gtf $gtf -o mirtop --sps $params.mirtrace_species --add-extra --gff mirtop/mirtop.gff\n    mirtop export --format isomir --hairpin $hairpin --gtf $gtf --sps $params.mirtrace_species -o mirtop mirtop/mirtop.gff\n    collapse_mirtop.r mirtop/mirtop.tsv\n    \"\"\"\n}"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/mirtop_bam_hairpin"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess BOWTIE2_REMOVAL_ALIGN {\n    tag \"${meta.id}-${options.suffix}\"\n    publishDir \"${params.outdir}/\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1\"\n    } else {\n        container \"quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.unmapped*.fastq.gz\") , emit: reads\n    path  \"*.mapped*.read_ids.txt\", optional:true , emit: read_ids\n    tuple val(meta), path(\"*.bowtie2.log\")        , emit: log\n    path  '*.version.txt'                         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    def sensitivity = params.host_removal_verysensitive ? \"--very-sensitive\" : \"--sensitive\"\n    def save_ids = params.host_removal_save_ids ? \"Y\" : \"N\"\n    if (!meta.single_end){\n        \"\"\"\n        bowtie2 -p ${task.cpus} \\\n                -x ${index[0].getSimpleName()} \\\n                -1 \"${reads[0]}\" -2 \"${reads[1]}\" \\\n                $sensitivity \\\n                --un-conc-gz ${prefix}.unmapped_%.fastq.gz \\\n                --al-conc-gz ${prefix}.mapped_%.fastq.gz \\\n                1> /dev/null \\\n                2> ${prefix}.bowtie2.log\n        if [ ${save_ids} = \"Y\" ] ; then\n            gunzip -c ${prefix}.mapped_1.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped_1.read_ids.txt\n            gunzip -c ${prefix}.mapped_2.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped_2.read_ids.txt\n        fi\n        rm -f ${prefix}.mapped_*.fastq.gz\n\n        echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        bowtie2 -p ${task.cpus} \\\n                -x ${index[0].getSimpleName()} \\\n                -U ${reads} \\\n                $sensitivity \\\n                --un-gz ${prefix}.unmapped.fastq.gz \\\n                --al-gz ${prefix}.mapped.fastq.gz \\\n                1> /dev/null \\\n                2> ${prefix}.bowtie2.log\n        if [ ${save_ids} = \"Y\" ] ; then\n            gunzip -c ${prefix}.mapped.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped.read_ids.txt\n        fi\n        rm -f ${prefix}.mapped.fastq.gz\n\n        echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/mag/nf-core__mag/BOWTIE2_REMOVAL_ALIGN"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 5, "tools": ["SAMtools", "Bowtie"], "nb_own": 4, "list_own": ["jianhong", "xiaoli-dong", "nf-core", "mahesh-panchal"], "nb_wf": 5, "list_wf": ["magph", "shotgun", "viralrecon", "test_nfcore_workflow_chain", "cutandrun"], "list_contrib": ["jordeu", "heuermh", "ewels", "maxulysse", "antunderwood", "ggabernet", "ktrns", "saramonzon", "dladd", "svarona", "stevekm", "nf-core-bot", "cjfields", "stevin-wilson", "xiaoli-dong", "charlotte-west", "jcurado-flomics", "chris-cheshire", "ErikaKvalem", "jianhong", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "MiguelJulia", "drpatelh"], "nb_contrib": 25, "codes": ["process BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2 bioconda::samtools=1.11 conda-forge::pigz=2.3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0' :\n        'quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    tuple val(meta), path('*.log'), emit: log\n    path  \"versions.yml\"          , emit: versions\n    tuple val(meta), path('*fastq.gz'), optional:true, emit: fastq\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4 bioconda::samtools=1.14 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:4d235f41348a00533f18e47c9669f1ecb327f629-0' :\n        'quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:4d235f41348a00533f18e47c9669f1ecb327f629-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   save_unaligned\n\n    output:\n    tuple val(meta), path('*.bam')    , emit: bam\n    tuple val(meta), path('*.log')    , emit: log\n    tuple val(meta), path('*fastq.gz'), emit: fastq, optional:true\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def unaligned = save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2 bioconda::samtools=1.11 conda-forge::pigz=2.3.4' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path('*.bam'), emit: bam\n    tuple val(meta), path('*.log'), emit: log\n    path  \"versions.yml\"          , emit: versions\n    tuple val(meta), path('*fastq.gz'), optional:true, emit: fastq\n\n    script:\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $options.args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $options.args2 -bhS -o ${prefix}.bam -\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $options.args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $options.args2 -bhS -o ${prefix}.bam -\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4 bioconda::samtools=1.14 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:4d235f41348a00533f18e47c9669f1ecb327f629-0' :\n        'quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:4d235f41348a00533f18e47c9669f1ecb327f629-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   save_unaligned\n\n    output:\n    tuple val(meta), path('*.bam')    , emit: bam\n    tuple val(meta), path('*.log')    , emit: log\n    tuple val(meta), path('*fastq.gz'), emit: fastq, optional:true\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def unaligned = save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BOWTIE2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4 bioconda::samtools=1.14 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:4d235f41348a00533f18e47c9669f1ecb327f629-0' :\n        'quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:4d235f41348a00533f18e47c9669f1ecb327f629-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   save_unaligned\n\n    output:\n    tuple val(meta), path('*.bam')    , emit: bam\n    tuple val(meta), path('*.log')    , emit: log\n    tuple val(meta), path('*fastq.gz'), emit: fastq, optional:true\n    path  \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        def unaligned = save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n        bowtie2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            $unaligned \\\\\n            $args \\\\\n            2> ${prefix}.bowtie2.log \\\\\n            | samtools view -@ $task.cpus $args2 -bhS -o ${prefix}.bam -\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["xiaoli-dong/magph/xiaoli-dong__magph/BOWTIE2_ALIGN", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BOWTIE2_ALIGN", "nf-core/cutandrun/nf-core__cutandrun/BOWTIE2_ALIGN", "nf-core/viralrecon/nf-core__viralrecon/BOWTIE2_ALIGN", "jianhong/shotgun/jianhong__shotgun/BOWTIE2_ALIGN"], "list_wf_names": ["jianhong/shotgun", "nf-core/cutandrun", "mahesh-panchal/test_nfcore_workflow_chain", "nf-core/viralrecon", "xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": [" process paraclu_peak_call {\n        tag \"$name\"\n        label 'process_low'\n        publishDir \"${params.outdir}/paraclu\", mode: params.publish_dir_mode\n\n        when:\n        'paraclu' in callers\n\n        input:\n        tuple val(name), path(xlinks) from ch_xlinks_paraclu\n\n        output:\n        tuple val(name), path(\"${name}.${min_value}_${max_cluster_length}nt_${min_density_increase}.peaks.bed.gz\") into ch_peaks_paraclu\n        path \"*.peaks.bed.gz\" into ch_paraclu_qc\n\n        script:\n        min_value = params.min_value\n        min_density_increase = params.min_density_increase\n        max_cluster_length = params.max_cluster_length\n        \"\"\"\n        pigz -d -c $xlinks | \\\\\n        awk '{OFS = \"\\t\"}{print \\$1, \\$6, \\$3, \\$5}' | \\\\\n        sort -k1,1 -k2,2 -k3,3n > paraclu_input.tsv\n\n        paraclu ${min_value} paraclu_input.tsv | \\\\\n        paraclu-cut -d ${min_density_increase} -l ${max_cluster_length} | \\\\\n        awk '{OFS = \"\\t\"}{print \\$1, \\$3-1, \\$4, \".\", \\$6, \\$2}' | \\\\\n        bedtools sort | \\\\\n        pigz > ${name}.${min_value}_${max_cluster_length}nt_${min_density_increase}.peaks.bed.gz\n        \"\"\"\n    }"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/paraclu_peak_call"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["FLASH"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process FLASH {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/flash:1.2.11--hed695b0_5' :\n        'quay.io/biocontainers/flash:1.2.11--hed695b0_5' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    flash \\\\\n        $args \\\\\n        -o ${prefix} \\\\\n        -z \\\\\n        ${reads[0]} \\\\\n        ${reads[1]}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        flash: \\$(echo \\$(flash --version 2>&1) | sed 's/^.*FLASH v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/FLASH"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess circulargenerator{\n    label 'sc_medium'\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/reference_genome/circularmapper_index\", mode: params.publish_dir_mode, saveAs: { filename -> \n            if (params.save_reference) filename \n            else if(!params.save_reference && filename == \"where_are_my_files.txt\") filename\n            else null\n    }\n\n\n    input:\n    file fasta from ch_fasta_for_circulargenerator\n\n    output:\n    file \"${prefix}.{amb,ann,bwt,sa,pac}\" into ch_circularmapper_indices\n    file \"*_elongated\" into ch_circularmapper_elongatedfasta\n\n    when: \n    params.mapper == 'circularmapper'\n\n    script:\n    prefix = \"${fasta.baseName}_${params.circularextension}.fasta\"\n    \"\"\"\n    circulargenerator -Xmx${task.memory.toGiga()}g -e ${params.circularextension} -i $fasta -s ${params.circulartarget}\n    bwa index $prefix\n    \"\"\"\n\n}"], "list_proc": ["nf-core/eager/nf-core__eager/circulargenerator"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["PIRATE"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process PIRATE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::pirate=1.0.4 bioconda::perl-bioperl=1.7.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pirate:1.0.4--hdfd78af_2' :\n        'quay.io/biocontainers/pirate:1.0.4--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(gff)\n\n    output:\n    tuple val(meta), path(\"results/*\")                                   , emit: results\n    tuple val(meta), path(\"results/core_alignment.fasta\"), optional: true, emit: aln\n    path \"versions.yml\"                                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    PIRATE \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --input ./ \\\\\n        --output results/\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pirate: \\$( echo \\$( PIRATE --version 2>&1) | sed 's/PIRATE //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/PIRATE"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 4, "tools": ["BWA"], "nb_own": 4, "list_own": ["nf-core", "ajodeh-juma", "tamara-hodgetts", "remiolsen"], "nb_wf": 4, "list_wf": ["bactmap", "nf-atac-seq", "hicscaff", "viclara"], "list_contrib": ["ajodeh-juma", "alexandregilardet", "thanhleviet", "tamara-hodgetts", "ewels", "avantonder", "antunderwood", "apeltzer", "ggabernet", "drpatelh", "remiolsen"], "nb_contrib": 11, "codes": ["\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"          , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bwa\n    bwa index $options.args $fasta -p bwa/${fasta.baseName}\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"index\"          , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    mkdir index\n    bwa index $options.args -p index/${fasta.baseName} $fasta \n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"          , emit: index\n    path \"*.version.txt\", emit: version\n\n                \n                                                              \n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bwa\n    bwa index $options.args -p bwa/${fasta.baseName} $fasta\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"          , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bwa\n    bwa index $options.args $fasta -p bwa/${fasta.baseName}\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["tamara-hodgetts/nf-atac-seq/tamara-hodgetts__nf-atac-seq/BWA_INDEX", "nf-core/bactmap/nf-core__bactmap/BWA_INDEX", "ajodeh-juma/viclara/ajodeh-juma__viclara/BWA_INDEX", "remiolsen/hicscaff/remiolsen__hicscaff/BWA_INDEX"], "list_wf_names": ["ajodeh-juma/viclara", "nf-core/bactmap", "tamara-hodgetts/nf-atac-seq", "remiolsen/hicscaff"]}, {"nb_reuse": 6, "tools": ["SortMeRna"], "nb_own": 4, "list_own": ["harleenduggal", "raygozag", "nf-core", "mahesh-panchal"], "nb_wf": 5, "list_wf": ["RNASEQ", "modules", "test_nfcore_workflow_chain", "nfcore-rnaseq", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "jordwil", "FelixKrueger", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "aanil", "silviamorins", "d4straub", "SPPearce", "Midnighter", "rannick", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "mvanins", "nebfield", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 146, "codes": ["process SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sortmerna:4.3.4--h9ee0642_0' :\n        'quay.io/biocontainers/sortmerna:4.3.4--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fastas\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $args\n\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $args\n\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sortmerna:4.3.4--h9ee0642_0' :\n        'quay.io/biocontainers/sortmerna:4.3.4--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fastas\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $args\n\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $args\n\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sortmerna:4.3.4--h9ee0642_0' :\n        'quay.io/biocontainers/sortmerna:4.3.4--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fastas\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $args\n\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $args\n\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sortmerna:4.3.4--h9ee0642_0' :\n        'quay.io/biocontainers/sortmerna:4.3.4--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fastas\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $args\n\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $args\n\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sortmerna:4.3.4--h9ee0642_0' :\n        'quay.io/biocontainers/sortmerna:4.3.4--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fastas\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $args\n\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $args\n\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.3.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sortmerna:4.3.4--h9ee0642_0' :\n        'quay.io/biocontainers/sortmerna:4.3.4--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fastas\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $args\n\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            ${'--ref '+fastas.join(' --ref ')} \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $args\n\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            sortmerna: \\$(echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["harleenduggal/RNASEQ/harleenduggal__RNASEQ/SORTMERNA", "nf-core/rnaseq/nf-core__rnaseq/SORTMERNA", "nf-core/modules/nf-core__modules/SORTMERNA", "raygozag/rnaseq/raygozag__rnaseq/SORTMERNA", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SORTMERNA", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SORTMERNA"], "list_wf_names": ["raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 4, "tools": ["STAR"], "nb_own": 2, "list_own": ["goodwright", "nf-core"], "nb_wf": 4, "list_wf": ["modules", "rnaseq", "rnavar", "imaps-nf"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "FelixKrueger", "jordwil", "rfara", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "silviamorins", "Midnighter", "aanil", "yuukiiwa", "samirelanduk", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "sofstam", "antunderwood", "george-hall-ucl", "veeravalli", "matrulda", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "nebfield", "mvanins", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "sruthipsuresh", "jun-wan", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "alexharston", "Gwennid", "Jeremy1805", "marc-jones", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "orionzhou", "sofiahaglund", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "CharlotteAnne", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "m3hdad", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 151, "codes": ["process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.7.9a bioconda::samtools=1.15.1 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1c4c32d87798d425c970ececfbadd155e7560277-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1c4c32d87798d425c970ececfbadd155e7560277-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args_list.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.7.9a bioconda::samtools=1.15.1 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1c4c32d87798d425c970ececfbadd155e7560277-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1c4c32d87798d425c970ececfbadd155e7560277-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args_list.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? conda_str : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        \"https://depot.galaxyproject.org/singularity/${container_id}\" :\n        \"quay.io/biocontainers/${container_id}\" }\"\n\n    input:\n    path fasta\n    path gtf\n    val  is_aws_igenome\n\n    output:\n    path \"star\"        , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n\n                                                                                  \n    conda_str = \"bioconda::star=2.7.10a bioconda::samtools=1.15.1 conda-forge::gawk=5.1.0\"\n    container_id = 'mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:afaaa4c6f5b308b4b6aa2dd8e99e1466b2a6b0cd-0'\n    if (is_aws_igenome) {\n        conda_str = \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\"\n        container_id = 'mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0'\n    }\n\n    def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args_list.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.7.9a bioconda::samtools=1.15.1 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1c4c32d87798d425c970ececfbadd155e7560277-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:1c4c32d87798d425c970ececfbadd155e7560277-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args_list.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $args\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n            gawk: \\$(echo \\$(gawk --version 2>&1) | sed 's/^.*GNU Awk //; s/, .*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/STAR_GENOMEGENERATE", "nf-core/rnavar/nf-core__rnavar/STAR_GENOMEGENERATE", "nf-core/rnaseq/nf-core__rnaseq/STAR_GENOMEGENERATE", "goodwright/imaps-nf/goodwright__imaps-nf/STAR_GENOMEGENERATE"], "list_wf_names": ["nf-core/rnaseq", "nf-core/rnavar", "nf-core/modules", "goodwright/imaps-nf"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process sourmash_compute_sketch_fastx_peptide {\n    tag \"${sig_id}\"\n    label \"low_memory\"\n    publishDir \"${params.outdir}/sketches_peptide/${sketch_id}\", mode: \"${params.publish_dir_mode}\",\n        saveAs: {filename ->\n            if (filename.indexOf(\".csv\") > 0) \"description/$filename\"\n            else if (filename.indexOf(\".sig\") > 0) \"sigs/$filename\"\n            else null\n        }\n\n    input:\n    val track_abundance\n    val sketch_value_parsed\n    val sketch_style_parsed\n    set val(sample_id), file(reads) from ch_protein_seq_to_sketch\n\n    output:\n    file(csv) into ch_sourmash_sig_describe_peptides\n    set val(sample_id), val(sketch_id), val(peptide_molecules_comma_separated), val(params.ksizes), file(sig) into sourmash_sketches_all_peptide\n\n    script:\n    sketch_id = make_sketch_id(\n      peptide_molecules_comma_separated, \n      params.ksizes, \n      sketch_value_parsed[0], \n      track_abundance, \n      sketch_style_parsed[0]\n    )\n\n    sketch_value_flag = make_sketch_value_flag(sketch_style_parsed[0], sketch_value_parsed[0])\n    track_abundance_flag = track_abundance ? '--track-abundance' : ''\n    sig_id = \"${sample_id}__${sketch_id}\"\n    sig = \"${sig_id}.sig\"\n    csv = \"${sig_id}.csv\"\n    \"\"\"\n      sourmash compute \\\\\n        ${sketch_value_flag} \\\\\n        --ksizes ${params.ksizes} \\\\\n        --input-is-protein \\\\\n        ${peptide_molecule_flags} \\\\\n        --name '${sample_id}' \\\\\n        --no-dna \\\\\n        $track_abundance_flag \\\\\n        --output ${sig} \\\\\n        $reads\n      sourmash sig describe --csv ${csv} ${sig}\n    \"\"\"\n    }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/sourmash_compute_sketch_fastx_peptide"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_qc && !params.skip_multiqc\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from fastqc_results.collect()\n    file ('trim_galore/*') from trimgalore_results.collect()\n    file ('mirtrace/*') from mirtrace_results.collect()\n    file ('samtools/*') from ch_sort_bam_flagstat_mqc.collect()\n    file ('samtools_genome/*') from ch_genome_bam_flagstat_mqc.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename $custom_config_file -m bowtie1 -m samtools -m cutadapt -m fastqc -m custom_content\n    \"\"\"\n}"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/multiqc"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess BOWTIE2_REMOVAL_BUILD {\n    tag \"$fasta\"\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1'\n    } else {\n        container 'quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1'\n    }\n\n    input:\n    path fasta\n\n    output:\n    path 'bt2_index_base*', emit: index\n    path '*.version.txt'  , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie\n    bowtie2-build --threads $task.cpus $fasta \"bt2_index_base\"\n    bowtie2 --version > ${software}_removal.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/BOWTIE2_REMOVAL_BUILD"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 8, "tools": ["SAMtools"], "nb_own": 7, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 8, "list_wf": ["saw.sarek", "NextflowModules", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "sawibo", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "rernst", "lescai", "melferink", "apeltzer", "ellendejong", "olgabot", "ywilke", "davidmasp"], "nb_contrib": 28, "codes": ["\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "process MPileup {\n                                                                                             \n    tag {\"Samtools MPileup ${sample_id}\"}\n    label 'Samtools_1_10'\n    label 'Samtools_1_10_MPileup'\n    container = 'quay.io/biocontainers/samtools:1.10--h9402c20_2'\n    shell = ['/bin/bash', '-euo', 'pipefail']\n\n    input:\n        tuple(sample_id, path(bam_file), path(bai_file))\n\n    output:\n        tuple(sample_id, path(\"${bam_file.baseName}.pileup\"), emit: pileup)\n\n    script:\n        \"\"\"\n        samtools mpileup ${params.optional} -f ${params.genome} ${bam_file} > ${bam_file.baseName}.pileup\n        \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/rmoran7__custom_sarek/Mpileup", "UMCUGenetics/NextflowModules/UMCUGenetics__NextflowModules/MPileup", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/Mpileup", "rmoran7/dx_sarek/rmoran7__dx_sarek/Mpileup", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/Mpileup", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/Mpileup", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/Mpileup", "nf-core/sarek/nf-core__sarek/Mpileup"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "UMCUGenetics/NextflowModules", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 4, "tools": ["ABRicate"], "nb_own": 2, "list_own": ["bactopia", "nf-core"], "nb_wf": 2, "list_wf": ["modules", "bactopia"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "Accio", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "fmaguire", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "TGotwig"], "nb_contrib": 108, "codes": ["\nprocess ABRICATE_SUMMARY {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1' :\n        'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\"\n\n    input:\n    tuple val(meta), path(reports)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: report\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        --summary \\\\\n        $reports > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"\n}", "process ABRICATE_SUMMARY {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::abricate=1.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1':\n        'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\"\n\n    input:\n    tuple val(meta), path(reports)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: report\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        --summary \\\\\n        $reports > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"\n}", "process ABRICATE_RUN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::abricate=1.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1':\n        'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\"\n\n    input:\n    tuple val(meta), path(assembly)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: report\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        $assembly \\\\\n        $args \\\\\n        --threads $task.cpus > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess ABRICATE_RUN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1' :\n        'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\"\n\n    input:\n    tuple val(meta), path(assembly)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: report\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        $assembly \\\\\n        $options.args \\\\\n        --threads $task.cpus > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["bactopia/bactopia/bactopia__bactopia/ABRICATE_SUMMARY", "nf-core/modules/nf-core__modules/ABRICATE_SUMMARY", "nf-core/modules/nf-core__modules/ABRICATE_RUN", "bactopia/bactopia/bactopia__bactopia/ABRICATE_RUN"], "list_wf_names": ["bactopia/bactopia", "nf-core/modules"]}, {"nb_reuse": 31, "tools": ["BEDTools"], "nb_own": 6, "list_own": ["vincenthhu", "nf-core", "mahesh-panchal", "CDCgov", "cguyomar", "jianhong"], "nb_wf": 9, "list_wf": ["cutandrun", "mycosnp-nf", "nf-core-hicar", "viralrecon", "modules", "test_nfcore_workflow_chain", "nf-ase", "nf-core-westest", "ssds"], "list_contrib": ["Danilo2771", "ajodeh-juma", "ktrns", "FelixKrueger", "dladd", "kmurat1", "yuxuth", "AntoniaSchuster", "stevekm", "erikrikarddaniel", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "mciprianoCDC", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "jcurado-flomics", "ErikaKvalem", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "MiguelJulia", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "saramonzon", "cjjossart", "cjfields", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "stevin-wilson", "Gwennid", "Jeremy1805", "charlotte-west", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "cguyomar", "fmalmeida", "jordeu", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "svarona", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "leebrian", "lassefolkersen", "nickhsmith", "vincenthhu", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 123, "codes": ["process BEDTOOLS_BAMTOBED {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        bamtobed \\\\\n        $args \\\\\n        -i $bam \\\\\n        | bedtools sort > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MAKEWINDOWS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--h7d7f7ad_1' :\n        'quay.io/biocontainers/bedtools:2.30.0--h7d7f7ad_1' }\"\n\n    input:\n    tuple val(meta), path(regions)\n    val(use_bed)\n\n    output:\n    tuple val(meta), path(\"*.tab\"), emit: tab\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def arg_input = use_bed ? \"-b $regions\" : \"-g $regions\"\n    \"\"\"\n    bedtools \\\\\n        makewindows \\\\\n        ${arg_input} \\\\\n        $args \\\\\n        > ${prefix}.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MASKFASTA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fa\"), emit: fasta\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        maskfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BEDTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path('*.bed'), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        merge \\\\\n        -i $bed \\\\\n        $options.args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BEDTOOLS_BAMTOBED {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        bamtobed \\\\\n        $options.args \\\\\n        -i $bam \\\\\n        | bedtools sort > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BEDTOOLS_INTERSECT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(intervals1), path(intervals2)\n    val extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: intersect\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        intersect \\\\\n        -a $intervals1 \\\\\n        -b $intervals2 \\\\\n        $options.args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $options.args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(intervals), val(scale)\n    path  sizes\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: genomecov\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def args_token = options.args.tokenize()\n    def args       = options.args\n    args += (scale > 0 && scale != 1) ? \" -scale $scale\" : \"\"\n\n    if (!args_token.contains('-bg') && (scale > 0 && scale != 1)) {\n        args += \" -bg\"\n    }\n\n    if (intervals.name =~ /\\.bam/) {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -ibam $intervals \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -i $intervals \\\\\n            -g $sizes \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BEDTOOLS_MASKFASTA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fa\"), emit: fasta\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        maskfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path('*.bed'), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        merge \\\\\n        -i $bed \\\\\n        $args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_BAMTOBED {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        bamtobed \\\\\n        $args \\\\\n        -i $bam \\\\\n        | bedtools sort > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MAKEWINDOWS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--h7d7f7ad_1' :\n        'quay.io/biocontainers/bedtools:2.30.0--h7d7f7ad_1' }\"\n\n    input:\n    tuple val(meta), path(regions)\n    val(use_bed)\n\n    output:\n    tuple val(meta), path(\"*.tab\"), emit: tab\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def arg_input = use_bed ? \"-b $regions\" : \"-g $regions\"\n    \"\"\"\n    bedtools \\\\\n        makewindows \\\\\n        ${arg_input} \\\\\n        $args \\\\\n        > ${prefix}.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path('*.bed'), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        merge \\\\\n        -i $bed \\\\\n        $args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GETFASTA {\n    tag \"$bed\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    path bed\n    path fasta\n\n    output:\n    path \"*.fa\"         , emit: fasta\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${bed.baseName}\"\n    \"\"\"\n    bedtools \\\\\n        getfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_COMPLEMENT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  sizes\n\n    output:\n    tuple val(meta), path('*.bed'), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        complement \\\\\n        -i $bed \\\\\n        -g $sizes \\\\\n        $args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals), val(scale)\n    path  sizes\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: genomecov\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    args += (scale > 0 && scale != 1) ? \" -scale $scale\" : \"\"\n    if (!args_list.contains('-bg') && (scale > 0 && scale != 1)) {\n        args += \" -bg\"\n    }\n\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (intervals.name =~ /\\.bam/) {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -ibam $intervals \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -i $intervals \\\\\n            -g $sizes \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BEDTOOLS_MASKFASTA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fa\"), emit: fasta\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        maskfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GETFASTA {\n    tag \"$bed\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    path bed\n    path fasta\n\n    output:\n    path \"*.fa\"         , emit: fasta\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${bed.baseName}\"\n    \"\"\"\n    bedtools \\\\\n        getfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_SLOP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        slop \\\\\n        -i $bed \\\\\n        -g $sizes \\\\\n        $args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals), val(scale)\n    path  sizes\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: genomecov\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    args += (scale > 0 && scale != 1) ? \" -scale $scale\" : \"\"\n    if (!args_list.contains('-bg') && (scale > 0 && scale != 1)) {\n        args += \" -bg\"\n    }\n\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (intervals.name =~ /\\.bam/) {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -ibam $intervals \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -i $intervals \\\\\n            -g $sizes \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BEDTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path('*.bed'), emit: bed\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        merge \\\\\n        -i $bed \\\\\n        $args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_SUBTRACT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals1), path(intervals2)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        subtract \\\\\n        -a $intervals1 \\\\\n        -b $intervals2 \\\\\n        $args \\\\\n        > ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_GETFASTA {\n    tag \"$bed\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    path bed\n    path fasta\n\n    output:\n    path \"*.fa\"         , emit: fasta\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${bed.baseName}\"\n    \"\"\"\n    bedtools \\\\\n        getfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MASKFASTA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fa\"), emit: fasta\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        maskfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_INTERSECT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals1), path(intervals2)\n    val extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: intersect\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        intersect \\\\\n        -a $intervals1 \\\\\n        -b $intervals2 \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_MASKFASTA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fa\"), emit: fasta\n    path \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        maskfasta \\\\\n        $args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "process BEDTOOLS_INTERSECT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals1), path(intervals2)\n    val extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: intersect\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        intersect \\\\\n        -a $intervals1 \\\\\n        -b $intervals2 \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ssds/nf-core__ssds/BEDTOOLS_BAMTOBED", "nf-core/ssds/nf-core__ssds/BEDTOOLS_MAKEWINDOWS", "nf-core/ssds/nf-core__ssds/BEDTOOLS_SORT", "CDCgov/mycosnp-nf/CDCgov__mycosnp-nf/BEDTOOLS_MASKFASTA", "nf-core/cutandrun/nf-core__cutandrun/BEDTOOLS_MERGE", "nf-core/cutandrun/nf-core__cutandrun/BEDTOOLS_BAMTOBED", "nf-core/cutandrun/nf-core__cutandrun/BEDTOOLS_INTERSECT", "nf-core/cutandrun/nf-core__cutandrun/BEDTOOLS_SORT", "nf-core/cutandrun/nf-core__cutandrun/BEDTOOLS_GENOMECOV", "nf-core/viralrecon/nf-core__viralrecon/BEDTOOLS_MASKFASTA", "nf-core/viralrecon/nf-core__viralrecon/BEDTOOLS_MERGE", "nf-core/modules/nf-core__modules/BEDTOOLS_BAMTOBED", "nf-core/modules/nf-core__modules/BEDTOOLS_MAKEWINDOWS", "nf-core/modules/nf-core__modules/BEDTOOLS_SORT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BEDTOOLS_MERGE", "nf-core/viralrecon/nf-core__viralrecon/BEDTOOLS_GETFASTA", "nf-core/modules/nf-core__modules/BEDTOOLS_COMPLEMENT", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/BEDTOOLS_GENOMECOV", "nf-core/modules/nf-core__modules/BEDTOOLS_MASKFASTA", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BEDTOOLS_GETFASTA", "jianhong/nf-core-hicar/jianhong__nf-core-hicar/BEDTOOLS_SORT", "nf-core/modules/nf-core__modules/BEDTOOLS_SLOP", "nf-core/modules/nf-core__modules/BEDTOOLS_GENOMECOV", "nf-core/modules/nf-core__modules/BEDTOOLS_MERGE", "nf-core/modules/nf-core__modules/BEDTOOLS_SUBTRACT", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/BEDTOOLS_SORT", "nf-core/modules/nf-core__modules/BEDTOOLS_GETFASTA", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/BEDTOOLS_MASKFASTA", "nf-core/modules/nf-core__modules/BEDTOOLS_INTERSECT", "cguyomar/nf-ase/cguyomar__nf-ase/BEDTOOLS_MASKFASTA", "vincenthhu/nf-core-westest/vincenthhu__nf-core-westest/BEDTOOLS_INTERSECT"], "list_wf_names": ["jianhong/nf-core-hicar", "cguyomar/nf-ase", "vincenthhu/nf-core-westest", "nf-core/ssds", "nf-core/cutandrun", "nf-core/modules", "nf-core/viralrecon", "mahesh-panchal/test_nfcore_workflow_chain", "CDCgov/mycosnp-nf"]}, {"nb_reuse": 3, "tools": ["Graphmap2", "ALIGN"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "nanoseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "cying111", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "alneberg", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "lwratten", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "csawye01", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 110, "codes": ["process GRAPHMAP2_INDEX {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::graphmap=0.6.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/graphmap:0.6.3--he513fc3_0' :\n        'quay.io/biocontainers/graphmap:0.6.3--he513fc3_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.gmidx\"      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    graphmap2 \\\\\n        align \\\\\n        -t $task.cpus \\\\\n        -I \\\\\n        $args \\\\\n        -r $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        graphmap2: \\$(echo \\$(graphmap2 align 2>&1) | sed 's/^.*Version: v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess GRAPHMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda     (params.enable_conda ? \"bioconda::graphmap=0.6.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/graphmap:0.6.3--he513fc3_0\"\n    } else {\n        container \"quay.io/biocontainers/graphmap:0.6.3--he513fc3_0\"\n    }\n\n    input:\n    tuple val(meta), path(fastq), path(fasta), path(sizes), val(gtf), val(bed), val(is_transcripts), path(index)\n\n    output:\n    tuple val(meta), path(sizes), val(is_transcripts), path(\"*.sam\"), emit: align_sam\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def preset    = (params.protocol == 'DNA' || is_transcripts) ? \"\" : \"-x rnaseq\"\n    def junctions = (params.protocol != 'DNA' && !is_transcripts && gtf) ? \"--gtf $gtf\" : \"\"\n    \"\"\"\n    graphmap2 \\\\\n        align \\\\\n        $preset \\\\\n        $junctions \\\\\n        -t $task.cpus \\\\\n        -r $fasta \\\\\n        -i $index \\\\\n        -d $fastq \\\\\n        -o ${meta.id}.sam \\\\\n        --extcigar\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(graphmap2 align 2>&1) | sed 's/^.*Version: v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GRAPHMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"bioconda::graphmap=0.6.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/graphmap:0.6.3--he513fc3_0' :\n        'quay.io/biocontainers/graphmap:0.6.3--he513fc3_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  fasta\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    graphmap2 \\\\\n        align \\\\\n        -t $task.cpus \\\\\n        -r $fasta \\\\\n        -i $index \\\\\n        -d $reads \\\\\n        -o ${prefix}.sam \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        graphmap2: \\$(echo \\$(graphmap2 align 2>&1) | sed 's/^.*Version: v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/GRAPHMAP2_INDEX", "nf-core/nanoseq/nf-core__nanoseq/GRAPHMAP2_ALIGN", "nf-core/modules/nf-core__modules/GRAPHMAP2_ALIGN"], "list_wf_names": ["nf-core/nanoseq", "nf-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess bowtie2 {\n    label 'mc_medium'\n    tag \"${libraryid}\"\n    publishDir \"${params.outdir}/mapping/bt2\", mode: params.publish_dir_mode\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(r1), file(r2) from ch_lanemerge_for_bt2\n    path index from bt2_index.collect()\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*.mapped.bam\"), path(\"*.{bai,csi}\") into ch_output_from_bt2\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, path(\"*_bt2.log\") into ch_bt2_for_multiqc\n\n    when: \n    params.mapper == 'bowtie2'\n\n    script:\n    def split_cpus = Math.floor(task.cpus/2)\n    def size = params.large_ref ? '-c' : ''\n    def fasta = \"${index}/${fasta_base}\"\n    def trim5 = params.bt2_trim5 != 0 ? \"--trim5 ${params.bt2_trim5}\" : \"\"\n    def trim3 = params.bt2_trim3 != 0 ? \"--trim3 ${params.bt2_trim3}\" : \"\"\n    def bt2n = params.bt2n != 0 ? \"-N ${params.bt2n}\" : \"\"\n    def bt2l = params.bt2l != 0 ? \"-L ${params.bt2l}\" : \"\"\n\n    if ( \"${params.bt2_alignmode}\" == \"end-to-end\"  ) {\n      switch ( \"${params.bt2_sensitivity}\" ) {\n        case \"no-preset\":\n        sensitivity = \"\"; break\n        case \"very-fast\":\n        sensitivity = \"--very-fast\"; break\n        case \"fast\":\n        sensitivity = \"--fast\"; break\n        case \"sensitive\":\n        sensitivity = \"--sensitive\"; break\n        case \"very-sensitive\":\n        sensitivity = \"--very-sensitive\"; break\n        default:\n        sensitivity = \"\"; break\n        }\n      } else if (\"${params.bt2_alignmode}\" == \"local\") {\n      switch ( \"${params.bt2_sensitivity}\" ) {\n        case \"no-preset\":\n        sensitivity = \"\"; break\n        case \"very-fast\":\n        sensitivity = \"--very-fast-local\"; break\n        case \"fast\":\n        sensitivity = \"--fast-local\"; break\n        case \"sensitive\":\n        sensitivity = \"--sensitive-local\"; break\n        case \"very-sensitive\":\n        sensitivity = \"--very-sensitive-local\"; break\n        default:\n        sensitivity = \"\"; break\n\n        }\n      }\n\n                                                             \n    if ( seqtype == 'PE' && ( params.skip_collapse || params.skip_adapterremoval ) ){\n    \"\"\"\n    bowtie2 -x ${fasta} -1 ${r1} -2 ${r2} -p ${split_cpus} ${sensitivity} ${bt2n} ${bt2l} ${trim5} ${trim3} --maxins ${params.bt2_maxins} --rg-id ILLUMINA-${libraryid} --rg SM:${samplename} --rg PL:illumina --rg PU:ILLUMINA-${libraryid}-${seqtype} 2> \"${libraryid}\"_bt2.log | samtools sort -@ ${split_cpus} -O bam > \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    samtools index \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size}\n    \"\"\"\n    } else {\n                               \n    \"\"\"\n    bowtie2 -x ${fasta} -U ${r1} -p ${split_cpus} ${sensitivity} ${bt2n} ${bt2l} ${trim5} ${trim3} --rg-id ILLUMINA-${libraryid} --rg SM:${samplename} --rg PL:illumina --rg PU:ILLUMINA-${libraryid}-${seqtype} 2> \"${libraryid}\"_bt2.log | samtools sort -@ ${split_cpus} -O bam > \"${libraryid}\"_\"${seqtype}\".mapped.bam\n    samtools index \"${libraryid}\"_\"${seqtype}\".mapped.bam ${size}\n    \"\"\"\n    }\n    \n}"], "list_proc": ["nf-core/eager/nf-core__eager/bowtie2"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 3, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 3, "list_wf": ["rnavar", "modules", "raredisease"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "m3hdad", "JoseEspinosa", "apeltzer", "ramprasadn", "mahesh-panchal", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process GATK4_BEDTOINTERVALLIST {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  dict\n\n    output:\n    tuple val(meta), path('*.interval_list'), emit: interval_list\n    path  \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK BedToIntervalList] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" BedToIntervalList \\\\\n        --INPUT $bed \\\\\n        --OUTPUT ${prefix}.interval_list \\\\\n        --SEQUENCE_DICTIONARY $dict \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.interval_list\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_BEDTOINTERVALLIST {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  dict\n\n    output:\n    tuple val(meta), path('*.interval_list'), emit: interval_list\n    path  \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK BedToIntervalList] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" BedToIntervalList \\\\\n        --INPUT $bed \\\\\n        --OUTPUT ${prefix}.interval_list \\\\\n        --SEQUENCE_DICTIONARY $dict \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.interval_list\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_BEDTOINTERVALLIST {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path  dict\n\n    output:\n    tuple val(meta), path('*.interval_list'), emit: interval_list\n    path  \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK BedToIntervalList] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" BedToIntervalList \\\\\n        --INPUT $bed \\\\\n        --OUTPUT ${prefix}.interval_list \\\\\n        --SEQUENCE_DICTIONARY $dict \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.interval_list\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/rnavar/nf-core__rnavar/GATK4_BEDTOINTERVALLIST", "nf-core/modules/nf-core__modules/GATK4_BEDTOINTERVALLIST", "nf-core/raredisease/nf-core__raredisease/GATK4_BEDTOINTERVALLIST"], "list_wf_names": ["nf-core/rnavar", "nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 2, "tools": ["QIIME", "BioMe"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_FILTERTAXA {\n    tag \"taxa:${exclude_taxa};min-freq:${min_frequency};min-samples:${min_samples}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n    path(repseq)\n    path(taxonomy)\n    val(min_frequency)\n    val(min_samples)\n    val(exclude_taxa)\n\n    output:\n    path(\"filtered-table.qza\"), emit: asv\n    path(\"filtered-table.tsv\"), emit: tsv\n    path(\"filtered-sequences.qza\"), emit: seq\n    path \"versions.yml\"       , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    if ! [ \\\"${exclude_taxa}\\\" = \\\"none\\\" ]; then\n        #filter sequences\n        qiime taxa filter-seqs \\\n            --i-sequences ${repseq} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-exclude ${exclude_taxa} --p-mode contains \\\n            --o-filtered-sequences tax_filtered-sequences.qza\n        #filter abundance table\n        qiime taxa filter-table \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-exclude ${exclude_taxa} --p-mode contains \\\n            --o-filtered-table tax_filtered-table.qza\n        filtered_table=\"tax_filtered-table.qza\"\n        filtered_sequences=\"tax_filtered-sequences.qza\"\n    else\n        filtered_table=${table}\n        filtered_sequences=${repseq}\n    fi\n    qiime feature-table filter-features \\\n        --i-table \\$filtered_table \\\n        --p-min-frequency ${min_frequency} \\\n        --p-min-samples ${min_samples} \\\n        --o-filtered-table filtered-table.qza\n\n    qiime feature-table filter-seqs \\\n        --i-data \\$filtered_sequences \\\n        --i-table filtered-table.qza \\\n        --o-filtered-data filtered-sequences.qza\n\n    #produce raw count table in biom format \"table/feature-table.biom\"\n    qiime tools export --input-path filtered-table.qza  \\\n        --output-path table\n    #produce raw count table\n    biom convert -i table/feature-table.biom \\\n        -o table/feature-table.tsv  \\\n        --to-tsv\n    cp table/feature-table.tsv filtered-table.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_FILTERTAXA {\n    tag \"taxa:${exclude_taxa};min-freq:${min_frequency};min-samples:${min_samples}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n    path(repseq)\n    path(taxonomy)\n    val(min_frequency)\n    val(min_samples)\n    val(exclude_taxa)\n\n    output:\n    path(\"filtered-table.qza\"), emit: asv\n    path(\"filtered-table.tsv\"), emit: tsv\n    path(\"filtered-sequences.qza\"), emit: seq\n    path \"versions.yml\"       , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    if ! [ \\\"${exclude_taxa}\\\" = \\\"none\\\" ]; then\n        #filter sequences\n        qiime taxa filter-seqs \\\n            --i-sequences ${repseq} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-exclude ${exclude_taxa} --p-mode contains \\\n            --o-filtered-sequences tax_filtered-sequences.qza\n        #filter abundance table\n        qiime taxa filter-table \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-exclude ${exclude_taxa} --p-mode contains \\\n            --o-filtered-table tax_filtered-table.qza\n        filtered_table=\"tax_filtered-table.qza\"\n        filtered_sequences=\"tax_filtered-sequences.qza\"\n    else\n        filtered_table=${table}\n        filtered_sequences=${repseq}\n    fi\n    qiime feature-table filter-features \\\n        --i-table \\$filtered_table \\\n        --p-min-frequency ${min_frequency} \\\n        --p-min-samples ${min_samples} \\\n        --o-filtered-table filtered-table.qza\n\n    qiime feature-table filter-seqs \\\n        --i-data \\$filtered_sequences \\\n        --i-table filtered-table.qza \\\n        --o-filtered-data filtered-sequences.qza\n\n    #produce raw count table in biom format \"table/feature-table.biom\"\n    qiime tools export --input-path filtered-table.qza  \\\n        --output-path table\n    #produce raw count table\n    biom convert -i table/feature-table.biom \\\n        -o table/feature-table.tsv  \\\n        --to-tsv\n    cp table/feature-table.tsv filtered-table.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_FILTERTAXA", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_FILTERTAXA"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 5, "tools": ["Salmon"], "nb_own": 4, "list_own": ["harleenduggal", "raygozag", "nf-core", "mahesh-panchal"], "nb_wf": 4, "list_wf": ["RNASEQ", "rnaseq", "test_nfcore_workflow_chain", "nfcore-rnaseq"], "list_contrib": ["Emiller88", "alneberg", "FriederikeHanssen", "ewels", "drejom", "arontommi", "maxulysse", "rsuchecki", "SpikyClip", "matrulda", "ggabernet", "george-hall-ucl", "jordwil", "veeravalli", "adomingues", "colindaven", "vezzi", "lpantano", "skrakau", "chuan-wang", "ppericard", "grst", "pcantalupo", "nf-core-bot", "mvanins", "Galithil", "jun-wan", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "na399", "robsyme", "BABS-STP1", "senthil10", "drpowell", "kviljoen", "rfenouil", "jburos", "chris-cheshire", "mashehu", "raygozag", "Hammarn", "sven1103", "jemten", "paulklemm", "pranathivemuri", "marchoeppner", "mahesh-panchal", "JoseEspinosa", "apeltzer", "KevinMenden", "aanil", "silviamorins", "d4straub", "olgabot", "drpatelh", "amayer21", "zxl124"], "nb_contrib": 60, "codes": ["process SALMON_TXIMPORT {\n    label \"process_medium\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-tximeta=1.8.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-tximeta:1.8.0--r40_0' :\n        'quay.io/biocontainers/bioconductor-tximeta:1.8.0--r40_0' }\"\n\n    input:\n    path (\"salmon/*\")\n    path  tx2gene\n\n    output:\n    path \"*gene_tpm.tsv\"                 , emit: tpm_gene\n    path \"*gene_counts.tsv\"              , emit: counts_gene\n    path \"*gene_counts_length_scaled.tsv\", emit: counts_gene_length_scaled\n    path \"*gene_counts_scaled.tsv\"       , emit: counts_gene_scaled\n    path \"*transcript_tpm.tsv\"           , emit: tpm_transcript\n    path \"*transcript_counts.tsv\"        , emit: counts_transcript\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:                                                                    \n    \"\"\"\n    salmon_tximport.r \\\\\n        NULL \\\\\n        salmon \\\\\n        salmon.merged\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        r-base: \\$(echo \\$(R --version 2>&1) | sed 's/^.*R version //; s/ .*\\$//')\n        bioconductor-tximeta: \\$(Rscript -e \"library(tximeta); cat(as.character(packageVersion('tximeta')))\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_TXIMPORT {\n    label \"process_medium\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-tximeta=1.8.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-tximeta:1.8.0--r40_0' :\n        'quay.io/biocontainers/bioconductor-tximeta:1.8.0--r40_0' }\"\n\n    input:\n    path (\"salmon/*\")\n    path  tx2gene\n\n    output:\n    path \"*gene_tpm.tsv\"                 , emit: tpm_gene\n    path \"*gene_counts.tsv\"              , emit: counts_gene\n    path \"*gene_counts_length_scaled.tsv\", emit: counts_gene_length_scaled\n    path \"*gene_counts_scaled.tsv\"       , emit: counts_gene_scaled\n    path \"*transcript_tpm.tsv\"           , emit: tpm_transcript\n    path \"*transcript_counts.tsv\"        , emit: counts_transcript\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:                                                                    \n    \"\"\"\n    salmon_tximport.r \\\\\n        NULL \\\\\n        salmon \\\\\n        salmon.merged\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        r-base: \\$(echo \\$(R --version 2>&1) | sed 's/^.*R version //; s/ .*\\$//')\n        bioconductor-tximeta: \\$(Rscript -e \"library(tximeta); cat(as.character(packageVersion('tximeta')))\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_TXIMPORT {\n    label \"process_medium\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-tximeta=1.8.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-tximeta:1.8.0--r40_0' :\n        'quay.io/biocontainers/bioconductor-tximeta:1.8.0--r40_0' }\"\n\n    input:\n    path (\"salmon/*\")\n    path  tx2gene\n\n    output:\n    path \"*gene_tpm.tsv\"                 , emit: tpm_gene\n    path \"*gene_counts.tsv\"              , emit: counts_gene\n    path \"*gene_counts_length_scaled.tsv\", emit: counts_gene_length_scaled\n    path \"*gene_counts_scaled.tsv\"       , emit: counts_gene_scaled\n    path \"*transcript_tpm.tsv\"           , emit: tpm_transcript\n    path \"*transcript_counts.tsv\"        , emit: counts_transcript\n    path \"versions.yml\"                  , emit: versions\n\n    script:                                                                    \n    \"\"\"\n    salmon_tximport.r \\\\\n        NULL \\\\\n        salmon \\\\\n        salmon.merged\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        r-base: \\$(echo \\$(R --version 2>&1) | sed 's/^.*R version //; s/ .*\\$//')\n        bioconductor-tximeta: \\$(Rscript -e \"library(tximeta); cat(as.character(packageVersion('tximeta')))\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_TXIMPORT {\n    label \"process_medium\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-tximeta=1.8.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-tximeta:1.8.0--r40_0' :\n        'quay.io/biocontainers/bioconductor-tximeta:1.8.0--r40_0' }\"\n\n    input:\n    path (\"salmon/*\")\n    path  tx2gene\n\n    output:\n    path \"*gene_tpm.tsv\"                 , emit: tpm_gene\n    path \"*gene_counts.tsv\"              , emit: counts_gene\n    path \"*gene_counts_length_scaled.tsv\", emit: counts_gene_length_scaled\n    path \"*gene_counts_scaled.tsv\"       , emit: counts_gene_scaled\n    path \"*transcript_tpm.tsv\"           , emit: tpm_transcript\n    path \"*transcript_counts.tsv\"        , emit: counts_transcript\n    path \"versions.yml\"                  , emit: versions\n\n    script:                                                                    \n    \"\"\"\n    salmon_tximport.r \\\\\n        NULL \\\\\n        salmon \\\\\n        salmon.merged\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        r-base: \\$(echo \\$(R --version 2>&1) | sed 's/^.*R version //; s/ .*\\$//')\n        bioconductor-tximeta: \\$(Rscript -e \"library(tximeta); cat(as.character(packageVersion('tximeta')))\")\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_TXIMPORT {\n    label \"process_medium\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-tximeta=1.8.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-tximeta:1.8.0--r40_0' :\n        'quay.io/biocontainers/bioconductor-tximeta:1.8.0--r40_0' }\"\n\n    input:\n    path (\"salmon/*\")\n    path  tx2gene\n\n    output:\n    path \"*gene_tpm.tsv\"                 , emit: tpm_gene\n    path \"*gene_counts.tsv\"              , emit: counts_gene\n    path \"*gene_counts_length_scaled.tsv\", emit: counts_gene_length_scaled\n    path \"*gene_counts_scaled.tsv\"       , emit: counts_gene_scaled\n    path \"*transcript_tpm.tsv\"           , emit: tpm_transcript\n    path \"*transcript_counts.tsv\"        , emit: counts_transcript\n    path \"versions.yml\"                  , emit: versions\n\n    script:                                                                    \n    \"\"\"\n    salmon_tximport.r \\\\\n        NULL \\\\\n        salmon \\\\\n        salmon.merged\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        r-base: \\$(echo \\$(R --version 2>&1) | sed 's/^.*R version //; s/ .*\\$//')\n        bioconductor-tximeta: \\$(Rscript -e \"library(tximeta); cat(as.character(packageVersion('tximeta')))\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["harleenduggal/RNASEQ/harleenduggal__RNASEQ/SALMON_TXIMPORT", "nf-core/rnaseq/nf-core__rnaseq/SALMON_TXIMPORT", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/SALMON_TXIMPORT", "raygozag/rnaseq/raygozag__rnaseq/SALMON_TXIMPORT", "harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/SALMON_TXIMPORT"], "list_wf_names": ["raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 2, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 2, "list_wf": ["modules", "gwas"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 106, "codes": ["process VCFTOOLS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::vcftools=0.1.16\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/vcftools:0.1.16--he513fc3_4' :\n        'quay.io/biocontainers/vcftools:0.1.16--he513fc3_4' }\"\n\n    input:\n                                                                                                                \n                                                                         \n                                                                                                                           \n                                                           \n    tuple val(meta), path(variant_file)\n    path  bed\n    path  diff_variant_file\n\n    output:\n    tuple val(meta), path(\"*.vcf\")                    , optional:true, emit: vcf\n    tuple val(meta), path(\"*.bcf\")                    , optional:true, emit: bcf\n    tuple val(meta), path(\"*.frq\")                    , optional:true, emit: frq\n    tuple val(meta), path(\"*.frq.count\")              , optional:true, emit: frq_count\n    tuple val(meta), path(\"*.idepth\")                 , optional:true, emit: idepth\n    tuple val(meta), path(\"*.ldepth\")                 , optional:true, emit: ldepth\n    tuple val(meta), path(\"*.ldepth.mean\")            , optional:true, emit: ldepth_mean\n    tuple val(meta), path(\"*.gdepth\")                 , optional:true, emit: gdepth\n    tuple val(meta), path(\"*.hap.ld\")                 , optional:true, emit: hap_ld\n    tuple val(meta), path(\"*.geno.ld\")                , optional:true, emit: geno_ld\n    tuple val(meta), path(\"*.geno.chisq\")             , optional:true, emit: geno_chisq\n    tuple val(meta), path(\"*.list.hap.ld\")            , optional:true, emit: list_hap_ld\n    tuple val(meta), path(\"*.list.geno.ld\")           , optional:true, emit: list_geno_ld\n    tuple val(meta), path(\"*.interchrom.hap.ld\")      , optional:true, emit: interchrom_hap_ld\n    tuple val(meta), path(\"*.interchrom.geno.ld\")     , optional:true, emit: interchrom_geno_ld\n    tuple val(meta), path(\"*.TsTv\")                   , optional:true, emit: tstv\n    tuple val(meta), path(\"*.TsTv.summary\")           , optional:true, emit: tstv_summary\n    tuple val(meta), path(\"*.TsTv.count\")             , optional:true, emit: tstv_count\n    tuple val(meta), path(\"*.TsTv.qual\")              , optional:true, emit: tstv_qual\n    tuple val(meta), path(\"*.FILTER.summary\")         , optional:true, emit: filter_summary\n    tuple val(meta), path(\"*.sites.pi\")               , optional:true, emit: sites_pi\n    tuple val(meta), path(\"*.windowed.pi\")            , optional:true, emit: windowed_pi\n    tuple val(meta), path(\"*.weir.fst\")               , optional:true, emit: weir_fst\n    tuple val(meta), path(\"*.het\")                    , optional:true, emit: heterozygosity\n    tuple val(meta), path(\"*.hwe\")                    , optional:true, emit: hwe\n    tuple val(meta), path(\"*.Tajima.D\")               , optional:true, emit: tajima_d\n    tuple val(meta), path(\"*.ifreqburden\")            , optional:true, emit: freq_burden\n    tuple val(meta), path(\"*.LROH\")                   , optional:true, emit: lroh\n    tuple val(meta), path(\"*.relatedness\")            , optional:true, emit: relatedness\n    tuple val(meta), path(\"*.relatedness2\")           , optional:true, emit: relatedness2\n    tuple val(meta), path(\"*.lqual\")                  , optional:true, emit: lqual\n    tuple val(meta), path(\"*.imiss\")                  , optional:true, emit: missing_individual\n    tuple val(meta), path(\"*.lmiss\")                  , optional:true, emit: missing_site\n    tuple val(meta), path(\"*.snpden\")                 , optional:true, emit: snp_density\n    tuple val(meta), path(\"*.kept.sites\")             , optional:true, emit: kept_sites\n    tuple val(meta), path(\"*.removed.sites\")          , optional:true, emit: removed_sites\n    tuple val(meta), path(\"*.singletons\")             , optional:true, emit: singeltons\n    tuple val(meta), path(\"*.indel.hist\")             , optional:true, emit: indel_hist\n    tuple val(meta), path(\"*.hapcount\")               , optional:true, emit: hapcount\n    tuple val(meta), path(\"*.mendel\")                 , optional:true, emit: mendel\n    tuple val(meta), path(\"*.FORMAT\")                 , optional:true, emit: format\n    tuple val(meta), path(\"*.INFO\")                   , optional:true, emit: info\n    tuple val(meta), path(\"*.012\")                    , optional:true, emit: genotypes_matrix\n    tuple val(meta), path(\"*.012.indv\")               , optional:true, emit: genotypes_matrix_individual\n    tuple val(meta), path(\"*.012.pos\")                , optional:true, emit: genotypes_matrix_position\n    tuple val(meta), path(\"*.impute.hap\")             , optional:true, emit: impute_hap\n    tuple val(meta), path(\"*.impute.hap.legend\")      , optional:true, emit: impute_hap_legend\n    tuple val(meta), path(\"*.impute.hap.indv\")        , optional:true, emit: impute_hap_indv\n    tuple val(meta), path(\"*.ldhat.sites\")            , optional:true, emit: ldhat_sites\n    tuple val(meta), path(\"*.ldhat.locs\")             , optional:true, emit: ldhat_locs\n    tuple val(meta), path(\"*.BEAGLE.GL\")              , optional:true, emit: beagle_gl\n    tuple val(meta), path(\"*.BEAGLE.PL\")              , optional:true, emit: beagle_pl\n    tuple val(meta), path(\"*.ped\")                    , optional:true, emit: ped\n    tuple val(meta), path(\"*.map\")                    , optional:true, emit: map_\n    tuple val(meta), path(\"*.tped\")                   , optional:true, emit: tped\n    tuple val(meta), path(\"*.tfam\")                   , optional:true, emit: tfam\n    tuple val(meta), path(\"*.diff.sites_in_files\")    , optional:true, emit: diff_sites_in_files\n    tuple val(meta), path(\"*.diff.indv_in_files\")     , optional:true, emit: diff_indv_in_files\n    tuple val(meta), path(\"*.diff.sites\")             , optional:true, emit: diff_sites\n    tuple val(meta), path(\"*.diff.indv\")              , optional:true, emit: diff_indv\n    tuple val(meta), path(\"*.diff.discordance.matrix\"), optional:true, emit: diff_discd_matrix\n    tuple val(meta), path(\"*.diff.switch\")            , optional:true, emit: diff_switch_error\n    path \"versions.yml\"                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def args_list = args.tokenize()\n\n    def bed_arg  = (args.contains('--bed')) ? \"--bed ${bed}\" :\n        (args.contains('--exclude-bed')) ? \"--exclude-bed ${bed}\" :\n        (args.contains('--hapcount')) ? \"--hapcount ${bed}\" : ''\n    args_list.removeIf { it.contains('--bed') }\n    args_list.removeIf { it.contains('--exclude-bed') }\n    args_list.removeIf { it.contains('--hapcount') }\n\n    def diff_variant_arg = (args.contains('--diff')) ? \"--diff ${diff_variant_file}\" :\n        (args.contains('--gzdiff')) ? \"--gzdiff ${diff_variant_file}\" :\n        (args.contains('--diff-bcf')) ? \"--diff-bcf ${diff_variant_file}\" : ''\n    args_list.removeIf { it.contains('--diff') }\n    args_list.removeIf { it.contains('--gzdiff') }\n    args_list.removeIf { it.contains('--diff-bcf') }\n\n    def input_file = (\"$variant_file\".endsWith(\".vcf\")) ? \"--vcf ${variant_file}\" :\n        (\"$variant_file\".endsWith(\".vcf.gz\")) ? \"--gzvcf ${variant_file}\" :\n        (\"$variant_file\".endsWith(\".bcf\")) ? \"--bcf ${variant_file}\" : ''\n\n    \"\"\"\n    vcftools \\\\\n        $input_file \\\\\n        --out $prefix \\\\\n        ${args_list.join(' ')} \\\\\n        $bed_arg \\\\\n        $diff_variant_arg\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        vcftools: \\$(echo \\$(vcftools --version 2>&1) | sed 's/^.*VCFtools (//;s/).*//')\n    END_VERSIONS\n    \"\"\"\n}", "process VCFTOOLS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::vcftools=0.1.16\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/vcftools:0.1.16--he513fc3_4' :\n        'quay.io/biocontainers/vcftools:0.1.16--he513fc3_4' }\"\n\n    input:\n                                                                                                                \n                                                                         \n                                                                                                                           \n                                                           \n    tuple val(meta), path(variant_file)\n    path  bed\n    path  diff_variant_file\n\n    output:\n    tuple val(meta), path(\"*.vcf\")                    , optional:true, emit: vcf\n    tuple val(meta), path(\"*.bcf\")                    , optional:true, emit: bcf\n    tuple val(meta), path(\"*.frq\")                    , optional:true, emit: frq\n    tuple val(meta), path(\"*.frq.count\")              , optional:true, emit: frq_count\n    tuple val(meta), path(\"*.idepth\")                 , optional:true, emit: idepth\n    tuple val(meta), path(\"*.ldepth\")                 , optional:true, emit: ldepth\n    tuple val(meta), path(\"*.ldepth.mean\")            , optional:true, emit: ldepth_mean\n    tuple val(meta), path(\"*.gdepth\")                 , optional:true, emit: gdepth\n    tuple val(meta), path(\"*.hap.ld\")                 , optional:true, emit: hap_ld\n    tuple val(meta), path(\"*.geno.ld\")                , optional:true, emit: geno_ld\n    tuple val(meta), path(\"*.geno.chisq\")             , optional:true, emit: geno_chisq\n    tuple val(meta), path(\"*.list.hap.ld\")            , optional:true, emit: list_hap_ld\n    tuple val(meta), path(\"*.list.geno.ld\")           , optional:true, emit: list_geno_ld\n    tuple val(meta), path(\"*.interchrom.hap.ld\")      , optional:true, emit: interchrom_hap_ld\n    tuple val(meta), path(\"*.interchrom.geno.ld\")     , optional:true, emit: interchrom_geno_ld\n    tuple val(meta), path(\"*.TsTv\")                   , optional:true, emit: tstv\n    tuple val(meta), path(\"*.TsTv.summary\")           , optional:true, emit: tstv_summary\n    tuple val(meta), path(\"*.TsTv.count\")             , optional:true, emit: tstv_count\n    tuple val(meta), path(\"*.TsTv.qual\")              , optional:true, emit: tstv_qual\n    tuple val(meta), path(\"*.FILTER.summary\")         , optional:true, emit: filter_summary\n    tuple val(meta), path(\"*.sites.pi\")               , optional:true, emit: sites_pi\n    tuple val(meta), path(\"*.windowed.pi\")            , optional:true, emit: windowed_pi\n    tuple val(meta), path(\"*.weir.fst\")               , optional:true, emit: weir_fst\n    tuple val(meta), path(\"*.het\")                    , optional:true, emit: heterozygosity\n    tuple val(meta), path(\"*.hwe\")                    , optional:true, emit: hwe\n    tuple val(meta), path(\"*.Tajima.D\")               , optional:true, emit: tajima_d\n    tuple val(meta), path(\"*.ifreqburden\")            , optional:true, emit: freq_burden\n    tuple val(meta), path(\"*.LROH\")                   , optional:true, emit: lroh\n    tuple val(meta), path(\"*.relatedness\")            , optional:true, emit: relatedness\n    tuple val(meta), path(\"*.relatedness2\")           , optional:true, emit: relatedness2\n    tuple val(meta), path(\"*.lqual\")                  , optional:true, emit: lqual\n    tuple val(meta), path(\"*.imiss\")                  , optional:true, emit: missing_individual\n    tuple val(meta), path(\"*.lmiss\")                  , optional:true, emit: missing_site\n    tuple val(meta), path(\"*.snpden\")                 , optional:true, emit: snp_density\n    tuple val(meta), path(\"*.kept.sites\")             , optional:true, emit: kept_sites\n    tuple val(meta), path(\"*.removed.sites\")          , optional:true, emit: removed_sites\n    tuple val(meta), path(\"*.singletons\")             , optional:true, emit: singeltons\n    tuple val(meta), path(\"*.indel.hist\")             , optional:true, emit: indel_hist\n    tuple val(meta), path(\"*.hapcount\")               , optional:true, emit: hapcount\n    tuple val(meta), path(\"*.mendel\")                 , optional:true, emit: mendel\n    tuple val(meta), path(\"*.FORMAT\")                 , optional:true, emit: format\n    tuple val(meta), path(\"*.INFO\")                   , optional:true, emit: info\n    tuple val(meta), path(\"*.012\")                    , optional:true, emit: genotypes_matrix\n    tuple val(meta), path(\"*.012.indv\")               , optional:true, emit: genotypes_matrix_individual\n    tuple val(meta), path(\"*.012.pos\")                , optional:true, emit: genotypes_matrix_position\n    tuple val(meta), path(\"*.impute.hap\")             , optional:true, emit: impute_hap\n    tuple val(meta), path(\"*.impute.hap.legend\")      , optional:true, emit: impute_hap_legend\n    tuple val(meta), path(\"*.impute.hap.indv\")        , optional:true, emit: impute_hap_indv\n    tuple val(meta), path(\"*.ldhat.sites\")            , optional:true, emit: ldhat_sites\n    tuple val(meta), path(\"*.ldhat.locs\")             , optional:true, emit: ldhat_locs\n    tuple val(meta), path(\"*.BEAGLE.GL\")              , optional:true, emit: beagle_gl\n    tuple val(meta), path(\"*.BEAGLE.PL\")              , optional:true, emit: beagle_pl\n    tuple val(meta), path(\"*.ped\")                    , optional:true, emit: ped\n    tuple val(meta), path(\"*.map\")                    , optional:true, emit: map_\n    tuple val(meta), path(\"*.tped\")                   , optional:true, emit: tped\n    tuple val(meta), path(\"*.tfam\")                   , optional:true, emit: tfam\n    tuple val(meta), path(\"*.diff.sites_in_files\")    , optional:true, emit: diff_sites_in_files\n    tuple val(meta), path(\"*.diff.indv_in_files\")     , optional:true, emit: diff_indv_in_files\n    tuple val(meta), path(\"*.diff.sites\")             , optional:true, emit: diff_sites\n    tuple val(meta), path(\"*.diff.indv\")              , optional:true, emit: diff_indv\n    tuple val(meta), path(\"*.diff.discordance.matrix\"), optional:true, emit: diff_discd_matrix\n    tuple val(meta), path(\"*.diff.switch\")            , optional:true, emit: diff_switch_error\n    path \"versions.yml\"                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def args_list = args.tokenize()\n\n    def bed_arg  = (args.contains('--bed')) ? \"--bed ${bed}\" :\n        (args.contains('--exclude-bed')) ? \"--exclude-bed ${bed}\" :\n        (args.contains('--hapcount')) ? \"--hapcount ${bed}\" : ''\n    args_list.removeIf { it.contains('--bed') }\n    args_list.removeIf { it.contains('--exclude-bed') }\n    args_list.removeIf { it.contains('--hapcount') }\n\n    def diff_variant_arg = (args.contains('--diff')) ? \"--diff ${diff_variant_file}\" :\n        (args.contains('--gzdiff')) ? \"--gzdiff ${diff_variant_file}\" :\n        (args.contains('--diff-bcf')) ? \"--diff-bcf ${diff_variant_file}\" : ''\n    args_list.removeIf { it.contains('--diff') }\n    args_list.removeIf { it.contains('--gzdiff') }\n    args_list.removeIf { it.contains('--diff-bcf') }\n\n    def input_file = (\"$variant_file\".endsWith(\".vcf\")) ? \"--vcf ${variant_file}\" :\n        (\"$variant_file\".endsWith(\".vcf.gz\")) ? \"--gzvcf ${variant_file}\" :\n        (\"$variant_file\".endsWith(\".bcf\")) ? \"--bcf ${variant_file}\" : ''\n\n    \"\"\"\n    vcftools \\\\\n        $input_file \\\\\n        --out $prefix \\\\\n        ${args_list.join(' ')} \\\\\n        $bed_arg \\\\\n        $diff_variant_arg\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        vcftools: \\$(echo \\$(vcftools --version 2>&1) | sed 's/^.*VCFtools (//;s/).*//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/VCFTOOLS", "nf-core/gwas/nf-core__gwas/VCFTOOLS"], "list_wf_names": ["nf-core/gwas", "nf-core/modules"]}, {"nb_reuse": 2, "tools": ["BUStools"], "nb_own": 2, "list_own": ["nf-core", "redst4r"], "nb_wf": 2, "list_wf": ["nf-10x-kallisto", "scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "redst4r", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 8, "codes": [" process bustools_count{\n                  \n     publishDir \"${params.outdir}/kallisto/bustools_counts\", mode: \"copy\"\n\n     input:\n     file bus from kallisto_corrected_sort_to_count\n     file t2g from kallisto_gene_map.collect()\n\n     output:\n     file \"${bus}_eqcount\"\n     file \"${bus}_genecount\"\n\n     script:\n     \"\"\"\n     mkdir -p ${bus}_eqcount\n     mkdir -p ${bus}_genecount\n     bustools count -o ${bus}_eqcount/tcc -g $t2g -e ${bus}/matrix.ec -t ${bus}/transcripts.txt ${bus}/output.corrected.sort.bus\n     bustools count -o ${bus}_genecount/gene -g $t2g -e ${bus}/matrix.ec -t ${bus}/transcripts.txt --genecounts ${bus}/output.corrected.sort.bus\n     \"\"\"\n }", "\nprocess bustools_count{\n    tag \"$bus\"\n    label 'mid_memory'\n    publishDir \"${params.outdir}/kallisto/bustools_counts\", mode: \"copy\"\n\n    input:\n    file bus from kallisto_corrected_sort_to_count\n    file t2g from kallisto_gene_map.collect()\n\n    output:\n    file \"${bus}_eqcount\"\n    file \"${bus}_genecount\"\n\n    script:\n    \"\"\"\n    mkdir -p ${bus}_eqcount\n    mkdir -p ${bus}_genecount\n    bustools count -o ${bus}_eqcount/tcc -g $t2g -e ${bus}/matrix.ec -t ${bus}/transcripts.txt ${bus}/output.corrected.sort.bus\n    bustools count -o ${bus}_genecount/gene -g $t2g -e ${bus}/matrix.ec -t ${bus}/transcripts.txt --genecounts ${bus}/output.corrected.sort.bus\n    \"\"\"\n}"], "list_proc": ["redst4r/nf-10x-kallisto/redst4r__nf-10x-kallisto/bustools_count", "nf-core/scrnaseq/nf-core__scrnaseq/bustools_count"], "list_wf_names": ["redst4r/nf-10x-kallisto", "nf-core/scrnaseq"]}, {"nb_reuse": 1, "tools": ["SKAT"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process ska_compare_sketches {\n    tag \"${sketch_id}\"\n    publishDir \"${params.outdir}/compare_sketches\", mode: params.publish_dir_mode\n\n    input:\n    set val(ksize), file (sketches) from ska_sketches.groupTuple()\n\n    output:\n                                                                     \n    file \"ksize_${ksize}*\"\n\n    script:\n    \"\"\"\n    ska distance -o ksize_${ksize} -s 25 -i 0.95 ${sketches}\n    \"\"\"\n\n    }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/ska_compare_sketches"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": ["\nprocess decont {\n    tag { \"Decontaminating \" + sample_id }\n    publishDir \"${params.outdir}/${sample_id}/reads/\", mode: 'copy'\n\n    input:\n        set sample_id, file(fq1), file(fq2) from trim_and_combine_ch\n        set file(cont_fasta), file(cont_amb), file(cont_ann), file(cont_bwt), \\\n            file(cont_pac), file(cont_sa) from cont_fasta_ch\n    output:\n        set sample_id, file(\"${sample_id}_trimmed_decont_1.fastq.gz\"), file(\"${sample_id}_trimmed_decont_2.fastq.gz\") into \\\n            fastq_for_tadpole, fastq_for_polish_assembly_ch, fastq_for_mapping_ch, fastq_for_kraken_ch\n        set file(\"${sample_id}_trimmed_decont_1_fastqc.zip\"), file(\"${sample_id}_trimmed_decont_2_fastqc.zip\"), \\\n            file(\"${sample_id}_trimmed_decont_1_fastqc.html\"), file(\"${sample_id}_trimmed_decont_2_fastqc.html\") into fastqc_ch\n    script:\n                                                                             \n        \"\"\"\n        decont.py -i ${fq1} ${fq2} -t ${task.cpus} -c 0.5 -r ${cont_fasta} -o ${sample_id}_trimmed_decont;\n        # since this is the last fastqc processing step, let's run fastqc here\n        fastqc -t {task.cpus} ${sample_id}_trimmed_decont_1.fastq.gz ${sample_id}_trimmed_decont_2.fastq.gz;\n        \"\"\"\n}"], "list_proc": ["nf-core/vipr/nf-core__vipr/decont"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 12, "tools": ["FREEC"], "nb_own": 5, "list_own": ["Genomic-Medicine-Linkoping", "rmoran7", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 6, "list_wf": ["saw.sarek", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 22, "codes": ["\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}"], "list_proc": ["nf-core/sarek/nf-core__sarek/ControlFREECSingle", "rmoran7/custom_sarek/rmoran7__custom_sarek/ControlFREEC", "rmoran7/custom_sarek/rmoran7__custom_sarek/ControlFREECSingle", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/ControlFREEC", "rmoran7/dx_sarek/rmoran7__dx_sarek/ControlFREEC", "rmoran7/dx_sarek/rmoran7__dx_sarek/ControlFREECSingle", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/ControlFREECSingle", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/ControlFREEC", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/ControlFREECSingle", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/ControlFREEC", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/ControlFREECSingle", "nf-core/sarek/nf-core__sarek/ControlFREEC"], "list_wf_names": ["Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": [" process makeBWAIndex {\n    label 'sc_medium'\n    tag \"${fasta}\"\n    publishDir path: \"${params.outdir}/reference_genome/bwa_index\", mode: params.publish_dir_mode, saveAs: { filename -> \n            if (params.save_reference) filename \n            else if(!params.save_reference && filename == \"where_are_my_files.txt\") filename\n            else null\n    }\n\n    input:\n    path fasta from ch_fasta_for_bwaindex\n    path where_are_my_files\n\n    output:\n    path \"BWAIndex\" into (bwa_index, bwa_index_bwamem)\n    path \"where_are_my_files.txt\"\n\n    script:\n    \"\"\"\n    bwa index $fasta\n    mkdir BWAIndex && mv ${fasta}* BWAIndex\n    \"\"\"\n    }"], "list_proc": ["nf-core/eager/nf-core__eager/makeBWAIndex"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 3, "tools": ["GATK"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 3, "list_wf": ["rnavar", "modules", "raredisease"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "m3hdad", "JoseEspinosa", "apeltzer", "ramprasadn", "mahesh-panchal", "SusiJo", "maxibor"], "nb_contrib": 107, "codes": ["process GATK4_INTERVALLISTTOOLS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.5.0--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.5.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n\n    output:\n    tuple val(meta), path(\"*_split/*/*.interval_list\"), emit: interval_list\n    path \"versions.yml\"                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IntervalListTools] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n\n    mkdir ${prefix}_split\n\n    gatk --java-options \"-Xmx${avail_mem}g\" IntervalListTools \\\\\n        --INPUT $intervals \\\\\n        --OUTPUT ${prefix}_split \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    python3 <<CODE\n    import glob, os\n    # The following python code snippet rename the output files into different name to avoid overwriting or name conflict\n    intervals = sorted(glob.glob(\"*_split/*/*.interval_list\"))\n    for i, interval in enumerate(intervals):\n        (directory, filename) = os.path.split(interval)\n        newName = os.path.join(directory, str(i + 1) + filename)\n        os.rename(interval, newName)\n    CODE\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mkdir -p ${prefix}_split/temp_0001_of_6\n    mkdir -p ${prefix}_split/temp_0002_of_6\n    mkdir -p ${prefix}_split/temp_0003_of_6\n    mkdir -p ${prefix}_split/temp_0004_of_6\n    touch ${prefix}_split/temp_0001_of_6/1scattered.interval_list\n    touch ${prefix}_split/temp_0002_of_6/2scattered.interval_list\n    touch ${prefix}_split/temp_0003_of_6/3scattered.interval_list\n    touch ${prefix}_split/temp_0004_of_6/4scattered.interval_list\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_INTERVALLISTTOOLS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n\n    output:\n    tuple val(meta), path(\"*_split/*/*.interval_list\"), emit: interval_list\n    path \"versions.yml\"                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IntervalListTools] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n\n    mkdir ${prefix}_split\n\n    gatk --java-options \"-Xmx${avail_mem}g\" IntervalListTools \\\\\n        --INPUT $intervals \\\\\n        --OUTPUT ${prefix}_split \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    python3 <<CODE\n    import glob, os\n    # The following python code snippet rename the output files into different name to avoid overwriting or name conflict\n    intervals = sorted(glob.glob(\"*_split/*/*.interval_list\"))\n    for i, interval in enumerate(intervals):\n        (directory, filename) = os.path.split(interval)\n        newName = os.path.join(directory, str(i + 1) + filename)\n        os.rename(interval, newName)\n    CODE\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mkdir -p ${prefix}_split/temp_0001_of_6\n    mkdir -p ${prefix}_split/temp_0002_of_6\n    mkdir -p ${prefix}_split/temp_0003_of_6\n    mkdir -p ${prefix}_split/temp_0004_of_6\n    touch ${prefix}_split/temp_0001_of_6/1scattered.interval_list\n    touch ${prefix}_split/temp_0002_of_6/2scattered.interval_list\n    touch ${prefix}_split/temp_0003_of_6/3scattered.interval_list\n    touch ${prefix}_split/temp_0004_of_6/4scattered.interval_list\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_INTERVALLISTTOOLS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.6.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.6.1--hdfd78af_0':\n        'quay.io/biocontainers/gatk4:4.2.6.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n\n    output:\n    tuple val(meta), path(\"*_split/*/*.interval_list\"), emit: interval_list\n    path \"versions.yml\"                               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IntervalListTools] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n\n    mkdir ${prefix}_split\n\n    gatk --java-options \"-Xmx${avail_mem}g\" IntervalListTools \\\\\n        --INPUT $intervals \\\\\n        --OUTPUT ${prefix}_split \\\\\n        --TMP_DIR . \\\\\n        $args\n\n    python3 <<CODE\n    import glob, os\n    # The following python code snippet rename the output files into different name to avoid overwriting or name conflict\n    intervals = sorted(glob.glob(\"*_split/*/*.interval_list\"))\n    for i, interval in enumerate(intervals):\n        (directory, filename) = os.path.split(interval)\n        newName = os.path.join(directory, str(i + 1) + filename)\n        os.rename(interval, newName)\n    CODE\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mkdir -p ${prefix}_split/temp_0001_of_6\n    mkdir -p ${prefix}_split/temp_0002_of_6\n    mkdir -p ${prefix}_split/temp_0003_of_6\n    mkdir -p ${prefix}_split/temp_0004_of_6\n    touch ${prefix}_split/temp_0001_of_6/1scattered.interval_list\n    touch ${prefix}_split/temp_0002_of_6/2scattered.interval_list\n    touch ${prefix}_split/temp_0003_of_6/3scattered.interval_list\n    touch ${prefix}_split/temp_0004_of_6/4scattered.interval_list\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/raredisease/nf-core__raredisease/GATK4_INTERVALLISTTOOLS", "nf-core/rnavar/nf-core__rnavar/GATK4_INTERVALLISTTOOLS", "nf-core/modules/nf-core__modules/GATK4_INTERVALLISTTOOLS"], "list_wf_names": ["nf-core/rnavar", "nf-core/modules", "nf-core/raredisease"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": [" process seqtype_merge {\n\n    label 'sc_tiny'\n    tag \"$libraryid\"\n\n    input:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(bam), file(bai) from ch_branched_for_seqtypemerge.merge_me\n\n    output:\n    tuple samplename, libraryid, lane, seqtype, organism, strandedness, udg, file(\"*_seqtypemerged.bam\"), file(\"*_seqtypemerged*.{bai,csi}\")  into ch_seqtypemerge_for_filtering\n\n    script:\n    def size = params.large_ref ? '-c' : ''\n    \"\"\"\n    samtools merge ${libraryid}_seqtypemerged.bam ${bam}\n    samtools index ${libraryid}_seqtypemerged.bam ${size}\n    \"\"\"\n    \n  }"], "list_proc": ["nf-core/eager/nf-core__eager/seqtype_merge"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["HiCapTools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process HICAP {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::hicap=1.0.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/hicap:1.0.3--py_0' :\n        'quay.io/biocontainers/hicap:1.0.3--py_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path database_dir\n    path model_fp\n\n    output:\n    tuple val(meta), path(\"*.gbk\"), emit: gbk, optional: true\n    tuple val(meta), path(\"*.svg\"), emit: svg, optional: true\n    tuple val(meta), path(\"*.tsv\"), emit: tsv, optional: true\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def database_args = database_dir ? \"--database_dir ${database_dir}\" : \"\"\n    def model_args = model_fp ? \"--model_fp ${model_fp}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n    hicap \\\\\n        --query_fp $fasta_name \\\\\n        $database_args \\\\\n        $model_args \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        -o ./\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        hicap: \\$( echo \\$( hicap --version 2>&1 ) | sed 's/^.*hicap //' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/HICAP"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_CLASSIFY {\n    tag \"${repseq},${trained_classifier}\"\n    label 'process_high'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(trained_classifier)\n    path(repseq)\n\n    output:\n    path(\"taxonomy.qza\"), emit: qza\n    path(\"taxonomy.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime feature-classifier classify-sklearn  \\\n        --i-classifier ${trained_classifier}  \\\n        --p-n-jobs ${task.cpus}  \\\n        --i-reads ${repseq}  \\\n        --o-classification taxonomy.qza  \\\n        --verbose\n    qiime metadata tabulate  \\\n        --m-input-file taxonomy.qza  \\\n        --o-visualization taxonomy.qzv  \\\n        --verbose\n    #produce \"taxonomy/taxonomy.tsv\"\n    qiime tools export --input-path taxonomy.qza  \\\n        --output-path taxonomy\n    qiime tools export --input-path taxonomy.qzv  \\\n        --output-path taxonomy\n    cp taxonomy/taxonomy.tsv .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_CLASSIFY {\n    tag \"${repseq},${trained_classifier}\"\n    label 'process_high'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(trained_classifier)\n    path(repseq)\n\n    output:\n    path(\"taxonomy.qza\"), emit: qza\n    path(\"taxonomy.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    qiime feature-classifier classify-sklearn  \\\n        --i-classifier ${trained_classifier}  \\\n        --p-n-jobs ${task.cpus}  \\\n        --i-reads ${repseq}  \\\n        --o-classification taxonomy.qza  \\\n        --verbose\n    qiime metadata tabulate  \\\n        --m-input-file taxonomy.qza  \\\n        --o-visualization taxonomy.qzv  \\\n        --verbose\n    #produce \"taxonomy/taxonomy.tsv\"\n    qiime tools export --input-path taxonomy.qza  \\\n        --output-path taxonomy\n    qiime tools export --input-path taxonomy.qzv  \\\n        --output-path taxonomy\n    cp taxonomy/taxonomy.tsv .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/laclac102__ampliseq/QIIME2_CLASSIFY", "nf-core/ampliseq/nf-core__ampliseq/QIIME2_CLASSIFY"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["hlatyping"], "list_contrib": ["nvk747", "KochTobi", "nf-core-bot", "ewels", "sven1103", "maxulysse", "ggabernet", "apeltzer", "pditommaso", "christopher-mohr"], "nb_contrib": 10, "codes": [" process pre_map_hla {\n        label 'process_medium'\n\n        input:\n        path(data_index) from params.base_index_path\n        set val(pattern), file(reads) from raw_reads\n\n        output:\n        set val(pattern), \"mapped_{1,2}.bam\" into fished_reads\n\n        script:\n        def full_index = \"$data_index/$base_index_name\"\n        if (params.single_end)\n            \"\"\"\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index $reads > output_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -b1 output_1.bam > mapped_1.bam\n            \"\"\"\n        else\n            \"\"\"\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index $reads > output.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x40 -b1 output.bam > mapped_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x80 -b1 output.bam > mapped_2.bam\n            \"\"\"\n    }"], "list_proc": ["nf-core/hlatyping/nf-core__hlatyping/pre_map_hla"], "list_wf_names": ["nf-core/hlatyping"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["scrnaseq"], "list_contrib": ["PeterBailey", "nf-core-bot", "maxulysse", "sk-sahu", "apeltzer", "ggabernet", "olgabot"], "nb_contrib": 7, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file multiqc_config from ch_multiqc_config\n    file ('software_versions/*') from ch_software_versions_yaml\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")    \n    file ('STAR/*') from star_log.collect().ifEmpty([])\n    file ('alevin/*') from alevin_logs.collect().ifEmpty([])\n    file ('kallisto/*') from kallisto_log_for_multiqc.collect().ifEmpty([])\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file \\\n      -m custom_content -m salmon -m star -m kallisto .\n    \"\"\"\n}"], "list_proc": ["nf-core/scrnaseq/nf-core__scrnaseq/multiqc"], "list_wf_names": ["nf-core/scrnaseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process multiqc {\n      publishDir \"${params.outdir}/MultiQC\", mode: \"${params.publish_dir_mode}\"\n      input:\n      file multiqc_config from ch_multiqc_config\n      file (\"sourmash_describe_sig_merge/\") from ch_sourmash_sig_describe_merged.collect().ifEmpty([])\n      file (\"sourmash_describe_peptides/\") from ch_sourmash_sig_describe_peptides.collect().ifEmpty([])\n      file (\"sourmash_describe_nucleotides/\") from ch_sourmash_sig_describe_nucleotides.collect().ifEmpty([])\n      file ('fastp/*') from ch_fastp_results.collect().ifEmpty([])\n      file ('sortmerna/*') from sortmerna_logs.collect().ifEmpty([])\n      file ('software_versions/*') from ch_software_versions_yaml.collect()\n      file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n      output:\n      file \"*multiqc_report.html\" into ch_multiqc_report\n      file \"*_data\"\n      file \"multiqc_plots\"\n\n      script:\n      rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n      rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n      custom_config_file = params.multiqc_config ? \"--config $multiqc_config\" : ''\n      \"\"\"\n      multiqc . -f $rtitle $rfilename $custom_config_file \\\\\n          -m custom_content \\\\\n          -m fastp \\\\\n          -m sortmerna\n      \"\"\"\n }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/multiqc"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": ["\nprocess polish_assembly {\n    tag { \"Polishing assembly for \" + sample_id }\n    publishDir \"${params.outdir}/${sample_id}/\", mode: 'copy'\n\n    input:\n        set sample_id, file(assembly_fa), file(assembly_gaps_bed), file(fq1), file(fq2) \\\n            from gap_filled_assembly_ch.join(fastq_for_polish_assembly_ch)\n    output:\n        set sample_id, file(\"${sample_id}_polished_assembly.fa\") into polished_assembly_ch\n    script:\n        \"\"\"\n        # downsample to 1M reads to increase runtime\n        seqtk sample -s 666 ${fq1} 1000000 | gzip > R1_ds.R1.fastq.gz;\n        seqtk sample -s 666 ${fq2} 1000000 | gzip > R2_ds.R2.fastq.gz;\n        polish_viral_ref.sh -t ${task.cpus} -1 R1_ds.R1.fastq.gz -2 R2_ds.R2.fastq.gz \\\n            -r ${assembly_fa} -o ${sample_id}_polished_assembly.fa\n        \"\"\"\n}"], "list_proc": ["nf-core/vipr/nf-core__vipr/polish_assembly"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 9, "tools": ["FreeBayes"], "nb_own": 7, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core"], "nb_wf": 8, "list_wf": ["saw.sarek", "sarek_ubec", "germline_somatic", "custom_sarek", "dx_sarek", "sarek", "test_nextflow_sarek", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "ffmmulder", "rmoran7", "lescai", "apeltzer", "olgabot", "davidmasp"], "nb_contrib": 23, "codes": ["\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n\n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess CallVariantsWithFreebayes {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        tuple val(idPatient), val(idSample), file(bam), file(bai), file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n        tuple val(idPatient), val(idSample), val(\"FreeBayes\"), file(\"${intervalBed.baseName}_${idSample}.vcf\")\n\n                                \n\n    script:\n    intervalsOptions = \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n\n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/rmoran7__dx_sarek/FreebayesSingle", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/FreebayesSingle", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/CallVariantsWithFreebayes", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/FreebayesSingle", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/FreebayesSingle", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/FreebayesSingle", "nf-core/sarek/nf-core__sarek/FreebayesSingle", "rmoran7/custom_sarek/rmoran7__custom_sarek/FreebayesSingle", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/FreebayesSingle"], "list_wf_names": ["UMCUGenetics/sarek_ubec", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "rmoran7/dx_sarek", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 18, "tools": ["VCFtools"], "nb_own": 11, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 18, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "layer_lab_chco", "layer_lab_caw", "layer_lab_vc", "germline_somatic", "sarek", "custom_sarek", "sarek-mirror", "dx_sarek", "pgp-chronek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "nf-core-sarek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess Vcftools {\n\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\")\n\n    when: !('vcftools' in skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\")\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\")\n\n    when: !('vcftools' in skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/Vcftools", "nf-core/sarek/nf-core__sarek/Vcftools", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/Vcftools", "cgpu/pgp-chronek/cgpu__pgp-chronek/Vcftools", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/Vcftools", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/Vcftools", "cgpu/sarek-mirror/cgpu__sarek-mirror/Vcftools", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/Vcftools", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/Vcftools", "rmoran7/custom_sarek/rmoran7__custom_sarek/Vcftools", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/Vcftools", "rmoran7/dx_sarek/rmoran7__dx_sarek/Vcftools", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/Vcftools", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/Vcftools", "cgpu/haplosarek/cgpu__haplosarek/Vcftools", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/Vcftools", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/Vcftools", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/Vcftools"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "ryanlayerlab/layer_lab_caw", "cgpu/haplosarek", "cgpu/sarek-mirror", "cgpu/sarek-mirror-cache", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/sarek-genomechronicler", "javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["FastQC", "MultiQC", "STAR", "BEDTools", "PiRaNhA", "Cutadapt", "SAMtools", "Bowtie", "preseq", "TRUmiCount"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    cutadapt --version > v_cutadapt.txt\n    bowtie2 --version > v_bowtie2.txt\n    STAR --version > v_star.txt\n    samtools --version > v_samtools.txt\n    umi_tools --version > v_umi_tools.txt\n    bedtools --version > v_bedtools.txt\n    preseq 2> v_preseq.txt\n    # subread-align -v 2> v_subread.txt\n    bam2fq.py --version > v_rseqc.txt\n    iCount --version > v_icount.txt\n    pureclip --version > v_pureclip.txt\n    Piranha -about 2> v_piranha.txt\n    echo \"9\" > v_paraclu.txt # Paraclu does not output a version\n    meme -version > v_meme.txt\n    python --version > v_python.txt\n    pygmentize -V > v_pygments.txt\n    pigz --version 2> v_pigz.txt\n    perl -v > v_perl.txt\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/get_software_versions"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process SAMTOOLS_BAM2FQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n\n    input:\n    tuple val(meta), path(inputbam)\n    val split\n\n    output:\n    tuple val(meta), path(\"*.fq.gz\"), emit: reads\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    if (split){\n        \"\"\"\n        samtools \\\\\n            bam2fq \\\\\n            $args \\\\\n            -@ $task.cpus \\\\\n            -1 ${prefix}_1.fq.gz \\\\\n            -2 ${prefix}_2.fq.gz \\\\\n            -0 ${prefix}_other.fq.gz \\\\\n            -s ${prefix}_singleton.fq.gz \\\\\n            $inputbam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools \\\\\n            bam2fq \\\\\n            $args \\\\\n            -@ $task.cpus \\\\\n            $inputbam | gzip --no-name > ${prefix}_interleaved.fq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["nf-core/modules/nf-core__modules/SAMTOOLS_BAM2FQ"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 3, "tools": ["BEDTools"], "nb_own": 3, "list_own": ["clairecoleman1", "nf-core", "oisinmccaffrey"], "nb_wf": 3, "list_wf": ["clipseq.nextflow", "clipseq1", "clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "CharlotteAnne", "drpatelh", "clairecoleman1", "oisinmccaffrey"], "nb_contrib": 8, "codes": ["\nprocess get_crosslinks {\n    tag \"$name\"\n    publishDir \"${params.outdir}/xlinks\", mode: 'copy'\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_dedup\n    path(fai) from ch_fai_crosslinks.collect()\n\n    output:\n    tuple val(name), path(\"${name}.xl.bed.gz\") into ch_xlinks_icount, ch_xlinks_paraclu, ch_xlinks_piranha\n    tuple val(name), path(\"${name}.xl.bedgraph.gz\") into ch_xlinks_bedgraphs\n    path \"*.xl.bed.gz\" into ch_xlinks_qc\n\n    script:\n    \"\"\"\n    bedtools bamtobed -i $bam > dedup.bed\n    bedtools shift -m 1 -p -1 -i dedup.bed -g $fai > shifted.bed\n    bedtools genomecov -dz -strand + -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"+\"}' > pos.bed\n    bedtools genomecov -dz -strand - -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"-\"}' > neg.bed\n    cat pos.bed neg.bed | sort -k1,1 -k2,2n | pigz > ${name}.xl.bed.gz\n    zcat ${name}.xl.bed.gz | awk '{OFS = \"\\t\"}{if (\\$6 == \"+\") {print \\$1, \\$2, \\$3, \\$5} else {print \\$1, \\$2, \\$3, -\\$5}}' | pigz > ${name}.xl.bedgraph.gz\n    \"\"\"\n}", "\nprocess get_crosslinks {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/xlinks\", mode: params.publish_dir_mode\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_dedup\n    path(fai) from ch_fai_crosslinks.collect()\n\n    output:\n    tuple val(name), path(\"${name}.xl.bed.gz\") into ch_xlinks_icount, ch_xlinks_paraclu, ch_xlinks_piranha\n    tuple val(name), path(\"${name}.xl.bedgraph.gz\") into ch_xlinks_bedgraphs\n    path \"*.xl.bed.gz\" into ch_xlinks_qc\n\n    script:\n    \"\"\"\n    bedtools bamtobed -i $bam > dedup.bed\n    bedtools shift -m 1 -p -1 -i dedup.bed -g $fai > shifted.bed\n    bedtools genomecov -dz -strand + -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"+\"}' > pos.bed\n    bedtools genomecov -dz -strand - -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"-\"}' > neg.bed\n    cat pos.bed neg.bed | sort -k1,1 -k2,2n | pigz > ${name}.xl.bed.gz\n    zcat ${name}.xl.bed.gz | awk '{OFS = \"\\t\"}{if (\\$6 == \"+\") {print \\$1, \\$2, \\$3, \\$5} else {print \\$1, \\$2, \\$3, -\\$5}}' | pigz > ${name}.xl.bedgraph.gz\n    \"\"\"\n}", "\nprocess get_crosslinks {\n    tag \"$name\"\n    publishDir \"${params.outdir}/xlinks\", mode: 'copy'\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_dedup\n    path(fai) from ch_fai_crosslinks.collect()\n\n    output:\n    tuple val(name), path(\"${name}.xl.bed.gz\") into ch_xlinks_icount, ch_xlinks_paraclu, ch_xlinks_piranha\n    tuple val(name), path(\"${name}.xl.bedgraph.gz\") into ch_xlinks_bedgraphs\n    path \"*.xl.bed.gz\" into ch_xlinks_qc\n\n    script:\n    \"\"\"\n    bedtools bamtobed -i $bam > dedup.bed\n    bedtools shift -m 1 -p -1 -i dedup.bed -g $fai > shifted.bed\n    bedtools genomecov -dz -strand + -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"+\"}' > pos.bed\n    bedtools genomecov -dz -strand - -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"-\"}' > neg.bed\n    cat pos.bed neg.bed | sort -k1,1 -k2,2n | pigz > ${name}.xl.bed.gz\n    zcat ${name}.xl.bed.gz | awk '{OFS = \"\\t\"}{if (\\$6 == \"+\") {print \\$1, \\$2, \\$3, \\$5} else {print \\$1, \\$2, \\$3, -\\$5}}' | pigz > ${name}.xl.bedgraph.gz\n    \"\"\"\n}"], "list_proc": ["clairecoleman1/clipseq1/clairecoleman1__clipseq1/get_crosslinks", "nf-core/clipseq/nf-core__clipseq/get_crosslinks", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/get_crosslinks"], "list_wf_names": ["clairecoleman1/clipseq1", "oisinmccaffrey/clipseq.nextflow", "nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["eager"], "list_contrib": ["drpatelh", "alexandregilardet", "phue", "ewels", "evanfloden", "maxulysse", "ggabernet", "alexhbnr", "jfy133", "ZandraFagernas", "nf-core-bot", "aidaanva", "TCLamnidis", "IdoBar", "charles-plessy", "ashildv", "sc13-bioinf", "apeltzer", "maxibor", "olgabot", "scarlhoff"], "nb_contrib": 21, "codes": ["\nprocess makeSeqDict {\n    label 'sc_medium'\n    tag \"${fasta}\"\n    publishDir path: \"${params.outdir}/reference_genome/seq_dict\", mode: params.publish_dir_mode, saveAs: { filename -> \n            if (params.save_reference) filename \n            else if(!params.save_reference && filename == \"where_are_my_files.txt\") filename\n            else null\n    }\n    \n    when: !params.seq_dict && params.fasta\n\n    input:\n    path fasta from ch_fasta_for_seqdict\n    path where_are_my_files\n\n    output:\n    path \"*.dict\" into ch_seq_dict\n    path \"where_are_my_files.txt\"\n\n    script:\n    \"\"\"\n    picard -Xmx${task.memory.toMega()}M CreateSequenceDictionary R=$fasta O=\"${fasta.baseName}.dict\"\n    \"\"\"\n}"], "list_proc": ["nf-core/eager/nf-core__eager/makeSeqDict"], "list_wf_names": ["nf-core/eager"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process MASH_DIST {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::mash=2.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1' :\n        'quay.io/biocontainers/mash:2.3--he348c14_1' }\"\n\n    input:\n    tuple val(meta), path(query)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: dist\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mash \\\\\n        dist \\\\\n        -p $task.cpus \\\\\n        $args \\\\\n        $reference \\\\\n        $query > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(mash --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/MASH_DIST"], "list_wf_names": ["nf-core/modules"]}, {"nb_reuse": 2, "tools": ["QIIME", "BioMe"], "nb_own": 2, "list_own": ["nf-core", "laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["emnilsson", "erikrikarddaniel", "nf-core-bot", "ewels", "maxulysse", "KevinMenden", "asafpr", "apeltzer", "jtangrot", "ggabernet", "DiegoBrambilla", "colindaven", "d4straub", "xingaulaglag", "drpatelh", "PhilPalmer"], "nb_contrib": 16, "codes": ["process QIIME2_EXPORT_RELTAX {\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    path(table)\n    path(taxonomy)\n    val(tax_agglom_min)\n    val(tax_agglom_max)\n\n    output:\n    path(\"*.tsv\")        , emit: tsv\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    ##on several taxa level\n    array=(\\$(seq ${tax_agglom_min} 1 ${tax_agglom_max}))\n\n    for i in \\${array[@]}\n    do\n        #collapse taxa\n        qiime taxa collapse \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-level \\$i \\\n            --o-collapsed-table table-\\$i.qza\n        #convert to relative abundances\n        qiime feature-table relative-frequency \\\n            --i-table table-\\$i.qza \\\n            --o-relative-frequency-table relative-table-\\$i.qza\n        #export to biom\n        qiime tools export --input-path relative-table-\\$i.qza \\\n            --output-path relative-table-\\$i\n        #convert to tab separated text file\n        biom convert \\\n            -i relative-table-\\$i/feature-table.biom \\\n            -o rel-table-\\$i.tsv --to-tsv\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "process QIIME2_EXPORT_RELTAX {\n    label 'process_low'\n\n                                                                                    \n                                             \n\n    input:\n    path(table)\n    path(taxonomy)\n    val(tax_agglom_min)\n    val(tax_agglom_max)\n\n    output:\n    path(\"*.tsv\")        , emit: tsv\n    path \"versions.yml\"  , emit: versions\n    path(\"*.html\")       , emit: html\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    ##on several taxa level\n    array=(\\$(seq ${tax_agglom_min} 1 ${tax_agglom_max}))\n\n    for i in \\${array[@]}\n    do\n        #collapse taxa\n        qiime taxa collapse \\\n            --i-table ${table} \\\n            --i-taxonomy ${taxonomy} \\\n            --p-level \\$i \\\n            --o-collapsed-table table-\\$i.qza\n        #convert to relative abundances\n        qiime feature-table relative-frequency \\\n            --i-table table-\\$i.qza \\\n            --o-relative-frequency-table relative-table-\\$i.qza\n        #export to biom\n        qiime tools export --input-path relative-table-\\$i.qza \\\n            --output-path relative-table-\\$i\n        #convert to tab separated text file\n        biom convert \\\n            -i relative-table-\\$i/feature-table.biom \\\n            -o rel-table-\\$i.tsv --to-tsv\n    done\n\n    #plot the composition bar plot from relative abundance table for each of level of taxonomy\n        comp_bar_plot.py\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/ampliseq/nf-core__ampliseq/QIIME2_EXPORT_RELTAX", "laclac102/ampliseq/laclac102__ampliseq/QIIME2_EXPORT_RELTAX"], "list_wf_names": ["nf-core/ampliseq", "laclac102/ampliseq"]}, {"nb_reuse": 6, "tools": ["fastPHASE"], "nb_own": 5, "list_own": ["ajodeh-juma", "avantonder", "nf-core", "ray1919", "peterk87"], "nb_wf": 6, "list_wf": ["nf-iav-illumina", "mag", "viclara", "bactmap", "assembleBAC", "lRNA-Seq"], "list_contrib": ["ajodeh-juma", "alexandregilardet", "heuermh", "alneberg", "ewels", "HadrienG", "maxulysse", "antunderwood", "ggabernet", "skrakau", "jfy133", "AntoniaSchuster", "nf-core-bot", "avantonder", "maxibor", "thanhleviet", "peterk87", "KevinMenden", "apeltzer", "ray1919", "d4straub", "drpatelh"], "nb_contrib": 22, "codes": ["\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (params.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path \"adapter.fasta\"\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_16'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["ajodeh-juma/viclara/ajodeh-juma__viclara/FASTP", "avantonder/assembleBAC/avantonder__assembleBAC/FASTP", "nf-core/bactmap/nf-core__bactmap/FASTP", "peterk87/nf-iav-illumina/peterk87__nf-iav-illumina/FASTP", "nf-core/mag/nf-core__mag/FASTP", "ray1919/lRNA-Seq/ray1919__lRNA-Seq/FASTP"], "list_wf_names": ["ajodeh-juma/viclara", "avantonder/assembleBAC", "ray1919/lRNA-Seq", "peterk87/nf-iav-illumina", "nf-core/mag", "nf-core/bactmap"]}, {"nb_reuse": 6, "tools": ["SAMtools", "HISAT2"], "nb_own": 4, "list_own": ["harleenduggal", "raygozag", "nf-core", "mahesh-panchal"], "nb_wf": 5, "list_wf": ["RNASEQ", "test_nfcore_workflow_chain", "modules", "nfcore-rnaseq", "rnaseq"], "list_contrib": ["Danilo2771", "ajodeh-juma", "drejom", "SpikyClip", "jordwil", "FelixKrueger", "kmurat1", "chuan-wang", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "Galithil", "avantonder", "lskatz", "jfnavarro", "na399", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "raygozag", "yocra3", "lescai", "pranathivemuri", "sateeshperi", "piotr-faba-ardigen", "aanil", "silviamorins", "d4straub", "SPPearce", "Midnighter", "rannick", "yuukiiwa", "zxl124", "phue", "FriederikeHanssen", "maxulysse", "rsuchecki", "matrulda", "veeravalli", "george-hall-ucl", "antunderwood", "sofstam", "rpetit3", "colindaven", "lpantano", "jfy133", "santiagorevale", "ppericard", "kevbrick", "mvanins", "nebfield", "ntoda03", "drpowell", "emnilsson", "rfenouil", "jburos", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "Hammarn", "fbdtemme", "sven1103", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "amayer21", "BatoolMM", "sima-r", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "adomingues", "pcantalupo", "GCJMackenzie", "jun-wan", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "BABS-STP1", "senthil10", "kviljoen", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "alneberg", "sysbiocoder", "arontommi", "ggabernet", "vezzi", "mjcipriano", "skrakau", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "nf-core-bot", "lassefolkersen", "nickhsmith", "c-mertes", "sofiahaglund", "orionzhou", "abhi18av", "pditommaso", "robsyme", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "marchoeppner", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor", "olgabot", "paulklemm"], "nb_contrib": 146, "codes": ["\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:0e773bb207600fcb4d38202226eb20a33c7909b6-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:0e773bb207600fcb4d38202226eb20a33c7909b6-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.15.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:0e773bb207600fcb4d38202226eb20a33c7909b6-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:0e773bb207600fcb4d38202226eb20a33c7909b6-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["harleenduggal/nfcore-rnaseq/harleenduggal__nfcore-rnaseq/HISAT2_ALIGN", "raygozag/rnaseq/raygozag__rnaseq/HISAT2_ALIGN", "harleenduggal/RNASEQ/harleenduggal__RNASEQ/HISAT2_ALIGN", "nf-core/rnaseq/nf-core__rnaseq/HISAT2_ALIGN", "mahesh-panchal/test_nfcore_workflow_chain/mahesh-panchal__test_nfcore_workflow_chain/HISAT2_ALIGN", "nf-core/modules/nf-core__modules/HISAT2_ALIGN"], "list_wf_names": ["raygozag/rnaseq", "harleenduggal/RNASEQ", "harleenduggal/nfcore-rnaseq", "nf-core/modules", "nf-core/rnaseq", "mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["kmermaid"], "list_contrib": ["nf-core-bot", "ewels", "pranathivemuri", "maxulysse", "snafees", "phoenixAja", "olgabot"], "nb_contrib": 7, "codes": [" process samtools_fastq_aligned {\n    tag \"${channel_id}\"\n    publishDir \"${params.outdir}/10x-fastqs/per-channel/aligned\", mode: params.publish_dir_mode\n    label \"mid_cpu\"\n\n    input:\n    set val(channel_id), file(bam) from tenx_bam_for_unaligned_fastq_ch\n\n    output:\n    set val(channel_id), val(\"aligned\"), file(reads) into tenx_reads_aligned_counting_ch, tenx_reads_aligned_concatenation_ch\n\n    script:\n    reads = \"${channel_id}__aligned.fastq.gz\"\n    \"\"\"\n    samtools view -ub -F 4 ${bam} \\\\\n        | samtools fastq --threads ${task.cpus} -T ${tenx_tags} - \\\\\n        | gzip -c - \\\\\n          > ${reads}\n    \"\"\"\n  }"], "list_proc": ["nf-core/kmermaid/nf-core__kmermaid/samtools_fastq_aligned"], "list_wf_names": ["nf-core/kmermaid"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["smrnaseq"], "list_contrib": ["sirselim", "lcabus-flomics", "Hammarn", "nf-core-bot", "ewels", "ErikDanielsson", "jemten", "maxulysse", "KevinMenden", "kstawiski", "apeltzer", "pericsson", "sdjebali", "pditommaso", "lpantano", "drpatelh", "chuan-wang", "mjsteinbaugh"], "nb_contrib": 18, "codes": [" process bowtie_indices {\n    label 'process_medium'\n    publishDir path: { params.save_reference ? \"${params.outdir}/references_parsed\" : params.outdir },\n               saveAs: { params.save_reference ? it : null }, mode: 'copy'\n\n    input:\n    file refgenome from reference_genome\n    file mature from reference_mature\n    file hairpin from reference_hairpin\n\n    output:\n    file 'genome.edited.fa' into fasta\n    file 'genome.*.ebwt' into indices_mirdeep2\n    file 'hairpin.fa' into hairpin\n    file 'mature.fa' into mature\n\n    script:\n    \"\"\"\n    # Uncompress FASTA reference files if necessary\n    MATURE=\"$mature\"\n    HAIRPIN=\"$hairpin\"\n    if [ \\${MATURE: -3} == \".gz\" ]; then\n        gunzip -f \\$MATURE\n        MATURE=\\${MATURE%%.gz}\n    fi\n    if [ \\${HAIRPIN: -3} == \".gz\" ]; then\n        gunzip -f \\$HAIRPIN\n        HAIRPIN=\\${HAIRPIN%%.gz}\n    fi\n\n    # Remove any special base characters from reference genome FASTA file\n    sed '/^[^>]/s/[^ATGCatgc]/N/g' $refgenome > genome.edited.fa\n\n    # Remove spaces from miRBase FASTA files\n    sed -i 's, ,_,g' \\$HAIRPIN\n    sed -i 's, ,_,g' \\$MATURE\n\n    # Build bowtie index\n    bowtie-build genome.edited.fa genome --threads ${task.cpus}\n    \"\"\"\n  }"], "list_proc": ["nf-core/smrnaseq/nf-core__smrnaseq/bowtie_indices"], "list_wf_names": ["nf-core/smrnaseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["mag"], "list_contrib": ["AntoniaSchuster", "heuermh", "nf-core-bot", "alneberg", "ewels", "d4straub", "HadrienG", "maxulysse", "KevinMenden", "ggabernet", "apeltzer", "maxibor", "skrakau", "jfy133"], "nb_contrib": 14, "codes": ["\nprocess BOWTIE2_ASSEMBLY_ALIGN {\n    tag \"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:assembly_meta, publish_by_meta:['assembler', 'id']) }\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.2 bioconda::samtools=1.11 conda-forge::pigz=2.3.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    }\n\n    input:\n    tuple val(assembly_meta), path(assembly), path(index), val(reads_meta), path(reads)\n\n    output:\n    tuple val(assembly_meta), path(assembly), path(\"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}.bam\"), path(\"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}.bam.bai\"), emit: mappings\n    tuple val(assembly_meta), val(reads_meta), path(\"*.bowtie2.log\")                                                                                                                                      , emit: log\n    path '*.version.txt'                                                                                                                                                                                  , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def name = \"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}\"\n    def input = params.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.rev.1.bt2l\" -o -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2l//' | sed 's/.rev.1.bt2//'`\n    bowtie2 -p \"${task.cpus}\" -x \\$INDEX $input 2> \"${name}.bowtie2.log\" | \\\n        samtools view -@ \"${task.cpus}\" -bS | \\\n        samtools sort -@ \"${task.cpus}\" -o \"${name}.bam\"\n    samtools index \"${name}.bam\"\n\n    if [ ${name} = \"${assembly_meta.assembler}-${assembly_meta.id}-${assembly_meta.id}\" ] ; then\n        mv \"${name}.bowtie2.log\" \"${assembly_meta.assembler}-${assembly_meta.id}.bowtie2.log\"\n    fi\n\n    echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}_assembly.version.txt\n    \"\"\"\n}"], "list_proc": ["nf-core/mag/nf-core__mag/BOWTIE2_ASSEMBLY_ALIGN"], "list_wf_names": ["nf-core/mag"]}, {"nb_reuse": 88, "tools": ["FastQC"], "nb_own": 64, "list_own": ["wslh-bio", "alesssia", "drejom", "HadrienG", "kevinpryan", "WhalleyT", "lifebit-ai", "pilm-bioinformatics", "qbicsoftware-archive", "bioatlas", "dfornika", "jiangweiyao", "oisinmccaffrey", "anwarMZ", "grst", "rnharmening", "heinzlab", "viktorlj", "UMCUGenetics", "jlboat", "UCL-BLIC", "anu9109", "rwtaylor", "edgano", "Gustius", "monikaBrandt", "glormph", "jgallowa07", "herczegrobert", "nf-core", "czbiohub", "Jeremy1805", "cometsong", "rbpisupati", "grbot", "SherineAwad", "genomicsITER", "StaPH-B", "FischbachLab", "likelet", "gavinf97", "clairecoleman1", "Gregor-Mendel-Institute", "gongyh", "propan2one", "vladsaveliev", "kerimoff", "dalmiaa", "davismcc", "marchoeppner", "KevinMenden", "steepale", "cgpu", "JackCurragh", "biocorecrg", "maxibor", "bioinfo-pf-curie", "BenNolann", "jiangfuqing", "drpatelh", "BarryDigby", "coraliegimonnet", "TDMedina", "remiolsen"], "nb_wf": 82, "list_wf": ["blastlca", "groupnextflow", "variantcallerbench", "nf-VIF", "microarray-qc-workflow", "wgsalign", "NextflowModules", "somatic-variant-caller", "cookiecutter", "metaphlan_nf", "nf-large-assembly", "nf-upcoast-v", "wgsfastqtobam", "ampliflow", "MesKit", "csrna", "Sample_NF", "sparkling-preprocessing", "SarGet", "kinseq", "nf-hipsci-fibro", "nf-methylpy", "umcaw", "riboseq_data_processing", "2018_coral_16S", "nf-core-test", "NanoCLUST", "nf-core-lfquandenser", "BS-Seq_pipeline", "pipelines-nf-circtools", "ownpipeline", "rsi_analysis", "exoseq", "nf-core-example", "YAMP", "nf-ginkgo", "demux", "nf-core-mynewpipeline", "omics-analyser", "clipseq1", "coregenome_align_nf", "nf-core-sgrnaquant", "coproID", "sparkling-gatk", "nf-multipleReferenceMapper", "nf-core-scgs", "smrna-seq-pipeline", "clipseq.nextflow", "CRISPR-Cas-off-target-identification", "mycosnp", "hybrid-assembly", "nextflow-pipelines", "Nextflow_test", "chip-seq-pipeline", "spriggan", "nf", "nf-core-mutenrich", "allele_specific_RNAseq", "TCoffee-NatureProtocol-nf", "Nextflow-Example", "bam2fastq-nf", "rtp_workshop", "nf-core-radseq", "admapipe", "qtlquant", "ChIP-seq_pipeline", "nf-bowtie", "kraken2_biom_nf", "crisprvar", "nf-haplocaller", "ATAC_Seq_nxf", "nf-core-influenzangs", "NextflowScripts", "nextflow_bits", "BioNextflow", "rnaseq_variant_calling", "neoantigen_prediction", "nf-core-cpo", "nf-readqc", "quaisar_nf", "nf-core-mytrial", "dryad"], "list_contrib": ["drejom", "HadrienG", "kalanir", "oisinmccaffrey", "erikrikarddaniel", "viktorlj", "MoritzBauer", "anu9109", "lucacozzuto", "jgallowa07", "sawibo", "Eletham", "skiniry", "Zethson", "cometsong", "ffmmulder", "toniher", "melferink", "AbigailShockey", "cgpu", "ywilke", "TDMedina", "alesssia", "maxulysse", "lpantano", "jfy133", "anwarMZ", "pufetin", "rwtaylor", "chenjy327", "ZIWEI-WONG", "clairecoleman1", "PhilPalmer", "gongyh", "Hammarn", "kerimoff", "sven1103", "dalmiaa", "steepale", "JackCurragh", "druvus", "ellendejong", "BarryDigby", "DiegoBrambilla", "c-guzman", "ewels", "jherrero", "kevinpryan", "dfornika", "edgano", "Gustius", "monikaBrandt", "senthil10", "herczegrobert", "lconde-ucl", "Niinleslie", "Jeremy1805", "k-florek", "grbot", "genomicsITER", "nservant", "gavinf97", "vladsaveliev", "LiuJie1117", "davismcc", "sarahdrury", "LeuThrAsp", "drpatelh", "coraliegimonnet", "alneberg", "WhalleyT", "Ninomoriaty", "Wangxin555", "jiangweiyao", "grst", "rnharmening", "lizheng141026", "jlboat", "pditommaso", "glormph", "rbpisupati", "likelet", "propan2one", "rernst", "marchoeppner", "KevinMenden", "sunitj", "apeltzer", "maxibor", "BenNolann", "olgabot", "remiolsen"], "nb_contrib": 92, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\tprocess fastqc {\n\t\tpublishDir \"${params.outdir}/fastqc\", mode: 'copy',\n\t\t\tsaveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\t\tinput:\n\t\t\tfile reads from reads_files_fastqc\n\t\toutput:\n\t\t\tfile '*_fastqc.{zip,html}' into fastqc_results\n\t\tscript:\n\t\t\"\"\"\n\t\t\tfastqc -q $reads\n\t\t\"\"\"\n\t}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from raw_reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    when:\n    !params.skipQC && !params.skipFastQC\n\n    input:\n    set val(name), val(experiment_info), file(reads) from raw_reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n  input:\n  set val(name), file(reads) from combined_reads\n\n  output:\n  file(\"*_fastqc.{zip,html}\") into fastqc_results, fastqc_multiqc\n\n  script:\n  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n  tag \"$name\"\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n  input:\n  set val(name), file(reads) from combined_reads\n\n  output:\n  file(\"*_fastqc.{zip,html}\") into fastqc_results, fastqc_multiqc\n\n  script:\n  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc_on_raw {\n\tpublishDir 'fastqc_on_raw', mode: 'copy'\n\t\n\tinput:\n\tfile raw_fastq from fastq_files_channel1\n\n\toutput:\n\tfile '*_fastqc.{zip,html}' into raw_fastqc_dir\n\n\t\"\"\"\n\tfastqc -q $raw_fastq \n\t\"\"\"\n}", "\tprocess fastqc {\n\t\t\ttag \"${pair_id}\"\n\t\t\tpublishDir \"${params.outdir}/fastQC\", mode: 'copy',\n\t\t\tsaveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n\t\t\tinput:\n\t\t\tset val(pair_id), file(reads) from ch_read_pairs_fastqc\n\n\t\t\toutput:\n\t\t\tfile \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n\t\t\twhen:\n\t\t\t!params.skip_fastqc\n\n\t\t\tscript: \n\t\t\t\"\"\"\n\t\t\tfastqc -q ${reads}\n\t\t\t\"\"\"\n\t\t}", "\tprocess fastqc_multi {\n\t\t\ttag \"${folder}${params.split}${pair_id}\"\n\t\t\tpublishDir \"${params.outdir}/fastQC\", mode: 'copy',\n\t\t\tsaveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n\t\t\tinput:\n\t\t\tset val(pair_id), file(reads), val(folder) from ch_read_pairs_fastqc\n\n\t\t\toutput:\n\t\t\tfile \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n\t\t\twhen:\n\t\t\t!params.skip_fastqc\n\n\t\t\tscript: \n\t\t\t\"\"\"\n\t\t\tfastqc -q ${reads}\n\t\t\t\"\"\"\n\t\t}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"${reads[0].baseName}\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    file reads from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n\n    conda 'bioconda::fastqc'\n\n    label 'normal'\n\n    cpus 1\n\n    publishDir \"${params.results}/fastqc\", mode: 'copy'\n\n    errorStrategy 'ignore'\n\n    input:\n        set val(name), file(reads) from reads_fastqc\n\n    output:\n        file '*_fastqc.{zip,html}' into fastqc_results\n    script:\n        \"\"\"\n        fastqc -q $reads\n        \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n        publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n        input:\n        set val(name), file(reads) from read_files_fastqc\n\n        output:\n        file '*_fastqc.{zip,html}' into fastqc_results\n        file '.command.out' into fastqc_stdout\n\n        script:\n        \"\"\"\n        fastqc -q $reads\n        \"\"\"\n}", "\tprocess fastqc {\n\t\ttag \"$pair_id\"\n\t\tpublishDir \"${params.outdir}/fastqc\", mode: 'copy',\n\t\t\tsaveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\t\tinput:\n\t\t\tset pair_id, file(reads) from read_files_fastqc\n\t\toutput:\n\t\t\tfile '*_fastqc.{zip,html}' into fastqc_results\n\t\tscript:\n\t\t\"\"\"\n\t\t\tfastqc -q $reads\n\t\t\"\"\"\n\t}", "\nprocess quality_control {\n\n    container 'quay.io/biocontainers/fastqc:0.11.9--0'\n\n    publishDir \"${baseDir}/fastqc_output/\"\n\n    input:\n        tuple sampleId, file(merged_reads) from append_1\n\n    output:\n       file(\"${output_name}\") into finish\n\n    exec:\n        \n        base_name = \"${merged_reads}\" - ~/\\.\\w+$/\n        output_name = \"${base_name}_fastqc.html\"\n\n    script:\n        \"\"\"\n        fastqc -t 4 ${merged_reads}\n        \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$id\"\n\n    publishDir \"${params.outdir}/fastqc\",  mode: params.publishDirMode,\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(id), file(reads) from raw_reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc{\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    set val(name), file(r) from raw_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $r\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n\n    input:\n        set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n        file '*_fastqc.{zip,html}' into fastqc_results\n    script:\n        \"\"\"\n        fastqc -q $reads\n        \"\"\"\n}", "\nprocess fastQC {\n    tag \"${fastq}\"\n    label (params.LABEL)\n    container params.CONTAINER\n    if (params.OUTPUT != \"\") { publishDir(params.OUTPUT, mode:'copy') }\n\n    input:\n    path(fastq)\n\n    output:\n    path(\"*_fastqc.*\") \n\n    script:\n\t\"\"\"\n\tfastqc -t ${task.cpus} ${fastq} \n\t\"\"\"\n}", "\nprocess fastQC2 {\n    tag \"${fastq}\"\n    label (params.LABEL)\n    container params.CONTAINER\n    if (params.OUTPUT != \"\") { publishDir(params.OUTPUT, mode:'copy') }\n\n    input:\n    tuple val(id), path(fastq)\n\n    output:\n    tuple val(id), path(\"*_fastqc.*\") \n\n    script:\n\t\"\"\"\n\tfastqc -t ${task.cpus} ${fastq} \n\t\"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'env_qual_small'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from short_reads_qc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    \n                            \n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files2\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    file(reads) from read_files_fastqc\n    val(name) from fastqc_names\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    \n                            \n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files, qc_files1\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    \n    errorStrategy 'ignore'\n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files, qc_files1\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "\nprocess fastqc {\n    \n                            \n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files, qc_files1\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "\nprocess QConRawReads {\n    tag  \"${read}\" \n    publishDir outputQC\n\n    input:\n    file(read) from reads_for_fastqc.flatten()\n\n    output:\n    file(\"*_fastqc.*\") into raw_fastqc_files\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} $read\n    \"\"\"\n\n}", " process fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n    container 'flowcraft/fastqc:0.11.7-1'\n\n    input:\n    set val(name), file(reads) from reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n  }", " process fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n    container 'flowcraft/fastqc:0.11.7-1'\n\n    input:\n    set val(name), file(reads) from reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n  }", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n   \n    when:\n    !params.skip_fastqc\n\n    input:\n    set val(name), file(reads) from reads_fastqc\n\n    output:\n    set val(prefix), file(\"${prefix}*.{zip,html}\") into fastqc_results\n\n    script:\n    prefix = reads[0].toString() - ~/(_1)?(_2)?(_R1)?(_R2)?(.R1)?(.R2)?(_val_1)?(_val_2)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc{\n                     \n\n    publishDir \"/home/ldotrang/C_auris_testing/nfBWAref/bwa_preprocess_output/qc_reports\", mode: 'copy'\n    tag \"$name\"\n\n    input:\n    tuple name, file(reads) from fastqc_downsampled_reads\n\n    output: \n    file(\"*_fastqc.{zip,html}\") into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q  ${reads}\n    \"\"\"\n\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess FastQC {\n\n      publishDir \"${params.outdir}/QC/raw\", mode:'copy'\n\n      input:\n\n      tuple val(key), file(reads) from fastqc_reads\n\n      output:\n\n      file(\"*.{html,zip}\") into fastqc_ch\n\n      script:\n      \"\"\"\n      fastqc -q $reads\n      \"\"\"\n}", "\nprocess quality_assessment {\n    tag \"$name\"\n    \n    container params.docker_container_fastqc\n    \n    publishDir \"${workingpath}/01_fastqc\"\n\n    input:\n    set val(name), file(reads) from read_files_fastqc.mix(qcd_reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_log\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess FASTQC{\n    tag \"${base}\"\n    publishDir params.outdir, mode: 'copy',\n        saveAs: { params.save_qc_intermediates ? \"fastqc/${it}\" : null }\n\n    when:\n    params.run_qc\n\n    input:\n    tuple val(base), file(reads) from ch_qc_reads\n\n    output:\n    tuple val(base), file(\"*.{html,zip}\") into ch_multiqc\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n    container 'flowcraft/fastqc:0.11.7-1'\n\n    when:\n    !params.skip_fastqc\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(reads) from reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/FastQC\", mode: 'copy'\n\n  input:\n    set sample_id, file(reads) from reads_fastqc_ch\n\n  output:\n    file fastqc_out into fastqc_results_ch\n    file(fastqc_out)\n  \n  script:\n  fastqc_out = \"*_fastqc.{zip,html}\"\n  \"\"\"\n    fastqc -q $reads\n  \"\"\"\n}", "\nprocess quality_assessment {\n\t\n    tag \"$name\"\n\t\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_fastqc\n    } else {\n        container params.docker_container_fastqc\n    }\n\t\n\tpublishDir \"${params.outdir}/${params.prefix}/fastqc\", mode: 'copy'    \n\n    input:\n    set val(name), file(reads) from read_files_fastqc.mix(qcd_reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_log\n\t\n\twhen:\n\tparams.mode != \"characterisation\"\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    set val(name), file(reads) from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n    file '.command.out' into fastqc_stdout\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess postFastqc {\n    publishDir \"results/fastqc/post/\"\n\n    input:\n    set val(dataset_id), file(r1) from trimmed_reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into trimmed_fastqc_results\n\n    clusterOptions = { \"--account=icbrbi --qos=icbrbi --time=4:00:00 --mem-per-cpu=2gb --cpus-per-task=1\" }\n    module 'fastqc'\n\n    script:\n    \"\"\"\n    fastqc -q ${r1} \n    \"\"\"\n}", "\nprocess preFastqc {\n    publishDir \"results/fastqc/pre/\"\n\n    input:\n    set val(dataset_id), file(r1) from fastqc_reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into raw_fastqc_results\n\n    clusterOptions = { \"--account=icbrbi --qos=icbrbi --time=4:00:00 --mem-per-cpu=2gb --cpus-per-task=1\" }\n    module 'fastqc'\n\n    script:\n    \"\"\"\n    fastqc -q ${r1}\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    set val(name), file(reads) from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n    file '.command.out' into fastqc_stdout\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$reads\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    file reads from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess FastQC {\n  tag \"${task.attempt}.${params.pipeline_name}-${sampleID}-${libID}-${laneID}\"\n\n  cpus 1\n  memory 2.GB\n\n  input:\n  set sampleID, libID, laneID, file(reads) from fastqFiles\n\n  output:\n  file('*_fastqc.{zip,html}') into fastqc_results\n\n  \"\"\"\n  /usr/local/bin/fastqc -q ${reads}\n  \"\"\"\n}", "\nprocess FastQC {\n\n          publishDir \"$params.outdir/FastQC/Raw\", mode:'copy'\n\n          input:\n              tuple val(base), file(fastq) from fastqc_reads\n\n          output:\n              file(\"*.{html,zip}\") into fastqc_raw\n\n          script:\n          \"\"\"\n          fastqc -q $fastq\n          \"\"\"\n}", " process FastQC_trim {\n\n        publishDir \"$params.outdir/FastQC/Trimmed\", mode:'copy'\n\n        input:\n            tuple val(base), file(fastq) from fastqc_trim_reads\n\n        output:\n            file (\"*.{html,zip}\") into fastqc_trimmed\n\n        script:\n        \"\"\"\n        fastqc -q $fastq\n        \"\"\"\n        }", "\nprocess fastqc{\n    tag \"${base}\"\n\n    when:\n    params.run_qc_trim\n\n    input:\n    tuple val(base), file(reads) from trimmed_reads_ch_1\n\n    output:\n    file(\"*.zip\") into fastqc_trim\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n\tinput:\n\ttuple val(sample), file(reads) from reads_for_fastqc\n\n\toutput:\n\tfile \"*_fastqc.{zip,html}\" into fastqc_results\n\n\tscript:\n\t\"\"\"\n\tfastqc -q ${reads}\n\t\"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from raw_reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    publishDir \"${params.outdir}/fastqc_rawdata\", mode: 'copy',\n    saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess testdocker {\n\n\t\"\"\"\n\tfastqc -h\n\t\"\"\"\n}", "\nprocess FastQC {\n\n      publishDir \"${params.outdir}/QC/raw\", mode:'copy'\n\n      input:\n\n      tuple val(key), file(reads) from fastqc_reads\n\n      output:\n\n      file(\"*.{html,zip}\") into fastqc_ch\n\n      script:\n      \"\"\"\n      fastqc -q $reads\n      \"\"\"\n}", "process fastQC{\n\n    tag { \"${params.prefix}/${sampleName}\" }\n    publishDir \"${params.outdir}/${params.prefix}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"*.{zip,html}\", mode: \"copy\"\n\n    label 'smallcpu'\n\n    input:\n    tuple(sampleName, path(forward), path(reverse))\n\n    output:\n    path(\"*.{zip,html}\"), emit: fastqc_files\n\n    \"\"\"\n    fastqc -t ${task.cpus} ${forward} ${reverse}\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    container \"quay.io/biocontainers/fastqc:0.11.8--1\"\n    memory 1.GB\n    time 8.h\n\n                                                                 \n                                                                                             \n\n    input:\n    set val(name), file(reads) from read_pairs_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "process FastQC {\n    tag {\"FastQC ${sample_id} - ${rg_id}\"}\n    label 'FastQC_0_11_5'\n    clusterOptions = workflow.profile == \"sge\" ? \"-l h_vmem=${params.mem}\" : \"\"\n    container = 'library://sawibo/default/bioinf-tools:fastqc-0.11.5'\n    shell = ['/bin/bash', '-euo', 'pipefail']\n\n    input:\n        tuple (sample_id, rg_id, path(fastq) )\n\n    output:\n        path(\"*_fastqc.{zip,html}\", emit: fastqc_reports)\n\n    script:\n        \"\"\"\n        fastqc ${params.optional} -t ${task.cpus} $fastq\n        \"\"\"\n}", "\nprocess FASTQC{\n    tag \"{base}\"\n    publishDir \"${params.outdir}/quality_control/fastqc\", mode: 'copy',\n        saveAs: { params.save_qc_intermediates ? \"fastqc/${it}\" : null }\n\n    input:\n    tuple val(base), file(reads) from ch_qc_reads\n\n    output:\n    file(\"*.{html,zip}\") into ch_multiqc\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag { \"${accID}_$reads\" }\n    label 'env_quality'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(accID), file(reads), val(library_id) from read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$sampleID\"\n    label 'fastqc'\n    publishDir \"${sample_outdir}\", pattern: \"*_fastqc.*\", mode: 'move'\n    cpus 2\n\n    input:\n    tuple sampleID, file(trimmedfqs) from trim_fastqc\n    output:\n    file(\"*_fastqc.*\")\n\n    script:\n    log.info \"-----FastQC running on ${sampleID}-----\"\n    \"\"\"\n    /opt/bin/FastQC/fastqc -t 2 ${trimmedfqs[0]} ${trimmedfqs[1]}\n    \"\"\"\n}", "\nprocess RunFastQC {\n  tag {idPatient}\n\n  publishDir \"${directoryMap.fastQC}/${idSample}\", mode: 'link'\n\n  input:\n    set idPatient, idSample, file(fastqFile1), file(fastqFile2) from fastqFilesforFastQC\n\n  output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  script:\n  \"\"\"\n  fastqc -q ${fastqFile1} ${fastqFile2}\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results1, fastqc_results2\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    container 'hadrieng/fastqc'\n\n    input:\n        file reads from trimmed_reads_se.concat(trimmed_reads_pe).collect()\n\n    output:\n        file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n        \"\"\"\n        fastqc -t 4 $reads\n        \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["grbot/nf-core-test/grbot__nf-core-test/fastqc", "coraliegimonnet/ChIP-seq_pipeline/coraliegimonnet__ChIP-seq_pipeline/fastqc", "marchoeppner/kinseq/marchoeppner__kinseq/fastqc", "pilm-bioinformatics/pipelines-nf-circtools/pilm-bioinformatics__pipelines-nf-circtools/fastqc", "nf-core/cookiecutter/nf-core__cookiecutter/fastqc", "nf-core/crisprvar/nf-core__crisprvar/fastqc", "vladsaveliev/umcaw/vladsaveliev__umcaw/fastqc", "grst/nf-core-test/grst__nf-core-test/fastqc", "wslh-bio/dryad/wslh-bio__dryad/fastqc", "propan2one/variantcallerbench/propan2one__variantcallerbench/fastqc", "wslh-bio/spriggan/wslh-bio__spriggan/fastqc", "qbicsoftware-archive/microarray-qc-workflow/qbicsoftware-archive__microarray-qc-workflow/fastqc", "JackCurragh/riboseq_data_processing/JackCurragh__riboseq_data_processing/fastqc_on_raw", "bioatlas/ampliflow/bioatlas__ampliflow/fastqc", "bioatlas/ampliflow/bioatlas__ampliflow/fastqc_multi", "czbiohub/blastlca/czbiohub__blastlca/fastqc", "czbiohub/demux/czbiohub__demux/fastqc", "czbiohub/nf-bowtie/czbiohub__nf-bowtie/fastqc", "maxibor/admapipe/maxibor__admapipe/fastqc", "nf-core/exoseq/nf-core__exoseq/fastqc", "coraliegimonnet/BS-Seq_pipeline/coraliegimonnet__BS-Seq_pipeline/fastqc", "jgallowa07/Nextflow-Example/jgallowa07__Nextflow-Example/quality_control", "czbiohub/nf-core-test/czbiohub__nf-core-test/fastqc", "Jeremy1805/nf-core-influenzangs/Jeremy1805__nf-core-influenzangs/fastqc", "WhalleyT/neoantigen_prediction/WhalleyT__neoantigen_prediction/fastqc", "czbiohub/nf-large-assembly/czbiohub__nf-large-assembly/fastqc", "maxibor/coproID/maxibor__coproID/fastqc", "biocorecrg/BioNextflow/biocorecrg__BioNextflow/fastQC", "biocorecrg/BioNextflow/biocorecrg__BioNextflow/fastQC2", "rbpisupati/nf-haplocaller/rbpisupati__nf-haplocaller/fastqc", "KevinMenden/hybrid-assembly/KevinMenden__hybrid-assembly/fastqc", "jiangfuqing/CRISPR-Cas-off-target-identification/jiangfuqing__CRISPR-Cas-off-target-identification/fastqc", "jiangweiyao/coregenome_align_nf/jiangweiyao__coregenome_align_nf/fastqc", "remiolsen/nf-core-radseq/remiolsen__nf-core-radseq/fastqc", "SherineAwad/nf-core-mytrial/SherineAwad__nf-core-mytrial/fastqc", "jiangweiyao/kraken2_biom_nf/jiangweiyao__kraken2_biom_nf/fastqc", "dalmiaa/Sample_NF/dalmiaa__Sample_NF/fastqc", "jiangweiyao/metaphlan_nf/jiangweiyao__metaphlan_nf/fastqc", "jiangweiyao/quaisar_nf/jiangweiyao__quaisar_nf/fastqc", "biocorecrg/allele_specific_RNAseq/biocorecrg__allele_specific_RNAseq/QConRawReads", "cgpu/sparkling-gatk/cgpu__sparkling-gatk/fastqc", "cgpu/sparkling-preprocessing/cgpu__sparkling-preprocessing/fastqc", "bioinfo-pf-curie/nf-VIF/bioinfo-pf-curie__nf-VIF/fastqc", "steepale/nf-core-mutenrich/steepale__nf-core-mutenrich/fastqc", "steepale/wgsfastqtobam/steepale__wgsfastqtobam/fastqc", "davismcc/nf-hipsci-fibro/davismcc__nf-hipsci-fibro/fastqc", "StaPH-B/mycosnp/StaPH-B__mycosnp/fastqc", "dfornika/nf-core-cpo/dfornika__nf-core-cpo/fastqc", "oisinmccaffrey/clipseq.nextflow/oisinmccaffrey__clipseq.nextflow/FastQC", "FischbachLab/nf-readqc/FischbachLab__nf-readqc/quality_assessment", "TDMedina/Nextflow_test/TDMedina__Nextflow_test/FASTQC", "lifebit-ai/somatic-variant-caller/lifebit-ai__somatic-variant-caller/fastqc", "rnharmening/nf-multipleReferenceMapper/rnharmening__nf-multipleReferenceMapper/fastqc", "alesssia/YAMP/alesssia__YAMP/quality_assessment", "heinzlab/chip-seq-pipeline/heinzlab__chip-seq-pipeline/fastqc", "jlboat/NextflowScripts/jlboat__NextflowScripts/postFastqc", "jlboat/NextflowScripts/jlboat__NextflowScripts/preFastqc", "heinzlab/csrna/heinzlab__csrna/fastqc", "likelet/MesKit/likelet__MesKit/fastqc", "monikaBrandt/nf-core-mynewpipeline/monikaBrandt__nf-core-mynewpipeline/fastqc", "heinzlab/smrna-seq-pipeline/heinzlab__smrna-seq-pipeline/fastqc", "herczegrobert/ownpipeline/herczegrobert__ownpipeline/fastqc", "rwtaylor/nextflow-pipelines/rwtaylor__nextflow-pipelines/FastQC", "BarryDigby/nextflow_bits/BarryDigby__nextflow_bits/FastQC", "BarryDigby/nextflow_bits/BarryDigby__nextflow_bits/FastQC_trim", "BenNolann/rsi_analysis/BenNolann__rsi_analysis/fastqc", "gavinf97/groupnextflow/gavinf97__groupnextflow/fastqc", "UCL-BLIC/nf-ginkgo/UCL-BLIC__nf-ginkgo/fastqc", "UCL-BLIC/rnaseq_variant_calling/UCL-BLIC__rnaseq_variant_calling/fastqc", "kerimoff/qtlquant/kerimoff__qtlquant/fastqc", "UCL-BLIC/wgsalign/UCL-BLIC__wgsalign/fastqc", "genomicsITER/NanoCLUST/genomicsITER__NanoCLUST/fastqc", "anu9109/nf/anu9109__nf/testdocker", "clairecoleman1/clipseq1/clairecoleman1__clipseq1/FastQC", "anwarMZ/nf-upcoast-v/anwarMZ__nf-upcoast-v/fastQC", "drejom/bam2fastq-nf/drejom__bam2fastq-nf/fastqc", "drpatelh/nf-core-sgrnaquant/drpatelh__nf-core-sgrnaquant/fastqc", "UMCUGenetics/NextflowModules/UMCUGenetics__NextflowModules/FastQC", "kevinpryan/rtp_workshop/kevinpryan__rtp_workshop/FASTQC", "Gregor-Mendel-Institute/nf-methylpy/Gregor-Mendel-Institute__nf-methylpy/fastqc", "cometsong/ATAC_Seq_nxf/cometsong__ATAC_Seq_nxf/fastqc", "viktorlj/SarGet/viktorlj__SarGet/RunFastQC", "glormph/nf-core-lfquandenser/glormph__nf-core-lfquandenser/fastqc", "Gustius/omics-analyser/Gustius__omics-analyser/fastqc", "gongyh/nf-core-scgs/gongyh__nf-core-scgs/fastqc", "HadrienG/2018_coral_16S/HadrienG__2018_coral_16S/fastqc", "edgano/TCoffee-NatureProtocol-nf/edgano__TCoffee-NatureProtocol-nf/fastqc", "grbot/nf-core-example/grbot__nf-core-example/fastqc"], "list_wf_names": ["Gregor-Mendel-Institute/nf-methylpy", "czbiohub/nf-core-test", "glormph/nf-core-lfquandenser", "HadrienG/2018_coral_16S", "qbicsoftware-archive/microarray-qc-workflow", "kerimoff/qtlquant", "lifebit-ai/somatic-variant-caller", "genomicsITER/NanoCLUST", "jiangweiyao/coregenome_align_nf", "drpatelh/nf-core-sgrnaquant", "viktorlj/SarGet", "clairecoleman1/clipseq1", "czbiohub/nf-bowtie", "wslh-bio/spriggan", "UCL-BLIC/rnaseq_variant_calling", "anu9109/nf", "coraliegimonnet/BS-Seq_pipeline", "KevinMenden/hybrid-assembly", "gongyh/nf-core-scgs", "dfornika/nf-core-cpo", "UMCUGenetics/NextflowModules", "grbot/nf-core-example", "BarryDigby/nextflow_bits", "nf-core/crisprvar", "grst/nf-core-test", "UCL-BLIC/wgsalign", "czbiohub/nf-large-assembly", "BenNolann/rsi_analysis", "maxibor/coproID", "jiangweiyao/quaisar_nf", "grbot/nf-core-test", "dalmiaa/Sample_NF", "nf-core/cookiecutter", "maxibor/admapipe", "oisinmccaffrey/clipseq.nextflow", "cgpu/sparkling-gatk", "czbiohub/demux", "likelet/MesKit", "steepale/wgsfastqtobam", "cometsong/ATAC_Seq_nxf", "biocorecrg/BioNextflow", "coraliegimonnet/ChIP-seq_pipeline", "biocorecrg/allele_specific_RNAseq", "jiangfuqing/CRISPR-Cas-off-target-identification", "rnharmening/nf-multipleReferenceMapper", "gavinf97/groupnextflow", "heinzlab/chip-seq-pipeline", "heinzlab/csrna", "drejom/bam2fastq-nf", "alesssia/YAMP", "FischbachLab/nf-readqc", "Gustius/omics-analyser", "SherineAwad/nf-core-mytrial", "jgallowa07/Nextflow-Example", "WhalleyT/neoantigen_prediction", "cgpu/sparkling-preprocessing", "StaPH-B/mycosnp", "edgano/TCoffee-NatureProtocol-nf", "bioatlas/ampliflow", "jiangweiyao/metaphlan_nf", "propan2one/variantcallerbench", "wslh-bio/dryad", "nf-core/exoseq", "jiangweiyao/kraken2_biom_nf", "steepale/nf-core-mutenrich", "bioinfo-pf-curie/nf-VIF", "remiolsen/nf-core-radseq", "herczegrobert/ownpipeline", "anwarMZ/nf-upcoast-v", "czbiohub/blastlca", "davismcc/nf-hipsci-fibro", "vladsaveliev/umcaw", "jlboat/NextflowScripts", "pilm-bioinformatics/pipelines-nf-circtools", "JackCurragh/riboseq_data_processing", "monikaBrandt/nf-core-mynewpipeline", "TDMedina/Nextflow_test", "marchoeppner/kinseq", "heinzlab/smrna-seq-pipeline", "Jeremy1805/nf-core-influenzangs", "rwtaylor/nextflow-pipelines", "kevinpryan/rtp_workshop", "UCL-BLIC/nf-ginkgo", "rbpisupati/nf-haplocaller"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["vipr"], "list_contrib": ["ewels", "apeltzer", "maxulysse", "alneberg"], "nb_contrib": 4, "codes": ["\nprocess var_calling {\n    tag { \"Final variant calling for \" + sample_id }\n    publishDir \"${params.outdir}/${sample_id}/\", mode: 'copy'\n\n    input:\n        set sample_id, file(ref_fa), file(bam), file(bai) from final_mapping_for_vcf_ch\n    output:\n        set sample_id, file(ref_fa), file(\"${sample_id}.vcf.gz\") into vcf_ch\n    script:\n        \"\"\"\n        samtools faidx ${ref_fa};\n        lofreq call-parallel --pp-threads ${task.cpus} -f ${ref_fa} \\\n           -d 1000000 --call-indels -o ${sample_id}.vcf.gz ${bam}\n        \"\"\"\n}"], "list_proc": ["nf-core/vipr/nf-core__vipr/var_calling"], "list_wf_names": ["nf-core/vipr"]}, {"nb_reuse": 21, "tools": ["GATK"], "nb_own": 11, "list_own": ["Genomic-Medicine-Linkoping", "chelauk", "rmoran7", "UMCUGenetics", "sripaladugu", "sickle-in-africa", "nf-core", "cgpu", "lifebit-ai", "javaidm", "ryanlayerlab"], "nb_wf": 18, "list_wf": ["haplosarek", "sarek-mirror-cache", "saw.sarek", "sarek_ubec", "PGP-UK-sarek", "layer_lab_chco", "layer_lab_caw", "layer_lab_vc", "germline_somatic", "custom_sarek", "nf-core-sarek", "sarek-mirror", "dx_sarek", "sarek", "GenomeChronicler-Sarek-nf", "test_nextflow_sarek", "sarek-genomechronicler", "pgp-chronek"], "list_contrib": ["alneberg", "FriederikeHanssen", "arontommi", "ewels", "maxulysse", "ggabernet", "skrakau", "BrunoGrandePhD", "pcantalupo", "szilvajuhos", "nf-core-bot", "jfnavarro", "jackmo375", "chelauk", "adrlar", "lconde-ucl", "malinlarsson", "javaidm", "ffmmulder", "rmoran7", "lescai", "cgpu", "apeltzer", "MSBradshaw", "olgabot", "davidmasp"], "nb_contrib": 26, "codes": ["\nprocess MergeMutect2Stats {\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSamplePair, file(\"${idSamplePair}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSamplePair}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2TNStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                                                                                                     \n        tuple idPatient, idSampleTumor, idSampleNormal, file(statsFiles)                         \n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\")\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2SingleStats {\n    label 'cpus_16'\n    tag {idSample}\n\n                                                                                                                              \n\n    input:\n        tuple idPatient, idSample, file(statsFiles)                         \n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.vcf.gz.stats\")\n\n    when: 'mutect2_single' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2TNStats {\n    label 'cpus_16'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                                                                                                     \n        tuple idPatient, idSampleTumor, idSampleNormal, file(statsFiles)                         \n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\")\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSamplePair, file(\"${idSamplePair}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSamplePair}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSamplePair, file(\"${idSamplePair}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSamplePair}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSamplePair, file(\"${idSamplePair}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSamplePair}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2TNStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                                                                                                     \n        tuple idPatient, idSampleTumor, idSampleNormal, file(statsFiles)                         \n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\")\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2SingleStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSample}\n\n                                                                                                                              \n\n    input:\n        tuple idPatient, idSample, file(statsFiles)                         \n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.vcf.gz.stats\")\n\n    when: 'mutect2_single' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2SingleStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSample}\n\n                                                                                                                              \n\n    input:\n        tuple idPatient, idSample, file(statsFiles)                         \n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.vcf.gz.stats\")\n\n    when: 'mutect2_single' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/rmoran7__dx_sarek/MergeMutect2Stats", "cgpu/haplosarek/cgpu__haplosarek/MergeMutect2Stats", "Genomic-Medicine-Linkoping/nf-core-sarek/Genomic-Medicine-Linkoping__nf-core-sarek/MergeMutect2Stats", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/MergeMutect2TNStats", "cgpu/sarek-mirror/cgpu__sarek-mirror/MergeMutect2Stats", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/MergeMutect2SingleStats", "javaidm/layer_lab_vc/javaidm__layer_lab_vc/MergeMutect2TNStats", "sripaladugu/germline_somatic/sripaladugu__germline_somatic/MergeMutect2Stats", "lifebit-ai/GenomeChronicler-Sarek-nf/lifebit-ai__GenomeChronicler-Sarek-nf/MergeMutect2Stats", "cgpu/pgp-chronek/cgpu__pgp-chronek/MergeMutect2Stats", "chelauk/test_nextflow_sarek/chelauk__test_nextflow_sarek/MergeMutect2Stats", "nf-core/sarek/nf-core__sarek/MergeMutect2Stats", "rmoran7/custom_sarek/rmoran7__custom_sarek/MergeMutect2Stats", "cgpu/sarek-genomechronicler/cgpu__sarek-genomechronicler/MergeMutect2Stats", "cgpu/PGP-UK-sarek/cgpu__PGP-UK-sarek/MergeMutect2Stats", "sickle-in-africa/saw.sarek/sickle-in-africa__saw.sarek/MergeMutect2Stats", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/MergeMutect2TNStats", "UMCUGenetics/sarek_ubec/UMCUGenetics__sarek_ubec/MergeMutect2Stats", "ryanlayerlab/layer_lab_caw/ryanlayerlab__layer_lab_caw/MergeMutect2SingleStats", "ryanlayerlab/layer_lab_chco/ryanlayerlab__layer_lab_chco/MergeMutect2SingleStats", "cgpu/sarek-mirror-cache/cgpu__sarek-mirror-cache/MergeMutect2Stats"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "cgpu/pgp-chronek", "UMCUGenetics/sarek_ubec", "cgpu/PGP-UK-sarek", "Genomic-Medicine-Linkoping/nf-core-sarek", "sripaladugu/germline_somatic", "chelauk/test_nextflow_sarek", "nf-core/sarek", "ryanlayerlab/layer_lab_caw", "cgpu/sarek-mirror", "cgpu/sarek-genomechronicler", "cgpu/sarek-mirror-cache", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek", "lifebit-ai/GenomeChronicler-Sarek-nf", "rmoran7/custom_sarek", "cgpu/haplosarek", "javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["TRUmiCount"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["clipseq"], "list_contrib": ["nf-core-bot", "ewels", "amchakra", "charlotte-west", "drpatelh", "CharlotteAnne"], "nb_contrib": 6, "codes": [" process icount_segment {\n            tag \"$gtf\"\n            publishDir \"${params.outdir}/icount\", mode: params.publish_dir_mode\n\n            input:\n            path(gtf) from ch_gtf_icount\n            path(fai) from ch_fai_icount\n\n            output:\n            path(\"icount_${gtf}\") into ch_segment\n\n            script:\n            \"\"\"\n            mkdir tmp\n            export ICOUNT_TMP_ROOT=\\$PWD/tmp\n            iCount segment $gtf icount_${gtf} $fai\n            \"\"\"\n        }"], "list_proc": ["nf-core/clipseq/nf-core__clipseq/icount_segment"], "list_wf_names": ["nf-core/clipseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nf-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["Danilo2771", "ajodeh-juma", "FelixKrueger", "kmurat1", "AntoniaSchuster", "erikrikarddaniel", "stevekm", "avantonder", "lskatz", "jfnavarro", "bunop", "thanhleviet", "FloWuenne", "mjakobs", "Darcy220606", "kojix2", "ljmesi", "candiceh08", "MGordon09", "yocra3", "lescai", "sateeshperi", "piotr-faba-ardigen", "rannick", "SPPearce", "d4straub", "Midnighter", "yuukiiwa", "phue", "FriederikeHanssen", "maxulysse", "sofstam", "antunderwood", "rpetit3", "jfy133", "santiagorevale", "kevbrick", "nebfield", "ntoda03", "emnilsson", "sidorov-si", "njspix", "aleksandrabliznina", "andersgs", "fbdtemme", "jemten", "MillironX", "riederd", "fullama", "kaurravneet4123", "sima-r", "BatoolMM", "alexandregilardet", "heuermh", "ewels", "Mxrcon", "GCJMackenzie", "sruthipsuresh", "hseabolt", "louperelo", "pericsson", "Gwennid", "Jeremy1805", "chris-cheshire", "oschwengers", "i-pletenev", "spficklin", "subwaystation", "annacprice", "jinmingda", "drpatelh", "fmalmeida", "RHReynolds", "Emiller88", "sysbiocoder", "arontommi", "ggabernet", "mjcipriano", "Erkison", "bjohnnyd", "grst", "sguizard", "tamara-hodgetts", "lassefolkersen", "nickhsmith", "c-mertes", "abhi18av", "pditommaso", "muffato", "projectoriented", "praveenraj2018", "tamuanand", "charles-plessy", "mashehu", "jianhong", "Mark-S-Hill", "suzannejin", "klkeys", "KevinMenden", "mahesh-panchal", "JoseEspinosa", "apeltzer", "ramprasadn", "SusiJo", "maxibor"], "nb_contrib": 105, "codes": ["process BWAMETH_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwameth=0.2.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwameth:0.2.2--py_1' :\n        'quay.io/biocontainers/bwameth:0.2.2--py_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ${index} -name \"*.bwameth.c2t\" | sed 's/.bwameth.c2t//'`\n\n    # Modify the timestamps so that bwameth doesn't complain about building the index\n    # See https://github.com/nf-core/methylseq/pull/217\n    touch -c -- *\n\n    bwameth.py \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        --reference \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $args2 -@ $task.cpus -bhS -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwameth: \\$(echo \\$(bwameth.py --version 2>&1) | cut -f2 -d\" \")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["nf-core/modules/nf-core__modules/BWAMETH_ALIGN"], "list_wf_names": ["nf-core/modules"]}]