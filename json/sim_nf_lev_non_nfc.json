[{"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess extract_pop {\n    tag \"extract_pop_vcf_${target_name}_${chrm}_${ref_name}\"\n                                                                                                                                         \n    label \"bigmem\"\n\n    input:\n        tuple val(chrm), val(target_name), val(ref_name), val(tagName), file(vcf), file(vcf_tbi), file(sample_to_extract), val(prefix)\n    output:\n        tuple val(chrm), val(target_name), val(ref_name), val(tagName), file(vcf_out), file(\"${vcf_out}.tbi\")\n    script:\n        base = file(vcf.baseName).baseName\n        vcf_out = \"${base}_${prefix}.vcf.gz\"\n        \"\"\"\n        bcftools \\\n            view \\\n             \\\n            --force-samples \\\n            --samples-file ${sample_to_extract} \\\n            ${vcf} \\\n            -Oz -o ${vcf_out} \n        tabix ${vcf_out} \n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/extract_pop"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess get_chromosome {\n    tag \"get_chromosome_${dataset}\"\n    label \"bigmem1\"\n\n    input:\n        tuple val(dataset), file(dataset_vcf), file(\"${dataset_vcf}.tbi\")\n    output:\n        tuple val(dataset), file(dataset_vcf), file(\"${dataset_vcf}.tbi\"), file(chrom_file)\n    script:\n        base = file(dataset_vcf.baseName).baseName\n        chrom_file = \"${base}_chromosomes.txt\"\n        \"\"\"\n        bcftools query -f '%CHROM\\\\t%POS\\\\n' ${dataset_vcf} >  ${chrom_file}\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/get_chromosome"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools", "SnpSift"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess split_vcf_chromosome {\n    tag \"split_vcf_chrm_${dataset}\"\n    label \"bigmem1\"\n\n    input:\n        tuple val(dataset), file(dataset_vcf)\n    output:\n        tuple val(dataset), file(\"${base}*.vcf.gz\")\n    script:\n        base = file(dataset_vcf.baseName).baseName\n        \"\"\"\n        bcftools view ${dataset_vcf} -Ou -o ${dataset_vcf.baseName}\n        SnpSift split ${dataset_vcf.baseName}\n        N=1\n        for FILE in ${base}.*.vcf\n        do\n            bcftools view \\${FILE} -Oz -o ${base}.\\$N.vcf.gz\n            rm \\${FILE}\n            N=\\$((N+1))\n        done\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/split_vcf_chromosome"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["SnpSift", "BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess split_vcf_chunk {\n    tag \"split_vcf_${dataset}_${chunk_size}\"\n    label \"bigmem1\"\n\n    input:\n        tuple val(dataset), file(dataset_vcf), val(chunk_size)\n    output:\n        tuple val(dataset), file(\"${base}*.vcf.gz\")\n    script:\n        base = file(dataset_vcf.baseName).baseName\n        \"\"\"\n        SnpSift split -l ${chunk_size} ${dataset_vcf}\n        N=1\n        for FILE in ${base}.*.vcf\n        do\n            bcftools view \\${FILE} -Oz -o ${base}.\\$N.vcf.gz\n            rm \\${FILE}\n            N=\\$((N+1))\n        done\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/split_vcf_chunk"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess qc_dupl {\n    tag \"dupl_qc_${dataset}_${chrm}_${start}_${end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), file(dataset_vcf)\n    output:\n        tuple val(dataset),val(chrm), val(start), val(end), file(dataset_qc_vcf)\n    script:\n        base = file(dataset_vcf.baseName).baseName\n        dataset_qc_vcf = \"${base}_${chrm}_${start}_${end}_qc.bcf\"\n        \"\"\"\n        tabix -f ${dataset_vcf}\n        bcftools norm  --rm-dup both ${dataset_vcf} -Ob -o ${dataset_qc_vcf}\n        tabix -f ${dataset_qc_vcf}\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/qc_dupl"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 2, "tools": ["BCFtools", "MultiQC"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 2, "list_wf": ["h3arefgraph", "chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "cjfields", "ameintjes", "jambler24"], "nb_contrib": 4, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess split_multi_allelic {\n    tag \"split_multi_${dataset}_${chrm}_${start}_${end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf)\n    output:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf_out)\n    script:\n        base = file(vcf.baseName).baseName\n        vcf_out = \"${base}.norm.bcf\"\n        \"\"\"\n        bcftools norm -m- ${vcf} -Ob -o ${vcf_out}\n        tabix -f ${vcf_out}\n        \"\"\"\n}"], "list_proc": ["h3abionet/h3arefgraph/multiqc", "h3abionet/chipimputation_evaluate_chips/split_multi_allelic"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips", "h3abionet/h3arefgraph"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess fill_tags_vcf {\n    tag \"fill_tags_${dataset}_${chrm}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf)\n    output:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf_out)\n    script:\n        vcf_out = \"${vcf.baseName}_af.bcf\"\n        \"\"\"\n        bcftools +fill-tags ${vcf}  -Ob -o ${vcf_out}\n        tabix -f ${vcf_out}\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/fill_tags_vcf"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 2, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 2, "list_wf": ["chipimputation_evaluate_chips", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess filter_min_ac {\n    tag \"min_ac_${dataset}_${chrm}_${start}_${end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf), val(params)\n\n    output:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf_out)\n\n    script:\n        base = file(vcf.baseName).baseName\n        vcf_out = \"${base}_ac.bcf\"\n        \"\"\"\n        bcftools view ${params} ${vcf} -Ob -o ${vcf_out}\n        tabix -f ${vcf_out}\n        \"\"\"\n }", "\nprocess fill_tags_VCF {\n    tag \"fill_tags_${dataset}_${chrm}\"\n    label \"hugemem\"\n\n    input:\n        tuple val(dataset), file(vcf), file(sample), val(chrm)\n    output:\n        tuple val(dataset), file(out_vcf), file(sample), val(chrm)\n    script:\n        base = file(vcf.baseName).baseName\n        out_vcf = \"${base}_AF.vcf.gz\"\n        \"\"\"\n        tabix -f ${vcf}\n        bcftools annotate --set-id '%CHROM\\\\_%POS\\\\_%REF\\\\_%ALT' ${vcf} | \\\n        bcftools +fill-tags -Oz -o ${out_vcf}\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/filter_min_ac", "h3abionet/popfreqs/fill_tags_VCF"], "list_wf_names": ["h3abionet/popfreqs", "h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess filter_f_missing {\n    tag \"min_ac_${dataset}_${chrm}_${start}_${end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf), val(params)\n\n    output:\n        tuple val(dataset), val(chrm), val(start), val(end), file(vcf_out)\n\n    script:\n        base = file(vcf.baseName).baseName\n        vcf_out = \"${base}_f-missing.bcf\"\n        \"\"\"\n        bcftools filter -i 'F_MISSING < 0.05' --threads ${task.cpus} ${vcf} -Ob -o ${vcf_out}\n        tabix -f ${vcf_out}\n        \"\"\"\n }"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/filter_f_missing"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess qc_site_missingness {\n    tag \"site_missingness_${target_name}_${chrm}:${chunk_start}-${chunk_end}_${ref_name}_${tagName}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(chrm), val(chunk_start), val(chunk_end), val(target_name), val(ref_name), file(impute_vcf), file(imputed_info), val(tagName), val(qc_params)\n    output:\n        tuple val(chrm), val(chunk_start), val(chunk_end), val(target_name), val(ref_name), file(impute_vcf_qc), file(imputed_info), val(tagName)\n    script:\n        base = file(impute_vcf.baseName).baseName\n        impute_vcf_qc = \"${base}_qc.vcf.gz\"\n        \"\"\"\n        vcftools --gzvcf ${impute_vcf} --keep-INFO-all ${qc_params} --recode --stdout | bgzip > ${impute_vcf_qc}\n        tabix ${impute_vcf_qc}\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/qc_site_missingness"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess sites_only {\n    tag \"sites_only_${target_name}_${chrm}:${chunk_start}-${chunk_end}_${ref_name}_${tagName}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(chrm), val(chunk_start), val(chunk_end), val(target_name), val(ref_name), file(impute_vcf_qc), file(imputed_info), val(tagName)\n\n    output:\n        tuple val(chrm), val(chunk_start), val(chunk_end), val(target_name), val(ref_name), file(sites_vcf), file(imputed_info), val(tagName)\n\n    script:\n        base = base = file(impute_vcf_qc.baseName).baseName\n        sites_vcf = \"${base}_sites.vcf.gz\"\n        \"\"\"\n        tabix ${impute_vcf_qc}\n        bcftools view ${impute_vcf_qc} --drop-genotypes  -Oz -o ${sites_vcf}\n        tabix ${sites_vcf}\n        \"\"\"\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/sites_only"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess combine_vcfs_chrm {\n   tag \"combine_${chrm}_${target_name}_${ref_name}_${tagName}\"\n   publishDir \"${params.outDir}/imputed/vcfs/${ref_name}/all/${target_name}/${tagName}\", overwrite: true, mode:'copy', pattern: '*vcf.gz*'\n   label \"bigmem\"\n   \n   input:\n       tuple val(chrm), val(target_name), val(ref_name), val(tagName), val(vcfs)\n   output:\n       tuple val(chrm), val(target_name), val(ref_name), val(tagName), file(vcf_out), file(\"${vcf_out}.tbi\")\n   script:\n       vcf_out = \"${target_name}_${ref_name}_${tagName}_chr${chrm}.vcf.gz\"\n       if(vcfs.size() > 1){\n            \"\"\"\n            bcftools concat ${vcfs.join(' ')} --allow-overlaps  | \\\n            bcftools sort -T .  -Oz -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n       }\n       else if(vcfs.size() == 1){\n            \"\"\"\n            cp ${vcfs.join(' ')} ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n       }\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/combine_vcfs_chrm"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["chipimputation_evaluate_chips"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess combine_vcfs {\n   tag \"combine_${dataset}_${ref_name}\"\n   publishDir \"${params.outdir}/${dataset}/vcfs\", overwrite: true, mode:'copy'\n   label \"bigmem\"\n   \n   input:\n       tuple val(dataset), val(ref_name), val(vcfs)\n   output:\n       tuple val(dataset), val(ref_name), file(vcf_out), file(\"${vcf_out}.tbi\")\n   script:\n       vcf_out = \"${dataset}_${ref_name}_phased.vcf.gz\"\n       if(vcfs.size() > 1){\n            \"\"\"\n            bcftools concat ${vcfs.join(' ')} --allow-overlaps  | \\\n            bcftools sort -T .  -Oz -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n       }\n       else if(vcfs.size() == 1){\n            \"\"\"\n            cp ${vcfs.join(' ')} ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n       }\n}"], "list_proc": ["h3abionet/chipimputation_evaluate_chips/combine_vcfs"], "list_wf_names": ["h3abionet/chipimputation_evaluate_chips"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["node-accreditation"], "list_contrib": ["cjfields"], "nb_contrib": 1, "codes": ["\nprocess sampleSeq {\n    tag \"Sample-${name}\"\n    publishDir \"${params.outdir}/Subsample\", mode: 'copy'\n    \n    input:\n    set val(name), file(reads) from ch_raw_reads_seqtk\n\n    output:\n    set val(name), file(\"*.fastq.gz\") into md5\n\n    script:\n    \"\"\"\n    seqtk sample -s 12345 ${reads[0]} 0.9 | pigz -p ${task.cpus} - > $name.12345.R1.fastq.gz\n    seqtk sample -s 12345 ${reads[1]} 0.9 | pigz -p ${task.cpus} - > $name.12345.R2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["h3abionet/node-accreditation/sampleSeq"], "list_wf_names": ["h3abionet/node-accreditation"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess get_chip_site_from_vcf {\n    tag \"get_chip_site_${chip_name}_${dataset}_${chrm}\"\n    publishDir \"${params.outdir}/chip_site\", mode: 'copy'\n    label \"bigmem\"\n\n    input:\n        tuple val(chip_name), file(chip_file), val(dataset), file(dataset_vcf), file(dataset_sample), val(chrm) from datasets_all\n\n    output:\n        tuple val(dataset), file(dataset_vcf), file(dataset_sample), val(chip_name), file(dataset_vcf_chip), val(chrm) into get_chip_site_from_vcf\n\n    script:\n    dataset_vcf_chip = \"${dataset}_${chip_name}_${chrm}.vcf.gz\"\n    \"\"\"\n    tabix -f ${dataset_vcf}\n    bcftools view --regions-file ${chip_file} ${dataset_vcf} -Oz -o ${dataset}_${chip_name}_tmp.vcf.gz\n    bcftools sort ${dataset}_${chip_name}_tmp.vcf.gz -T . -Oz -o ${dataset_vcf_chip}\n    rm -f ${dataset}_${chip_name}_tmp*.vcf.gz\n    \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/get_chip_site_from_vcf"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess split_population {\n    tag \"split_pop_${dataset}_${pop}_${chrm}\"\n    label \"bigmem\"\n                             \n\n    input:\n        tuple val(dataset), val(pop), file(dataset_vcf), file(dataset_sample), val(chrm)\n\n    output:\n        tuple val(pop), val(dataset), file(pop_vcf), file(dataset_vcf), val(chrm)\n\n    script:\n        pop_vcf = \"${pop}_${dataset}_${chrm}.vcf.gz\"\n        \"\"\"\n        awk '\\$2==\"${pop}\" {print \\$1}' ${dataset_sample} > ${pop}.samples\n        ## Keep only samples for population and Recalculate AC, AN, AF\n        bcftools view --samples-file ${pop}.samples --force-samples ${dataset_vcf} | \\\n        bcftools +fill-tags -- -t AC,AN,AF,MAF | \\\n        bcftools annotate --set-id '%CHROM\\\\_%POS\\\\_%REF\\\\_%ALT' | \\\n        bcftools view --drop-genotypes -Oz -o ${pop_vcf}\n        \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/split_population"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess get_bed_vcf {\n    tag \"get_bed_${name}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(name), file(bed_file), file(vcf)\n\n    output:\n        tuple val(name), file(bed_vcf)\n\n    script:\n        bed_vcf = \"${name}.vcf.gz\"\n        \"\"\"\n        tabix ${vcf}\n        bcftools view --regions-file ${bed_file} ${vcf} | \\\n        bcftools sort -T . -Oz -o ${bed_vcf}\n        \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/get_bed_vcf"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess concat_vcf_chrms {\n   tag \"concat_${name}\"\n   label \"bigmem\"\n                                                           \n   \n   input:\n        tuple val(name), val(vcfs)\n   output:\n        tuple val(name), file(vcf_out)\n   script:\n        vcf_out = \"${name}.bcf\"\n        if(vcfs.size() > 1){\n            \"\"\"\n            bcftools concat ${vcfs.join(' ')} |\\\n            bcftools sort -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n        else{\n            \"\"\"\n            bcftools sort ${vcfs.join(' ')} -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n}"], "list_proc": ["h3abionet/popfreqs/concat_vcf_chrms"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess extract_fields1 {\n   tag \"extract_fields_${name}\"\n   label \"bigmem\"\n   publishDir \"${params.outdir}/${prefix}\", mode: 'copy'\n   \n   input:\n        tuple val(name), val(vcf)\n   output:\n        tuple val(name), file(csv)\n   script:\n        csv = \"${name}.csv\"\n        \"\"\"\n        echo -e 'ID\\\\tCHROM\\\\tPOS\\\\tREF\\\\tALT\\\\tgnomad_b38_AF\\\\tgnomad_b38_AC\\\\tgnomad_b38_AN' > ${csv}\n        bcftools query \\\n            -f '%ID\\\\t%CHROM\\\\t%POS\\\\t%REF\\\\t%ALT\\\\t%INFO/gnomad_b38_AF\\\\t%INFO/gnomad_b38_AC\\\\t%INFO/gnomad_b38_AN\\\\n' \\\n            ${vcf} >> ${csv}\n        \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/extract_fields1"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools", "SnpSift"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess extract_fields {\n   tag \"extract_fields_${name}\"\n   label \"bigmem\"\n   publishDir \"${params.outdir}\", mode: 'copy'\n   \n   input:\n        tuple val(name), val(vcf), val(fields)\n   output:\n        tuple val(name), file(csv)\n   script:\n        csv = \"${name}.csv\"\n        \"\"\"\n        echo -e '${fields}' > ${csv}\n        bcftools view ${vcf} |\\\n        snpsift \\\n            extractFields \\\n            - -e \".\" ${fields} \\\n            > ${csv}\n        \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/extract_fields"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya", "ameintjes"], "nb_contrib": 2, "codes": ["\nprocess get_maf {\n    tag \"get_maf_${pop}_${dataset}_${chrm}_${header}\"\n                                                                                                                      \n    label \"bigmem\"\n\n    input:\n        tuple val(pop), val(dataset), file(pop_vcf), val(chrm), val(info_field)\n\n    output:\n        tuple val(pop), val(dataset), file(pop_maf), val(chrm)\n\n    script:\n                                                                                                                                 \n        header = info_field.replaceAll('INFO/','')                                      \n                                                                                                     \n        pop_maf = \"${pop}_${dataset}_${chrm}_${header}.csv\"\n        \"\"\"\n        ## Compute frequency\n        echo \"CHROM\\tPOS\\tPOS\\tID\\t${pop}_${header}\" > ${pop_maf}\n        bcftools query -f '%CHROM\\\\t%POS\\\\t%POS\\\\t%CHROM\\\\_%POS\\\\_%REF\\\\_%ALT\\\\t%${info_field}\\\\n' ${pop_vcf} >> ${pop_maf}\n        \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/get_maf"], "list_wf_names": ["h3abionet/popfreqs"]}, {"nb_reuse": 2, "tools": ["FastQC", "BCFtools", "MultiQC"], "nb_own": 2, "list_own": ["h3abionet", "marchoeppner"], "nb_wf": 2, "list_wf": ["popfreqs", "trinoflow"], "list_contrib": ["mamanambiya", "ameintjes", "marchoeppner"], "nb_contrib": 3, "codes": ["\nprocess sites_only {\n    tag \"sites_only_${dataset}_${chrm}:${start}-${end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), val(chip), file(vcf)\n\n    output:\n        tuple val(dataset), val(chrm), val(start), val(end), val(chip), file(sites_vcf)\n\n    script:\n        sites_vcf = \"${vcf.getSimpleName()}_sites.bcf\"\n        \"\"\"\n        tabix ${vcf}\n        bcftools view ${vcf} --drop-genotypes --threads ${task.cpus} -Ob -o ${sites_vcf}\n        tabix ${sites_vcf}\n        \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/sites_only", "marchoeppner/trinoflow/get_software_versions"], "list_wf_names": ["h3abionet/popfreqs", "marchoeppner/trinoflow"]}, {"nb_reuse": 2, "tools": ["BCFtools", "MultiQC"], "nb_own": 2, "list_own": ["h3abionet", "marchoeppner"], "nb_wf": 2, "list_wf": ["popfreqs", "trinoflow"], "list_contrib": ["mamanambiya", "ameintjes", "marchoeppner"], "nb_contrib": 3, "codes": ["\nprocess merge_datasets {\n   tag \"merge_datasets_${dataset}_${chrm}_${start}_${end}\"\n   label \"bigmem\"\n   \n   input:\n        tuple val(datasets), val(chrm), val(start), val(end), val(chips), val(vcfs)\n   output:\n        tuple val(dataset), val(chrm), val(start), val(end), val(chip), file(vcf_out)\n   script:\n        dataset = datasets.sort().join('-')\n        chip = chips.sort().join('-')\n        vcf_out = \"${dataset}_${chrm}_${start}_${end}.bcf\"\n        if(vcfs.size() > 1){\n            \"\"\"\n            bcftools merge ${vcfs.join(' ')} |\\\n            bcftools sort -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n        else{\n            \"\"\"\n            bcftools sort ${vcfs.join(' ')} -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/merge_datasets", "marchoeppner/trinoflow/multiqc"], "list_wf_names": ["h3abionet/popfreqs", "marchoeppner/trinoflow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools", "BWA"], "nb_own": 2, "list_own": ["h3abionet", "marcocrotti"], "nb_wf": 2, "list_wf": ["radseq-processing-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcocrotti"], "nb_contrib": 3, "codes": ["\nprocess concat_chrms {\n   tag \"concat_${dataset}_${chrm}_${prefix}\"\n   label \"bigmem\"\n   publishDir \"${params.outdir}/${prefix}\", mode: 'copy'\n                           \n                  \n   \n   input:\n        tuple val(dataset), val(chrm), val(vcfs), val(prefix)\n   output:\n        tuple val(dataset), val(chrm), file(vcf_out)\n   script:\n        vcf_out = \"${prefix}_${dataset}_${chrm}.bcf\"\n                        \n        if(vcfs.size() > 1){\n            \"\"\"\n            bcftools concat ${vcfs.join(' ')} |\\\n            bcftools sort -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n        else{\n            \"\"\"\n            bcftools sort ${vcfs.join(' ')} -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n}", "\nprocess alignment {\t\n\n\ttag \"$genomeName\"\n\t\n\tpublishDir params.resultsAlign, mode: params.saveMode\n\t\n\t\n\tinput:\t\n\ttuple val(genomeName), file(genomeReads) from trim_paired_out_ch\n        file index from genome_index_ch.first()\n    \tfile genome\n    \n\toutput:\n\tfile(\"${genomeName}.bam\") into aligned_ch\n\t\n\tscript:\n\t\t\n\t\"\"\"\t\n\tbwa mem ${genome} ${genomeReads[0]} ${genomeReads[1]} | samtools view -bSq 20 | samtools sort -O bam > ${genomeName}.bam\n\t\"\"\"\n\n}"], "list_proc": ["h3abionet/popfreqs/concat_chrms", "marcocrotti/radseq-processing-nf/alignment"], "list_wf_names": ["h3abionet/popfreqs", "marcocrotti/radseq-processing-nf"]}, {"nb_reuse": 2, "tools": ["Salmon", "BCFtools"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["demo-shpc-nf", "popfreqs"], "list_contrib": ["vsoch", "ameintjes", "mamanambiya", "marcodelapierre"], "nb_contrib": 4, "codes": ["\nprocess quant {\n    tag \"$pair_id\"\n\n    input:\n    file index from index_ch\n    set pair_id, file(reads) from read_pairs_ch\n\n    output:\n    file(pair_id) into quant_ch\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}", "\nprocess merge_groups {\n    tag \"merge_groups_${dataset}_${prefix}\"\n    label \"largemem\"\n                                                             \n\n    input:\n        tuple val(dataset), val(chrms), val(vcfs), val(prefix)\n    output:\n        tuple val(dataset), file(vcf_out)\n    script:\n        vcf_out = \"${prefix}.vcf.gz\"\n        if(vcfs.size() > 1){\n            \"\"\"\n            bcftools merge ${vcfs.join(' ')} |\\\n            bcftools sort -T . -Ob -o ${vcf_out}\n            \"\"\"\n        }\n        else{\n            \"\"\"\n            bcftools sort ${vcfs.join(' ')} -T . -Ob -o ${vcf_out}\n            \"\"\"\n        }\n        \n}"], "list_proc": ["marcodelapierre/demo-shpc-nf/quant", "h3abionet/popfreqs/merge_groups"], "list_wf_names": ["marcodelapierre/demo-shpc-nf", "h3abionet/popfreqs"]}, {"nb_reuse": 2, "tools": ["BCFtools", "MultiQC"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["demo-shpc-nf", "popfreqs"], "list_contrib": ["vsoch", "ameintjes", "mamanambiya", "marcodelapierre"], "nb_contrib": 4, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n    \n    input:\n    file('data*/*') from quant_ch.mix(fastqc_ch).collect()\n    file(config) from multiqc_file\n\n    output:\n    file('multiqc_report.html') optional true\n\n    script:\n    \"\"\"\n    cp $config/* .\n    echo \"custom_logo: \\$PWD/logo.png\" >> multiqc_config.yaml\n    multiqc -v .\n    \"\"\"\n}", "\nprocess get_map {\n    tag \"get_map_${dataset}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), file(dataset_vcf)\n    output:\n        tuple val(dataset), file(dataset_vcf), file(dataset_map)\n    script:\n        base = file(dataset_vcf.baseName).baseName\n        dataset_map = \"${base}.map\"\n        \"\"\"\n        bcftools query -f '%CHROM\\\\t%POS\\\\n' ${dataset_vcf} > ${dataset_map}\n        \"\"\"\n}"], "list_proc": ["marcodelapierre/demo-shpc-nf/multiqc", "h3abionet/popfreqs/get_map"], "list_wf_names": ["marcodelapierre/demo-shpc-nf", "h3abionet/popfreqs"]}, {"nb_reuse": 4, "tools": ["BCFtools", "SAMtools", "DEPTH", "FastQC", "GATK"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 3, "list_wf": ["recalling", "illumina-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "grbot", "marcodelapierre"], "nb_contrib": 4, "codes": ["\nprocess run_genotype_gvcf_on_genes {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rGGoG\" }\n    label \"bigmem\"\n    publishDir \"${params.out_dir}/\", mode: 'copy', overwrite: false\n    input:\n        set chr, file(gvcf_file), file(bed_file) from split_bed_to_chr\n    output:\n        set chr, file(vcf_out), file(vcf_index_out) into vcf\n    script:\n         call_conf = 30               \n         if ( params.sample_coverage == \"high\" )\n           call_conf = 30\n         else if ( params.sample_coverage == \"low\" )\n           call_conf = 10\n        base = file(file(gvcf_file.baseName).baseName).baseName\n        vcf_out = \"${base}_chr${chr}_genes.vcf.gz\"\n        vcf_index_out = \"${base}_chr${chr}_genes.vcf.gz.tbi\"\n        \"\"\"\n        ${params.tabix_base}/tabix ${gvcf_file}\n        ${params.gatk_base}/gatk \\\n            GenotypeGVCFs \\\n            -R ${params.ref_seq} \\\n            -L ${bed_file} \\\n            -V ${gvcf_file} \\\n            -stand-call-conf ${call_conf} \\\n            -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n            -O ${vcf_out}\n        \"\"\"\n}", "\nprocess get_vcf_site {\n    tag \"get_vcf_site_${dataset}\"\n    label \"bigmem\"\n\n    input:\n        tuple dataset, file(vcf)\n    \n    output:\n        tuple dataset, file(vcf), file(vcf_sites)\n    \n    script:\n        base = file(vcf.baseName).baseName\n        vcf_sites = \"${base}.sites\"\n        \"\"\"\n        echo -e 'ID' > ${vcf_sites}\n        bcftools query -f \"%CHROM\\\\_%POS\\\\_%REF\\\\_%ALT\\\\n\" ${vcf} >> ${vcf_sites}\n        \"\"\"\n}", "\nprocess sam_post_map_contigs {\n  tag \"${dir}/${name}\"\n  publishDir \"${dir}/${params.outprefix}${name}\", mode: 'copy'\n\n  input:\n  tuple val(dir), val(name), path('mapped_contigs_sub_unsorted.sam')\n\n  output:\n  tuple val(dir), val(name), path('mapped_contigs_sub.bam'), path('mapped_contigs_sub.bam.bai'), emit: bam\n  tuple val(dir), val(name), path('depth_contigs_sub.dat'), emit: depth\n\n  script:\n  \"\"\"\n  samtools \\\n    view -b -o mapped_contigs_sub_unsorted.bam \\\n    mapped_contigs_sub_unsorted.sam\n\n  samtools \\\n    sort -o mapped_contigs_sub.bam \\\n    mapped_contigs_sub_unsorted.bam\n\n  samtools \\\n    index mapped_contigs_sub.bam\n\n  samtools \\\n    depth -aa mapped_contigs_sub.bam \\\n    >depth_contigs_sub.dat\n  \"\"\"\n}", "\nprocess qc_post_trim {\n  tag \"${dir}/${name}\"\n  publishDir \"${dir}/${params.outprefix}${name}\", mode: 'copy'\n\n  input:\n  tuple val(dir), val(name), path('clean.fastq.gz')\n\n  output:\n  tuple val(dir), val(name), path('clean_fastqc.html'), path('clean_fastqc.zip')\n\n  script:\n  \"\"\"\n  fastqc clean.fastq.gz\n  \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_genotype_gvcf_on_genes", "h3abionet/popfreqs/get_vcf_site", "marcodelapierre/illumina-nf/sam_post_map_contigs", "marcodelapierre/illumina-nf/qc_post_trim"], "list_wf_names": ["h3abionet/popfreqs", "marcodelapierre/illumina-nf", "h3abionet/recalling"]}, {"nb_reuse": 3, "tools": ["BCFtools", "SAMtools", "mpileup", "DEPTH", "Consensus"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["illumina-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcodelapierre"], "nb_contrib": 3, "codes": ["\nprocess sam_post_map_contigs {\n  tag \"${dir}/${name}\"\n  publishDir \"${dir}/${params.outprefix}${name}\", mode: 'copy'\n\n  input:\n  tuple val(dir), val(name), path('mapped_contigs_sub_unsorted.sam')\n\n  output:\n  tuple val(dir), val(name), path('mapped_contigs_sub.bam'), path('mapped_contigs_sub.bam.bai'), emit: bam\n  tuple val(dir), val(name), path('depth_contigs_sub.dat'), emit: depth\n\n  script:\n  \"\"\"\n  samtools \\\n    view -b -o mapped_contigs_sub_unsorted.bam \\\n    mapped_contigs_sub_unsorted.sam\n\n  samtools \\\n    sort -o mapped_contigs_sub.bam \\\n    mapped_contigs_sub_unsorted.bam\n\n  samtools \\\n    index mapped_contigs_sub.bam\n\n  samtools \\\n    depth -aa mapped_contigs_sub.bam \\\n    >depth_contigs_sub.dat\n  \"\"\"\n}", "\nprocess get_vcf_site1 {\n    tag \"get_vcf_site_${dataset}\"\n    label \"bigmem\"\n\n    input:\n        tuple dataset, file(vcf)\n    \n    output:\n        tuple dataset, file(vcf), file(vcf_sites)\n    \n    script:\n        base = file(vcf.baseName).baseName\n        vcf_sites = \"${base}.sites\"\n        \"\"\"\n        echo -e 'ID' > ${vcf_sites}\n        bcftools query -f \"%CHROM\\\\_%POS\\\\_%REF\\\\_%ALT\\\\n\" ${vcf} >> ${vcf_sites}\n        \"\"\"\n}", "\nprocess bcf_post_map_contigs {\n  tag \"${dir}/${name}\"\n  publishDir \"${dir}/${params.outprefix}${name}\", mode: 'copy'\n\n  input:\n  tuple val(dir), val(name), path('mapped_contigs_sub.bam'), path('mapped_contigs_sub.bam.bai'), path('contigs_sub.fasta')\n\n  output:\n  tuple val(dir), val(name), path('calls_contigs_sub.vcf.gz'), emit: call\n  tuple val(dir), val(name), path('consensus_contigs_sub.fasta'), emit: cons\n\n  script:\n  \"\"\"\n  bcftools \\\n    mpileup -Ou -f contigs_sub.fasta \\\n    mapped_contigs_sub.bam \\\n    | bcftools \\\n    call --ploidy 1 -mv -Oz \\\n    -o calls_contigs_sub.vcf.gz\n\n  bcftools \\\n    tabix calls_contigs_sub.vcf.gz\n\n  bcftools \\\n    consensus -f contigs_sub.fasta \\\n    -o consensus_contigs_sub.fasta \\\n    calls_contigs_sub.vcf.gz\n  \"\"\"\n}"], "list_proc": ["marcodelapierre/illumina-nf/sam_post_map_contigs", "h3abionet/popfreqs/get_vcf_site1", "marcodelapierre/illumina-nf/bcf_post_map_contigs"], "list_wf_names": ["h3abionet/popfreqs", "marcodelapierre/illumina-nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools", "SnpSift"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["illumina-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcodelapierre"], "nb_contrib": 3, "codes": ["\nprocess sam_post_seqfile {\n  tag \"${seqid}\"\n  publishDir \"${params.refdir}\", mode: 'copy', saveAs: { filename -> \"refseq_${seqid}.fasta\" }\n\n  input:\n  tuple val(order), val(seqid), path('refseq.fasta')\n\n  output:\n  tuple val(order), val(seqid), path('refseq.fasta')\n\n  script:\n  \"\"\"\n  seqid=\"${seqid}\"\n  if [ \"\\${seqid: -3}\" == \"_RC\" ] ; then\n    samtools faidx \\\n      -i -o refseq_revcom.fasta \\\n      refseq.fasta \\${seqid%_RC}\n\n    mv refseq_revcom.fasta refseq.fasta\n\n    sed -i '/^>/ s/ .*//g' refseq.fasta\n  fi\n  \"\"\"\n}", "\nprocess get_vcf_site2 {\n    tag \"get_vcf_site_${dataset}\"\n    publishDir \"${params.outdir}/sites\", mode: 'copy', pattern: \"*.sites\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), file(vcf)\n    \n    output:\n        tuple val(dataset), file(vcf), file(vcf_sites)\n    \n    script:\n        base = file(vcf.baseName).baseName\n        vcf_sites = \"${base}_with_gene.sites\"\n        \"\"\"\n        bcftools view ${vcf} |\\\n        SnpSift -Xmx${task.memory.toGiga()}g \\\n            extractFields \\\n            - -e \".\" CHROM POS ANN[0].GENE ANN[0].EFFECT \\\n            > ${vcf_sites}\n        \"\"\"\n}"], "list_proc": ["marcodelapierre/illumina-nf/sam_post_seqfile", "h3abionet/popfreqs/get_vcf_site2"], "list_wf_names": ["h3abionet/popfreqs", "marcodelapierre/illumina-nf"]}, {"nb_reuse": 2, "tools": ["Consensus", "BCFtools", "mpileup"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["illumina-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcodelapierre"], "nb_contrib": 3, "codes": ["\nprocess annotate_mafs {\n    tag \"mafs_${dataset_name}_${mafs_dataset}\"\n    label \"bigmem\"\n    \n    input:\n        tuple dataset_name, file(dataset_vcf), mafs_dataset, file(maf_annot), file(header_annot)\n    \n    output:\n        tuple dataset_name, file(outVCF)\n    \n    script:\n        base = \"${dataset_name}_${maf_annot.baseName}\"\n        outVCF = \"${base}.vcf.gz\"\n        \"\"\"\n        columns=\\$(awk 'NR==1{print \\$0}' ${maf_annot} | sed -e 's/\\\\s\\\\+/,/g')\n        tail -n+2 ${maf_annot} | sort -k1,1n -k2,2n -V > ${maf_annot}.annot.tsv\n        bgzip ${maf_annot}.annot.tsv\n        tabix -s1 -b2 -e3 ${maf_annot}.annot.tsv.gz\n        bcftools sort ${dataset_vcf} -T . -Oz -o ${dataset_vcf}.sorted.vcf.gz\n        tabix ${dataset_vcf}.sorted.vcf.gz\n        bcftools annotate -a ${maf_annot}.annot.tsv.gz -h ${header_annot} -c \\${columns} -Oz -o ${outVCF} ${dataset_vcf}.sorted.vcf.gz\n        tabix ${outVCF}\n        rm ${dataset_vcf}.sorted.vcf.gz\n        \"\"\"\n}", "\nprocess bcf_post_map_refs {\n  tag \"${dir}/${name}/${seqid}\"\n  publishDir \"${dir}/${params.outprefix}${name}${params.hash_cascade}/${seqid}\", mode: 'copy'\n\n  input:\n  tuple val(dir), val(name), val(seqid), path('mapped_refseq.bam'), path('mapped_refseq.bam.bai'), path('refseq.fasta')\n  \n  output:\n  tuple val(dir), val(name), val(seqid), path('calls_refseq.vcf.gz'), emit: call\n  tuple val(dir), val(name), val(seqid), path('consensus_refseq.fasta'), emit: cons\n\n  script:\n  \"\"\"\n  bcftools \\\n    mpileup -Ou -f refseq.fasta \\\n    mapped_refseq.bam \\\n    | bcftools \\\n    call --ploidy 1 -mv -Oz \\\n    -o calls_refseq.vcf.gz\n\n  bcftools \\\n    tabix calls_refseq.vcf.gz\n\n  bcftools \\\n    consensus -f refseq.fasta \\\n    -o consensus_refseq.fasta \\\n    calls_refseq.vcf.gz\n  \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/annotate_mafs", "marcodelapierre/illumina-nf/bcf_post_map_refs"], "list_wf_names": ["h3abionet/popfreqs", "marcodelapierre/illumina-nf"]}, {"nb_reuse": 2, "tools": ["BCFtools", "G-BLASTN"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["nanopore-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcodelapierre"], "nb_contrib": 3, "codes": ["\nprocess annotate_annot_chunk {\n    tag \"annot_${dataset_name}_${annot}\"\n    label \"bcftools\"\n    \n    input:\n        tuple val(dataset_name), val(chrm), val(start), val(end), val(chip), file(dataset_vcf), val(annot), file(annot_bed), file(annot_hdr), file(annot_col)\n    \n    output:\n        tuple val(dataset_name), val(chrm), val(start), val(end), file(outVCF)\n    \n    script:\n        base = \"${dataset_name}_${annot}_${chrm}_${start}_${end}\"\n        outVCF = \"${base}.vcf.gz\"\n        \"\"\"\n        columns=\\$(cat ${annot_col})\n        tabix -s1 -b2 -e3 ${annot_bed}\n        bcftools annotate -x INFO,^FORMAT/GT,FORMAT/PL ${dataset_vcf} | \\\n        bcftools sort  -T . | \\\n        bcftools annotate -a ${annot_bed} -h ${annot_hdr} -c \\${columns} | \\\n        bcftools annotate -x ID -Oz -o ${outVCF}\n        tabix ${outVCF}\n        \"\"\"\n}", "\nprocess blast {\ntag \"${dir}/${name}\"\npublishDir \"${dir}/${params.outsuffix}${name}\", mode: 'copy'\n\ninput:\nset dir, name, file('Denovo_subset.fa') from denovo_ch\n\noutput:\nset dir, name, file('blast.tsv') into blast_ch\nset dir, name, file('blast.xml') into blast_xml_ch\n\nwhen:\n!params.diamond\n\nscript:\n\"\"\"\nblastn \\\n  -query Denovo_subset.fa -db ${params.blast_db} \\\n  -outfmt 11 -out blast.asn \\\n  -evalue ${params.evalue} \\\n  -num_threads ${task.cpus}\n\nblast_formatter \\\n  -archive blast.asn \\\n  -outfmt 5 -out blast.xml\n\nblast_formatter \\\n  -archive blast.asn \\\n  -outfmt \"6 qaccver saccver pident length evalue bitscore stitle\" -out blast_unsort.tsv\n\nsort -n -r -k 6 blast_unsort.tsv >blast.tsv\n\"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/annotate_annot_chunk", "marcodelapierre/nanopore-nf/blast"], "list_wf_names": ["h3abionet/popfreqs", "marcodelapierre/nanopore-nf"]}, {"nb_reuse": 2, "tools": ["Trinity", "BCFtools", "snpEff"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["trinity-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcodelapierre"], "nb_contrib": 3, "codes": ["\nprocess snpeff_vcf {\n    tag \"snpeff_${dataset}_${chrm}\"\n    label \"snpeff_bcftools\"\n    input:\n        tuple val(dataset), val(chrm), file(vcf_file)\n    output:\n        tuple val(dataset), val(chrm), file(vcf_out)\n    script:\n        base = vcf_file.getSimpleName()\n        vcf_out = \"${base}_snpeff.vcf.gz\"\n        \"\"\"\n        snpEff \\\n            -Xmx${task.memory.toGiga()}g \\\n            ${params.snpEff_human_db} \\\n            -lof \\\n            -stats ${base}_snpeff.html \\\n            -csvStats ${base}_snpeff.csv \\\n            -dataDir ${params.snpEff_database} \\\n            ${vcf_file} |\\\n        bcftools view -Oz -o ${vcf_out}\n        tabix ${vcf_out}\n        #rm ${base}_snpeff.vcf\n        \"\"\"\n}", "\nprocess jellyfish {\n  tag \"${dir}/${name}\"\n  stageInMode { params.copyinput ? 'copy' : 'symlink' }\n\n  input:\n  tuple val(dir), val(name), path(read1), path(read2), path(\"${params.overtaskfile}\")\n\n  output:\n  tuple val(dir), val(name), path(read1), path(read2), path(\"${params.taskoutdir}\"), path(\"${params.overtaskfile}\")\n\n  script:\n  \"\"\"\n  mem='${task.memory}'\n  mem=\\${mem%B}\n  mem=\\${mem// /}\n\n  Trinity \\\n    --left $read1 \\\n    --right $read2 \\\n    --seqType fq \\\n    --no_normalize_reads \\\n    --verbose \\\n    --no_version_check \\\n    --output ${params.taskoutdir} \\\n    --max_memory \\${mem} \\\n    --CPU ${task.cpus} \\\n    --no_run_inchworm\n  \"\"\"\n}"], "list_proc": ["h3abionet/popfreqs/snpeff_vcf", "marcodelapierre/trinity-nf/jellyfish"], "list_wf_names": ["marcodelapierre/trinity-nf", "h3abionet/popfreqs"]}, {"nb_reuse": 2, "tools": ["Trinity", "BCFtools"], "nb_own": 2, "list_own": ["h3abionet", "marcodelapierre"], "nb_wf": 2, "list_wf": ["trinity-nf", "popfreqs"], "list_contrib": ["mamanambiya", "ameintjes", "marcodelapierre"], "nb_contrib": 3, "codes": ["\nprocess chrysalis {\n  tag \"${dir}/${name}\"\n\n  input:\n  tuple val(dir), val(name), path(read1), path(read2), path(\"${params.taskoutdir}\"), path(\"${params.overtaskfile}\")\n\n  output:\n  tuple val(dir), val(name), path{ params.localdisk ? \"chunk*.tgz\" : \"${params.taskoutdir}/read_partitions/**inity.reads.fa\" }\n\n  script:\n  \"\"\"\n  if [ \"${params.localdisk}\" == \"true\" ] ; then\n    here=\\$PWD\n    rm -rf ${params.localdir}\n    mkdir ${params.localdir}\n    cp -r \\$( readlink $read1 ) ${params.localdir}/\n    cp -r \\$( readlink $read2 ) ${params.localdir}/\n    cp -r \\$( readlink ${params.taskoutdir} ) ${params.localdir}/\n    cd ${params.localdir}\n  fi\n\n  mem='${task.memory}'\n  mem=\\${mem%B}\n  mem=\\${mem// /}\n\n  Trinity \\\n    --left $read1 \\\n    --right $read2 \\\n    --seqType fq \\\n    --no_normalize_reads \\\n    --verbose \\\n    --no_version_check \\\n    --output ${params.taskoutdir} \\\n    --max_memory \\${mem} \\\n    --CPU ${task.cpus} \\\n    --no_distributed_trinity_exec\n\n  if [ \"${params.localdisk}\" == \"true\" ] ; then\n    find ${params.taskoutdir}/read_partitions -name \"*inity.reads.fa\" >output_list\n    split -l ${params.bf_collate} -a 4 output_list chunk\n    for f in chunk* ; do\n      tar -cz -h -f \\${f}.tgz -T \\${f}\n    done\n    cd \\$here\n    cp ${params.localdir}/chunk*.tgz .\n    rm -r ${params.localdir}\n  fi\n  \"\"\"\n}", "\nprocess update_rsid_vcf {\n    tag \"update_rsid_vcf_${dataset}_${chrm}\"\n    label \"bigmem\"\n    input:\n        tuple val(dataset), val(chrm), file(vcf_file), file(dbsnp_vcf)\n    output:\n        tuple val(dataset), val(chrm), file(vcf_out)\n    script:\n        base = vcf_file.getSimpleName()\n        vcf_out = \"${base}_dbsnp.vcf.gz\"\n        \"\"\"\n        tabix -f ${vcf_file}\n        tabix -f ${dbsnp_vcf}\n        bcftools annotate ${vcf_file} -a ${dbsnp_vcf} -c ID --threads ${task.cpus} -Oz -o ${vcf_out}\n        tabix ${vcf_out}\n        \"\"\"\n}"], "list_proc": ["marcodelapierre/trinity-nf/chrysalis", "h3abionet/popfreqs/update_rsid_vcf"], "list_wf_names": ["marcodelapierre/trinity-nf", "h3abionet/popfreqs"]}, {"nb_reuse": 2, "tools": ["BCFtools", "Bowtie"], "nb_own": 2, "list_own": ["h3abionet", "markgene"], "nb_wf": 2, "list_wf": ["cutnrun", "popfreqs"], "list_contrib": ["sofiahaglund", "mamanambiya", "ameintjes", "Rotholandus", "markgene", "winni2k", "ewels", "apeltzer", "tiagochst", "chuan-wang", "drpatelh"], "nb_contrib": 11, "codes": [" process Bowtie2Index {\n        tag \"$fasta\"\n        label 'process_high'\n        publishDir path: { params.save_reference ? \"${params.outdir}/reference_genome\" : params.outdir },\n            saveAs: { params.save_reference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta from ch_fasta\n\n        output:\n        file \"Bowtie2Index\" into ch_bowtie2_index\n\n        script:\n        \"\"\"\n        bowtie2-build --threads $task.cpus $fasta $fasta\n        mkdir Bowtie2Index && mv ${fasta}* Bowtie2Index\n        \"\"\"\n    }", "\nprocess annot_vcf_chrm {\n    tag \"annot_vcf_chrm_${dataset}_${chrm}\"\n    label \"medium\"\n    publishDir \"/cbio/users/mamana/exome_aibst/data/AIBST/VCF/CHRS/\", mode: 'copy'\n\n    input:\n        tuple val(dataset), val(chrm), file(annot_vcf), file(vcf_file)\n    output:\n        tuple val(dataset), val(chrm), file(vcf_chrm)\n    script:\n        base = file(vcf_file.baseName).baseName\n        vcf_chrm = \"${base}.annot.vcf.gz\"\n        \"\"\"\n        tabix ${vcf_file}\n        tabix ${annot_vcf}\n        bcftools view --regions ${chrm} ${vcf_file} --threads ${task.cpus} -Oz -o ${base}_${chrm}.annot.vcf.gz\n        bcftools annotate -a ${annot_vcf} -c INFO ${vcf_file} --threads ${task.cpus} -Oz -o ${vcf_chrm}\n        tabix ${vcf_chrm}\n        \"\"\"\n}"], "list_proc": ["markgene/cutnrun/Bowtie2Index", "h3abionet/popfreqs/annot_vcf_chrm"], "list_wf_names": ["h3abionet/popfreqs", "markgene/cutnrun"]}, {"nb_reuse": 2, "tools": ["FastQC", "BCFtools"], "nb_own": 2, "list_own": ["h3abionet", "markgene"], "nb_wf": 2, "list_wf": ["cutnrun", "popfreqs"], "list_contrib": ["sofiahaglund", "mamanambiya", "ameintjes", "Rotholandus", "markgene", "winni2k", "ewels", "apeltzer", "tiagochst", "chuan-wang", "drpatelh"], "nb_contrib": 11, "codes": ["\nprocess FastQC {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.endsWith(\".zip\") ? \"zips/$filename\" : \"$filename\"\n                }\n\n    when:\n    !params.skip_fastqc\n\n    input:\n    set val(name), file(reads) from ch_raw_reads_fastqc\n\n    output:\n    file \"*.{zip,html}\" into ch_fastqc_reports_mqc\n\n    script:\n                                                                           \n    if (params.single_end) {\n        \"\"\"\n        [ ! -f  ${name}.fastq.gz ] && ln -s $reads ${name}.fastq.gz\n        fastqc -q -t $task.cpus ${name}.fastq.gz\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${name}_1.fastq.gz ] && ln -s ${reads[0]} ${name}_1.fastq.gz\n        [ ! -f  ${name}_2.fastq.gz ] && ln -s ${reads[1]} ${name}_2.fastq.gz\n        fastqc -q -t $task.cpus ${name}_1.fastq.gz\n        fastqc -q -t $task.cpus ${name}_2.fastq.gz\n        \"\"\"\n    }\n}", "\nprocess split_target_to_chunk_sites {\n    tag \"split_${dataset}_${chrm}:${chunk_start}-${chunk_end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(chunk_start), val(chunk_end), val(chip), file(dataset_vcf)\n    output:\n        tuple val(dataset), val(chrm), val(chunk_start), val(chunk_end), val(chip), file(vcf_chunk_out)\n    script:\n        base = file(dataset_vcf.baseName).baseName\n        vcf_chunk_out = \"${base}_${chrm}_${chunk_start}-${chunk_end}_${chip}.sites.bcf\"\n        \"\"\"\n        tabix ${dataset_vcf}\n        bcftools view --regions ${chrm}:${chunk_start}-${chunk_end} ${dataset_vcf} --drop-genotypes --threads ${task.cpus} -Ob -o ${vcf_chunk_out}\n        tabix ${vcf_chunk_out}\n        \"\"\"\n}"], "list_proc": ["markgene/cutnrun/FastQC", "h3abionet/popfreqs/split_target_to_chunk_sites"], "list_wf_names": ["h3abionet/popfreqs", "markgene/cutnrun"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_genotype_gvcf_on_genome {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rGGoG\" }\n    label \"bigmem\"\n    publishDir \"${params.out_dir}/${params.cohort_id}/genome-calling\", mode: 'copy', overwrite: false\n    input:\t\t\n    val (gvcf_file) from gvcf_file_ch\n    each chr from chroms\n\n    output:\n    set chr, file(\"${params.cohort_id}.${chr}.vcf.gz\"), file(\"${params.cohort_id}.${chr}.vcf.gz.tbi\") into gg_vcf\n\n    script:\n    call_conf = 30               \n    if ( params.sample_coverage == \"high\" )\n      call_conf = 30\n    else if ( params.sample_coverage == \"low\" )\n      call_conf = 10\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n    GenotypeGVCFs \\\n    -R ${params.ref_seq} \\\n    -L $chr \\\n    -V ${gvcf_file} \\\n    -stand-call-conf ${call_conf} \\\n    -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n    -O \"${params.cohort_id}.${chr}.vcf.gz\" \n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_genotype_gvcf_on_genome"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_concat_vcf {\n     tag { \"${params.project_name}.${params.cohort_id}.rCV\" }\n     label \"bigmem\"\n     publishDir \"${params.out_dir}/${params.cohort_id}/genome-calling\", mode: 'copy', overwrite: false\n \n     input:\n     file(vcf) from concat_ready\n\n     output:\n\t   set val(\"${params.cohort_id}\"), file(\"${params.cohort_id}.recal-SNP.recal-INDEL.vcf.gz\"), file(\"${params.cohort_id}.recal-SNP.recal-INDEL.vcf.gz.tbi\") into combine_calls\n\n     script:\n     \"\"\"\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.1\\\\.*recal-SNP.recal-INDEL.vcf.gz\" > ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.2\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.3\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.4\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.5\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.6\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.7\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.8\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.9\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.10\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.11\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.12\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.13\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.14\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.15\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.16\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.17\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.18\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.19\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.20\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.21\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n     echo \"${vcf.join('\\n')}\" | grep \"\\\\.22\\\\.*recal-SNP.recal-INDEL.vcf.gz\" >> ${params.cohort_id}.vcf.list\n    \n     ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\"  \\\n     GatherVcfs \\\n     -I ${params.cohort_id}.vcf.list \\\n     -O ${params.cohort_id}.recal-SNP.recal-INDEL.vcf.gz # GatherVCF does not index the VCF. The VCF will be indexed in the next tabix operation.\n     ${params.tabix_base}/tabix -p vcf ${params.cohort_id}.recal-SNP.recal-INDEL.vcf.gz \n     \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_concat_vcf"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_vqsr_on_snps {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rVoS\" }\n    label \"bigmem\"\n    publishDir \"${params.out_dir}/${params.cohort_id}/genome-calling\", mode: 'copy', overwrite: false\n    input:\t\t\n    set val (chr), file(vcf), file(vcf_index) from gg_vcf\n\n    output:\n    set chr, file(vcf), file(vcf_index), file(\"${params.cohort_id}.${chr}.vcf.recal-SNP.recal\"), file(\"${params.cohort_id}.${chr}.vcf.recal-SNP.recal.idx\"), file(\"${params.cohort_id}.${chr}.vcf.recal-SNP.tranches\") into snps_vqsr_recal\n\n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n    VariantRecalibrator \\\n   -R ${params.ref_seq} \\\n   -L $chr \\\n   -resource hapmap,known=false,training=true,truth=true,prior=15.0:${params.hapmap} \\\n   -resource omni,known=false,training=true,truth=true,prior=12.0:${params.omni} \\\n   -resource 1000G,known=false,training=true,truth=false,prior=10.0:${params.phase1_snps} \\\n   -resource dbsnp,known=true,training=false,truth=false,prior=2.0:${params.dbsnp} \\\n   -an DP \\\n   -an FS \\\n   -an SOR \\\n   -an MQ \\\n   -an MQRankSum \\\n   -an QD \\\n   -an ReadPosRankSum \\\n   -mode SNP \\\n    --max-gaussians \"${params.max_gaussians_snps}\" \\\n   -V ${vcf} \\\n   -O \"${params.cohort_id}.${chr}.vcf.recal-SNP.recal\" \\\n   --tranches-file \"${params.cohort_id}.${chr}.vcf.recal-SNP.tranches\"\n   \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_vqsr_on_snps"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_apply_vqsr_on_snps {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rAVoS\" }\n    label \"bigmem\"\n    publishDir \"${params.out_dir}/${params.cohort_id}/genome-calling\", mode: 'copy', overwrite: false\n    input:\t\t\n    set val (chr), file(vcf), file(vcf_index), file(snp_recal), file(snp_recal_index), file(snp_tranches) from snps_vqsr_recal\n\n    output:\n    set chr, file(\"${params.cohort_id}.${chr}.recal-SNP.vcf.gz\"), file(\"${params.cohort_id}.${chr}.recal-SNP.vcf.gz.tbi\") into snps_vqsr_vcf\n\n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n    ApplyVQSR \\\n    -R ${params.ref_seq} \\\n    --recal-file ${snp_recal} \\\n    --tranches-file ${snp_tranches} \\\n    -mode SNP \\\n    -ts-filter-level \"${params.ts_filter_level_snps}\" \\\n    -V ${vcf} \\\n    -O \"${params.cohort_id}.${chr}.recal-SNP.vcf.gz\"\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_apply_vqsr_on_snps"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_vqsr_on_indels {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rVoI\" }\n    label \"bigmem\"\n    publishDir \"${params.out_dir}/${params.cohort_id}/genome-calling\", mode: 'copy', overwrite: false\n    input:\n    set val (chr), file(vcf), file(vcf_index) from snps_vqsr_vcf\n\n    output:\n    set chr, file(vcf), file(vcf_index), file(\"${params.cohort_id}.${chr}.recal-SNP.vcf.recal-INDEL.recal\"), file(\"${params.cohort_id}.${chr}.recal-SNP.vcf.recal-INDEL.recal.idx\"), file(\"${params.cohort_id}.${chr}.recal-SNP.vcf.recal-INDEL.tranches\") into indel_vqsr_recal\n\n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n    VariantRecalibrator \\\n   -R ${params.ref_seq} \\\n   -L $chr \\\n   -resource mills,known=false,training=true,truth=true,prior=12.0:${params.golden_indels} \\\n   -resource dbsnp,known=true,training=false,truth=false,prior=2.0:${params.dbsnp} \\\n   -an DP \\\n   -an FS \\\n   -an SOR \\\n   -an MQ \\\n   -an MQRankSum \\\n   -an QD \\\n   -an ReadPosRankSum \\\n   -mode INDEL \\\n    --max-gaussians \"${params.max_gaussians_indels}\" \\\n   -V ${vcf} \\\n   -O \"${params.cohort_id}.${chr}.recal-SNP.vcf.recal-INDEL.recal\" \\\n   --tranches-file \"${params.cohort_id}.${chr}.recal-SNP.vcf.recal-INDEL.tranches\"\n   \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_vqsr_on_indels"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_apply_vqsr_on_indels {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rAVoI\" }\n    label \"bigmem\"\n    publishDir \"${params.out_dir}/${params.cohort_id}/genome-calling\", mode: 'copy', overwrite: false\n    input:\t\t\n    set val (chr), file(vcf), file(vcf_index), file(indel_recal), file(indel_recal_index), file(indel_tranches) from indel_vqsr_recal\n\n    output:\n    set chr, file(\"${params.cohort_id}.${chr}.recal-SNP.recal-INDEL.vcf.gz\"), file(\"${params.cohort_id}.${chr}.recal-SNP.recal-INDEL.vcf.gz.tbi\") into indel_vqsr_vcf\n    file(\"${params.cohort_id}.${chr}.recal-SNP.recal-INDEL.vcf.gz\") into vcf_concat_ready\n\n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n    ApplyVQSR \\\n    -R ${params.ref_seq} \\\n    --recal-file ${indel_recal} \\\n    --tranches-file ${indel_tranches} \\\n    -mode INDEL \\\n    -ts-filter-level \"${params.ts_filter_level_indels}\" \\\n    -V ${vcf} \\\n    -O \"${params.cohort_id}.${chr}.recal-SNP.recal-INDEL.vcf.gz\"\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_apply_vqsr_on_indels"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess collate {\n    tag { \"${params.project_name}.${sample_id}.C\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'symlink', overwrite: false\n    input:\n    set val(sample_id), val(bam_file) from samples\n\n    output:\n    set val(sample_id), file(\"${sample_id}.collate.bam\") into collate\n\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools collate --reference ${params.ref_seq} -o ${sample_id}.collate.bam  ${bam_file} tmp.collate  \n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/collate"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess cram_to_fastq {\n    tag { \"${params.project_name}.${sample_id}.ctF\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n    input:\n    set val(sample_id), file(bam_file) from collate\n\n    output:\n    set val(sample_id), file(\"${sample_id}_R1.fastq.gz\"), file(\"${sample_id}_R2.fastq.gz\") into fastq\n\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools fastq -1 ${sample_id}_R1.fastq.gz -2 ${sample_id}_R2.fastq.gz -0 /dev/null -s /dev/null -N -F 0x900 --reference ${params.ref_seq} ${bam_file}\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/cram_to_fastq"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_combine_gvcfs {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rCG\" }\n    label 'bigmem'\n    publishDir \"${params.out_dir}/${params.cohort_id}\", mode: 'copy', overwrite: false\n\n    input:\n    file(gvcf_list)\n    each chr from chroms\n\n    output:\n    file(\"${params.cohort_id}.${chr}.g.vcf.gz\")  into cohort_chr_calls\n    file(\"${params.cohort_id}.${chr}.g.vcf.gz.tbi\") into cohort_chr_indexes\n\n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\"  \\\n    CombineGVCFs \\\n    -R ${params.ref_seq} \\\n    --L ${chr} \\\n    --variant ${gvcf_list} \\\n    -O ${params.cohort_id}.${chr}.g.vcf.gz\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_combine_gvcfs"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_concat_combine_gvcf {\n     tag { \"${params.project_name}.${params.cohort_id}.rCCG\" }\n     label 'bigmem'\n     publishDir \"${params.out_dir}/${params.cohort_id}\", mode: 'copy', overwrite: false\n\n     input:\n     file(gvcf) from cohort_calls\n\n     output:\n\t   set val(params.cohort_id), file(\"${params.cohort_id}.g.vcf.gz\") into combine_calls\n\t   set val(params.cohort_id), file(\"${params.cohort_id}.g.vcf.gz.tbi\") into combine_calls_indexes\n\n     script:\n     \"\"\"\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.1\\\\.g.vcf.gz\" > ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.2\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.3\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.4\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.5\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.6\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.7\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.8\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.9\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.10\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.11\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.12\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.13\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.14\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.15\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.16\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.17\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.18\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.19\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.20\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.21\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.22\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.X\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.Y\\\\.g.vcf.gz\" >> ${params.cohort_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.MT\\\\.g\\\\.vcf\\\\.gz\" >> ${params.cohort_id}.gvcf.list\n    \n     ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\"  \\\n     GatherVcfs \\\n     -I ${params.cohort_id}.gvcf.list \\\n     -O ${params.cohort_id}.g.vcf.gz # GatherVCF does not index the VCF. The VCF will be indexed in the next tabix operation.\n     ${params.tabix_base}/tabix -p vcf ${params.cohort_id}.g.vcf.gz \n     \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_concat_combine_gvcf"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools"], "nb_own": 2, "list_own": ["h3abionet", "harleenduggal"], "nb_wf": 2, "list_wf": ["recalling", "RNASEQ"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 61, "codes": ["\nprocess filter_short_snps_indels {\n    tag { \"${params.project_name}.${params.cohort_id}.fSSI\" }\n    publishDir \"${out_dir}/${params.cohort_id}/filter-vcf\", mode: 'copy', overwrite: false\n    input:\n      set val (file_name), file (vcf) from vcfs1\n    output:\n    file(\"${filebase}.filter-pass.vcf.gz\") into vcf_short_out\n\n    script:\n    filebase = (file(vcf[0].baseName)).baseName\n    \"\"\"\n    ${params.bcftools_base}/bcftools view \\\n    --include \"FILTER='PASS'\" \\\n    -O z \\\n    -o \"${filebase}.filter-pass.vcf.gz\" \\\n    ${vcf[0]} \n    \"\"\"\n}", "process CUSTOM_GETCHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path  \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        custom: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/filter_short_snps_indels", "harleenduggal/RNASEQ/CUSTOM_GETCHROMSIZES"], "list_wf_names": ["harleenduggal/RNASEQ", "h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess filter_other {\n    tag { \"${params.project_name}.${params.cohort_id}.fO\" }\n    publishDir \"${out_dir}/${params.cohort_id}/filter-vcf\", mode: 'copy', overwrite: false\n    input:\n      set val (file_name), file (vcf) from vcfs2\n    output:\n    file(\"${filebase}.filter-other.vcf.gz\") into vcf_long_out\n\n    script:\n    filebase = (file(vcf[0].baseName)).baseName\n    \"\"\"\n    ${params.bcftools_base}/bcftools view \\\n    --include \"FILTER='.'\" \\\n    -O z \\\n    -o \"${filebase}.filter-other.vcf.gz\" \\\n    ${vcf[0]}\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/filter_other"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_bwa {\n    tag { \"${params.project_name}.${sample_id}.rBwa\" }\n    memory { 4.GB * task.attempt }\n    cpus { \"${params.bwa_threads}\" }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), val(fastq_r1_file), val(fastq_r2_file) from samples_2\n\n    output:\n    set val(sample_id), file(\"${sample_id}.bam\")  into raw_bam\n\n    script:\n    readgroup_info=\"@RG\\\\tID:$sample_id.0\\\\tLB:LIBA\\\\tSM:$sample_id\\\\tPL:Illumina\"\n    \"\"\"\n    ${params.bwa_base}/bwa mem \\\n    -R \\\"${readgroup_info}\\\" \\\n    -t ${params.bwa_threads}  \\\n    -M \\\n    ${params.ref_seq} \\\n    ${fastq_r1_file} \\\n    ${fastq_r2_file} | \\\n    ${params.samtools_base}/samtools sort \\\n    --threads ${params.bwa_threads} \\\n    -m 2G \\\n    - > ${sample_id}.bam\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_bwa"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["MarkDuplicates (IP)", "GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_mark_duplicates {\n    tag { \"${params.project_name}.${sample_id}.rMD\" }\n    memory { 4.GB * task.attempt }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), file(bam_file) from raw_bam\n\n    output:\n    set val(sample_id), file(\"${sample_id}.md.bam\"), file(\"${sample_id}.md.bai\")  into md_bam\n    \n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_md_mem}\"  \\\n    MarkDuplicates \\\n    --MAX_RECORDS_IN_RAM 50000 \\\n    --INPUT ${bam_file} \\\n    --METRICS_FILE ${bam_file}.metrics \\\n    --TMP_DIR ${params.gatk_tmp_dir} \\\n    --ASSUME_SORT_ORDER coordinate \\\n    --CREATE_INDEX true \\\n    --OUTPUT ${sample_id}.md.bam\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_mark_duplicates"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_create_recalibration_table {\n    tag { \"${params.project_name}.${sample_id}.rCRT\" }\n    memory { 8.GB * task.attempt }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), file(bam_file), file(bam_file_index) from md_bam\n\n    output:\n    set val(sample_id), file(\"${sample_id}.md.bam\"), file(\"${sample_id}.md.bai\"), file(\"${sample_id}.recal.table\")  into recal_table\n    \n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options  \"-Xmx${task.memory.toGiga()}g\" \\\n    BaseRecalibrator \\\n    --input ${bam_file} \\\n    --output ${sample_id}.recal.table \\\n    --TMP_DIR ${params.gatk_tmp_dir} \\\n    -R ${params.ref_seq} \\\n    --known-sites ${params.dbsnp} \\\n    --known-sites ${params.known_indels_1} \\\n    --known-sites ${params.known_indels_2}\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_create_recalibration_table"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_recalibrate_bam {\n    tag { \"${params.project_name}.${sample_id}.rRB\" }\n    memory { 8.GB * task.attempt }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), file(bam_file), file(bam_file_index), file(recal_table_file) from recal_table\n\n    output:\n    set val(sample_id), file(\"${sample_id}.md.recal.bam\")  into recal_bam\n    set val(sample_id), file(\"${sample_id}.md.recal.bai\")  into recal_bam_index\n    \n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options  \"-Xmx${task.memory.toGiga()}g\" \\\n     ApplyBQSR \\\n    --input ${bam_file} \\\n    --output ${sample_id}.md.recal.bam \\\n    --TMP_DIR ${params.gatk_tmp_dir} \\\n    -R ${params.ref_seq} \\\n    --create-output-bam-index true \\\n    --bqsr-recal-file ${recal_table_file}\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_recalibrate_bam"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_samtools_stats {\n    tag { \"${params.project_name}.${sample_id}.rSS\" }\n    memory { 4.GB * task.attempt } \n    cpus { \"${params.bwa_threads}\" }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), file(bam_file) from recal_bam\n\n    output:\n    set val(sample_id), file(\"${sample_id}.md.recal.stats\")  into recal_stats\n\n    \"\"\"\n    ${params.samtools_base}/samtools stats  \\\n    --threads ${params.bwa_threads} \\\n    ${bam_file} > ${sample_id}.md.recal.stats  \\\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_samtools_stats"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_remove_samples_combined_gvcf {\n    tag { \"${params.project_name}.${params.cohort_id}.${chr}.rRSCG\" }\n    label 'bigmem'\n    publishDir \"${params.out_dir}/${params.cohort_id}\", mode: 'copy', overwrite: false\n\n    input:\n    file (gvcf) from file(params.combined_gvcf)\n    file (gvcf_index) from file(params.combined_gvcf_index)\n    each chr from chroms\n\n    output:\n    file(\"${params.cohort_id}.${chr}.g.vcf.gz\")  into cohort_chr_calls\n    file(\"${params.cohort_id}.${chr}.g.vcf.gz.tbi\") into cohort_chr_indexes\n\n    script:\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${task.memory.toGiga()}g\"  \\\n    SelectVariants \\\n    -R ${params.ref_seq} \\\n    --L ${chr} \\\n    -V ${gvcf} \\\n    --exclude-sample-name ${params.exclude_samples} \\\n    -O ${params.cohort_id}.${chr}.g.vcf.gz\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_remove_samples_combined_gvcf"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess bam_to_cram {\n    tag { \"${params.project_name}.${sample_id}.btC\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'symlink', overwrite: false\n    input:\n    set val(sample_id), file(bam_file) from samples\n\n    output:\n    set val(sample_id), file(\"${bam_file.baseName}.cram\") into cram_file\n\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools view \\\n    --reference ${params.ref_seq} \\\n    --output-fmt cram,version=3.0 \\\n    -o ${bam_file.baseName}.cram  ${bam_file}  \n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/bam_to_cram"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess index_cram {\n    tag { \"${params.project_name}.${sample_id}.iC\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'symlink', overwrite: false\n    input:\n    set val(sample_id), file(cram_file) from cram_file_1\n\n    output:\n    set val(sample_id), file(\"${cram_file}.crai\") into cram_index\n\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools index  \\\n    ${cram_file} ${cram_file}.crai\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/index_cram"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_flagstat {\n    tag { \"${params.project_name}.${sample_id}.rF\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'symlink', overwrite: false\n    input:\n    set val(sample_id), file(bam_file) from samples\n\n    output:\n    set val(sample_id), file(\"${bam_file}.flagstat\") into cram_file\n\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools flagstat \\\n    -@ 1 \\\n    ${bam_file} > ${bam_file}.flagstat  \\\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_flagstat"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess index_bam {\n    tag { \"${sample_id}\" }\n    echo true\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n    input:\n    set val(sample_id), file(bam_file) from samples\n    output:\n    file(\"*.bai\")\n    script:\n    \"\"\"\n    ${params.samtools_base}/samtools index ${bam_file}\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/index_bam"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 9, "tools": ["SAMtools", "MultiQC", "RapidNJ", "snippy", "Roary", "FastQC", "shovill", "ClonalFrameML", "GATK"], "nb_own": 3, "list_own": ["h3abionet", "happykhan", "harleenduggal"], "nb_wf": 5, "list_wf": ["nf-klebtest", "RNASEQ", "nf-scripts", "recalling", "NEXTFLOW-fastqc"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "happykhan", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "harleenduggal", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 63, "codes": ["\nprocess run_haplotype_caller_on_autosomes {\n    tag { \"${params.project_name}.${sample_id}.${chr}.rHCoA\" }\n    memory { 4.GB * task.attempt }\n    cpus { 4 }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), val(gender), val(bam_file) from samples_2\n\t  each chr from autosomes\n\n    output:\n\t  set val(sample_id), file(\"${sample_id}.${chr}.g.vcf.gz\")  into autosome_calls\n\t  set val(sample_id), file(\"${sample_id}.${chr}.g.vcf.gz.tbi\") into autosome_calls_indexes\n\n    script:\n    call_conf = 30               \n    if ( params.sample_coverage == \"high\" )\n      call_conf = 30\n    else if ( params.sample_coverage == \"low\" )\n      call_conf = 10\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n    HaplotypeCaller \\\n    -R ${params.ref_seq} \\\n    -I $bam_file \\\n    --emit-ref-confidence GVCF \\\n    --dbsnp ${params.dbsnp_sites} \\\n    --L $chr \\\n    --genotyping-mode DISCOVERY \\\n    -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n    -stand-call-conf ${call_conf} \\\n    --sample-ploidy 2 \\\n    -O ${sample_id}.${chr}.g.vcf.gz\n    \"\"\"\n}", "\nprocess SHOVILL {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveContigs(filename:filename, publish_dir:'ass', meta:meta) }\n\n    conda (params.enable_conda ? \"bioconda::shovill=1.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/shovill:1.1.0--0\"\n    } else {\n        container \"quay.io/biocontainers/shovill:1.1.0--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"${meta.id}.fa\")                         , emit: contigs\n    tuple val(meta), path(\"${meta.id}.shovill.log\")                        , emit: log\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n    \"\"\"\n    shovill \\\\\n        --R1 ${reads[0]} \\\\\n        --R2 ${reads[1]} \\\\\n        $args \\\\\n        --cpus $task.cpus \\\\\n        --outdir ./ \\\\\n        --force\n    mv contigs.fa ${meta.id}.fa\n    mv shovill.log ${meta.id}.shovill.log\n    shovill --version | sed -e 's/^.*shovill //' > shovill.version.txt\n    \"\"\"\n    stub:\n\n    \"\"\"\n      touch ${meta.id}.fa\n      touch ${meta.id}.shovill.log\n      shovill --version | sed -e 's/^.*shovill //' > shovill.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess roary {\n    cpus 20\n    time '47h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'roary', mode: 'copy', overwrite: true\n \n    input:\n    file(genome) from gff_roary.collect()\n\n    output:\n    file \"roary_out/*\" into roary_all_results\n\n    script:\n    \"\"\"\n    roary -p ${task.cpus} -ne -f roary_out ${genome}\n    \"\"\"\n}", "\nprocess snippy {\n   cpus 20\n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 80.GB * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   input:\n   set name, file(read1), file(read2) from reads_snippy\n   file ref \n\n   output:\n   file \"${name}\" into core_aln_results\n\n   script:\n   \"\"\"\n   snippy --cpus ${task.cpus} --ref ${ref} --R1 ${read1} --R2 ${read2} --outdir ${name} --cleanup\n   \"\"\"\n}", "\nprocess rapidnj  {\n    publishDir 'rapidnj', mode: 'copy', overwrite: true\n    queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n    executor 'slurm'\n \n    input:\n    file core from nj_core_align \n\n    output:\n    file 'rapidnj.tree' into njtree \n\n    script:\n    \"\"\"\n    rapidnj -n -i fa ${core} > rapidnj.tree\n    \"\"\"\n}", "\nprocess clonal{\n   publishDir 'clonal', mode: 'copy', overwrite: true    \n   time '5d'\n   cpus 5 \n   memory '120 GB'   \n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 120.GB * task.attempt }\n   time { 2.d * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   when:\n   params.tree\n\n   input: \n   file tree from iqtreefastout\n   file align from clonal_align\n   \n   output:\n   file 'clonal*' into clonalout\n\n   script:\n   \"\"\"\n   ClonalFrameML ${tree} ${align} clonal\n   \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    set sample_id, file(reads) from read_pairs2_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs  -q ${reads}\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_autosomes", "happykhan/nf-klebtest/SHOVILL", "happykhan/nf-klebtest/MULTIQC", "happykhan/nf-scripts/roary", "happykhan/nf-scripts/snippy", "happykhan/nf-scripts/rapidnj", "happykhan/nf-scripts/clonal", "harleenduggal/NEXTFLOW-fastqc/fastqc", "harleenduggal/RNASEQ/SAMTOOLS_INDEX"], "list_wf_names": ["harleenduggal/NEXTFLOW-fastqc", "harleenduggal/RNASEQ", "happykhan/nf-scripts", "happykhan/nf-klebtest", "h3abionet/recalling"]}, {"nb_reuse": 8, "tools": ["SAMtools", "STAR", "RapidNJ", "snippy", "Roary", "FastQC", "ClonalFrameML", "GATK"], "nb_own": 3, "list_own": ["h3abionet", "happykhan", "harleenduggal"], "nb_wf": 4, "list_wf": ["recalling", "NEXTFLOW-fastqc", "RNASEQ", "nf-scripts"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "happykhan", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "harleenduggal", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 63, "codes": ["\nprocess run_haplotype_caller_on_x_par1_male {\n     tag { \"${params.project_name}.${sample_id}.rHCoXP1M\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_male_1\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.X_PAR1.g.vcf.gz\") into x_par1_calls\n\t   set val(sample_id), file(\"${sample_id}.X_PAR1.g.vcf.gz.tbi\") into x_par1_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L X:60001-2699520 \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 2 \\\n     -O ${sample_id}.X_PAR1.g.vcf.gz\n     \"\"\"\n}", "\nprocess roary {\n    cpus 20\n    time '47h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'roary', mode: 'copy', overwrite: true\n \n    input:\n    file(genome) from gff_roary.collect()\n\n    output:\n    file \"roary_out/*\" into roary_all_results\n\n    script:\n    \"\"\"\n    roary -p ${task.cpus} -ne -f roary_out ${genome}\n    \"\"\"\n}", "\nprocess snippy {\n   cpus 20\n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 80.GB * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   input:\n   set name, file(read1), file(read2) from reads_snippy\n   file ref \n\n   output:\n   file \"${name}\" into core_aln_results\n\n   script:\n   \"\"\"\n   snippy --cpus ${task.cpus} --ref ${ref} --R1 ${read1} --R2 ${read2} --outdir ${name} --cleanup\n   \"\"\"\n}", "\nprocess rapidnj  {\n    publishDir 'rapidnj', mode: 'copy', overwrite: true\n    queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n    executor 'slurm'\n \n    input:\n    file core from nj_core_align \n\n    output:\n    file 'rapidnj.tree' into njtree \n\n    script:\n    \"\"\"\n    rapidnj -n -i fa ${core} > rapidnj.tree\n    \"\"\"\n}", "\nprocess clonal{\n   publishDir 'clonal', mode: 'copy', overwrite: true    \n   time '5d'\n   cpus 5 \n   memory '120 GB'   \n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 120.GB * task.attempt }\n   time { 2.d * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   when:\n   params.tree\n\n   input: \n   file tree from iqtreefastout\n   file align from clonal_align\n   \n   output:\n   file 'clonal*' into clonalout\n\n   script:\n   \"\"\"\n   ClonalFrameML ${tree} ${align} clonal\n   \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    set sample_id, file(reads) from read_pairs2_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs  -q ${reads}\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"        , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = (task.ext.args ?: '').tokenize()\n    def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_x_par1_male", "happykhan/nf-scripts/roary", "happykhan/nf-scripts/snippy", "happykhan/nf-scripts/rapidnj", "happykhan/nf-scripts/clonal", "harleenduggal/NEXTFLOW-fastqc/fastqc", "harleenduggal/RNASEQ/SAMTOOLS_INDEX", "harleenduggal/RNASEQ/STAR_GENOMEGENERATE"], "list_wf_names": ["harleenduggal/RNASEQ", "harleenduggal/NEXTFLOW-fastqc", "happykhan/nf-scripts", "h3abionet/recalling"]}, {"nb_reuse": 7, "tools": ["SAMtools", "STAR", "RapidNJ", "BEDTools", "FastQC", "ClonalFrameML", "GATK"], "nb_own": 3, "list_own": ["h3abionet", "happykhan", "harleenduggal"], "nb_wf": 4, "list_wf": ["recalling", "NEXTFLOW-fastqc", "RNASEQ", "nf-scripts"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "happykhan", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "harleenduggal", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 63, "codes": ["\nprocess run_haplotype_caller_on_x_par2_male {\n     tag { \"${params.project_name}.${sample_id}.rHCoXP2M\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_male_2\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.X_PAR2.g.vcf.gz\") into x_par2_calls\n\t   set val(sample_id), file(\"${sample_id}.X_PAR2.g.vcf.gz.tbi\") into x_par2_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L X:154931044-155260560 \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 2 \\\n     -O ${sample_id}.X_PAR2.g.vcf.gz\n     \"\"\"\n}", "\nprocess rapidnj  {\n    publishDir 'rapidnj', mode: 'copy', overwrite: true\n    queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n    executor 'slurm'\n \n    input:\n    file core from nj_core_align \n\n    output:\n    file 'rapidnj.tree' into njtree \n\n    script:\n    \"\"\"\n    rapidnj -n -i fa ${core} > rapidnj.tree\n    \"\"\"\n}", "\nprocess clonal{\n   publishDir 'clonal', mode: 'copy', overwrite: true    \n   time '5d'\n   cpus 5 \n   memory '120 GB'   \n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 120.GB * task.attempt }\n   time { 2.d * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   when:\n   params.tree\n\n   input: \n   file tree from iqtreefastout\n   file align from clonal_align\n   \n   output:\n   file 'clonal*' into clonalout\n\n   script:\n   \"\"\"\n   ClonalFrameML ${tree} ${align} clonal\n   \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    set sample_id, file(reads) from read_pairs2_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs  -q ${reads}\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"        , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = (task.ext.args ?: '').tokenize()\n    def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_x_par2_male", "happykhan/nf-scripts/rapidnj", "happykhan/nf-scripts/clonal", "harleenduggal/NEXTFLOW-fastqc/fastqc", "harleenduggal/RNASEQ/SAMTOOLS_INDEX", "harleenduggal/RNASEQ/STAR_GENOMEGENERATE", "harleenduggal/RNASEQ/BEDTOOLS_GENOMECOV"], "list_wf_names": ["harleenduggal/RNASEQ", "harleenduggal/NEXTFLOW-fastqc", "happykhan/nf-scripts", "h3abionet/recalling"]}, {"nb_reuse": 6, "tools": ["HISAT2", "SAMtools", "STAR", "BEDTools", "FastQC", "GATK"], "nb_own": 2, "list_own": ["h3abionet", "harleenduggal"], "nb_wf": 3, "list_wf": ["recalling", "NEXTFLOW-fastqc", "RNASEQ"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "harleenduggal", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 62, "codes": ["\nprocess run_haplotype_caller_on_x_nonpar_male {\n     tag { \"${params.project_name}.${sample_id}.rHCoXNPM\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_male_3\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.X_nonPAR.g.vcf.gz\") into x_nonpar_calls\n\t   set val(sample_id), file(\"${sample_id}.X_nonPAR.g.vcf.gz.tbi\") into x_nonpar_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L X -XL X:60001-2699520 -XL X:154931044-155260560 \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 1 \\\n     -O ${sample_id}.X_nonPAR.g.vcf.gz\n     \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    set sample_id, file(reads) from read_pairs2_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs  -q ${reads}\n    \"\"\"\n}", "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"        , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = (task.ext.args ?: '').tokenize()\n    def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_x_nonpar_male", "harleenduggal/NEXTFLOW-fastqc/fastqc", "harleenduggal/RNASEQ/SAMTOOLS_INDEX", "harleenduggal/RNASEQ/STAR_GENOMEGENERATE", "harleenduggal/RNASEQ/BEDTOOLS_GENOMECOV", "harleenduggal/RNASEQ/HISAT2_ALIGN"], "list_wf_names": ["harleenduggal/RNASEQ", "harleenduggal/NEXTFLOW-fastqc", "h3abionet/recalling"]}, {"nb_reuse": 5, "tools": ["HISAT2", "SAMtools", "BEDTools", "STAR", "GATK"], "nb_own": 2, "list_own": ["h3abionet", "harleenduggal"], "nb_wf": 2, "list_wf": ["recalling", "RNASEQ"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 61, "codes": ["\nprocess run_haplotype_caller_on_y_par1_male {\n     tag { \"${params.project_name}.${sample_id}.rHCoYP1M\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_male_4\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.Y_PAR1.g.vcf.gz\") into y_par1_calls\n\t   set val(sample_id), file(\"${sample_id}.Y_PAR1.g.vcf.gz.tbi\") into y_par1_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L Y:10001-2649520 \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 2 \\\n     -O ${sample_id}.Y_PAR1.g.vcf.gz\n     \"\"\"\n}", "process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"        , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = (task.ext.args ?: '').tokenize()\n    def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"versions.yml\"                        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_y_par1_male", "harleenduggal/RNASEQ/STAR_GENOMEGENERATE", "harleenduggal/RNASEQ/BEDTOOLS_GENOMECOV", "harleenduggal/RNASEQ/HISAT2_ALIGN", "harleenduggal/RNASEQ/SAMTOOLS_SORT"], "list_wf_names": ["harleenduggal/RNASEQ", "h3abionet/recalling"]}, {"nb_reuse": 4, "tools": ["HISAT2", "SAMtools", "Mgenome", "STAR", "GATK"], "nb_own": 2, "list_own": ["h3abionet", "harleenduggal"], "nb_wf": 2, "list_wf": ["recalling", "RNASEQ"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 61, "codes": ["\nprocess run_haplotype_caller_on_y_par2_male {\n     tag { \"${params.project_name}.${sample_id}.rHCoYP2M\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_male_5\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.Y_PAR2.g.vcf.gz\") into y_par2_calls\n\t   set val(sample_id), file(\"${sample_id}.Y_PAR2.g.vcf.gz.tbi\") into y_par2_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L Y:59034050-59363566 \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 2 \\\n     -O ${sample_id}.Y_PAR2.g.vcf.gz\n     \"\"\"\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_y_par2_male", "harleenduggal/RNASEQ/HISAT2_ALIGN", "harleenduggal/RNASEQ/SAMTOOLS_SORT", "harleenduggal/RNASEQ/RSEM_PREPAREREFERENCE"], "list_wf_names": ["harleenduggal/RNASEQ", "h3abionet/recalling"]}, {"nb_reuse": 3, "tools": ["gffread", "STAR", "GATK", "Mgenome"], "nb_own": 2, "list_own": ["h3abionet", "harleenduggal"], "nb_wf": 2, "list_wf": ["recalling", "RNASEQ"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 61, "codes": ["\nprocess run_haplotype_caller_on_y_nonpar_male {\n     tag { \"${params.project_name}.${sample_id}.rHCoYNPM\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_male_6\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.Y_nonPAR.g.vcf.gz\") into y_nonpar_calls\n\t   set val(sample_id), file(\"${sample_id}.Y_nonPAR.g.vcf.gz.tbi\") into y_nonpar_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L Y -XL Y:10001-2649520 -XL Y:59034050-59363566 \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 1 \\\n     -O ${sample_id}.Y_nonPAR.g.vcf.gz\n    \"\"\"\n}", "process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_y_nonpar_male", "harleenduggal/RNASEQ/RSEM_PREPAREREFERENCE", "harleenduggal/RNASEQ/GFFREAD"], "list_wf_names": ["harleenduggal/RNASEQ", "h3abionet/recalling"]}, {"nb_reuse": 2, "tools": ["Salmon", "GATK"], "nb_own": 2, "list_own": ["h3abionet", "harleenduggal"], "nb_wf": 2, "list_wf": ["recalling", "RNASEQ"], "list_contrib": ["JoseEspinosa", "grbot", "amayer21", "rfenouil", "alneberg", "arontommi", "abhi18av", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "c-mertes", "drpatelh", "lpantano", "ppericard", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "SpikyClip", "Emiller88", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "chris-cheshire", "mashehu", "jburos", "chuan-wang", "mamanambiya", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "KevinMenden", "george-hall-ucl", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "adomingues", "BABS-STP1", "mahesh-panchal", "Hammarn", "zxl124", "drpowell"], "nb_contrib": 61, "codes": ["\nprocess run_haplotype_caller_on_x_female {\n     tag { \"${params.project_name}.${sample_id}.rHCoXF\" }\n     memory { 4.GB * task.attempt }\n     cpus { 4 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), val(gender), val(bam_file) from samples_female\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.X.g.vcf.gz\") into x_calls\n\t   set val(sample_id), file(\"${sample_id}.X.g.vcf.gz.tbi\") into x_calls_indexes\n\n     script:\n     call_conf = 30               \n     if ( params.sample_coverage == \"high\" )\n       call_conf = 30\n     else if ( params.sample_coverage == \"low\" )\n       call_conf = 10\n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n     HaplotypeCaller \\\n     -R ${params.ref_seq} \\\n     -I $bam_file \\\n     --emit-ref-confidence GVCF \\\n     --dbsnp ${params.dbsnp_sites} \\\n     --L X \\\n     --genotyping-mode DISCOVERY \\\n     -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n     -stand-call-conf ${call_conf} \\\n     --sample-ploidy 2 \\\n     -O ${sample_id}.X.g.vcf.gz\n     \"\"\"\n\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_x_female", "harleenduggal/RNASEQ/SALMON_INDEX"], "list_wf_names": ["harleenduggal/RNASEQ", "h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess run_haplotype_caller_on_mt {\n    tag { \"${params.project_name}.${sample_id}.rHCoMT\" }\n    memory { 4.GB * task.attempt }\n    cpus { 4 }\n    publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n    input:\n    set val(sample_id), val(gender), file(bam_file) from samples_5\n\n    output:\n\t  set val(sample_id), file(\"${sample_id}.MT.g.vcf.gz\") into mt_calls\n\t  set val(sample_id), file(\"${sample_id}.MT.g.vcf.gz.tbi\") into mt_calls_indexes\n\n    script:\n    call_conf = 30               \n    if ( params.sample_coverage == \"high\" )\n      call_conf = 30\n    else if ( params.sample_coverage == \"low\" )\n      call_conf = 10\n    \"\"\"\n    ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_hc_mem}\"  \\\n    HaplotypeCaller \\\n    -R ${params.ref_seq} \\\n    -I $bam_file \\\n    --emit-ref-confidence GVCF \\\n    --dbsnp ${params.dbsnp_sites} \\\n    --L MT \\\n    --genotyping-mode DISCOVERY \\\n    -A Coverage -A FisherStrand -A StrandOddsRatio -A MappingQualityRankSumTest -A QualByDepth -A RMSMappingQuality -A ReadPosRankSumTest \\\n    -stand-call-conf ${call_conf} \\\n    --sample-ploidy 2 \\\n    -O ${sample_id}.MT.g.vcf.gz\n    \"\"\"\n}"], "list_proc": ["h3abionet/recalling/run_haplotype_caller_on_mt"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["h3abionet"], "nb_wf": 1, "list_wf": ["recalling"], "list_contrib": ["mamanambiya", "grbot"], "nb_contrib": 2, "codes": ["\nprocess combine_gVCFs {\n     tag { \"${params.project_name}.${sample_id}.cCgVCF\" }\n     memory { 4.GB * task.attempt }\n     cpus { 20 }\n     publishDir \"${params.out_dir}/${sample_id}\", mode: 'copy', overwrite: false\n\n     input:\n     set val(sample_id), file(gvcf) from all_calls\n\n     output:\n\t   set val(sample_id), file(\"${sample_id}.g.vcf.gz\") into combine_calls\n\t   set val(sample_id), file(\"${sample_id}.g.vcf.gz.tbi\") into combine_calls_indexes\n\n     script:\n     if (gvcf.size() == 29)                              \n     \"\"\"\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_sv_mem}\"  \\\n     SortVcf \\\n     -I ${sample_id}.X_PAR1.g.vcf.gz \\\n     -I ${sample_id}.X_PAR2.g.vcf.gz \\\n     -I ${sample_id}.X_nonPAR.g.vcf.gz \\\n     -O ${sample_id}.X.g.vcf.gz\n\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_sv_mem}\"  \\\n     SortVcf \\\n     -I ${sample_id}.Y_PAR1.g.vcf.gz \\\n     -I ${sample_id}.Y_PAR2.g.vcf.gz \\\n     -I ${sample_id}.Y_nonPAR.g.vcf.gz \\\n     -O ${sample_id}.Y.g.vcf.gz\n\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.1\\\\.g.vcf.gz\" > ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.2\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.3\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.4\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.5\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.6\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.7\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.8\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.9\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.10\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.11\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.12\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.13\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.14\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.15\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.16\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.17\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.18\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.19\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.20\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.21\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.22\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo ${sample_id}.X.g.vcf.gz >> ${sample_id}.gvcf.list\n     echo ${sample_id}.Y.g.vcf.gz >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.MT\\\\.g\\\\.vcf\\\\.gz\" >> ${sample_id}.gvcf.list\n    \n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_gv_mem}\"  \\\n     GatherVcfs \\\n     -I ${sample_id}.gvcf.list \\\n     -O ${sample_id}.g.vcf.gz # GatherVCF does not index the VCF. The VCF will be indexed in the next tabix operation.\n\n     ${params.tabix_base}/tabix -p vcf ${sample_id}.g.vcf.gz \n     \"\"\"\n     else if (gvcf.size() == 24)                                 \n     \"\"\"\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.1\\\\.g.vcf.gz\" > ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.2\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.3\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.4\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.5\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.6\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.7\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.8\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.9\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.10\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.11\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.12\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.13\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.14\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.15\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.16\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.17\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.18\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.19\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.20\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.21\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.22\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.X\\\\.g.vcf.gz\" >> ${sample_id}.gvcf.list\n     echo \"${gvcf.join('\\n')}\" | grep \"\\\\.MT\\\\.g\\\\.vcf\\\\.gz\" >> ${sample_id}.gvcf.list\n\n     ${params.gatk_base}/gatk --java-options \"-Xmx${params.gatk_gv_mem}\"  \\\n     GatherVcfs \\\n     -I ${sample_id}.gvcf.list \\\n     -O ${sample_id}.g.vcf.gz # GatherVCF does not index the VCF. The VCF will be indexed in the next tabix operation.\n     \n     ${params.tabix_base}/tabix -p vcf ${sample_id}.g.vcf.gz\n     \"\"\"\n}"], "list_proc": ["h3abionet/recalling/combine_gVCFs"], "list_wf_names": ["h3abionet/recalling"]}, {"nb_reuse": 1, "tools": ["CATH"], "nb_own": 1, "list_own": ["harryscholes"], "nb_wf": 1, "list_wf": ["giga"], "list_contrib": ["harryscholes"], "nb_contrib": 1, "codes": ["\nprocess concatenate_mgnify_and_cath_domains {\n    publishDir params.publish_dir, mode: \"copy\"\n\n    container \"julia:1.2.0-buster\"\n\n    input:\n    file cath_domain_sequences\n    file mgy_domain_sequences\n\n    output:\n    file \"cath_mgy_${params.superfamily}.dom.fa\" into cath_mgy_domain_sequences\n\n    \"\"\"\n    #!/usr/bin/env julia\n\n    using CATHBase\n\n    cath = readfasta(\"$cath_domain_sequences\")\n    mgy = filter!(readfasta(\"$mgy_domain_sequences\"), FullLengthSequence())\n    writefasta(\"cath_mgy_${params.superfamily}.dom.fa\", [cath; mgy])\n    \"\"\"\n\n       \"\"\"\n    // cat $cath_domain_sequences $mgy_domain_sequences > cath_mgy_${params.superfamily}.dom.fa\n    // \"\"\"\n}"], "list_proc": ["harryscholes/giga/concatenate_mgnify_and_cath_domains"], "list_wf_names": ["harryscholes/giga"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess FASTQC {\n    publishDir \"${params.outdir}/figures\", mode: \"copy\"\n    label \"high_computation\"\n\n    container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n\n    input:\n    path fastqs\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    fastqc -o . --threads $task.cpus $fastqs\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/FASTQC"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["noreturn", "TCGenerators", "SUMSTAT", "DNABarcodes", "nucloc", "Cgrid"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess ERROR_MODEL {\n    publishDir \"${params.outdir}/interm/error_model\", mode: \"copy\"\n    label \"process_high\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::pandas conda-forge::h5py conda-forge::scikit-learn conda-forge::bokeh\" : null)\n\n    input:\n    path h5\n    path meta_file\n\n    output:\n    path \"*.h5\", emit: h5\n    path \"*.html\", emit: html\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import argparse\n    from glob import glob\n\n    import pandas as pd\n    import numpy as np\n    import h5py\n    from sklearn.isotonic import IsotonicRegression\n\n    from bokeh.io import output_file, save\n    from bokeh.plotting import figure\n    from bokeh.layouts import gridplot\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def get_h5_keys():\n        h5_file = glob(\"*.h5\")[0]\n        handle = h5py.File(h5_file, 'r')\n\n        return list(handle.get('seq').keys())\n\n    def load_h5():\n        orientations = get_h5_keys()\n\n        indexes = {orient: [] for orient in orientations}\n        sequences = {orient: [] for orient in orientations}\n        qualities = {orient: [] for orient in orientations}\n\n        for h5 in sorted(glob(\"*.h5\")):\n            handle = h5py.File(h5, 'r')\n            for orient in orientations:\n                indexes[orient].append(handle.get('index/{}'.format(orient))[:])\n                sequences[orient].append(handle.get('seq/{}'.format(orient))[:])\n                qualities[orient].append(handle.get('qual/{}'.format(orient))[:])\n            handle.close()\n\n        fastq_data = {\n            orient: {'index': np.concatenate(indexes[orient]),\n                     'seq': np.vstack(sequences[orient]),\n                     'qual': np.vstack(qualities[orient])}\n            for orient in orientations\n        }\n        return fastq_data\n\n    def idx2seq(nb):\n        repr_b5 = \"{:08}\".format(int(np.base_repr(nb, 5)))\n        nucl = repr_b5.translate(str.maketrans(\"01234\", ''.join(NUCLS)))\n        return nucl\n\n\n    def compute_transition_matrix(fastq_data, barcodes, max_dist=$params.max_mismatches, \n                                  n_bases_max=int($params.n_bases), eps=1e-6):\n        '''\n        - Barcodes in a numpy array of nucleotide index (base 5)\n        -\n        '''\n\n        # barcodes_index = np.array([int(''.join(map(str, bc)), 5) for bc in barcodes])\n        bc_len = len(barcodes[0])\n        transition_matrix = np.zeros((4, 5, 40)) # Transitions (A,C,G,T) to (A,C,G,T,N) and quality_scores\n        generator = zip(fastq_data['index'], fastq_data['seq'], fastq_data['qual'])\n        n_bases = 0\n\n        mem = {}\n        while n_bases < n_bases_max:\n\n            try:\n                (code, intlist, qual) = next(generator)\n            except StopIteration:\n                print('Warning: not enough bases for the error model ({:,}/{:,})'\n                      .format(n_bases, n_bases_max))\n\n            if n_bases % 10*bc_len == 0:\n                print(\"{:,}/{:,}\".format(n_bases, n_bases_max), end='\\\\r')\n\n            if code in mem:\n                matching_bc = mem[code]\n            else:\n                matching_bc = barcodes[np.sum(barcodes != intlist[None, :], axis=1) <= max_dist]\n                mem[code] = matching_bc\n\n            if len(matching_bc) == 0:\n                continue\n\n            for bc in matching_bc:\n                tup, counts = np.unique(np.vstack((bc, intlist, qual-1)),\n                                        return_counts=True, axis=1)\n                transition_matrix[tup[0, :], tup[1, :], tup[2, :]] += counts\n                n_bases += bc_len\n\n        sums = transition_matrix.sum(axis=0)\n        sums[sums==0] = eps\n\n        transition_matrix /= sums\n\n        return transition_matrix\n    \n    def regression_model(freqs, deg=2, same=False, method='isotonic'):\n        '''\n        - qual: all measured quality scores when a transition was observed\n        - proportion of transitions for a given quality\n        '''\n\n        observed_transitions = (~np.isnan(freqs)) & (freqs>0)\n\n        x = np.arange(1, 41)[observed_transitions]\n        y = -np.log10(freqs[observed_transitions])\n\n        if len(x) == 0:\n            return np.zeros(40)\n\n        if method == 'polynomial':\n            z = np.polyfit(x, y, 3)\n            polynom = np.poly1d(z)\n            y_interp = 10**-polynom(np.arange(1, 41))\n        elif method == 'isotonic':\n            ir = IsotonicRegression(y_min=0, out_of_bounds='clip', increasing=not same)\n            ir.fit(x, y)\n            y_interp = 10**-ir.predict(np.arange(1, 41))\n        else:\n            print('Unknown method: {}. Aborting.'.format(method))\n            exit(1)\n        return y_interp\n\n    def compute_error_models(idx, barcodes, orient='fwd'):\n\n        barcodes_int = np.array(\n            [list(seq.translate(MAPPING_BASE5)) for seq in barcodes]\n        ).astype('uint8')\n\n        transitions = compute_transition_matrix(idx, barcodes_int)\n\n        transitions_interp = np.array(\n            [[regression_model(err_prob, same=(i==j))\n              for i, err_prob in enumerate(transition_from_orig)]\n             for j, transition_from_orig in enumerate(transitions)])\n\n        display_models_bokeh(transitions, transitions_interp, orient=orient)\n        print('Error model computed')\n\n        return transitions_interp\n\n    def display_models_bokeh(trans_matrix, trans_matrix_interp, orient='fwd'):\n        data = []\n\n        for matrix in [trans_matrix, trans_matrix_interp]:\n            # For each matrix, stores non-zero probabilities with transitions and quality\n            nz_info = np.nonzero(matrix)\n            data.append(np.vstack((matrix[nz_info], *nz_info)).T)\n\n        labels = np.array(['y1']*len(data[0]) + ['y_hat']*len(data[1]))\n\n        data = np.vstack(data)\n\n        transitions = [\"{}->{}\".format(NUCLS[i], NUCLS[j]) for i, j in data[:, [1, 2]].astype(int)]\n\n        data = pd.DataFrame({'transition': transitions,\n                             'quality': 1 + data[:, 3].astype(int),\n                             'P_err': data[:, 0],\n                             'type': labels})\n\n        data.set_index(['transition', 'type'], inplace=True)\n\n        plots = []\n        tools = ['hover', 'box_zoom', 'reset']\n\n        observed_transitions = set(pd.MultiIndex.get_level_values(data.index,'transition'))\n        for i, nucl_i in enumerate(NUCLS[:-1]):\n            for j, nucl_j in enumerate(NUCLS):\n                transition_label = \"{}->{}\".format(nucl_i, nucl_j)\n                if transition_label not in observed_transitions:\n                    plots.append(None)\n                    continue\n\n                data_s = data.loc[transition_label].sort_values(by='quality')\n\n                p = figure(title=\"{}->{}\".format(nucl_i, nucl_j), tooltips=[], tools=tools,\n                           x_range=(0, 40), y_range=(-0.1, 1.1))\n                p.circle(x='quality', y='P_err', source=data_s.loc[['y1']], alpha=0.7, size=8)\n                p.line(x='quality', y='P_err', source=data_s.loc[['y_hat']], color='red',\n                       line_dash=\"dashed\", line_width=2)\n                plots.append(p)\n\n        grid = gridplot(plots, ncols=5, plot_width=300, plot_height=300)\n        output_file(\"error_model_{}.html\".format(orient))\n        save(grid)\n\n    # main script\n    barcodes = (pd.read_csv(\"$meta_file\", dtype=str, names=[\"sample_name\", \"fwd\", \"rev\"])\n                .dropna(axis=1, how='all')\n                .drop('sample_name', axis=1))\n\n    fastq_data = load_h5()\n\n    transition_h5 = h5py.File('transition_probs.h5', 'w')\n    for (orient, val) in fastq_data.items():\n        error_trans = compute_error_models(val, barcodes[orient].unique(), orient=orient)\n        transition_h5.create_dataset(orient, data=error_trans, dtype='f4')\n    transition_h5.close()\n    \"\"\"\n    \n}"], "list_proc": ["hawaiidatascience/metaflowmics/ERROR_MODEL"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["chopsticks", "GOsummaries", "Count"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess SAMPLE_SIZE_DISTRIBUTION {\n    publishDir \"${params.outdir}/figures\", mode: \"copy\"\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::pandas conda-forge::bokeh\" : null)\n\n    input:\n    path counts\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from math import log10\n    from pathlib import Path\n\n    import numpy as np\n    import pandas as pd\n\n    from bokeh.plotting import figure\n    from bokeh.io import save, output_file\n    from bokeh.models import tickers\n\n\n    def count_samples(folder='.'):\n        '''\n        Count the number of read pair/sample in [folder]\n        '''\n\n        files = Path(folder).glob(\"sample_counts*.csv\")\n        summaries = pd.concat([pd.read_csv(summary, index_col=0, dtype=str, header=None)\n                               for summary in files],\n                              axis=1, sort=True)\n\n        return summaries.fillna(0).astype(int).sum(axis=1)\n\n    def plot_bokeh(counts):\n\n        hist, edges = np.histogram(np.log10(counts), bins=max(5, len(counts)//10), density=False)\n        hist_df = pd.DataFrame({'count': hist,\n                                \"left\": edges[:-1],\n                                \"right\": edges[1:]})\n        hist_df[\"interval\"] = [\"{:,} - {:,}\".format(int(10**left), int(10**right))\n                               for left, right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n\n        x_min = int(min(edges))\n        x_max = max(4, 1+int(max(edges)))\n\n        p = figure(plot_height=800, plot_width=800,\n                   x_range=[x_min, x_max], tools='hover,box_zoom',\n                   tooltips=[('Size range', '@interval'),\n                             ('#Samples in interval', str(\"@count\"))],\n                    title=\"Sample size distribution\",\n                    x_axis_label=\"Sample read count\",\n                    y_axis_label=\"Occurrences\")\n\n        p.quad(bottom=0, top=\"count\", left=\"left\", \n               right=\"right\", source=hist_df, fill_color=\"SteelBlue\", \n               line_color=\"black\", fill_alpha=0.7,\n               hover_fill_alpha=1.0, hover_fill_color=\"Tan\")\n\n        ticks = list(range(x_min, x_max))\n        minor_ticks = np.log10([i*10**j for i in range(1, 10) for j in ticks])\n\n        p.xaxis.ticker = tickers.FixedTicker(ticks=ticks, minor_ticks=minor_ticks)\n        p.xaxis.major_label_overrides = {tick: \"{:,}\".format(int(10**tick)) for tick in ticks}\n        p.yaxis.minor_tick_line_color = None\n\n        p.axis.major_label_text_font_size = \"12pt\"\n        p.axis.axis_label_text_font_size = \"14pt\"\n        p.title.text_font_size = \"18pt\"\n\n        output_file('sample_sizes.html')\n        save(p)\n\n    # main script\n    counts = count_samples()\n    counts.to_csv('sample_sizes.csv', header=False)\n    plot_bokeh(counts)\n    \"\"\"\n    \n}"], "list_proc": ["hawaiidatascience/metaflowmics/SAMPLE_SIZE_DISTRIBUTION"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["IDSM"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess WRITE_SAMPLES_TO_FASTQ {\n    tag \"$split\"\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(split), path(fastq), path(mapping)\n\n    output:\n    path \"*.fastq\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from Bio.SeqIO.QualityIO import FastqGeneralIterator\n    import pandas as pd\n\n    demux_info = pd.read_csv(\"$mapping\", header=None, sep=\"\\\\t\", dtype=str, index_col=0)\n    \n    if demux_info[2].count() == 0:\n        demux_info.drop(columns=[2], inplace=True)\n        demux_info.columns = [\"fwd\", \"sample_name\", \"mismatches\"]\n    else:\n        demux_info.columns = [\"fwd\", \"rev\", \"sample_name\", \"mismatches\"]\n\n    read_orient = [\"fwd\", \"rev\"][:${fastq.size()}]\n    \n    print(\"Preparing handles.\")\n    handles = {}\n    for sample in demux_info[\"sample_name\"].unique():\n        if not pd.isnull(sample):\n            for i, orient in enumerate(read_orient, 1):\n                handles[sample+orient] = open(f\"{sample}_R{i}.fastq\", \"w\")\n\n    parsers = [FastqGeneralIterator(open(fastq, \"r\")) for fastq in \"$fastq\".split()]\n\n    print(\"Starting demultiplexing\")\n    for seq_nb, sequences in enumerate(zip(*parsers)):\n        ids = [seq[0].split()[0] for seq in sequences]\n\n        if len(ids) > 1:\n            if ids[0] != ids[1]:\n                print(\"Sequence #{}: {} (fwd) and {} (rev) do not match. The forward and reverse read files seem to be out of order\"\n                      .format(seq_nb, *ids))\n                exit(42)\n\n        sample_assignment = demux_info.loc[ids[0], \"sample_name\"]\n\n        if pd.isnull(sample_assignment):\n            continue\n\n        for orient, seq in zip(read_orient, sequences):\n            handles[sample_assignment + orient].write(\"@{}\\\\n{}\\\\n+\\\\n{}\\\\n\".format(*seq))\n\n    for sample in demux_info[\"sample_name\"].unique():\n        if not pd.isnull(sample):\n            for orient in read_orient:\n                handles[sample + orient].close()\n\n    print(\"Demultiplexing finished.\")\n    \"\"\"\n    \n}"], "list_proc": ["hawaiidatascience/metaflowmics/WRITE_SAMPLES_TO_FASTQ"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Alphabet"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess COUNT_KMERS {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::numpy\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"ref*.npy\", emit: kmers\n    path \"freqs*.npy\", emit: freqs\n\n    script:\n    if (params.feature == 'nucl') {\n        alphabet = \"ACGT\"\n        base = 2\n    } else {\n        alphabet = \"ARNDCQEGHILKMFPSTWYV\"\n        base = 5\n    }\n    \"\"\"\n    #!/usr/bin/env python\n\n    from itertools import product\n    import numpy as np\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    alphabet = \"$alphabet\"\n    N = len(alphabet)\n    k = $params.k\n\n    kmer_idx = {''.join(x): i for i, x in enumerate(product(alphabet, repeat=k))}\n\n    n_db = sum(1 for l in open(\"$fasta\") if l.startswith('>'))\n\n    ref = np.zeros((n_db, N**k), dtype=np.uint16)\n\n    with open(\"$fasta\") as r:\n        for i, (_, s) in enumerate(SimpleFastaParser(r)):\n            # remove potential gaps\n            s = s.replace('-', '')\n            # compute kmer counts\n            kmer_indices = [kmer_idx[s[i:i+k]] for i in range(len(s)-k+1) if s[i:i+k] in kmer_idx]\n            occurrences = np.bincount(kmer_indices, minlength=N**k)\n\n            ref[i] = occurrences\n\n            if i % 1000 == 0:\n                print(f\"{i:,}/{n_db:,} sequences processed\")\n\n    kmer_usage = np.argsort(-(ref > 0).sum(axis=0))\n\n    np.save(f\"freqs_{k}mers.npy\", kmer_usage)\n    np.save(f\"ref_{k}mers.npy\", ref)\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/COUNT_KMERS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["iRefScape", "LineagePulse"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess UPDATE_MSA_WITH_REF {\n    label \"process_low\"\n    publishDir params.outdir, mode: params.publish_dir_mode\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge:pandas\" : null)\n\n    input:\n    path aln\n    path ref\n\n    output:\n    path \"*.updated.afa\", emit: afa\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    def map_on_ref(ref, query):\n        query_updated = []\n        pos = 0\n        for aa in ref:\n           if aa != '-':\n               query_updated.append(query[pos])\n               pos += 1\n           else:\n               query_updated.append('-')\n               \n        return ''.join(query_updated)\n\n    refs = {}\n    with open(\"$ref\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            refs[tax] = seq\n\n    with open(\"$aln\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            seq_updated = map_on_ref(refs[tax], seq)\n      \n            with open(f\"{tax}.updated.afa\", \"a\") as writer:\n                writer.write(f\">{title}\\\\n{seq_updated}\\\\n\")    \n    \"\"\"    \n}"], "list_proc": ["hawaiidatascience/metaflowmics/UPDATE_MSA_WITH_REF"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_REMOVE_LINEAGE {\n    tag \"$otu_id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.44.1--hf0cea05_2\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.44.1\" : null)\n\n    input:\n    tuple val(otu_id), file(list), file(constax), file(shared), file(count_table)\n\n    output:\n    tuple val(otu_id), path(\"${outprefix}.cons.taxonomy\"), emit: constaxonomy\n    tuple val(otu_id), path(\"${outprefix}.list\"), emit: list\n    tuple val(otu_id), path(\"${outprefix}.shared\"), emit: shared\n    tuple val(otu_id), path(\"${outprefix}.count_table\"), emit: count_table    \n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = options.suffix ? \"${options.suffix}.${otu_id}\" : \"${procname}.${otu_id}\"\n    \"\"\"\n    mothur \"#remove.lineage(constaxonomy=$constax, list=$list, shared=$shared, taxon='$params.taxa_to_filter');list.seqs(list=current);get.seqs(accnos=current,count=$count)\"\n    # rename outputs\n    mv *.pick.list ${outprefix}.list\n    mv *.pick.cons.taxonomy ${outprefix}.cons.taxonomy\n    mv *.pick.shared ${outprefix}.shared\n    mv *.pick.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d'=' -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_REMOVE_LINEAGE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["chimera", "UniqueProt"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DADA2_CHIMERA {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(contigs)\n\n    output:\n    tuple val(meta), path(\"*.RDS\"), emit: rds    \n    path(\"*.csv\"), emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n\n    derep <- readRDS(\"$contigs\")\n    chimera <- isBimeraDenovo(derep, verbose=T)\n\n    kept <- (1:length(chimera))[!chimera]\n\n    # Remove uniques sequences identified as chimeric\n    derep[[\"uniques\"]] <- derep[[\"uniques\"]][kept]\n\n    # Find new maximum read length (in case we removed the longest)\n    max_len <- max(sapply(names(derep[[\"uniques\"]]), nchar))\n    # Subset the qualities\n    derep[[\"quals\"]] <- derep[[\"quals\"]][kept, 1:max_len]\n\n    # Subset the read -> unique mapping\n    derep[[\"map\"]] <- derep[[\"map\"]][which(derep[[\"map\"]] %in% kept)]\n\n    # Write counts\n    counts <- getUniques(derep)\n    data <- sprintf(\"chimera,,${meta.id},%s,%s\",sum(counts),sum(counts>0))\n    write(data, \"summary.csv\")\n\n    saveRDS(derep, \"${meta.id}.nochim.RDS\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DADA2_CHIMERA"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["sabund"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DADA2_DADA {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-stringr\" : null)\n\n    input:\n    tuple val(meta), path(reads), path(errors)\n\n    output:\n    tuple val(meta), path(\"*.denoised.RDS\"), emit: denoised\n    tuple val(meta), path(\"*.derep.RDS\"), emit: derep, optional: true\n    path \"summary.csv\", optional: true, emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def suffix = meta.paired_end ? \"_${meta.orient}\" : \"\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n\n    derep_files <- sort(list.files(\".\", pattern=\".[^(errors)].RDS\"))\n    sample_names <- gsub(\"${suffix}\\\\\\\\.[a-z]+.RDS\", \"\", derep_files)\n    derep <- lapply(derep_files, readRDS)\n    names(derep) <- sample_names\n\n    err <- readRDS(\"$errors\")\n\n    denoised <- dada(derep, err=err, multithread=TRUE, pool=$params.pool)\n\n    if (length(sample_names) == 1) {\n        denoised <- list($meta.id=denoised)\n    }\n\n    sapply(names(denoised), function(x) saveRDS(denoised[[x]], sprintf(\"%s${suffix}.denoised.RDS\", x)))\n\n    # Write counts\n    if (\"$meta.orient\" == \"R1\") {\n        counts <- lapply(denoised, getUniques)\n        abund <- sapply(counts, sum)\n        richness <- sapply(counts, function(x) sum(x>0))\n        data <- sprintf(\"denoising,,%s,%s,%s\",sample_names,abund,richness)\n        write(data, \"summary.csv\")\n    }\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DADA2_DADA"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_CLASSIFY_OTUS {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count), file(tax)\n\n    output:\n    tuple val(meta), path(\"*.cons.taxonomy\"), emit: taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    list.seqs(list=$list); get.seqs(taxonomy=$tax, accnos=current);\n    classify.otu(taxonomy=current, list=current, count=$count, probs=f)\"\n\n    # rename output\n    mv *.cons.taxonomy ${outprefix}.cons.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_CLASSIFY_OTUS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_CLUSTER {\n    tag \"${otu_id}\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(count), path(tax)\n    each otu_id\n\n    output:\n    tuple val(meta_upd), path(\"*.list\"), emit: list\n    tuple val(meta_upd), path(\"*.shared\"), emit: shared\n    tuple val(meta_upd), path(fasta), emit:fasta\n    tuple val(meta_upd), path(count), emit:count_table\n    tuple val(meta_upd), path(tax), emit:taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    meta_upd = meta + [id: \"${otu_id}\", otu_id: otu_id]\n    def ext = [\"rep.fasta\", \"cons.taxonomy\", \"shared\", \"list\", \"database\"]\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta_upd.id}\"\n    \"\"\"\n    mothur \"#\n    cluster(count=$count, fasta=$fasta, method=${otu_id == 100 ? \"unique\" : \"dgc\"}, cutoff=${(100-meta_upd.otu_id) / 100});\n    make.shared(count=current, list=current)\"\n\n    mv *.list ${outprefix}.list\n    mv *.shared ${outprefix}.shared\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_CLUSTER"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["VSEARCH"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess VSEARCH_SINTAX{\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"\n    conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    path database\n\n    output:\n    tuple val(meta), path(\"annotations_*.tsv\"), emit: taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch \\\\\n        --threads $task.cpus \\\\\n        --db $database \\\\\n        --sintax $fasta \\\\\n        --sintax_cutoff $params.tax_confidence \\\\\n        --tabbedout annotations_sintax-${meta.id}.tsv\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/VSEARCH_SINTAX"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["FastTree"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess FASTTREE {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? \"bioconda::fasttree\" : null)\n    container \"quay.io/biocontainers/fasttree:2.1.8--h779adbc_6\"\n\n    input:\n    tuple val(meta), path(repfasta)\n\n    output:\n    tuple val(meta), path(\"*.nwk\"), emit: nwk\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\t\n    \"\"\"\n    FastTree -nt $repfasta > fasttree.${meta.id}.nwk\n\n    fasttree 2>&1 >/dev/null | head -1 | sed 's/.*version \\\\([0-9\\\\.]*\\\\).*/\\\\1/g' \\\\\n    > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/FASTTREE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["SummaryAUC"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess SUMMARIZE_TABLE {\n    tag \"$step\"\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)\n\n    input:\n    tuple val(meta), val(step), path(table)\n\n    output:\n    tuple val(meta), file('summary.csv')\n\n    script:\n    drop = 'NULL'\n    sep = 'auto'\n    rownames = 1\n    taxa_are_rows = 'T'\n\totu_id = meta.otu_id ?: ''\n    \n    if (table.getExtension() == 'count_table') {\n        drop = \"'total'\"\n        sep = '\\\\t'\n        rownames = 1\n        taxa_are_rows = 'T'\n    }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    table <- data.frame(\n        fread(\"$table\", drop=$drop, sep='$sep'),\n        row.names=$rownames, check.names=F\n    )\n\n    if (!$taxa_are_rows) {\n        table <- t(table)\n    }\n    \n    summary <- cbind(\n        rep('$step', ncol(table)),\n        rep('$otu_id', ncol(table)),\n        colnames(table),\n        colSums(table),\n        colSums(table > 0)\n    )\n\n    write.table(summary, 'summary.csv', quote=F, row.names=F, col.names=F, sep=',')    \n    \"\"\"\n    \n}"], "list_proc": ["hawaiidatascience/metaflowmics/SUMMARIZE_TABLE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Threshold-seq"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess GET_SUBSAMPLING_THRESHOLD {\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)\n\n    input:\n    path count\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    table <- data.frame(fread(\"$count\", drop=c(2)), row.names=1, check.names=F)\n    sample_sizes <- colSums(table)\n    threshold <- floor(quantile(sample_sizes, ${params.subsampling_quantile}, names=F))\n\n    if (threshold < $params.min_subsampling) {\n        threshold <- ifelse($params.min_subsampling < max(sample_sizes), \n                            $params.min_subsampling,\n                            threshold)\n    }\n\n    cat(threshold)\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/GET_SUBSAMPLING_THRESHOLD"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["sabund"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess CONVERT_TO_MOTHUR_FORMAT {\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }\t\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)\n\n    input:\n    tuple val(meta), path(\"abundance.tsv\"), path(\"taxonomy.csv\")\n\n    output:\n    tuple val(meta), path(\"*.shared\"), emit: shared\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    abund <- data.frame(fread(\"abundance.tsv\"), row.names=1, check.names=F)\n\n    if ($params.taxa_are_rows) {\n        abund <- t(abund)\n    }\n    \n    shared <- cbind(\n        rep(1-$meta.otu_id/100, nrow(abund)),\n        rownames(abund),\n        rep(ncol(abund), nrow(abund)),\n        abund\n    )\n    colnames(shared) <- c('label', 'Group', 'numOtus', colnames(abund))\n    write.table(shared, \"OTUs.${meta.id}.shared\", quote=F, sep='\\\\t', row.names=F)\n\n    tax <- data.frame(fread(\"taxonomy.csv\"), row.names=1, check.names=F)\n    rownames(tax) <- gsub(';.*', '', rownames(tax))\n    rank_names <- colnames(tax)\n\n    tax <- cbind(\n        rownames(tax), \n        colSums(abund), \n        apply(tax, 1, function(x) paste(x, collapse=';'))\n    )\n    colnames(tax) <- c('OTU', 'Size', 'Taxonomy')\n    tax[, 'Taxonomy'] <- gsub('[a-z]__', '', tax[, 'Taxonomy'])\n\n    write.table(tax, \"OTUs.${meta.id}.taxonomy\", quote=F, sep='\\\\t', row.names=F)\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/CONVERT_TO_MOTHUR_FORMAT"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 2, "tools": ["sabund", "RTREE", "Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_GET_SEQS {\n    tag \"$meta.id\"\n    label \"process_low\"\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(ref), file(filt)\n\n    output:\n    tuple val(meta), path(\"*.pick.${filt.getExtension()}\")\n\n    script:\n    def ref_ext = ref.getExtension()\n    def arg = filt.getExtension().replaceAll(\"_table\", \"\")\n    \"\"\"\n    mothur \"#list.seqs(${ref_ext}=$ref);get.seqs(accnos=current,$arg=$filt)\"\n    \"\"\"\n}", "\nprocess PHYLOSEQ_UNIFRAC {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process))}\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-phyloseq::1.34.0 conda-forge::r-data.table\" : null)\n\n    input:\n    tuple val(meta), path(shared), path(tree)\n\n    output:\n    tuple val(meta), path(\"unifrac*.csv\"), emit: csv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n    library(phyloseq)\n\n    abund <- data.frame(\n        fread(\"$shared\", drop=c(\"label\", \"numOtus\"), sep=\"\\\\t\"),\n        row.names=1, check.names=F\n    )\n\n    tree <- read_tree(\"$tree\")\n\n    ps <- phyloseq(\n        otu_table(as.matrix(abund), taxa_are_rows=FALSE),\n        phy_tree(tree)\n    )\n\n    dists <- UniFrac(ps, weighted=(\"$params.unifrac\"==\"weighted\"), parallel=TRUE)\n\n    write.csv(as.matrix(dists), \"unifrac_${params.unifrac}_${meta.id}.csv\")\n\n    writeLines(paste0(packageVersion(\"phyloseq\")), \"${software}.version.txt\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_GET_SEQS", "hawaiidatascience/metaflowmics/PHYLOSEQ_UNIFRAC"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 2, "tools": ["Consensus", "Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess COMPUTE_MSA_REPRESENTATIVE {\n    label \"process_low\"\n    publishDir params.outdir, mode: params.publish_dir_mode\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge:pandas\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"*.repr.fa\", emit: repr\n\n    script:\n    def prefix = fasta.getSimpleName()\n\tdef skip_gap = params.skip_gap ? \"if letter == '-': continue\" :\"\"\n    \"\"\"\n    #!/usr/bin/env python\n\n    from collections import defaultdict\n    import re\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n    import pandas as pd\n\n    aln_len = len(next(SimpleFastaParser(open(\"$fasta\")))[1])\n\n    freqs = [defaultdict(lambda: 0) for _ in range(aln_len)]\n\n    with open(\"$fasta\", \"r\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            for (i, letter) in enumerate(seq):\n\t            $skip_gap\n\t            freqs[i][letter] += 1\n\n    freqs = pd.DataFrame(freqs).fillna(0).astype(int)\n\n    consensus = ''.join(pd.DataFrame(freqs).idxmax(axis=1))\n    cons_lineage = title.split()[1]\n\n    with open(\"${prefix}.repr.fa\", \"w\") as writer:\n        writer.write(f\">repr|${prefix} {cons_lineage}\\\\n{consensus}\\\\n\")\n    \"\"\"\n}", "\nprocess MOTHUR_MAKE_SHARED {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count)\n\n    output:\n    tuple val(meta), path(\"*.shared\"), emit: shared\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#make.shared(count=$count, list=$list)\"\n\n    # rename output\n    mv *.shared ${outprefix}.shared\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/COMPUTE_MSA_REPRESENTATIVE", "hawaiidatascience/metaflowmics/MOTHUR_MAKE_SHARED"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_ALIGN_SEQS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    path db_aln\n\n    output:\n    tuple val(meta), path(\"*.fasta\"), emit: fasta\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    align.seqs(fasta=$fasta, reference=$db_aln);\n    filter.seqs(fasta=current, vertical=T);\"\n\n    mv *.filter.fasta ${outprefix}.fasta\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_ALIGN_SEQS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Proteins API", "ProteinART", "LineagePulse"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess TRANSLATE {\n    tag \"$meta\"\n    label \"process_low\"\n    publishDir params.outdir, mode: params.publish_dir_mode\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::bipython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*_main.faa\"), emit: main\n    tuple val(meta), path(\"*_mult.faa\"), emit: mult\n    tuple val(meta), path(\"*_other.faa\"), emit: other\n                                        \n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n    from Bio.Seq import Seq\n    from Bio.Data import CodonTable\n\n    def translate(seq, table):\n        proteins = []\n\n        for frame in range(0, 3):\n            nucl_length = len(seq) - frame\n            prot_length = nucl_length // 3\n            dna = seq[frame:3*prot_length+frame]\n            protein = seq.translate(table=table)\n            if '*' not in protein:\n                proteins.append((frame, protein))\n                continue\n\n            protein = dna.reverse_complement().translate(table=table)\n            if '*' not in protein:\n                protein.description = \"{protein.description} rc\"\n                proteins.append((frame, protein))\n        return proteins\n\n    with open(\"$fasta\", \"r\") as reader, \\\\\n         open(\"${fasta.getBaseName()}_main.faa\", \"w\") as writer, \\\\\n         open(\"${fasta.getBaseName()}_mult.faa\", \"w\") as writer_mult, \\\\\n         open(\"${fasta.getBaseName()}_other.faa\", \"w\") as writer_other:\n        for (title, seq) in SimpleFastaParser(reader):\n\n            if any(x not in \"ACGT\" for x in seq):\n                writer_other.write(f\">{title} | unknown_nucleotide \\\\n{seq}\\\\n\")\n                continue\n\n            lineage = dict(zip(\n                [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"subfamily\", \"genus\", \"species\"],\n                title.split(';')\n            ))\n\n            seq = Seq(seq)\n\n            proteins = translate(seq, $params.table)\n\n            if len(proteins) == 0:\n                proteins = [prot for table in CodonTable.ambiguous_generic_by_id.keys()\n                            for prot in translate(seq, table)]\n                handle = writer_other\n            elif len(proteins) == 1:\n                handle = writer\n            elif len(proteins) > 1:\n                handle = writer_mult\n            \n            for (frame, protein) in proteins:\n                handle.write(f\">{title}|frame={frame};table={$params.table}\\\\n{protein}\\\\n\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/TRANSLATE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_MAKE_DATABASE {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }    \n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(shared), file(constax), file(repfasta), file(repcount)\n\n    output:\n    tuple val(meta), path(\"*.database\"), emit: database\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#create.database(shared=$shared,repfasta=$repfasta,constaxonomy=$constax,count=$repcount)\"\n    mv *.database ${outprefix}.database\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_MAKE_DATABASE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_CLASSIFY_SEQS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }        \n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(count)\n    path(db_aln)\n    path(db_tax)\n\n    output:\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#classify.seqs(fasta=$fasta, count=$count, template=$db_aln, taxonomy=$db_tax, cutoff=$params.classification_consensus)\"\n    # rename output\n    mv *.wang.taxonomy ${outprefix}.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_CLASSIFY_SEQS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["VSEARCH"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess VSEARCH_CLUSTER {\n    tag \"$otu_id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"\n    conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    each otu_id\n\n    output:\n    tuple val(meta_upd), path(\"vsearch_OTUs-*.tsv\"), emit: tsv\n    tuple val(meta_upd), path(\"vsearch_OTUs-*.fasta\"), emit: fasta\n                                        \n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n                                    \n\tmeta_upd = meta.clone()\n\tmeta_upd[\"id\"] = \"${otu_id}\"\n\tmeta_upd[\"otu_id\"] = otu_id\t\n    \"\"\"\n    #!/usr/bin/env bash\n    vsearch $options.args \\\\\n        --threads $task.cpus \\\\\n        --sizein \\\\\n        --sizeout \\\\\n        --fasta_width 0 \\\\\n        --id ${otu_id/100} \\\\\n        --strand plus \\\\\n        --cluster_size ${fasta} \\\\\n        --uc clusters${otu_id}.uc \\\\\n        --relabel OTU${otu_id}_ \\\\\n        --centroids vsearch_OTUs-${otu_id}.fasta \\\\\n        --otutabout vsearch_OTUs-${otu_id}.tsv\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/VSEARCH_CLUSTER"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_DIST_SEQS {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(fasta)\n\n    output:\n    tuple val(meta), path(\"$outprefix*.dist\"), emit: dist\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def cutoff = params.cutoff ?: 1\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"    \n    \"\"\"\n    # Manually rename sequences\n    # sed \"/^>/s/.*\\\\(Otu[0-9]*\\\\)\\\\(.*\\\\)/>\\\\1\\\\t\\\\1\\\\2/\" $fasta > renamed.fasta\n\n    mothur \"#filter.seqs(fasta=$fasta, trump=.); dist.seqs(fasta=current, cutoff=$cutoff)\"\n\n    if [ \"${params.format.toLowerCase()}\" == \"vsearch\" ]; then\n        awk '{OFS=\"\\\\t\"}{print \\$1,\\$2,100*(1-\\$3)}' *.dist > ${outprefix}.dist\n    else\n        mv *.dist ${outprefix}.dist\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_DIST_SEQS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_SUMMARY_SINGLE {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        pattern: \"*.summary\",\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), val(step), file(shared)\n\n    output:\n    tuple val(meta), path(\"*.{csv,summary}\"), emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def ext = shared.getBaseName()\n    \"\"\"\n    mothur \"#summary.single(shared=$shared, calc=$params.calc)\"\n\n    # summary\n    if [ \"$params.calc\" = \"nseqs-sobs\" ]; then\n        cut -f2- *.groups.summary | tail -n+2 \\\\\n        | awk '{OFS=\",\"}{print \"$step\",\"$meta.otu_id\",\\$1,int(\\$2),int(\\$3)}' \\\\\n        | tr \"\\\\t\" \",\" \\\\\n        > ${ext}.csv\n        rm -f *.groups.summary\n    else\n        mv *.summary alpha-diversity.${meta.otu_id}.summary\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_SUMMARY_SINGLE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["RANKS"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess CLEAN_TAXONOMY {\n    label \"process_low\"\n    publishDir \"${params.outdir}/clean_taxonomy\", mode: params.publish_dir_mode\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"*.with_tax.fasta\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n\timport re\n\timport pandas as pd\n\tfrom Bio.SeqIO.FastaIO import SimpleFastaParser\n\n\tRANKS = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n\n\tdef format_taxon_name(taxon):\n\t    taxon_clean = re.sub(\"[^A-Za-z0-9_-]\", \"_\", taxon.strip())\n\t    taxon_clean = re.sub(\"__+\", \"_\", taxon_clean)\n\t    return taxon_clean\n\n\tdef fill_missing_ranks(lineage):\n\t\tmissing = [i for i,v in enumerate(lineage) if not v][::-1]\n\t\twhile missing:\n\t\t\tidx = missing.pop()\n\t\t\tprev = lineage[idx-1] # assume the highest level is always known\n\t\t\tprev_rank = RANKS[idx-1][0]\n\t\t\tparts = prev.split(\"__\")\n\n\t\t\tnew_name = f\"{prev_rank}__{parts[-1]}\"\n\t\t\tif len(parts) > 1:\n\t\t\t\tprefix = \"__\".join(parts[:-1])\n\t\t\t\tnew_name = f\"{prefix}__{new_name}\"\n\t\t\tlineage[idx] = new_name\n\t\treturn lineage\n\n\tcriteria = {}\n\tuniq_seq = {}\n\n\twith open(\"$fasta\", \"r\") as reader:\n\t    for i, (title, seq) in enumerate(SimpleFastaParser(reader)):\n\t        info = title.strip().split()\n\t        lineage = \" \".join(info[1:])\n\t        lineage = lineage.split(\"$params.sep\")\n\t        lineage = [format_taxon_name(taxon) for taxon in lineage]\n\n\t        n_missing = sum(1 for x in lineage if not x)\n\t        # check if duplicated\n\t        if seq in uniq_seq and n_missing >= criteria[seq]:\n\t            continue\n\t        criteria[seq] = n_missing\n\n\t        lineage = \"$params.sep\".join(\n              fill_missing_ranks(lineage)\n            )\n\t        uniq_seq[seq] = f'{info[0]} {lineage}'\n\n\twith open(\"${fasta.getBaseName()}.with_tax.fasta\", \"w\") as writer:\n\t    for (seq, header) in uniq_seq.items():\n\t        writer.write(f'>{header}\\\\n{seq}\\\\n')\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/CLEAN_TAXONOMY"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_SUMMARY_SHARED {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(shared)\n\n    output:\n    path \"*.summary\", emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def ext = shared.getBaseName()\n    \"\"\"\n    mothur \"#summary.shared(shared=$shared, calc=${params.calc}, distance=lt)\"\n    mv *.summary beta-diversity.${meta.otu_id}.summary\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_SUMMARY_SHARED"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["seqtk", "tranalign"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess EMBOSS_TRANALIGN {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-biotools:0.0.1\"\n    conda (params.enable_conda ? \"bioconda::emboss=6.6.0\" : null)\n\n    input:\n    tuple val(meta), path(afa), path(fna)\n\n    output:\n    tuple val(meta), path(\"*.codons.afa\"), emit: fna\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix = fna.getSimpleName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    tranalign \\\\\n      -asequence $fna \\\\\n      -bsequence $afa \\\\\n      -outseq stdout \\\\\n      -table 5 |\n        seqtk seq -l0 > rev.afa\n\n    [ -s rev.afa ] && echo \"tranalign was successfull\" || (echo \"Something went wrong. Are sequences in the same order?\" && exit 1)\n\n    aln_len=\\$(grep -v '>' rev.afa | wc -L)\n\n    awk -v L=\\$aln_len '{if (NR % 2 == 0 && length(\\$1)<1707) \\$1=\\$1\"-\"; print}' rev.afa \\\\\n        > ${prefix}.codons.afa\n\n    tranalign -h 2>&1 | grep -i version | cut -d':' -f3 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/EMBOSS_TRANALIGN"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["AbDesign"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess BUILD_ASV_TABLE {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-r:0.0.1\"    \n    conda (params.enable_conda ? \"conda-forge::r-stringr bioconda::bioconductor-dada2=1.18 conda-forge::r-seqinr\" : null)\n\n    input:\n    path(rds)\n\n    output:\n    path \"ASVs_duplicates_to_cluster.fasta\", optional: true, emit: fasta_dup\n    path \"ASVs-100.fasta\", emit: fasta    \n    path \"ASVs-100.{count_table,tsv}\", emit: count_table\n                                          \n\n    script:\n                                                   \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n    library(seqinr)\n\n    # Collect denoised reads\n    denoised <- list.files(path=\".\", pattern=\"*.denoised.RDS\")\n\n    sample_names <- unname(sapply(denoised, function(x) gsub(\".denoised.RDS\", \"\", x)))\n    merged <- lapply(denoised, function (x) readRDS(x))\n    names(merged) <- sample_names\n    \n    # Retrieve merged object\n    asv_table <- makeSequenceTable(merged)\n    asv_ids <- sprintf(\"asv_%s\", 1:dim(asv_table)[2])\n\n    # Write ASV sequences\n    uniquesToFasta(asv_table, \"ASVs-100.fasta\", ids=asv_ids)\n\n    # Format count table\n    count_table <- cbind(\n        asv_ids, \n        colSums(asv_table),\n        t(asv_table[rowSums(asv_table)>0,])\n    )\n    colnames(count_table) <- c(\"Representative_Sequence\", \"total\", sample_names)\n\n    if (\"${params.format.toLowerCase()}\" == \"mothur\") {\n        # Write abundances\n        write.table(count_table, file=\"ASVs-100.count_table\", quote=F, sep=\"\\\\t\",\n                    row.names=F, col.names=T)\n    } else {\n        # Write abundances\n        write.table(count_table[, -c(2)],\"ASVs-100.tsv\", quote=F, row.names=F, sep=\"\\\\t\")\n\n        # Write duplicated fasta sequences with header formatted for VSEARCH\n        list.fasta <- list()\n        i = 1\n        for(seq in colnames(asv_table)) {\n            for(sample in rownames(asv_table)) {\n                abd = asv_table[sample, seq]\n                if(abd > 0) {\n                    seq_id = sprintf(\"ASV_%s;sample=%s;size=%s\", i, sample, abd)\n                    list.fasta[seq_id] = seq\n                }\n            }\n            i <- i+1\n        }\n        write.fasta(list.fasta, names=names(list.fasta), \n                    file.out='ASVs_duplicates_to_cluster.fasta')\n    }\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/BUILD_ASV_TABLE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["msa"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess BACKTRANSLATE {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::bipython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(meta), path(afa), path(fna)\n\n    output:\n    tuple val(meta), path(\"*.codons.afa\"), emit: fna\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio import SeqIO, AlignIO\n    from Bio.SeqRecord import SeqRecord\n    from Bio.Seq import Seq\n    from Bio.Data import CodonTable\n    from Bio.Align import MultipleSeqAlignment\n    from Bio.codonalign import build\n\n    msa = {aln.id: aln for aln in AlignIO.read(\"$afa\", \"fasta\")}\n    msa_clean = []\n\n    dna_all = []\n    for dna in SeqIO.parse(\"$fna\", \"fasta\"):\n\n        if dna.id not in msa:\n            continue\n    \n        prot = msa[dna.id]\n        dna.description = prot.description\n\n        frame = int(dna.description.split()[-1])\n        offset = (frame-1) % 3\n        aa_len = (len(dna)-offset) // 3\n        dna.seq = dna.seq[offset:3*aa_len+offset]\n\n        if frame > 3:\n           dna.seq = dna.seq.reverse_complement()\n\n        prot_seq = prot.seq.ungap('-').ungap('.').upper()\n        \n        if len(prot_seq) > len(dna):\n            # a partial codon was translated\n            prot_seq = prot_seq.tomutable()\n            prot_seq.pop()\n            prot_seq = prot_seq.toseq()\n            \n        prot.seq = prot_seq\n        msa_clean.append(prot)\n        dna_all.append(dna)\n\n    msa_clean = MultipleSeqAlignment(msa_clean)\n    codon_aln = build(msa_clean, dna_all, codon_table=CodonTable.generic_by_id[5])\n\n    with open(\"${meta.id}.codons.afa\", \"w\") as writer:\n        for seq in codon_aln:\n            ref = msa[seq.id]\n            codons = seq.seq\n            seq_str = ''.join(\n                '...' if ref.seq[i] == '.'\n                else codons.get_codon(i).lower() if ref.seq[i].islower()\n                else codons.get_codon(i)\n                for i in range(len(ref))\n            )\n            writer.write(f\">{ref.description}\\\\n{seq_str}\\\\n\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/BACKTRANSLATE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Taxa"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DADA2_ASSIGN_TAXONOMY {\n    tag \"${meta.id}\"\n    label \"process_high\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    path db\n\n    output:\n    tuple val(meta), path(\"taxonomy*.csv\"), emit: taxonomy\n    tuple val(meta), path(\"confidence*.csv\"), emit: confidence\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n    library(ShortRead)\n\n    sr <- readFasta(\"$fasta\")\n    seq_ids <- as.character(id(sr))\n    seq <- as.character(sread(sr))\n    assignments <- assignTaxonomy(\n        setNames(seq, seq_ids), \"$db\",\n        minBoot=$params.tax_confidence,\n        tryRC=TRUE,\n        outputBootstraps=TRUE\n    )\n    taxa = cbind(seq_ids, assignments\\$tax)\n    write.table(taxa, \"taxonomy_${meta.id}.csv\", quote=F, row.names=F, sep=\",\")\n\n    scores <- cbind(seq_ids, assignments\\$boot)\n    write.table(scores, \"confidence_${meta.id}.csv\", quote=F, row.names=F, sep=\",\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DADA2_ASSIGN_TAXONOMY"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["16S classifier"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess RDP_CLASSIFY {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? \"bioconda::rdptools=2.0.3\" : null)\n    container \"quay.io/biocontainers/rdptools:2.0.3--hdfd78af_1\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path rdp_db_files\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = fasta.getBaseName()\n    \"\"\"\n    classifier -Xmx${task.memory.getGiga()}g classify -t rRNAClassifier.properties -o ${prefix}.tsv $fasta -c $params.rdp_confidence\n\tawk -F'\\\\t' '{printf \\$1\"\"FS; for(i=6;i<=NF;i=i+3) printf \\$i\";\"; print \"\"}' ${prefix}.tsv > ${prefix}.taxonomy\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/RDP_CLASSIFY"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["MUSCLE"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MUSCLE {\n    tag \"${fasta.getBaseName()}\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/muscle:5.0.1278\"\n                                                                    \n                                                                  \n\n    input:\n    path fasta\n\n    output:\n    path \"*.afa\", emit: afa\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def arg = fasta.getExtension() == \"afa\" ? \"-profile\" : \"\"\n    def prefix = fasta.getBaseName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    n=\\$(grep -c \"^>\" $fasta)\n\n    if [ \"$arg\" == \"\" ] && [ \\$n -ge $params.max_seq ] ; then\n        opt=\"-maxiters 1 -diags -sv -distance1 kbit20_3\"\n    else\n        opt=\"\"\n    fi\n\n    # Run muscle and convert to 2-line fasta\n    cat $fasta | muscle $arg \\$opt -in - \\\\\n        | awk '/^>/ {printf(\"\\\\n%s\\\\n\",\\$0);next; } { printf(\"%s\",\\$0);}  END {printf(\"\\\\n\");}' \\\\\n        | tail -n+2 > ${prefix}.afa\n\n    echo \\$(muscle -version 2>&1) | cut -d\" \" -f2  > ${software}.version.txt\n\n    [ -s ${prefix}.afa ] && echo \"muscle was successfull\" || (echo \"Something went wrong. Did muscle segfault?\" && exit 1)\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MUSCLE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Pafig"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DADA2_FILTERANDTRIM {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq, optional: true\n    path(\"*.csv\"), emit: summary\n    path \"*.png\", emit: png, optional: true\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def rmphix = params.keep_phix ? \"TRUE\" : \"FALSE\"\n    def n = meta.paired_end ? 2 : 1\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(ggplot2)\n\n    filt_params <- data.frame(\n        minLen=c($params.min_read_len),\n        truncLen=c($params.trunc_len),\n        truncQ=c($params.trunc_quality),\n        maxEE=c($params.max_expected_error)\n    )[1:$n,]\n\n    if (\"$meta.paired_end\" == \"true\") {\n        io <- list(fwd=\"${reads[0]}\", filt=\"${meta.id}_R1.trimmed.fastq.gz\",\n                   rev=\"${reads[1]}\", filt.rev=\"${meta.id}_R2.trimmed.fastq.gz\")\n    } else {\n        io <- list(fwd=\"${reads[0]}\", filt=\"${meta.id}.trimmed.fastq.gz\")\n    }\n\n    params <- c(io, filt_params, list(rm.phix=${rmphix}))\n\n    read_count <- do.call(filterAndTrim, params)[, \"reads.out\"]\n    write(sprintf(\"qc,,$meta.id,%s,\", read_count), \"summary.csv\")\n\n    # Plot if we kept all reads\n    if (file.exists(io[[\"filt\"]])) {\n        fig <- plotQualityProfile(io)\n        ggsave(\"quality-profile_${meta.id}.png\", plot=fig, type=\"cairo-png\" )\n    }\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DADA2_FILTERANDTRIM"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["ITSxpress"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess ITSXPRESS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bbmap=38.69 bioconda::itsxpress=1.8.0\" : null)\n    container \"quay.io/biocontainers/itsxpress:1.8.0--py_1\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta_upd), path(\"*.fastq.gz\"), emit: fastq\n    path \"*.log\", emit: log\n    path \"*.version.txt\", emit: version\n\n    script:\n\tmeta_upd = meta.clone()\n\tmeta_upd[\"paired\"] = false\t\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    def rev_arg = params.paired_end ? \"--fastq2 ${reads[1]}\" : \"--single_end\"\n    \"\"\"\n    itsxpress --taxa All --region $params.locus --threads $task.cpus \\\\\n        --outfile ${meta.id}-${params.locus}.fastq.gz --log ITSxpress_${meta.id}.log \\\\\n        --fastq ${reads[0]} $rev_arg\n\n    pip show itsxpress | grep ^Version | sed 's/.*: //' > \"${software}.version.txt\"\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/ITSXPRESS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_CHIMERA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(count)\n\n    output:\n    tuple val(meta), path(\"${outprefix}.fasta\"), emit: fasta\n    tuple val(meta), path(\"${outprefix}.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    chimera.${params.chimera_tool}(fasta=$fasta, count=$count, dereplicate=t);\n    remove.seqs(fasta=current, accnos=current, dups=f)\"\n\n    # rename outputs\n    mv *.pick.fasta ${outprefix}.fasta\n    mv *.denovo.${params.chimera_tool}.pick.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_CHIMERA"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_SUBSAMPLE {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count)\n    each level\n\n    output:\n    tuple val(meta), path(\"*.shared\"), emit: shared\n    tuple val(meta), path(\"*.list\"), emit: list\n    tuple val(meta), path(\"*.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    sub.sample(list=$list, count=$count, size=$level, persample=true);\n    make.shared(list=current, count=current)\"\n\n    # rename outputs\n    mv *.subsample.count_table ${outprefix}.count_table\n    mv *.shared ${outprefix}.shared\n\n    [ -f *subsample*.list ] && mv *subsample*.list ${outprefix}.list || cp $list ${outprefix}.list\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_SUBSAMPLE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess CUTADAPT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.2' : null)\n    container 'quay.io/biocontainers/cutadapt:3.2--py38h0213d0e_0'\n\n    input:\n    tuple val(meta), path(reads)\n    path barcodes\n\n    output:\n    tuple val(meta), path('*.fastq.gz'), emit: reads\n    path '*.log', emit: log\n    path '*.version.txt', emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def trimmed  = params.single_end ?\n        \"-o {name}.fastq.gz\" :\n        \"-o {name}_R1.fastq.gz -p {name}_R2.fastq.gz\"\n\n    demux_opt = \"\"\n    if (params.paired_end && params.linked_bc) {\n        demux_opt += \" -a file:${barcodes[0]}\"\n    } else {\n        if (params.forward_bc.matches('5|3')) {\n            demux_opt += params.forward_bc == '5' ?\n                      \" -g file:${barcodes[0]}\" :\n                      \" -a file:${barcodes[0]}\"\n        }\n        if (params.reverse_bc.matches('5|3')) {\n            demux_opt += params.reverse_bc == '5' ?\n                \" -G file:${barcodes[1]}\" :\n                \" -A file:${barcodes[1]}\"\n        }\n    }\n\n    demux_opt += params.single_end ? \" --rc\" : \" --pair-adapters\"\n\n    \"\"\"\n    cutadapt --discard-untrimmed \\\\\n        --cores $task.cpus \\\\\n        -e $params.max_error_rate \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > cutadapt.log\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/CUTADAPT"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess CUTADAPT_JAMP {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.2' : null)\n    container 'quay.io/biocontainers/cutadapt:3.2--py38h0213d0e_0'\n\n    input:\n    tuple val(meta), path(reads)\n    path barcodes\n\n    output:\n    tuple val(meta), path('trimmed/*.fastq.gz'), emit: reads\n    path '*.log', emit: log\n    path '*.version.txt', emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n\tdef adapter1 = \"GGWACWGGWTGAACWGTWTAYCCYCC\"\n\tdef adapter2 = \"TANACYTCNGGRTGNCCRAARAAYCA\"\n    \"\"\"\n\tmkdir demux trimmed\n\n\t# remove index first\n    cutadapt --discard-untrimmed --pair-adapters \\\\\n\t    -g file:$barcodes \\\\\n        -G file:$barcodes \\\\\n        --cores $task.cpus \\\\\n        -e $params.max_error_rate \\\\\n\t    -o demux/{name}_R1.fastq.gz -p demux/{name}_R2.fastq.gz \\\\\n        $reads \\\\\n        > cutadapt_demux.log\n\n\t# remove adapters\n\tfor f in \\$(ls demux/*_R1.fastq.gz); do\n        s=\\$(basename \\${f/_R1.fastq.gz//}); \n        cutadapt --discard-untrimmed --pair-adapters \\\\\n\t        --cores $task.cpus \\\\\n            -g ^$adapter1 -G ^$adapter2 \\\\\n            -g ^$adapter2 -G ^$adapter1 \\\\\n            -o trimmed/\\${s}_R1.fastq.gz -p trimmed/\\${s}_R2.fastq.gz \\\\\n            demux/\\${s}_R*.fastq.gz; >> cutadapt_trimming.log\n\tdone\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/CUTADAPT_JAMP"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["LineagePulse"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess SPLIT_FASTA {\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"*.main.faa\", emit: main\n    path \"*.others.faa\", optional: true, emit: others\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import re\n    from collections import Counter, defaultdict\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    sequences = defaultdict(lambda: [])\n    \n    with open(\"$fasta\", \"r\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            sequences[tax].append(f\">{title}\\\\n{seq}\")\n\n    for (taxa, entries) in sequences.items():        \n        seq_str = \"\\\\n\".join(entries) + \"\\\\n\"\n\n        if len(entries) < $params.min_group_size:\n            fname = f\"{taxa}.others.faa\"\n        else:\n            fname = f\"{taxa}.main.faa\"\n        \n        with open(fname, \"w\") as writer:\n            writer.write(seq_str)\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/SPLIT_FASTA"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Pafig"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DADA2_LEARNERRORS {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:['id']) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.RDS\"), emit: rds, optional: true\n    path \"*.png\", emit: png, optional: true\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def suffix = meta.paired_end ? \"_${meta.orient}\" : \"\"\n    def outprefix = \"${meta.id}${suffix}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n    library(ggplot2)\n    \n    files <- sort(list.files(\".\", pattern=\"${suffix}.*.RDS\"))\n    sample_names <- gsub(\"${suffix}.*.RDS\", \"\", files)\n    reads <- lapply(files, readRDS)\n    names(reads) <- sample_names\n\n    tryCatch(\n        expr={ errors <- learnErrors(reads, multithread=TRUE, randomize=TRUE, nbases=1e7)\n               saveRDS(errors, \"${outprefix}.errors.RDS\") \n               fig <- plotErrors(errors, nominalQ=TRUE)\n               ggsave(\"profile_${outprefix}.png\", plot=fig, type=\"cairo-png\") },\n        error=function(e) { print(e) }            \n    )\n\n    writeLines(paste0(packageVersion('dada2')), \"${software}.version.txt\")    \n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DADA2_LEARNERRORS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["iConvert"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DOWNLOAD_UNITE {\n    tag \"$params.db_release\"\n    label \"process_low\"\n\n    conda (params.enable_conda ? \"bash:5.0.018\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    output:\n    path \"*.fasta\", emit: fasta\n\n    script:\n    def root_url = \"https://files.plutof.ut.ee/public/orig\"\n\n    if (params.db_release == 'fungi') {\n        url_base = \"${root_url}/1E/66\"\n        file = \"1E662B6EB320312A61E7E3218327F34C7DB09CFF8E4686A89EF47886822DA6AB.gz\"\n        \"\"\"\n        wget -qO- $url_base/$file | tar xz\n        iconv -f utf-8 -t ascii sh_general_release*/*.fasta \\\n            > unite_fungi.fasta\n        rm -rf sh_general_release*\n        \"\"\"\n    } else {\n        url_base = \"${root_url}/BF/49\"\n        file = \"BF49FBF4B47314A1CC5238B280FC58BFB8CEBD44A8D45F4A2BF5B8A466715693.gz\"\n        \"\"\"\n        wget -qO- $url_base/$file \\\\\n            | gunzip \\\\\n            | iconv -f utf-8 -t ascii \\\\\n            > unite_all_eukaryotes.fasta\n        rm -f $file\n        \"\"\"\n    }\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DOWNLOAD_UNITE"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["SummaryAUC", "AbDesign"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess DADA2_MERGEPAIRS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2 conda-forge::r-stringr conda-forge::r-seqinr conda-forge::r-dplyr conda-forge::r-tidyr\" : null)\n\n    input:\n    path(derep)\n    path(denoised)\n\n    output:\n    path \"*.RDS\", emit: rds\n    tuple val(meta), path(\"ASVs-100.{count_table,tsv}\"), emit: count_table\n    tuple val(meta), path(\"ASVs-100.fasta\"), emit: fasta\n    tuple val(meta), path(\"ASVs_duplicates_to_cluster.fasta\"), optional: true, emit: fasta_dup\n    path \"*_summary.tsv\", emit: merge_summary, optional: true\n    path \"*.version.txt\", emit: version\n\n    script:\n    meta = [id: \"100\"]\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n    library(reshape2)\n    library(dplyr)\n    library(tidyr)\n    library(seqinr)\n\n    paired <- length(list.files(path=\".\", pattern=\"_R2.denoised.RDS\")) > 1\n\n    load_rds_dada2 <- function(type, orient=\"\") {\n        ext <- sprintf(\"%s.%s.RDS\", orient, type)\n        files <- list.files(path=\".\", pattern=ext)\n        obj <- lapply(files, readRDS)\n        names(obj) <- gsub(ext, \"\", files)\n        return(obj)\n    }\n\n    ## Merge if paired_end (or merging was not done previously)\n    if (paired) {\n        merged <- mergePairs(\n            derepF=load_rds_dada2(\"derep\", \"_R1\"),\n            dadaF=load_rds_dada2(\"denoised\", \"_R1\"),\n            derepR=load_rds_dada2(\"derep\", \"_R2\"),\n            dadaR=load_rds_dada2(\"denoised\", \"_R2\"),\n            minOverlap=${params.min_overlap},\n            maxMismatch=${params.max_mismatch},\n            returnReject=TRUE\n        )\n        saveRDS(merged, \"merged.RDS\")\n\n        ## Mismatch summary\n        mismatches <- lapply(merged, function(df) table(df\\$nmismatch))\n        summary <- reshape2::melt(mismatches, varnames=\"nmismatch\") %>% \n          pivot_wider(values_from=value, names_from=L1) %>% \n          arrange(nmismatch)\n        write.table(summary, \"mismatch_summary.tsv\", sep=\"\\\\t\", quote=F, row.names=F)\n\n        ## Removes the reads not matching the merging criteria\n        merged <- lapply(merged, function(df) df[df\\$accept,])\n    } else { ## not paired\n        merged <- load_rds_dada2(\"denoised\", \"\")\n        saveRDS(merged, \"merged.RDS\")\n    }\n\n    ## Make the ASV table\n    asv_table <- makeSequenceTable(merged)\n    asv_table <- asv_table[rowSums(asv_table)>0, ]\n    sample_names <- rownames(asv_table)\n\n    ## Write ASV sequences\n    asv_ids <- sprintf(\"ASV_%s\", c(1:dim(asv_table)[2]))\n    uniquesToFasta(asv_table, \"ASVs-100.fasta\", ids=asv_ids)\n\n    ## Compute count table\n    count_table <- t(asv_table)\n    rownames(count_table) <- asv_ids\n\n    count_table <- cbind(asv_ids, rowSums(count_table), count_table)\n    colnames(count_table) <- c(\"Representative_Sequence\", \"total\", sample_names)\n\n    if (\"${params.format.toLowerCase()}\" == \"mothur\") {\n        write.table(count_table, file=\"ASVs-100.count_table\", row.names=F, col.names=T, quote=F, sep=\"\\\\t\")\n    } else {\n        write.table(count_table[, -c(2)], \"ASVs-100.tsv\", quote=F, row.names=F, sep=\"\\\\t\")\n\n        # Write duplicated fasta sequences with header formatted for VSEARCH\n        list.fasta <- list()\n        i = 1\n        for(seq in colnames(asv_table)) {\n            for(sample in rownames(asv_table)) {\n                abd = asv_table[sample, seq]\n                if(abd > 0) {\n                    seq_id = sprintf(\"ASV_%s;sample=%s;size=%s\", i, sample, abd)\n                    list.fasta[seq_id] = seq\n                }\n            }\n            i <- i+1\n        }\n        write.fasta(list.fasta, names=names(list.fasta), \n                    file.out='ASVs_duplicates_to_cluster.fasta')        \n    }\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/DADA2_MERGEPAIRS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Mothur"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess MOTHUR_GET_OTUS {\n    tag \"$meta.id\"\n    label \"process_low\"\n\n    container \"quay.io/biocontainers/mothur:1.46.1--h7165306_0\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.46.1\" : null)\n\n    input:\n    tuple val(meta), file(ref), file(filt)\n\n    output:\n    tuple val(meta), path(\"*.pick.${filt.getExtension()}\")\n\n    script:\n    def ref_ext = ref.getExtension()\n    def arg = filt.getExtension()\n    \"\"\"\n    mothur \"#list.otus(${ref_ext}=$ref);get.otus(accnos=current,$arg=$filt)\"\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/MOTHUR_GET_OTUS"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["Alphabet"], "nb_own": 1, "list_own": ["hawaiidatascience"], "nb_wf": 1, "list_wf": ["metaflowmics"], "list_contrib": ["Puumanamana"], "nb_contrib": 1, "codes": ["\nprocess KMER_FILTER {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::numpy\" : null)\n\n    input:\n    tuple val(meta), path(query)\n    path db\n\n    output:\n    tuple val(meta), path(\"*.single.fasta\")\n\n    script:\n    prefix = \"${query.getSimpleName()}.single\"\n\n    if (params.feature == 'nucl') {\n        alphabet = \"ACGT\"\n        base = 2\n    } else {\n        alphabet = \"ARNDCQEGHILKMFPSTWYV\"\n        base = 5\n    }\n    \"\"\"\n    #!/usr/bin/env python\n\n    from itertools import product, groupby\n    import numpy as np\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n\n    alphabet = \"$alphabet\"\n    N = len(alphabet)\n    k = $params.k\n\n    kmer_usage = set(np.load(\"$db\")[:$params.n_sub])\n    kmer_db = {''.join(x) for i, x in enumerate(product(alphabet, repeat=k)) if i in kmer_usage}\n\n    with open(\"$query\") as r, open(\"${prefix}.fasta\", \"w\") as w:\n        for i, (_, entries) in enumerate(\n            groupby(SimpleFastaParser(r), lambda x: x[0].split()[0])\n        ):\n            best_score = 0\n            selected = ()\n\n            for (t, s) in entries:\n                # remove potential gaps\n                s = s.replace('-', '')\n                # compute kmer counts\n                score = sum(1 for i in range(len(s)-k+1) if s[i:i+k] in kmer_db)\n\n                if score > best_score:\n                    selected = (t.split('|')[0], s)\n                    best_score = score\n\n            w.write('>{}\\\\n{}\\\\n'.format(*selected))\n\n            if i % 1000 == 0:\n                print(f\"{i:,} sequences processed\")\n    \"\"\"\n}"], "list_proc": ["hawaiidatascience/metaflowmics/KMER_FILTER"], "list_wf_names": ["hawaiidatascience/metaflowmics"]}, {"nb_reuse": 1, "tools": ["GWAS4D"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess lasso {\n\n    publishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n    input:\n        file BED from bed\n        file FAM from fam\n        file BIM from bim\n\n    output:\n        file 'scored_snps.lasso.tsv'\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(biglasso)\n    library(snpStats)\n    library(tidyverse)\n\n    # read dataset\n    gwas <- read.plink(\"${BED}\", \"${BIM}\", \"${FAM}\")\n    X <- as(gwas[['genotypes']], 'numeric')\n    ## remove NAs\n    X[is.na(X)] <- 0\t\n    X <- as.big.matrix(X) \n    y <- gwas[['fam']][['affected']] - 1\n    names(y) <- gwas[['fam']][['pedigree']] %>% as.character\n\n    rm(gwas)\n\n    # train and evaluate classifier\n    cvfit <- cv.biglasso(X, y, penalty = 'lasso', family = \"binomial\")\n\n    tibble(snp = rownames(cvfit\\$fit\\$beta), \n\t   beta = cvfit\\$fit\\$beta[,cvfit\\$lambda == cvfit\\$lambda.min][-1]) %>%\n        write_tsv('scored_snps.lasso.tsv')\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/lasso"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["Tab2MAGE"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess create_network {\n\n    input:\n        file RGWAS from rgwas_network\n        val NET from params.network\n        file TAB2 from tab2\n        file SNP2GENE from snp2gene\n\n    output:\n        file 'net.RData' into RNET\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(martini)\n    library(tidyverse)\n    library(igraph)\n\n    load(\"${RGWAS}\")\n    netType <- \"${NET}\"\n\n    if (netType == \"gs\") {\n        net <- get_GS_network(gwas)\n    } else if (netType %in% c('gm', 'gi')) {\n        snp2gene <- read_tsv(\"${SNP2GENE}\")\n\n        if (netType == \"gm\") {\n            net <- get_GM_network(gwas, snpMapping = snp2gene)\n        } else if (netType == \"gi\") {\n            tab2 <- read_tsv(\"${TAB2}\", col_types = cols(.default = col_character())) %>%\n                rename(gene1 = `Official Symbol Interactor A`, gene2 = `Official Symbol Interactor B`) %>%\n                select(gene1, gene2)\n            net <- get_GI_network(gwas, snpMapping = snp2gene, ppi = tab2)\n        }\n    } else {\n        stop(\"network type not recognized.\")\n    }\n\n    save(net, file = \"net.RData\")\n    \"\"\"\n}"], "list_proc": ["hclimente/gwas-tools/create_network"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 2, "tools": ["FastQC", "PLINK"], "nb_own": 2, "list_own": ["hclimente", "heuermh"], "nb_wf": 2, "list_wf": ["gwas-tools", "artic"], "list_contrib": ["drpatelh", "hclimente", "heuermh"], "nb_contrib": 3, "codes": ["\nprocess FASTQC {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.endsWith(\".zip\") ? \"zips/$filename\" : \"$filename\"\n                }\n\n    when:\n    !params.skip_fastqc && !params.skip_qc\n\n    input:\n    set val(sample), val(single_end), val(long_reads), file(reads) from ch_reads_fastqc\n\n    output:\n    file \"*.{zip,html}\" into ch_fastqc_reports_mqc\n\n    script:\n                                                                           \n    if (single_end || long_reads) {\n        \"\"\"\n        [ ! -f  ${sample}.fastq.gz ] && ln -s $reads ${sample}.fastq.gz\n        fastqc -q -t $task.cpus ${sample}.fastq.gz\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${sample}_1.fastq.gz ] && ln -s ${reads[0]} ${sample}_1.fastq.gz\n        [ ! -f  ${sample}_2.fastq.gz ] && ln -s ${reads[1]} ${sample}_2.fastq.gz\n        fastqc -q -t $task.cpus ${sample}_1.fastq.gz\n        fastqc -q -t $task.cpus ${sample}_2.fastq.gz\n        \"\"\"\n    }\n}", " process run_regression {\n\n       input:\n           each I from 1..params.N\n           val N from params.N\n           file BED from bed\n           file BIM from bim\n           file FAM from fam\n\n       output:\n           file \"part.epi.??.${I}\" into parts\n\n       \"\"\"\n       plink --bfile ${BED.baseName} --epistasis --parallel ${I} ${N} --allow-no-sex --out part\n       \"\"\"\n\n    }"], "list_proc": ["heuermh/artic/FASTQC", "hclimente/gwas-tools/run_regression"], "list_wf_names": ["hclimente/gwas-tools", "heuermh/artic"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": [" process run_regression_with_covars {\n\n       input:\n           each I from 1..params.N\n           val N from params.N\n           file BED from bed\n           file BIM from bim\n           file FAM from fam\n           file COVAR from covar\n\n       output:\n           file \"part.epi.??.${I}\" into parts\n\n       \"\"\"\n       plink --bfile ${BED.baseName} --epistasis --covar ${COVAR} --parallel ${I} ${N} --allow-no-sex --out part\n       \"\"\"\n\n    }"], "list_proc": ["hclimente/gwas-tools/run_regression_with_covars"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["FIT"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess high_order_glm {\n    \n    errorStrategy 'ignore'\n\n    input:\n        file RGWAS from rgwas\n        file INTERACTIONS from interactions\n        val I from 1..(interactions.countLines() - 1)\n        file COVARS from covars\n\n    output:\n        file \"scores_${I}.tsv\" into scores\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(tidyverse)\n\n    snps <- strsplit(read_tsv('${INTERACTIONS}')\\$snp_sets[${I}], '_') %>% unlist\n    covars <- read_tsv('${COVARS}')\n    # do some kind of matching using sample ids\n\n    load('${RGWAS}')\n\n    X <- as(gwas[['genotypes']], 'numeric')\n    X <- X[, snps]\n    X[is.na(X)] = 0 # TODO change?\n    X[X == 0] = 'AA'\n    X[X == 1] = 'Aa'\n    X[X == 2] = 'aa'\n\n    y <- gwas[['fam']][['affected']] - 1\n\n    rm(gwas)\n\n    intx <- model.matrix(~.^100,data = as.data.frame(X)) %>%\n        as_tibble %>%\n        mutate(y = y) %>%\n        bind_cols(covars)\n\n    rm(X,y)\n\n    fit <- glm(y ~ ., data = intx)\n    pvals <- coef(summary(fit))[,'Pr(>|t|)']\n\n    tibble(snp_set = paste(snps, collapse = '_'),\n           test = names(pvals),\n           p_value = pvals) %>%\n        write_tsv('scores_${I}.tsv', col_names = FALSE)\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/high_order_glm"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PPI", "VEGAS2"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess dmgwas {\n\n    publishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n    input:\n        file VEGAS_OUT\n        file TAB2\n        val D from params.d\n        val R from params.r\n\n    output:\n        file 'selected_genes.dmgwas.txt'\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dmGWAS)\n    library(tidyverse)\n\n    vegas <- read_tsv('${VEGAS_OUT}') %>% \n        select(Gene, Pvalue) %>%\n        mutate(Pvalue = ifelse(Pvalue == 1, 0.99999, Pvalue)) %>%\n        as.data.frame\n    ppi <- read_tsv('${TAB2}',\n\t\t    col_types = 'cccccccccccccccccccccccc') %>%\n        rename(interactorA = `Official Symbol Interactor A`, \n               interactorB = `Official Symbol Interactor B`) %>%\n        select(interactorA, interactorB)\n\n    modules <- dms(ppi, vegas, expr1 = NULL, expr2 = NULL, r = ${R}, d = ${D})\n    top <- simpleChoose(modules)\n\n    tibble(gene = names(V(top\\$subnetwork))) %>%\n        write_tsv('selected_genes.dmgwas.txt')\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/dmgwas"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["GenEpi"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess genepi {\n\n    publishDir \"$params.out\", overwrite: true, mode: 'copy'\n\n    input:\n    file GEN from gen\n    file PHENO from pheno\n    val PHENO_TYPE from pheno_type \n\n    output:\n    file 'crossGeneResult/Result.csv'\n\n    script:\n    \"\"\"\n    GenEpi -g ${GEN} -p ${PHENO} -m ${PHENO_TYPE} -o ./\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/genepi"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PHENO"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess gpuepiscan {\n\n        publishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n        input:\n          file RGWAS from rgwas\n\n        output:\n          file 'scored_interactions.episcan.tsv'\n\n        script:\n        \"\"\"\n        #!/usr/bin/env Rscript\n\n        library(gpuEpiScan)\n        library(tidyverse)\n        library(bigmemory)\n\n        load('${RGWAS}')\n        \n        # read dataset\n        X <- as(gwas[['genotypes']], 'numeric') # %>% as.big.matrix\n        y <- gwas[['fam']][['affected']] - 1\n\n        gpuEpiScan(geno1 = X,\n                   pheno = y,\n                   phetype = \"quantitative\",\n                   outfile = \"scored_interactions.episcan\", \n                   suffix = \".txt\" \n                  )\n        \n        system2('sed', args = c(\"'s/ /\\t/g'\", 'scored_interactions.episcan.txt'),\n                stdout = 'scored_interactions.episcan.tsv')\n        \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/gpuepiscan"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PHENO", "PLINK"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess preprocess_data {\n\n\tinput:\n\t\tfile BED from bed\n\t\tfile bim\n\t\tfile FAM from fam\n\t\tfile SNP2SNP from snp2snp_1 \n\t\tfile EXCLUDED from excluded_snps\n\t\tval I from params.nperm\n\n\toutput:\n\t\tset 'post_qc.bed', 'post_qc.bim', 'post_qc.fam', 'pheno' into filtered_bed_pipeline, filtered_bed_qc\n\t\tfile 'snps_75.prune.in' into qc_snps\n\n\t\"\"\"\n\t# QC + keep only SNPs in models + remove excluded SNPs\n\tcut -f3 ${SNP2SNP} >tmp\n\tcut -f4 ${SNP2SNP} >>tmp\n\tsort tmp | uniq >snps_in_models\n\tcomm -3 snps_in_models <(sort ${EXCLUDED}) >included_snps\n\tplink -bfile ${BED.baseName} -extract included_snps -maf 0.05 -hwe 0.001 -make-bed -out filtered \n\n\t# LD pruning\n\tplink -bfile filtered -indep-pairwise 50 5 0.75 -out snps_75\n\tplink -bfile filtered -extract snps_75.prune.in -make-bed -out post_qc\n\n\t# create pheno file\n\tcut -d' ' -f6 ${FAM} >orig\n\tcut -d' ' -f1,2,6 ${FAM} | sed 's/ /\\t/g'  >pheno\n\tfor i in {1..${I}}\n\tdo\n\t\tshuf orig >tmp1\n\t\tpaste pheno tmp1 >tmp2\n\t\tmv tmp2 pheno\n\tdone\n\t\"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/preprocess_data"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess snp_epistasis {\n\n\ttag { \"${I}\" }\n\n\tinput:\n\t\teach I from 1..(params.nperm + 1)\n\t\tset file(BED), file(BIM), file(FAM), file(PHENO) from filtered_bed_pipeline\n\t\tfile SNP2SNP from snp2snp_2\n\n\toutput:\n\t\tset val(I), 'scored_interactions.regression.txt' into snp_pairs, snp_pairs_null \n\n\t\"\"\"\n\tepistasis_regression.nf --bfile ${BED.baseName} --pheno ${PHENO} --i ${I} -profile cluster\n\n\tif [ `wc -l < scored_interactions.regression.txt` -gt \"1\" ]; then\n\t\t# exhaustive LD pruning\n\t\tcut -f1 scored_interactions.regression.txt >tmp\n\t\tcut -f2 scored_interactions.regression.txt >>tmp\n\t\tsort tmp | uniq >included_snps\n\n\t\tplink -bfile ${BED.baseName} -extract included_snps -allow-no-sex -r2\n\t\tsed 's/^ \\\\+//' plink.ld | sed 's/ \\\\+/\\t/g' | sed 's/\\t\\$//' >ld.tsv\n\n\t\tR -e '\n\t\tlibrary(tidyverse); \n\t\tld_ok <- read_tsv(\"ld.tsv\", col_types = \"idcidcd\") %>%\n\t\tfilter(R2 < 0.75) %>%\n\t\tmutate(uniq_snp_id = cbind(SNP_A, SNP_B) %>% apply(1, sort) %>% apply(2, paste, collapse = \"_\")) %>%\n\t\t.\\$uniq_snp_id\n\t\tsnp_models <- read_tsv(\"${SNP2SNP}\") %>%\n\t\t.\\$uniq_snp_id\t\n\t\tread_tsv(\"scored_interactions.regression.txt\", col_types = \"ccccddd\") %>%\n\t\tmutate(uniq_snp_id = cbind(SNP1, SNP2) %>% apply(1, sort) %>% apply(2, paste, collapse = \"_\")) %>%\n\t\tfilter(uniq_snp_id %in% ld_ok & uniq_snp_id %in% snp_models) %>%\n\t\twrite_tsv(\"scored_interactions.regression.txt\")'\n\telse\n\t\techo \"`cat scored_interactions.regression.txt`\\tuniq_snp_id\" >header\n\t\tsed 's/\\tP\\t/\\t0.0001\\t/' header | sed 's/\\tSTAT\\t/\\t0\\t/' | sed 's/\\tBETA_INT\\t/\\t0\\t/' >vals\n\t\tcat header vals >scored_interactions.regression.txt\n\tfi\n\t\"\"\" \n\n}"], "list_proc": ["hclimente/gwas-tools/snp_epistasis"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["UniqueProt", "Threshold-seq"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess gene_epistasis {\n\t\n\ttag { \"${I}\" }\n\n\tinput:\n\t\tfile 'permuted_association_*' from snp_pairs_null_filtered .collect()\n\t\tset I, file(SNP_PAIRS) from snp_pairs\n\t\tfile SNP2GENE from snp2gene\n\t\tfile SNP2SNP from snp2snp_3 \n\t\tfile TAB2 from tab2\n\n\toutput:\n\t\tset val(I), 'scored_gene_pairs.tsv' into scored_gene_pairs, scored_gene_pairs_null\n\t\tfile 'sign_snp_pairs.tsv' optional true into sign_snp_pairs\n\n\t\"\"\"\n\t#!/usr/bin/env Rscript\n\n\tlibrary(tidyverse)\n\tlibrary(data.table)\n\n\tsnp2snp <- read_tsv('$SNP2SNP', col_types = 'cccccc') %>%\n\t\tdata.table\n\tsnp2gene <- read_tsv('$SNP2GENE', col_types = 'cc')\n\tthreshold <- lapply(list.files(pattern = 'permuted_association_'), function(x) {\n\t\t\tread_tsv(x, col_types = 'ccccddd') %>%\n\t\t\t\tmutate(uniq_snp_id = cbind(SNP1, SNP2) %>% apply(1, sort) %>% apply(2, paste, collapse = '_')) %>%\n\t\t\t\tdata.table %>%\n\t\t\t\tmerge(snp2snp, by = 'uniq_snp_id', allow.cartesian = TRUE) %>%\n\t\t\t\tarrange(P) %>%\n\t\t\t\thead(1) %>%\n\t\t\t\tselect(P)\n\t\t\t}) %>%\n\t\tdo.call(bind_rows, .) %>%\n\t\t.\\$P %>%\n\t\tquantile(.05)\n\n\tsnp_pairs <- read_tsv('${SNP_PAIRS}', col_types = 'ccccddd') %>%\n\t\tmutate(uniq_snp_id = cbind(SNP1, SNP2) %>% apply(1, sort) %>% apply(2, paste, collapse = '_')) %>%\n\t\tfilter(P < threshold) %>%\n\t\tmutate(Padj = (P * .05) / threshold) %>%\n\t\tinner_join(snp2snp, by = 'uniq_snp_id') %>%\n\t\tselect(uniq_snp_id, Padj)\n\n\tif (${I} == 1) {\n\t\tseparate(snp_pairs, uniq_snp_id, into = c('snp_1','snp_2'), sep = '_') %>%\n\t\t\tunique %>%\n\t\t\twrite_tsv('sign_snp_pairs.tsv')\n\t}\n\n\tread_tsv('${TAB2}', col_types = cols(.default = col_character())) %>%\n\t\tinner_join(snp2gene, ., by = c('gene' = 'Official Symbol Interactor A')) %>%\n\t\tinner_join(snp2gene, ., by = c('gene' = 'Official Symbol Interactor B'), suffix = c('_1','_2')) %>%\n\t\trename(gene_1 = gene) %>%\n\t\tmutate(uniq_snp_id = cbind(snp_1, snp_2) %>% apply(1, sort) %>% apply(2, paste, collapse = '_'),\n\t\t\t   uniq_gene_id = cbind(gene_1, gene_2) %>% apply(1, sort) %>% apply(2, paste, collapse = '_')) %>%\n\t\tinner_join(snp_pairs, by = 'uniq_snp_id') %>%\n\t\tgroup_by(uniq_gene_id) %>%\n\t\tsummarize(tau_05 = prod(Padj[Padj < 0.05]),\n\t\t\t\t  tau_01 = prod(Padj[Padj < 0.01]),\n\t\t\t\t  tau_001 = prod(Padj[Padj < 0.001]),\n\t\t\t \t  snp_pairs = paste(unique(uniq_snp_id), collapse = ',')) %>%\n        write_tsv('scored_gene_pairs.tsv')\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/gene_epistasis"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["Tab2MAGE", "UniqueProt"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess pathway_epistasis {\n\n\tpublishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n\tinput:\n\t\tfile 'permuted_association_*' from gene_pairs_null .collect()\n\t\tfile GENE_PAIRS from gene_pairs\n\t\tfile SNP2GENE from snp2gene\n\t\tfile TAB2 from tab2\n\t\tfile SNPS from qc_snps                                \n\n\toutput:\n\t\tfile 'sign_gene_pairs.tsv'\n\t\tfile 'sign_pathways.tsv'\n\n\t\"\"\"\n    #!/usr/bin/env Rscript\n\n    library(tidyverse)\n    library(igraph)\n    library(clusterProfiler)\n    library(msigdbr)\n\n    # compute gene-pair association\n    gene_pairs <- read_tsv('${GENE_PAIRS}') %>%\n        mutate(experiment = 'original')\n    studied_gene_pairs <- gene_pairs\\$uniq_gene_id\t\t\n    permutations <- list.files(pattern = 'permuted_association_')\n    N <- length(permutations) + 1\n\n    gene_pairs_assoc <- lapply(permutations, function(x) {\n        read_tsv(x, col_types = 'cdddc') %>%\n            filter(uniq_gene_id %in% studied_gene_pairs)\n    }) %>%\n        do.call(bind_rows, .) %>%\n        mutate(experiment = 'permutation') %>%\n        bind_rows(gene_pairs) %>%\n        group_by(uniq_gene_id) %>%\n        mutate(P_tau_05 = rank(tau_05) / N,\n               P_tau_01 = rank(tau_01) / N, \n               P_tau_001 = rank(tau_001) / N,\n               min_P = pmin(P_tau_05, P_tau_01, P_tau_001),\n               P = rank(min_P) / N ) %>%\n        filter(experiment == 'original') %>%\n        separate(uniq_gene_id, into = c('gene_1', 'gene_2'), sep = '_') %>%\n        select(-experiment) %>%\n        select(gene_1, gene_2, P, everything())\n    write_tsv(gene_pairs_assoc, 'sign_gene_pairs.tsv')\n    \n    sign_pairs <- filter(gene_pairs_assoc, P < .05)\n    sign_genes <- c(sign_pairs\\$gene_1, sign_pairs\\$gene_2) %>% unique\n\n    # compute pathway association\n    tab2 <- read_tsv('${TAB2}', col_types = cols(.default = col_character()))\n    net <- select(tab2, `Official Symbol Interactor A`, `Official Symbol Interactor B`) %>%\n        graph_from_data_frame(directed = FALSE)\n \n\t# create background\n    snps <- read_tsv('${SNPS}', col_names = FALSE)\\$X1\n    snp2gene <- read_tsv('${SNP2GENE}', col_types = 'cc')\n    bg <- snp2gene %>%\n        filter(!is.na(gene) & !is.na(snp)) %>%\n        filter(gene %in% c(tab2\\$`Official Symbol Interactor A`, tab2\\$`Official Symbol Interactor B`)) %>%\n        filter(snp %in% snps) %>%\n        .\\$gene\n\n    # import MSigDB Gene Sets\n    m_df = msigdbr(species = \"Homo sapiens\")\n    m_t2g = m_df %>% dplyr::select(gs_name, gene_symbol) %>% as.data.frame()\n\n    mapply(function(gene_1, gene_2) {\n        neighborhood <- delete_edges(net, paste(gene_1, gene_2, sep = '|')) %>%\n            shortest_paths(from = gene_1, to = gene_2) %>%\n            .\\$vpath %>% unlist %>% names %>%\n            intersect(sign_genes) %>%\n            sort\n        if (length(neighborhood) >= 3) {\n            enricher(neighborhood, TERM2GENE = m_t2g, universe = bg, pAdjustMethod = 'bonferroni')\n        }\n    }, sign_pairs\\$gene_1, sign_pairs\\$gene_2) %>%\n        lapply(as_tibble) %>%\n        bind_rows %>%\n        unique %>%\n        write_tsv('sign_pathways.tsv')\t\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/pathway_epistasis"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PRS", "GWAS4D"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\tprocess adjust_snp_pairs {\n\n\t\tpublishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n\t\tinput:\n\t\t\tfile SIGN_SNP_PAIRS from sign_snp_pairs\n\t\t\tfile PRS from file(params.prs)\n\t\t\tset file(BED), file(BIM), file(FAM), file(PHENO) from filtered_bed_qc\n\n\t\toutput:\n\t\t\tfile 'prs_adjusted_sign_snp_pairs.tsv'\n\n\t\t\"\"\"\n\t\t#!/usr/bin/env Rscript\n\n\t\tlibrary(tidyverse)\n\t\tlibrary(snpStats)\n\n\t\tsnp_pairs <- read_tsv('${SIGN_SNP_PAIRS}', col_types = 'ccd')\n\t\tgwas <- read.plink(\"${BED}\", \"${BIM}\", \"${FAM}\")\n\t\tprs <- read_tsv('${PRS}')\\$prs\n\n\t\tX <- as(gwas[['genotypes']], 'numeric')\n\t\tX[X == 0] = 'AA'\n\t\tX[X == 1] = 'Aa'\n\t\tX[X == 2] = 'aa'\n\t\ty <- gwas[['fam']][['affected']]\n\n\t\tgwas <- as_tibble(X) %>%\n\t\t\tmutate(y = y, prs = prs)\n\t\trm(X,y)\n\n\t\tprs_adjust <- function(snp_1, snp_2, ...) {\n\t\t\tdf <- as.data.frame(gwas[, c('y', 'prs', snp_1, snp_2)])\n\t\t\tcolnames(df) <- c('Y', 'PRS', 'SNP1', 'SNP2')\n\t\t\tPRS_adjusted = lm(Y ~ PRS + SNP1 + SNP2 + SNP1*SNP2, df, na.action=na.exclude)\n        \t        summary(PRS_adjusted)\\$coefficients[5, 4]\n\t\t}\n\n\t\tsnp_pairs %>%\n\t\t\tmutate(prs_adjusted_p =  pmap_dbl(snp_pairs, prs_adjust)) %>%\n\t\t\twrite_tsv('prs_adjusted_sign_snp_pairs.tsv')\n\t\"\"\"\n\t}"], "list_proc": ["hclimente/gwas-tools/adjust_snp_pairs"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["GWAS4D"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess random_forest {\n\n    input:\n        file RGWAS from rgwas_train\n\n    output:\n        file 'forest.RData' into forest\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(ranger)\n    library(tidyverse)\n\n    load(\"${RGWAS}\")\n\n    X <- as(gwas\\$genotypes, \"numeric\")\n    X[is.na(X)] <- 0 # safeguard against missing genotypes\n    Y <- gwas\\$fam\\$affected\n\n    gwas <- as_tibble(X) %>%\n        mutate(phenotype = Y)\n\n    rf <- ranger(dependent.variable.name = 'phenotype', data = gwas, importance = 'impurity')\n    save(rf, file = 'forest.RData')\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/random_forest"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["SNP2APA"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess extract_epistatic_snps {\n\n    publishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n    input:\n        file FOREST from forest\n\n    output:\n        file 'scored_interactions.rf.txt' into pairs\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(ranger)\n    library(tidyverse)\n\n    load(\"${FOREST}\")\n\n    snps <- lapply(seq(1, 500), function(i) {\n        nodes <- treeInfo(rf,i)[['splitvarName']]\n        names(nodes) <- treeInfo(rf,i)[['nodeID']]\n        bind_rows(treeInfo(rf,i)[,c('nodeID','leftChild')] %>% rename(otherNodeID = leftChild),\n                  treeInfo(rf,i)[,c('nodeID','rightChild')] %>% rename(otherNodeID = rightChild)) %>%\n            mutate(snp1 = nodes[nodeID + 1], snp2 = nodes[otherNodeID + 1]) %>%\n            filter(!is.na(snp2) & snp1 != snp2) %>%\n            mutate(uniq_snp_id = cbind(snp1, snp2) %>% apply(1, sort) %>% apply(2, paste, collapse = '_')) %>%\n            select(uniq_snp_id)\n        }) %>%\n        do.call(rbind, .) %>% \n        group_by(uniq_snp_id) %>%\n        summarize(n = n()) %>%\n        ungroup() %>%\n        separate(uniq_snp_id, into=c('snp1','snp2'), sep='_')\n\n    imp <- importance(rf)\n    top_snps <- tail(sort(imp), n = floor(length(imp) * 0.2)) %>% names\n    snps %>%\n        filter(snp1 %in% top_snps | snp2 %in% top_snps) %>%\n        write_tsv('scored_interactions.rf.txt')\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/extract_epistatic_snps"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["MoDEL"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess get_gene_models {\n\n    publishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n    input:\n        file LOKI from loki\n        file GENES from genes\n\n    output:\n        file 'biofilter.gene.models' into gene_models\n\n    \"\"\"\n    cat << EOF >parameters.txt\n    KNOWLEDGE           ${LOKI}\n    GENE_FILE           ${GENES}\n    MODEL               gene\n    EOF\n\n    biofilter.py parameters.txt\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/get_gene_models"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess snp2gene {\n\n\tinput:\n\t\tfile snps_bed\n\t\tfile genes_bed\n\n\toutput:\n\t\tfile 'snp2ensembl.tsv' into snp2ensembl\n\n\t\"\"\"\n\tbedtools intersect -a $snps_bed -b $genes_bed -wa -wb >tmp\n\tcut -f4,16 tmp | sed 's/\\\\tID=/\\\\t/' | sed 's/\\\\.[0-9]\\\\+;.\\\\+//' >snp2ensembl.tsv\n\t\"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/snp2gene"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": [" process compute_chisq {\n\n          input:\n              file BED from bed\n              file BIM from bim\n              file FAM from fam\n\n          output:\n              file 'snp_association' into snp_association\n\n          \"\"\"\n          plink --bed ${BED} --bim ${BIM} --fam ${FAM} --assoc \n          awk 'NR > 1 && \\$9 != \"NA\" { print \\$2,\\$9 }' OFS='\\\\t' plink.assoc  >snp_association\n          \"\"\"\n\n      }"], "list_proc": ["hclimente/gwas-tools/compute_chisq"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": [" process regress_phenotypes_with_covars {\n\n        input:\n            file BED from bed\n            file BIM from bim\n            file FAM from fam\n            file COVAR from covar\n\n        output:\n            file 'snp_association' into snp_association\n\n        \"\"\"\n        plink --bed ${BED} --bim ${BIM} --fam ${FAM} --logistic --covar ${COVAR}\n        awk 'NR > 1 && \\$5 == \"ADD\" && \\$9 != \"NA\" { print \\$2,\\$9 }' OFS='\\\\t' plink.assoc.logistic >snp_association\n        \"\"\"\n\n    }"], "list_proc": ["hclimente/gwas-tools/regress_phenotypes_with_covars"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": [" process extract_controls {\n\n      input:\n          file BED from bed\n          file bim\n          file fam\n\n      output:\n          file 'plink.bed' into bed_controls\n          file 'plink.bim' into bim_controls\n          file 'plink.fam' into fam_controls\n\n      \"\"\"\n      plink --bfile ${BED.baseName} --filter-controls --make-bed\n      \"\"\"\n\n  }"], "list_proc": ["hclimente/gwas-tools/extract_controls"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 1, "tools": ["VEGAS2"], "nb_own": 1, "list_own": ["hclimente"], "nb_wf": 1, "list_wf": ["gwas-tools"], "list_contrib": ["hclimente"], "nb_contrib": 1, "codes": ["\nprocess heinz {\n\n    publishDir \"$params.out\", overwrite: true, mode: \"copy\"\n\n    input:\n        file VEGAS from vegas\n        file TAB2 from tab2\n        val FDR from params.fdr\n\n    output:\n        file 'selected_genes.heinz.txt' \n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(tidyverse)\n    library(igraph)\n    library(BioNet)\n\n    vegas <- read_tsv('${VEGAS}')\n    net <- read_tsv(\"${TAB2}\") %>%\n        rename(gene1 = `Official Symbol Interactor A`, \n               gene2 = `Official Symbol Interactor B`) %>%\n        filter(gene1 %in% vegas\\$Gene & gene2 %in% vegas\\$Gene) %>%\n        select(gene1, gene2) %>%\n        graph_from_data_frame(directed = FALSE)\n    vegas <- filter(vegas, Gene %in% names(V(net)))\n\n    # search subnetworks\n    pvals <- vegas\\$Pvalue\n    names(pvals) <- vegas\\$Gene\n    fb <- fitBumModel(pvals, plot = FALSE)\n    scores <- scoreNodes(net, fb, fdr = ${FDR})\n\n    if (sum(scores > 0)) {\n        selected <- runFastHeinz(net, scores)    \n    \ttibble(gene = names(V(selected))) %>% \n            write_tsv('selected_genes.heinz.txt')\n    } else {\n        write_tsv(tibble(gene = character()), 'selected_genes.heinz.txt')\n    }\n    \"\"\"\n\n}"], "list_proc": ["hclimente/gwas-tools/heinz"], "list_wf_names": ["hclimente/gwas-tools"]}, {"nb_reuse": 2, "tools": ["FastQC", "Bowtie"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 2, "list_wf": ["smrna-seq-pipeline", "chip-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": [" process ngi_visualizations {\n        tag \"$bowtie2_bam\"\n        publishDir \"${params.outdir}/bowtie2/ngi_visualizations\", mode: 'copy'\n\n        input:\n        file gtf from gtf\n        file bowtie2_bam\n\n        output:\n        file '*.{png,pdf}' into bowtie2_ngi_visualizations\n\n        script:\n                                                          \n                                                                                   \n        \"\"\"\n        #!/usr/bin/env python\n        from ngi_visualizations.biotypes import count_biotypes\n        count_biotypes.main('$gtf','$bowtie2_bam')\n        \"\"\"\n    }", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    set val(name), file(reads) from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n    file '.command.out' into fastqc_stdout\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/ngi_visualizations", "heinzlab/chip-seq-pipeline/fastqc"], "list_wf_names": ["heinzlab/smrna-seq-pipeline", "heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["chip-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess bwa {\n    tag \"$prefix\"\n    publishDir path: { params.saveAlignedIntermediates ? \"${params.outdir}/bwa\" : params.outdir }, mode: 'copy',\n               saveAs: {filename -> params.saveAlignedIntermediates ? filename : null }\n\n    input:\n    file reads from trimmed_reads\n    file index from bwa_index.first()\n\n    output:\n    file '*.bam' into bwa_bam\n\n    script:\n    prefix = reads[0].toString() - ~/(.R1)?(_1)?(_R1)?(_trimmed)?(_val_1)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    filtering = params.allow_multi_align ? '' : \"| samtools view -b -q 1 -F 4 -F 256\"\n    \"\"\"\n    bwa mem -t ${task.cpus} -M ${index}/genome.fa $reads | samtools view -bT $index - $filtering > ${prefix}.bam\n    \"\"\"\n}"], "list_proc": ["heinzlab/chip-seq-pipeline/bwa"], "list_wf_names": ["heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Minimap2", "MultiQC", "BEDTools", "FastQC"], "nb_own": 2, "list_own": ["heinzlab", "heuermh"], "nb_wf": 2, "list_wf": ["chip-seq-pipeline", "artic"], "list_contrib": ["heuermh", "drpatelh", "c-guzman"], "nb_contrib": 3, "codes": ["\nprocess samtools {\n    tag \"${bam.baseName}\"\n    publishDir path: \"${params.outdir}/bwa\", mode: 'copy',\n               saveAs: { filename ->\n                   if (filename.indexOf(\".stats.txt\") > 0) \"stats/$filename\"\n                   else params.saveAlignedIntermediates ? filename : null\n               }\n\n    input:\n    file bam from bwa_bam\n\n    output:\n    file '*.sorted.bam' into bam_picard, bam_for_mapped\n    file '*.sorted.bam.bai' into bwa_bai, bai_for_mapped\n    file '*.sorted.bed' into bed_total\n    file '*.stats.txt' into samtools_stats\n\n    script:\n    \"\"\"\n    samtools sort $bam -o ${bam.baseName}.sorted.bam\n    samtools index ${bam.baseName}.sorted.bam\n    bedtools bamtobed -i ${bam.baseName}.sorted.bam | sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 > ${bam.baseName}.sorted.bed\n    samtools stats ${bam.baseName}.sorted.bam > ${bam.baseName}.stats.txt\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    NanoPlot --version &> v_nanoplot.txt\n    fastqc --version > v_fastqc.txt\n    echo \\$(bwa 2>&1) > v_bwa.txt\n    minimap2 --version &> v_minimap2.txt\n    samtools --version > v_samtools.txt\n    bedtools --version > v_bedtools.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["heinzlab/chip-seq-pipeline/samtools", "heuermh/artic/get_software_versions"], "list_wf_names": ["heuermh/artic", "heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FastQC"], "nb_own": 2, "list_own": ["heinzlab", "heuermh"], "nb_wf": 1, "list_wf": ["chip-seq-pipeline", "nf-core-hlatyping2"], "list_contrib": ["heuermh", "c-guzman"], "nb_contrib": 2, "codes": ["\nprocess bwa_mapped {\n    tag \"${input_files[0].baseName}\"\n    publishDir \"${params.outdir}/bwa/mapped\", mode: 'copy'\n\n    input:\n    file input_files from bam_for_mapped.collect()\n    file bai from bai_for_mapped.collect()\n\n    output:\n    file 'mapped_refgenome.txt' into bwa_mapped\n\n    script:\n    \"\"\"\n    for i in $input_files\n    do\n      samtools idxstats \\${i} | awk -v filename=\"\\${i}\" '{mapped+=\\$3; unmapped+=\\$4} END {print filename,\"\\t\",mapped,\"\\t\",unmapped}'\n    done > mapped_refgenome.txt\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["heinzlab/chip-seq-pipeline/bwa_mapped"], "list_wf_names": ["heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard", "yara", "BEDTools"], "nb_own": 2, "list_own": ["heinzlab", "heuermh"], "nb_wf": 1, "list_wf": ["chip-seq-pipeline", "nf-core-hlatyping2"], "list_contrib": ["heuermh", "c-guzman"], "nb_contrib": 2, "codes": ["\nprocess picard {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/picard\", mode: 'copy'\n\n    input:\n    file bam from bam_picard\n\n    output:\n    file '*.dedup.sorted.bam' into bam_dedup_ssp, bam_dedup_ngsplot, bam_dedup_deepTools, bam_dedup_macs, bam_dedup_saturation\n    file '*.dedup.sorted.bam.bai' into bai_dedup_deepTools, bai_dedup_ngsplot, bai_dedup_macs, bai_dedup_ssp\n    file '*.dedup.sorted.bed' into bed_dedup\n    file '*.picardDupMetrics.txt' into picard_reports\n\n    script:\n    prefix = bam[0].toString() - ~/(\\.sorted)?(\\.bam)?$/\n    if( task.memory == null ){\n        log.warn \"[Picard MarkDuplicates] Available memory not known - defaulting to 6GB ($prefix)\"\n        avail_mem = 6000\n    } else {\n        avail_mem = task.memory.toMega()\n        if( avail_mem <= 0){\n            avail_mem = 6000\n            log.warn \"[Picard MarkDuplicates] Available memory 0 - defaulting to 6GB ($prefix)\"\n        } else if( avail_mem < 250){\n            avail_mem = 250\n            log.warn \"[Picard MarkDuplicates] Available memory under 250MB - defaulting to 250MB ($prefix)\"\n        }\n    }\n    \"\"\"\n    picard MarkDuplicates \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.dedup.bam \\\\\n        ASSUME_SORTED=true \\\\\n        REMOVE_DUPLICATES=true \\\\\n        METRICS_FILE=${prefix}.picardDupMetrics.txt \\\\\n        VALIDATION_STRINGENCY=LENIENT \\\\\n        PROGRAM_RECORD_ID='null'\n    samtools sort ${prefix}.dedup.bam -o ${prefix}.dedup.sorted.bam\n    samtools index ${prefix}.dedup.sorted.bam\n    bedtools bamtobed -i ${prefix}.dedup.sorted.bam | sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 > ${prefix}.dedup.sorted.bed\n    \"\"\"\n}", "process YARA_MAPPER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::yara=1.0.2 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-f13549097a0d1ca36f9d4f017636fb3609f6c083:f794a548b8692f29264c8984ff116c2141b90d9e-0' :\n        'quay.io/biocontainers/mulled-v2-f13549097a0d1ca36f9d4f017636fb3609f6c083:f794a548b8692f29264c8984ff116c2141b90d9e-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path index\n\n    output:\n    tuple val(meta), path(\"*.mapped.bam\"), emit: bam\n    tuple val(meta), path(\"*.mapped.bam.bai\"), emit: bai\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        yara_mapper \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f bam \\\\\n            ${index}/yara \\\\\n            $reads | samtools view -@ $task.cpus -hb -F4 | samtools sort -@ $task.cpus > ${prefix}.mapped.bam\n\n        samtools index -@ $task.cpus ${prefix}.mapped.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            yara: \\$(echo \\$(yara_mapper --version 2>&1) | sed 's/^.*yara_mapper version: //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        yara_mapper \\\\\n            $args \\\\\n            -t $task.cpus \\\\\n            -f bam \\\\\n            ${index}/yara \\\\\n            ${reads[0]} \\\\\n            ${reads[1]} > output.bam\n\n        samtools view -@ $task.cpus -hF 4 -f 0x40 -b output.bam | samtools sort -@ $task.cpus > ${prefix}_1.mapped.bam\n        samtools view -@ $task.cpus -hF 4 -f 0x80 -b output.bam | samtools sort -@ $task.cpus > ${prefix}_2.mapped.bam\n\n        samtools index -@ $task.cpus ${prefix}_1.mapped.bam\n        samtools index -@ $task.cpus ${prefix}_2.mapped.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            yara: \\$(echo \\$(yara_mapper --version 2>&1) | sed 's/^.*yara_mapper version: //; s/ .*\\$//')\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["heinzlab/chip-seq-pipeline/picard"], "list_wf_names": ["heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 2, "tools": ["SSP", "gffread"], "nb_own": 2, "list_own": ["heinzlab", "higsch"], "nb_wf": 2, "list_wf": ["chip-seq-pipeline", "togetherforever"], "list_contrib": ["higsch", "c-guzman"], "nb_contrib": 2, "codes": ["\nprocess GetNucleotideSequences {\n\n  tag \"$sample\"\n\n  input:\n  set val(sample), file(gtf) from gtfs\n  file genome_fasta\n\n  output:\n  set val(\"${sample}\"), file(\"${sample}.fasta\") into nucleotide_fastas\n\n  script:\n  \"\"\"\n  gffread -F -w ${sample}.fasta -g $genome_fasta ${gtf}\n  \"\"\"\n\n}", "\nprocess ssp {\n    tag \"${bam[0].baseName}\"\n    publishDir \"${params.outdir}/ssp\", mode: \"copy\"\n\n    when:\n    !params.skip_qc\n\n    input:\n    file bam from bam_dedup_ssp\n    file bai from bai_dedup_ssp\n    file chrom_sizes\n\n    output:\n    file 'sspout/*.{txt,pdf}' into ssp_results\n\n    script:\n    prefix = bam[0].toString() - ~/(\\.sorted)?(\\.bam)?$/\n    if (!params.singleEnd) {\n        \"\"\"\n        ssp -i $bam -o ${prefix} --gt $chrom_sizes -p ${task.cpus} --pair\n        \"\"\"\n    } else {\n        \"\"\"\n        ssp -i $bam -o ${prefix} --gt $chrom_sizes -p ${task.cpus}\n        \"\"\"\n    }\n}"], "list_proc": ["higsch/togetherforever/GetNucleotideSequences", "heinzlab/chip-seq-pipeline/ssp"], "list_wf_names": ["higsch/togetherforever", "heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 2, "tools": ["FastQC", "MultiQC"], "nb_own": 2, "list_own": ["hirenbioinfo", "heinzlab"], "nb_wf": 2, "list_wf": ["Nanopipe", "chip-seq-pipeline"], "list_contrib": ["hirenbioinfo", "c-guzman"], "nb_contrib": 2, "codes": ["\nprocess runFastQC{\n    publishDir \"${out_dir}/qc/raw/${sample}\", mode: 'copy', overwrite: false\n\n    input:\n        set sample, file(in_fastq) from read_pair\n\n    output:\n        file(\"${sample}_fastqc/*.zip\") into fastqc_files\n\n    \"\"\"\n    mkdir ${sample}_fastqc\n    fastqc --outdir ${sample}_fastqc \\\n    -t ${task.cpus} \\\n    ${in_fastq.get(0)} \\\n    ${in_fastq.get(1)}\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file (fastqc:'fastqc/*') from fastqc_results.collect()\n    file ('samtools/*') from samtools_stats.collect()\n    file ('picard/*') from picard_reports.collect()\n    file ('deeptools/*') from deepTools_multiqc.collect()\n\n    output:\n    file '*multiqc_report.html' into multiqc_report\n    file '*_data' into multiqc_data\n    file '.command.err' into multiqc_stderr\n    val prefix into multiqc_prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . 2>&1\n    \"\"\"\n}"], "list_proc": ["hirenbioinfo/Nanopipe/runFastQC", "heinzlab/chip-seq-pipeline/multiqc"], "list_wf_names": ["hirenbioinfo/Nanopipe", "heinzlab/chip-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    set val(name), file(reads) from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n    file '.command.out' into fastqc_stdout\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["heinzlab/csrna/fastqc"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": [" process bowtie2 {\n        tag \"$reads\"\n        publishDir path: { params.saveAlignedIntermediates ? \"${params.outdir}/bowtie2\" : params.outdir }, mode: 'copy',\n               saveAs: {filename -> params.saveAlignedIntermediates ? filename : null }\n\n        input:\n        file reads from trimmed_reads_bowtie2\n        file bt2_indices\n\n        output:\n        file '*.bowtie2.bam' into bowtie2_bam, bowtie2_bam_ngi\n\n        script:\n        index_base = bt2_indices[0].toString()  - ~/\\.\\d+\\.bt2/\n        prefix = reads.toString() - ~/(.R1)?(_R1)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        if (params.singleEnd) {\n            \"\"\"\n            bowtie2 \\\\\n                -x $index_base \\\\\n                -U $reads \\\\\n                --very-sensitive \\\\\n                -p ${task.cpus} \\\\\n                -t \\\\\n                | samtools view -bT $index_base - | samtools sort - > ${prefix}.bowtie2.bam\n            \"\"\"\n        } else {\n            \"\"\"\n            bowtie2 \\\\\n                -x $index_base \\\\\n                -1 $reads[0] \\\\\n                -2 $reads[1] \\\\\n                --very-sensitive \\\\\n                -p ${task.cpus} \\\\\n                -t \\\\\n                | samtools view -bT $index_base - | samtools sort - > ${prefix}.bowtie2.bam\n            \"\"\"\n        }\n    }"], "list_proc": ["heinzlab/csrna/bowtie2"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie"], "nb_own": 2, "list_own": ["hoelzer-lab", "heinzlab"], "nb_wf": 2, "list_wf": ["treat", "csrna"], "list_contrib": ["lmfaber", "hoelzer", "c-guzman"], "nb_contrib": 3, "codes": [" process ngi_visualizations {\n        tag \"$bowtie2_bam\"\n        publishDir \"${params.outdir}/bowtie2/ngi_visualizations\", mode: 'copy'\n\n        input:\n        file gtf from gtf\n        file bowtie2_bam from bowtie2_bam_ngi\n\n        output:\n        file '*.{png,pdf}' into bowtie2_ngi_visualizations\n\n        script:\n                                                          \n                                                                                   \n        \"\"\"\n        #!/usr/bin/env python\n        from ngi_visualizations.biotypes import count_biotypes\n        count_biotypes.main('$gtf','$bowtie2_bam')\n        \"\"\"\n    }", "\nprocess SAMTOOLS_FLAGSTATS {\n  label 'HISAT2'\n  input:\n    tuple val(name), file(assembly)\n\n  output:\n    tuple val (name), file('flagstats.txt')\n  \n  shell:\n  \"\"\"\n  samtools flagstat ${assembly} > \"flagstats.txt\"\n  \"\"\"\n}"], "list_proc": ["heinzlab/csrna/ngi_visualizations", "hoelzer-lab/treat/SAMTOOLS_FLAGSTATS"], "list_wf_names": ["heinzlab/csrna", "hoelzer-lab/treat"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess samtools {\n    tag \"${bam.baseName}\"\n    publishDir path: \"${params.outdir}/bowtie2\", mode: 'copy',\n               saveAs: { filename ->\n                   if (filename.indexOf(\".stats.txt\") > 0) \"stats/$filename\"\n                   else params.saveAlignedIntermediates ? filename : null\n               }\n\n    input:\n    file bam from bowtie2_bam\n\n    output:\n    file '*.sorted.bam' into bam_picard, bam_for_mapped\n    file '*.sorted.bam.bai' into bowtie2_bai, bai_for_mapped\n    file '*.sorted.bed' into bed_total\n    file '*.stats.txt' into samtools_stats\n\n    script:\n    \"\"\"\n    samtools sort $bam -o ${bam.baseName}.sorted.bam\n    samtools index ${bam.baseName}.sorted.bam\n    bedtools bamtobed -i ${bam.baseName}.sorted.bam | sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 > ${bam.baseName}.sorted.bed\n    samtools stats ${bam.baseName}.sorted.bam > ${bam.baseName}.stats.txt\n    \"\"\"\n}"], "list_proc": ["heinzlab/csrna/samtools"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess bowtie2_mapped {\n    tag \"${input_files[0].baseName}\"\n    publishDir \"${params.outdir}/bowtie2/mapped\", mode: 'copy'\n\n    input:\n    file input_files from bam_for_mapped.collect()\n    file bai from bai_for_mapped.collect()\n\n    output:\n    file 'mapped_refgenome.txt' into bwa_mapped\n\n    script:\n    \"\"\"\n    for i in $input_files\n    do\n      samtools idxstats \\${i} | awk -v filename=\"\\${i}\" '{mapped+=\\$3; unmapped+=\\$4} END {print filename,\"\\t\",mapped,\"\\t\",unmapped}'\n    done > mapped_refgenome.txt\n    \"\"\"\n}"], "list_proc": ["heinzlab/csrna/bowtie2_mapped"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard", "BEDTools"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess picard {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/picard\", mode: 'copy'\n\n    input:\n    file bam from bam_picard\n\n    output:\n    file '*.dedup.sorted.bam' into bam_dedup_ssp, bam_dedup_deepTools\n    file '*.dedup.sorted.bam.bai' into bai_dedup_deepTools, bai_dedup_ssp\n    file '*.dedup.sorted.bed' into bed_dedup\n    file '*.picardDupMetrics.txt' into picard_reports\n\n    script:\n    prefix = bam[0].toString() - ~/(\\.sorted)?(\\.bam)?$/\n    if( task.memory == null ){\n        log.warn \"[Picard MarkDuplicates] Available memory not known - defaulting to 6GB ($prefix)\"\n        avail_mem = 6000\n    } else {\n        avail_mem = task.memory.toMega()\n        if( avail_mem <= 0){\n            avail_mem = 6000\n            log.warn \"[Picard MarkDuplicates] Available memory 0 - defaulting to 6GB ($prefix)\"\n        } else if( avail_mem < 250){\n            avail_mem = 250\n            log.warn \"[Picard MarkDuplicates] Available memory under 250MB - defaulting to 250MB ($prefix)\"\n        }\n    }\n    \"\"\"\n    picard MarkDuplicates \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.dedup.bam \\\\\n        ASSUME_SORTED=true \\\\\n        REMOVE_DUPLICATES=false \\\\\n        METRICS_FILE=${prefix}.picardDupMetrics.txt \\\\\n        VALIDATION_STRINGENCY=LENIENT \\\\\n        PROGRAM_RECORD_ID='null'\n    samtools sort ${prefix}.dedup.bam -o ${prefix}.dedup.sorted.bam\n    samtools index ${prefix}.dedup.sorted.bam\n    bedtools bamtobed -i ${prefix}.dedup.sorted.bam | sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 > ${prefix}.dedup.sorted.bed\n    \"\"\"\n}"], "list_proc": ["heinzlab/csrna/picard"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 1, "tools": ["SSP"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess ssp {\n    tag \"${bam[0].baseName}\"\n    publishDir \"${params.outdir}/ssp\", mode: \"copy\"\n\n    when:\n    !params.skip_qc\n\n    input:\n    file bam from bam_dedup_ssp\n    file bai from bai_dedup_ssp\n    file chrom_sizes\n\n    output:\n    file 'sspout/*.{txt,pdf}' into ssp_results\n\n    script:\n    prefix = bam[0].toString() - ~/(\\.sorted)?(\\.bam)?$/\n    if (!params.singleEnd) {\n        \"\"\"\n        ssp -i $bam -o ${prefix} --gt $chrom_sizes -p ${task.cpus} --pair\n        \"\"\"\n    } else {\n        \"\"\"\n        ssp -i $bam -o ${prefix} --gt $chrom_sizes -p ${task.cpus}\n        \"\"\"\n    }\n}"], "list_proc": ["heinzlab/csrna/ssp"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["csrna"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    tag \"$prefix\"\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n    file (fastqc:'fastqc/*') from fastqc_results.collect()\n    file ('samtools/*') from samtools_stats.collect()\n    file ('picard/*') from picard_reports.collect()\n    file ('deeptools/*') from deepTools_multiqc.collect()\n\n    output:\n    file '*multiqc_report.html' into multiqc_report\n    file '*_data' into multiqc_data\n    file '.command.err' into multiqc_stderr\n    val prefix into multiqc_prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . 2>&1\n    \"\"\"\n}"], "list_proc": ["heinzlab/csrna/multiqc"], "list_wf_names": ["heinzlab/csrna"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["smrna-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess makeBowtieIndex {\n\n    publishDir path: { params.saveReference ? \"${params.outdir}/bowtie/reference\" : params.outdir },\n               saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n    input:\n    file mature from mature\n    file hairpin from hairpin\n\n    output:\n    file 'mature_idx.*' into mature_index\n    file 'hairpin_idx.*' into hairpin_index\n\n    script:\n    \"\"\"\n    fasta_formatter -w 0 -i $mature -o mature_igenome.fa\n    fasta_nucleotide_changer -d -i mature_igenome.fa -o mature_idx.fa\n    bowtie-build mature_idx.fa mature_idx\n    fasta_formatter -w 0 -i $hairpin -o hairpin_igenome.fa\n    fasta_nucleotide_changer -d -i hairpin_igenome.fa -o hairpin_idx.fa\n    bowtie-build hairpin_idx.fa hairpin_idx\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/makeBowtieIndex"], "list_wf_names": ["heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["smrna-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    tag \"$reads\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    when:\n    !params.skip_qc && !params.skip_fastqc\n\n    input:\n    file reads from raw_reads_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/fastqc"], "list_wf_names": ["heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["smrna-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess bowtie_miRBase_mature {\n    tag \"$reads\"\n    publishDir \"${params.outdir}/bowtie/miRBase_mature\", mode: 'copy', pattern: '*.mature_unmapped.fq.gz'\n\n    input:\n    file reads from trimmed_reads_bowtie\n    file index from mature_index\n\n    output:\n    file '*.mature.bam' into miRBase_mature_bam\n    file '*.mature_unmapped.fq.gz' into mature_unmapped_reads\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.toString() - ~/(.R1)?(_R1)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -q <(zcat $reads) \\\\\n        -p 2 \\\\\n        -t \\\\\n        -k 1 \\\\\n        -m 1 \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        --un ${prefix}.mature_unmapped.fq \\\\\n        -S \\\\\n        | samtools view -bS - > ${prefix}.mature.bam\n    gzip ${prefix}.mature_unmapped.fq\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/bowtie_miRBase_mature"], "list_wf_names": ["heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["smrna-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess bowtie_miRBase_hairpin {\n    tag \"$reads\"\n    publishDir \"${params.outdir}/bowtie/miRBase_hairpin\", mode: 'copy', pattern: '*.hairpin_unmapped.fq.gz'\n\n    input:\n    file reads from mature_unmapped_reads\n    file index from hairpin_index\n\n    output:\n    file '*.hairpin.bam' into miRBase_hairpin_bam\n    file '*.hairpin_unmapped.fq.gz' into hairpin_unmapped_reads\n\n    script:\n    index_base = index.toString().tokenize(' ')[0].tokenize('.')[0]\n    prefix = reads.toString() - '.mature_unmapped.fq.gz'\n    \"\"\"\n    bowtie \\\\\n        $index_base \\\\\n        -p 2 \\\\\n        -t \\\\\n        -k 1 \\\\\n        -m 1 \\\\\n        --best \\\\\n        --strata \\\\\n        -e 99999 \\\\\n        --chunkmbs 2048 \\\\\n        -q <(zcat $reads) \\\\\n        --un ${prefix}.hairpin_unmapped.fq \\\\\n        -S \\\\\n        | samtools view -bS - > ${prefix}.hairpin.bam\n    gzip ${prefix}.hairpin_unmapped.fq\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/bowtie_miRBase_hairpin"], "list_wf_names": ["heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["smrna-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": ["\nprocess miRBasePostAlignment {\n    tag \"$input\"\n    publishDir \"${params.outdir}/bowtie\", mode: 'copy', saveAs: wrap_mature_and_hairpin\n\n    input:\n    file input from miRBase_mature_bam.mix(miRBase_hairpin_bam)\n\n    output:\n    file \"${input.baseName}.count\" into miRBase_counts\n    file \"${input.baseName}.sorted.bam\" into miRBase_bam\n    file \"${input.baseName}.sorted.bam.bai\" into miRBase_bai\n\n    script:\n    \"\"\"\n    samtools sort ${input.baseName}.bam -o ${input.baseName}.sorted.bam\n    samtools index ${input.baseName}.sorted.bam\n    samtools idxstats ${input.baseName}.sorted.bam > ${input.baseName}.count\n    \"\"\"\n}"], "list_proc": ["heinzlab/smrna-seq-pipeline/miRBasePostAlignment"], "list_wf_names": ["heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["heinzlab"], "nb_wf": 1, "list_wf": ["smrna-seq-pipeline"], "list_contrib": ["c-guzman"], "nb_contrib": 1, "codes": [" process bowtie2 {\n        tag \"$reads\"\n        publishDir \"${params.outdir}/bowtie2\", mode: 'copy'\n\n        input:\n        file reads from trimmed_reads_bowtie2\n        file bt2_indices\n\n        output:\n        file '*.bowtie2.bam' into bowtie2_bam, bowtie2_bam_for_unmapped\n\n        script:\n        index_base = bt2_indices[0].toString()  - ~/\\.\\d+\\.bt2/\n        prefix = reads.toString() - ~/(.R1)?(_R1)?(_trimmed)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        \"\"\"\n        bowtie2 \\\\\n            -x $index_base \\\\\n            -U $reads \\\\\n            -k 10 \\\\\n            --very-sensitive \\\\\n            -p 8 \\\\\n            -t \\\\\n            | samtools view -bT $index_base - > ${prefix}.bowtie2.bam\n        \"\"\"\n    }"], "list_proc": ["heinzlab/smrna-seq-pipeline/bowtie2"], "list_wf_names": ["heinzlab/smrna-seq-pipeline"]}, {"nb_reuse": 16, "tools": ["PLINK", "HISAT2", "SAMtools", "STAR", "MMseqs", "MultiQC", "FeatureCounts", "FREEC", "BEDTools", "G-BLASTN", "FastQC", "MiXCR", "GATK"], "nb_own": 17, "list_own": ["msmallegan", "seandavi", "sripaladugu", "rikenbit", "markgene", "supark87", "herczegrobert", "kevbrick", "lobleya", "mbosio85", "szabogtamas", "jiangweiyao", "wslh-bio", "markxiao", "zamanianlab", "ralsallaq", "juneb4869"], "nb_wf": 16, "list_wf": ["cutnrun", "prac_nextflow", "metaGx_nf", "RNAseq-VC-nf", "coregenome_align_nf", "ownpipeline", "rmghc-workshop-19", "GATK4_WGS", "repertoireseq_container", "RamDAQ-prototype", "new_meripseqpipe", "germline_somatic", "nf-core-gatkcohortcall", "cmgd_coordinator", "ssds_nfcore", "spriggan", "PRS-dev"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "supark87", "kevbrick", "szabogtamas", "jiangweiyao", "tiagochst", "markxiao", "davidmasp", "drpatelh", "Rotholandus", "k-florek", "markgene", "pcantalupo", "malinlarsson", "olgabot", "jrdemasi", "manabuishii", "skrakau", "msmallegan", "sofiahaglund", "seandavi", "lescai", "yuifu", "ewels", "herczegrobert", "szilvajuhos", "FriederikeHanssen", "chuan-wang", "juneb4869", "nf-core-bot", "maxulysse", "ggabernet", "winni2k", "apeltzer", "wheelern", "lobleya", "mbosio85", "adrlar", "AbigailShockey", "myoshimura080822", "ralsallaq", "hynesgra"], "nb_contrib": 46, "codes": ["\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess run_hisat2  {\n\n    tag {\"${proj_id}\"}\n    publishDir \"output_${proj_id}/${run_id}/04_hisat2_rrna\", mode: 'copy', overwrite: true\n\n    clusterOptions = '-S /bin/bash -l nc=8'\n    container \"docker.io/myoshimura080822/hisat2_set:190611\"\n\n    input:\n    val proj_id\n    set run_id, fastq_name, file(fastq), option_name, option, strandedness_name, strandedness, index_name, index, file(index_files), pipeline_class from hisat2_conditions\n\n    output:\n    set run_id, pipeline_class, fastq_name, file(\"*.bam\") into hisat2_output, hisat2_output_forsummary\n    file \"*.bai\"\n    file \"*.command.err\"\n    file \"*.bam\" into hisat2_output_to_count\n\n    script:\n    def scripts_dir = workflow.scriptFile.parent.parent + \"/bamtools_scripts\"\n\n    \"\"\"\n    hisat2 $option -x $index -U $fastq $strandedness 2> ${fastq_name}.hisat2.command.err | samtools view -bS - | samtools sort - -o ${fastq_name}_rrna_trim.sort.bam \n    samtools index ${fastq_name}_rrna_trim.sort.bam \n    \"\"\"\n}", "\nprocess split_reads {\n\n    input:\n        tuple val(id), file(bam) from duplicate_bams\n\n    output:\n        tuple id, file(\"${id}_split.bam\") into split_bams1, split_bams2\n\n    when:\n        params.bam\n\n    \"\"\"\n        gatk \\\n          --java-options \\\n          -Xmx4g \\\n          SplitNCigarReads \\\n          -R \"${aedesgenome}/genome.fa\" \\\n          -I ${bam} \\\n          -O ${id}_split.bam \\\n          -tmp-dir /home/BIOTECH/zamanian/tmp\n    \"\"\"\n}", "\nprocess VariantRecalibrator_INDELs {\n\tpublishDir \"${params.outdir}/VariantRecalibrator\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile reference\n\tfile reference_fai\n\tfile reference_dict\n\tfile recalibrated_snps_raw_indels\n\tfile dbsnp\n\tfile dbsnp_idx\n\tfile golden_indel\n\tfile golden_indel_idx\n\n\toutput:\n\tfile 'recalibrate_INDEL.recal' into variantrecalibrator_indel_recal\n\tfile 'recalibrate_INDEL.recal.idx' into variantrecalibrator_indel_recal_idx\n\tfile 'recalibrate_INDEL.tranches' into variantrecalibrator_indel_tranches\n\t\n\tscript:\n\t\"\"\"\n\tgatk VariantRecalibrator \\\n\t-V $recalibrated_snps_raw_indels \\\n \t-R $reference \\\n\t--resource mills,known=false,training=true,truth=true,prior=12.0:./$golden_indel \\\n    \t--resource dbsnp,known=true,training=false,truth=false,prior=2.0:./$dbsnp \\\n\t-an QD \\\n    \t-an DP \\\n    \t-an FS \\\n\t-an SOR \\\n    \t-an MQRankSum \\\n    \t-an ReadPosRankSum \\\n    \t-mode INDEL \\\n    \t-tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \\\n\t--max-gaussians 4 \\\n    \t-O recalibrate_INDEL.recal \\\n    \t--tranches-file recalibrate_INDEL.tranches \\\n\t\"\"\"\n}", " process MeyerPrepration{\n        label 'build_index'\n        tag \"onecore_peak\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/meyerPrepration\" : params.outdir },\n                saveAs: { params.saveReference ? it : null }, mode: 'copy'       \n\n        input:\n        file fasta\n        file chromsizesfile from chromsizesfile.collect()\n\n        output:\n        file \"chrName.txt\" into chrNamefile\n        file \"genome.bin25.bed\" into bin25file\n        file \"genomebin\" into genomebin\n\n        when:\n        !params.skip_meyer && !params.skip_peakCalling\n\n        shell:\n        '''\n        awk '{print $1}' !{chromsizesfile} > chrName.txt\n        mkdir genomebin\n        bedtools makewindows -g !{chromsizesfile} -w 25 > genome.bin25.bed\n        awk '{print \"cat genome.bin25.bed | grep \"$1\" > genomebin/\"$1\".bin25.bed\"}' chrName.txt | xargs -iCMD -P!{task.cpus} bash -c CMD\n        '''\n    }", "\nprocess fastqc {\n    \n                            \n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files2\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "\nprocess samtools {\n  tag \"$name\"\n\n  publishDir \"${params.outdir}/alignments\", mode: 'copy',pattern:\"*.sorted.*\"\n  publishDir \"${params.outdir}/alignments\", mode: 'copy', pattern:\"*.stats.txt*\"\n  publishDir \"${params.outdir}/coverage\", mode: 'copy', pattern:\"*.depth.tsv*\"\n\n  input:\n  set val(name), file(sam) from sam_files\n\n  output:\n  file(\"${name}.depth.tsv\") into cov_files\n  file(\"${name}.stats.txt\") into stats_multiqc\n  file(\"*.sorted.*\")\n\n  shell:\n  \"\"\"\n  samtools view -S -b ${name}.sam > ${name}.bam\n  samtools sort ${name}.bam > ${name}.sorted.bam\n  samtools index ${name}.sorted.bam\n  samtools depth -a ${name}.sorted.bam > ${name}.depth.tsv\n  samtools stats ${name}.sorted.bam > ${name}.stats.txt\n  \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess blastrun{\n    container 'my-image-spades'\n\n    tag \"$sample_id\"\n    publishDir \"${params.outdir}/blast_out_files\",mode:'copy'\n    \n    input:\n    path cpmp from params.references1\n    val sample_id_db from blastdb_name \n    path dbdir from blastdb_dir\n\n    output:\n    file (\"${sample_id_db}.blast\") into blastout_ch\n    file(\"${sample_id_db}.fasta\") into sequences\n    val \"${sample_id_db}\" into seq_name\n    file(\"${sample_id_db}.fasta\") into cpmp_all\n    \n    script:\n\n\n    \"\"\"\n    blastn -query $cpmp -db $dbdir/$sample_id_db  -outfmt \"6 qseqid sseqid pident qlen qstart qend sstart send sseq\" -out ${sample_id_db}.blast\n    cat ${sample_id_db}.blast | awk '\\$8-\\$7 > 250 {print \">\" \\$2 \"_${sample_id_db}_\" \"\\\\n\" \\$9}' > ${sample_id_db}.fasta\n    \n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    \n    label 'memory_singleCPU_2_task'\n    label 'cpus_4'\n\t\n\ttag { chr }\n                                                                                                         \n\t\n\n    input:\n\ttuple chr, file (workspace) from gendb_ch\n   \tfile genome from ch_fasta\n    file genomefai from ch_fastaFai\n    file genomedict from Channel.value(file(params.dict ))\n    file dbsnp_resource_vcf from Channel.value(file(params.dbsnp ))\n    file dbsnp_resource_vcf_idx from Channel.value(file(params.dbsnpIndex ))\n\n\toutput:\n    tuple chr, file(\"${params.cohort}.${chr}.vcf\"), file(\"${params.cohort}.${chr}.vcf.idx\") into vcf_ch\n    \n    script:\n\t\"\"\"\n\n    WORKSPACE=\\$( basename ${workspace} )\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g  \\\n     GenotypeGVCFs \\\n     -R ${genome} \\\n     -O ${params.cohort}.${chr}.vcf \\\n     -D ${dbsnp_resource_vcf} \\\n     -G StandardAnnotation \\\n     --only-output-calls-starting-in-intervals \\\n     --use-new-qual-calculator \\\n     -V gendb://\\$WORKSPACE \\\n     -L ${chr}\n\n\t\"\"\"\n}", "\nprocess map {\n\n  maxForks 1\n  publishDir 'results/bam'\n\n  input:\n  set sample_id, file(reads), file(index) from reads_for_mapping.combine(index)\n\n  output:\n  set sample_id, file(\"*Aligned.out.bam\") into mapped_genome\n  set sample_id, file(\"*toTranscriptome.out.bam\") into mapped_transcriptome\n  file '*' into star\n\n  script:\n  \"\"\"\n  STAR  --runThreadN 5 \\\n  --genomeDir ${index} \\\n  --readFilesIn ${reads.findAll{ it =~ /\\_R1\\./ }.join(',')} \\\n                ${reads.findAll{ it =~ /\\_R2\\./ }.join(',')} \\\n  --readFilesCommand zcat \\\n  --outSAMtype BAM Unsorted \\\n  --outSAMunmapped Within \\\n  --outSAMattributes NH HI NM MD AS \\\n  --outReadsUnmapped Fastx \\\n  --quantMode TranscriptomeSAM \\\n  --outFileNamePrefix ${sample_id}_\n  \"\"\"\n}", "\nprocess reconstructBCRepertoireMiXCR {\n\n    publishDir params.clonotype_dir, pattern: '*.txt', mode: 'copy'\n\n    input:\n        val species_alias from params.species_alias\n        tuple sample, \"${sample}_trim_1.fastq\", \"${sample}_trim_2.fastq\" from trimmed_fastqs\n\n    output:\n        file \"${sample}_repertoire.txt\" into repertoire_reports\n        tuple sample, \"${sample}.clns\" into repertoire_clns\n\n    \"\"\"\n    /usr/share/mixcr/mixcr-3.0.13/mixcr analyze shotgun\\\n    --only-productive\\\n    --starting-material rna\\\n    --receptor-type bcr\\\n    --species $species_alias\\\n    --report ${sample}_repertoire.txt\\\n    ${sample}_trim_1.fastq ${sample}_trim_2.fastq\\\n    $sample\n    \"\"\"\n}", "\nprocess ConsensusPeakSetDESeq {\n    tag \"${antibody}\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/macs/${PEAK_TYPE}/consensus/${antibody}/deseq2\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.endsWith(\".igv.txt\")) null\n                      else filename\n                }\n\n    when:\n    params.macs_gsize && replicatesExist && multipleGroups && !params.skip_diff_analysis\n\n    input:\n    set val(antibody), val(replicatesExist), val(multipleGroups), file(bams) ,file(saf) from ch_group_bam_deseq\n    file deseq2_pca_header from ch_deseq2_pca_header\n    file deseq2_clustering_header from ch_deseq2_clustering_header\n\n    output:\n    file \"*featureCounts.txt\" into ch_macs_consensus_counts\n    file \"*featureCounts.txt.summary\" into ch_macs_consensus_counts_mqc\n    file \"*.{RData,results.txt,pdf,log}\" into ch_macs_consensus_deseq_results\n    file \"sizeFactors\" into ch_macs_consensus_deseq_factors\n    file \"*vs*/*.{pdf,txt}\" into ch_macs_consensus_deseq_comp_results\n    file \"*vs*/*.bed\" into ch_macs_consensus_deseq_comp_bed\n    file \"*igv.txt\" into ch_macs_consensus_deseq_comp_igv\n    file \"*.tsv\" into ch_macs_consensus_deseq_mqc\n\n    script:\n    prefix = \"${antibody}.consensus_peaks\"\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    bam_ext = params.single_end ? \".mLb.clN.sorted.bam\" : \".mLb.clN.bam\"\n    pe_params = params.single_end ? '' : \"-p --donotsort\"\n    \"\"\"\n    featureCounts \\\\\n        -F SAF \\\\\n        -O \\\\\n        --fracOverlap 0.2 \\\\\n        -T $task.cpus \\\\\n        $pe_params \\\\\n        -a $saf \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bam_files.join(' ')}\n\n    featurecounts_deseq2.r -i ${prefix}.featureCounts.txt -b '$bam_ext' -o ./ -p $prefix -s .mLb\n\n    sed 's/deseq2_pca/deseq2_pca_${task.index}/g' <$deseq2_pca_header >tmp.txt\n    sed -i -e 's/DESeq2:/${antibody} DESeq2:/g' tmp.txt\n    cat tmp.txt ${prefix}.pca.vals.txt > ${prefix}.pca.vals_mqc.tsv\n\n    sed 's/deseq2_clustering/deseq2_clustering_${task.index}/g' <$deseq2_clustering_header >tmp.txt\n    sed -i -e 's/DESeq2:/${antibody} DESeq2:/g' tmp.txt\n    cat tmp.txt ${prefix}.sample.dists.txt > ${prefix}.sample.dists_mqc.tsv\n\n    find * -type f -name \"*.FDR0.05.results.bed\" -exec echo -e \"bwa/mergedLibrary/macs/${PEAK_TYPE}/consensus/${antibody}/deseq2/\"{}\"\\\\t255,0,0\" \\\\; > ${prefix}.igv.txt\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", " process buildMMseqDB {\n        container \"${container_mmseqs2}\"\n        label 'io_mem'\n                                                           \n        \n        input:\n        file geneExDBFAA\n\n                                                             \n        when:\n        ! params.useDiamond\n        \n        output:\n        file('targetDBMMseq*') into targetMMseq_ch\n        \n        \"\"\"\n        mmseqs createdb ${geneExDBFAA}  targetDBMMseq --dbtype 0 --shuffle 1 --createdb-mode 0 --id-offset 0 --compressed 0 -v 3\n        \"\"\"\n    }", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    tuple val(sample_id), path(reads) from read_pairs2_ch\n\n    output:\n    path \"fastqc_${sample_id}_logs\" into fastqc_ch\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}", "\nprocess target_qc_sex_check {\n\n    publishDir \"${params.outdir}/intermediate_files\", mode: 'copy'\n\n    input:\n    tuple prefix, file(bed), file(bim), file(cov), file(fam), file(height) from target_data.copy4\n    tuple _tmp, file(irem), file(hh), file(snplist), file(fam1), file(prune_in), file(prune_out), file(het) from target_qc_standard_gwas_qc_1_output_copy3\n    file valid_sample from target_qc_standard_gwas_qc_2_output\n\n    output:\n    tuple file(\"${prefix_qc}.sexcheck\"), file(\"${prefix_qc}.valid\"), file(\"${prefix_qc}.hh\") into target_qc_sex_check_output\n\n    script:\n    prefix_qc = \"${prefix}.QC\"\n    \"\"\"\n    plink --bfile ${prefix} --extract ${prune_in} --keep ${valid_sample} --check-sex --out ${prefix_qc}\n    if test -f \"${prefix_qc}.log\";\n        then mv ${prefix_qc}.log ${params.outdir}/logs/${prefix_qc}.log.4\n    fi\n\n    R --file=${bin}/target_qc_sex_check.R --args ${valid_sample} ${prefix_qc}.sexcheck ${prefix_qc}.valid\n    \"\"\"  \n}"], "list_proc": ["kevbrick/ssds_nfcore/FASTQC", "rikenbit/RamDAQ-prototype/run_hisat2", "zamanianlab/RNAseq-VC-nf/split_reads", "lobleya/GATK4_WGS/VariantRecalibrator_INDELs", "juneb4869/new_meripseqpipe/MeyerPrepration", "jiangweiyao/coregenome_align_nf/fastqc", "wslh-bio/spriggan/samtools", "sripaladugu/germline_somatic/ControlFREEC", "supark87/prac_nextflow/blastrun", "msmallegan/rmghc-workshop-19/map", "szabogtamas/repertoireseq_container/reconstructBCRepertoireMiXCR", "markgene/cutnrun/ConsensusPeakSetDESeq", "herczegrobert/ownpipeline/multiqc", "ralsallaq/metaGx_nf/buildMMseqDB", "seandavi/cmgd_coordinator/fastqc", "markxiao/PRS-dev/target_qc_sex_check"], "list_wf_names": ["juneb4869/new_meripseqpipe", "kevbrick/ssds_nfcore", "sripaladugu/germline_somatic", "markgene/cutnrun", "herczegrobert/ownpipeline", "msmallegan/rmghc-workshop-19", "zamanianlab/RNAseq-VC-nf", "ralsallaq/metaGx_nf", "szabogtamas/repertoireseq_container", "wslh-bio/spriggan", "jiangweiyao/coregenome_align_nf", "supark87/prac_nextflow", "markxiao/PRS-dev", "seandavi/cmgd_coordinator", "rikenbit/RamDAQ-prototype", "lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["heuermh"], "nb_wf": 1, "list_wf": ["artic"], "list_contrib": ["heuermh", "drpatelh"], "nb_contrib": 2, "codes": ["\nprocess MULTIQC {\n    publishDir \"${params.outdir}/multiqc\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_multiqc\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_reports_mqc.collect().ifEmpty([])\n            'samtools/*'                                                  \n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename $custom_config_file -m custom_content -m fastqc -m samtools\n    \"\"\"\n}"], "list_proc": ["heuermh/artic/MULTIQC"], "list_wf_names": ["heuermh/artic"]}, {"nb_reuse": 1, "tools": ["eFetch Pmc"], "nb_own": 1, "list_own": ["heuermh"], "nb_wf": 1, "list_wf": ["sars-cov-2"], "list_contrib": ["heuermh"], "nb_contrib": 1, "codes": ["\nprocess efetch {\n  publishDir \"${params.resultsDir}/genbank\", mode: 'copy'\n  container \"quay.io/biocontainers/entrez-direct:13.8--pl526h375a9b1_0\"\n\n  input:\n  val acc from accessions\n\n  output:\n  file \"${acc.get(0)}-${acc.get(1)}.gb\" into sequences\n\n  \"\"\"\n  sleep 1\n  efetch -db nucleotide -format gb -mode text -id ${acc.get(2)} > ${acc.get(0)}-${acc.get(1)}.gb\n  \"\"\"\n}"], "list_proc": ["heuermh/sars-cov-2/efetch"], "list_wf_names": ["heuermh/sars-cov-2"]}, {"nb_reuse": 1, "tools": ["Percolator"], "nb_own": 1, "list_own": ["higsch"], "nb_wf": 1, "list_wf": ["togetherforever"], "list_contrib": ["higsch"], "nb_contrib": 1, "codes": ["\nprocess Percolator {\n\n  tag \"$set $fractions\"\n\n  publishDir 'results', mode: \"copy\" \n\n  input:\n  set val(set), val(fractions), val(samples), file(\"mzid?\") from mzids2pin\n\n  output:\n  set val(set), file(\"Set${set}.perco.xml\") into percolated\n\n  \"\"\"\n  echo $samples\n  mkdir mzids\n  count=1;for sam in ${samples.join(' ')}; do ln -s `pwd`/mzid\\$count mzids/\\${sam}.mzid; echo mzids/\\${sam}.mzid >> metafile; ((count++));done\n  msgf2pin -o percoin.xml -e trypsin -P \"DECOY_\" metafile\n  percolator -j percoin.xml -X \"Set${set}.perco.xml\" -N 500000 --decoy-xml-output -y\n  \"\"\"\n}"], "list_proc": ["higsch/togetherforever/Percolator"], "list_wf_names": ["higsch/togetherforever"]}, {"nb_reuse": 2, "tools": ["FastQC", "MultiQC"], "nb_own": 2, "list_own": ["hirenbioinfo", "kviljoen"], "nb_wf": 2, "list_wf": ["Nanopipe", "fastq_QC"], "list_contrib": ["hirenbioinfo", "alesssia", "kviljoen"], "nb_contrib": 3, "codes": ["\nprocess runFastQC {\n    cache 'deep'\n    tag { \"rFQC.${pairId}\" }\n\n    publishDir \"${params.outdir}/FilterAndTrim\", mode: \"copy\"\n\n    input:\n        set pairId, file(in_fastq) from ReadPairsToQual\n\n    output:\n        file(\"${pairId}_fastqc/*.zip\") into fastqc_files\n\n    \"\"\"\n    mkdir ${pairId}_fastqc\n    fastqc --outdir ${pairId}_fastqc \\\n    ${in_fastq.get(0)} \\\n    ${in_fastq.get(1)}\n    \"\"\"\n}", "\nprocess runMultiQC{\n    tag { \"${params.projectName}.rMQC\" }\n    publishDir \"${out_dir}/qc/raw\", mode: 'copy', overwrite: false\n\n    input:\n        file('*') from fastqc_files.collect()\n\n    output:\n        file('multiqc_report.html')\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["kviljoen/fastq_QC/runFastQC", "hirenbioinfo/Nanopipe/runMultiQC"], "list_wf_names": ["kviljoen/fastq_QC", "hirenbioinfo/Nanopipe"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["hirenbioinfo"], "nb_wf": 1, "list_wf": ["Nanopipe"], "list_contrib": ["hirenbioinfo"], "nb_contrib": 1, "codes": ["\nprocess assembly {\n    input:\n  file(filtread) from file(trimmed_reads)\n\n    output:\n  file('assembly.fasta') into filtlong_reads\n\nscript:\n        \"\"\"\n    flye --nano-raw ${filtread} --genome-size 1m --out-dir ./flye_output\n        \"\"\"\n}"], "list_proc": ["hirenbioinfo/Nanopipe/assembly"], "list_wf_names": ["hirenbioinfo/Nanopipe"]}, {"nb_reuse": 1, "tools": ["Filtlong"], "nb_own": 1, "list_own": ["hirenbioinfo"], "nb_wf": 1, "list_wf": ["nextflow_pipeline"], "list_contrib": ["hirenbioinfo"], "nb_contrib": 1, "codes": ["\nprocess filter_long {\n    input:\n  file reads from trimmed_for_assembly\n\n    output:\n  file('trimmed.fastq.gz') into filtlong_reads\n\n\tscript:\n        \"\"\"\n    filtlong --min_length 1000 ${reads} | gzip > trimmed.fastq.gz\n    \n        \"\"\"\n}"], "list_proc": ["hirenbioinfo/nextflow_pipeline/filter_long"], "list_wf_names": ["hirenbioinfo/nextflow_pipeline"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["hirenbioinfo"], "nb_wf": 1, "list_wf": ["nextflow_pipeline"], "list_contrib": ["hirenbioinfo"], "nb_contrib": 1, "codes": ["\nprocess assembly {\n\npublishDir \"Assembly_out\", mode: 'copy', pattern: 'assembly.fasta'\n\n    input:\n  \tfile reads from filtlong_reads\n  \n    output:\n  \tfile 'assembly.fasta' into assembly\n\n\tscript:\n\tif(params.assembler == 'flye')\n      \n        \"\"\"\n        mkdir fly_out\n    \tflye --nano-raw ${reads} --genome-size 1m --out-dir  fly_out\n    \tmv fly_out/assembly.fasta assembly.fasta\n    \t\n    \t\"\"\"\n    \n    else if(params.assembler == 'canu')\n       \n        \"\"\"\n        canu -p assembly -d canu_out genomeSize=2m -nanopore-raw \"${reads}\" maxThreads=8 gnuplotTested=true useGrid=false \n        mv canu_out/assembly.contigs.fasta assembly.fasta\n       \n        \"\"\"\n    else if(params.assembler == 'unicycler')\n        \"\"\"\n        unicycler -l \"${reads}\" -o unicycler_out\n        mv unicycler_out/assembly.fasta assembly.fasta\n        \"\"\"        \n}"], "list_proc": ["hirenbioinfo/nextflow_pipeline/assembly"], "list_wf_names": ["hirenbioinfo/nextflow_pipeline"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["hoelzer-lab"], "nb_wf": 1, "list_wf": ["treat"], "list_contrib": ["lmfaber", "hoelzer"], "nb_contrib": 2, "codes": ["\nprocess SALMON_INDEX {\n  label 'Ex90N50'\n\n  input:\n  tuple val(name), file(assembly)\n\n  output:\n  file(\"salmon_${name}/\")\n  tuple val(name), file(assembly)\n\n  shell:\n    \"\"\"\n    salmon index --keepDuplicates -t ${assembly} -i salmon_${name} -p !{params.threads}\n    \"\"\"\n}"], "list_proc": ["hoelzer-lab/treat/SALMON_INDEX"], "list_wf_names": ["hoelzer-lab/treat"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["hoelzer-lab"], "nb_wf": 1, "list_wf": ["treat"], "list_contrib": ["lmfaber", "hoelzer"], "nb_contrib": 2, "codes": ["\nprocess SAMTOOLS_SORT {\n  label 'DETONATE'\n\n  input:\n  file(sorted_bam)\n  \n  output:\n  file(\"rsem_expr.transcript.sorted.bam\")\n  \n  shell:\n  \"\"\"\n  samtools sort --threads !{params.threads} -o rsem_expr.transcript.sorted.bam rsem_expr.transcript.bam\n  \"\"\"\n}"], "list_proc": ["hoelzer-lab/treat/SAMTOOLS_SORT"], "list_wf_names": ["hoelzer-lab/treat"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["hoelzer-lab"], "nb_wf": 1, "list_wf": ["treat"], "list_contrib": ["lmfaber", "hoelzer"], "nb_contrib": 2, "codes": ["\nprocess READ_LENGTH {\n  label 'DETONATE'\n\n  input:\n  file(sorted_bam)\n\n  output:\n  stdout()\n  \n  shell:\n  \"\"\"\n    # Get read length and insert size info via the mapped bam file from bowtie\n    samtools stats rsem_expr.transcript.sorted.bam | grep -e '^SN\\tmaximum length' | awk -F\" \" '{printf(\\$4)}'\n  \"\"\"\n}"], "list_proc": ["hoelzer-lab/treat/READ_LENGTH"], "list_wf_names": ["hoelzer-lab/treat"]}, {"nb_reuse": 1, "tools": ["BLAT"], "nb_own": 1, "list_own": ["hoelzer-lab"], "nb_wf": 1, "list_wf": ["treat"], "list_contrib": ["lmfaber", "hoelzer"], "nb_contrib": 2, "codes": ["\nprocess BLAT {\n  label 'DETONATE'\n\n  input:\n  tuple val(name), file(assembly)\n  file('ta_0.fa')\n\n  output:\n  tuple file(\"ta_0_to_${name}.psl\"), file(\"${name}_to_ta_0.psl\")\n  tuple val(name), file(assembly)\n  file('ta_0.fa')\n  \n  shell:\n  \"\"\"\n  blat -minIdentity=80 ${assembly} ta_0.fa ta_0_to_${name}.psl\n  blat -minIdentity=80 ta_0.fa ${assembly} ${name}_to_ta_0.psl\n  \"\"\"\n}"], "list_proc": ["hoelzer-lab/treat/BLAT"], "list_wf_names": ["hoelzer-lab/treat"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["hoelzer-lab"], "nb_wf": 1, "list_wf": ["treat"], "list_contrib": ["lmfaber", "hoelzer"], "nb_contrib": 2, "codes": ["\nprocess HISAT2_SINGLE {\n  label 'HISAT2'\n  publishDir \"${params.output}/${params.dir}/\", mode:'copy', pattern: \"${name}.sorted.bam\"\n\n  input:\n  tuple val(name), file(assembly)\n  file(reads)\n\n  output:\n  tuple val(name), file(\"${name}.sorted.bam\")\n  \n  shell:\n  '''\n  hisat2-build !{assembly} !{name} \n  hisat2 -x !{name} -U !{reads} -p !{params.threads} | samtools view -bS | samtools sort -T tmp --threads !{params.threads} > !{name}.sorted.bam\n  ''' \n}"], "list_proc": ["hoelzer-lab/treat/HISAT2_SINGLE"], "list_wf_names": ["hoelzer-lab/treat"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["NEMO"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process blast {\n    label 'blast'\n    publishDir \"${params.output}/\", mode: 'copy', pattern: \"${name}.blast\"\n\n    input:\n    tuple val(name), file(fasta) \n    file db\n    \n    output:\n\tfile(\"${name}.blast\") \n    \n    script:\n    \"\"\"\n    makeblastdb -in ${db} -dbtype nucl\n    blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db ${db} -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" > ${name}.blast\n    \"\"\"\n}"], "list_proc": ["hoelzer/NEMO/blast"], "list_wf_names": ["hoelzer/NEMO"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["clean"], "list_contrib": ["hoelzer", "MarieLataretu"], "nb_contrib": 2, "codes": ["process fastqc {\n  label 'fastqc'\n\n  input:\n  tuple val(name), val(type), path(reads)\n\n  output:\n  tuple val(name), val(type), path(\"*_fastqc.zip\"), emit: zip\n\n  script:\n  \"\"\"\n  fastqc --noextract -t ${task.cpus} ${reads}\n  \"\"\"\n}"], "list_proc": ["hoelzer/clean/fastqc"], "list_wf_names": ["hoelzer/clean"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["clean"], "list_contrib": ["hoelzer", "MarieLataretu"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n  label 'multiqc'\n                                                                                                                              \n   \n  publishDir \"${params.output}/${params.multiqc_dir}\", pattern: 'multiqc_report.html'\n  \n  input:\n  path(config)\n  path(fastqc)\n  path(nanoplot)\n  path(quast)\n  path(mapping_stats)\n    \n  output:\n  path \"multiqc_report.html\"\n  \n  script:\n  \"\"\"\n  multiqc . -s -c ${config}\n  \"\"\"\n}"], "list_proc": ["hoelzer/clean/multiqc"], "list_wf_names": ["hoelzer/clean"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["clean"], "list_contrib": ["hoelzer", "MarieLataretu"], "nb_contrib": 2, "codes": ["\nprocess minimap2_fasta {\n  label 'minimap2'\n\n  publishDir \"${params.output}/${name}/minimap2\", mode: 'copy', pattern: \"*.contamination.sorted.bam*\"\n  publishDir \"${params.output}/${name}/minimap2\", mode: 'copy', pattern: \"*clean.fasta.gz\"\n  publishDir \"${params.output}/${name}/minimap2\", mode: 'copy', pattern: \"*contamination.fasta.gz\"\n\n  input: \n    tuple val(name), path(fasta)\n    path db\n\n  output:\n    tuple val(name), path ('idxstats.tsv'), env(TOTALCONTIGS), emit: stats\n    tuple val(name), val('clean'), path('*clean.fasta.gz'), emit: cleaned_contigs\n    tuple val(name), val('contamination'), path('*contamination.fasta.gz'), emit: contaminated_contigs\n    path '*.contamination.sorted.bam*'\n\n  script:\n  \"\"\"\n  if [[ ${fasta} =~ \\\\.gz\\$ ]]; then\n    TOTALCONTIGS=\\$(zgrep '^>' ${fasta} | wc -l)\n  else\n    TOTALCONTIGS=\\$(grep '^>' ${fasta} | wc -l)\n  fi\n\n  minimap2 -ax asm5 -N 5 --secondary=no -t ${task.cpus} -o ${name}.sam ${db} ${fasta}\n\n  samtools fasta -f 4 -0 ${name}.clean.fasta ${name}.sam\n  samtools fasta -F 4 -0 ${name}.contamination.fasta ${name}.sam\n  pigz -p ${task.cpus} ${name}.clean.fasta\n  pigz -p ${task.cpus} ${name}.contamination.fasta\n\n  samtools view -b -F 2052 ${name}.sam | samtools sort -o ${name}.contamination.sorted.bam --threads ${task.cpus}\n  samtools index ${name}.contamination.sorted.bam\n  samtools idxstats ${name}.contamination.sorted.bam > idxstats.tsv\n\n  rm -f ${name}.sam\n  \"\"\"\n}"], "list_proc": ["hoelzer/clean/minimap2_fasta"], "list_wf_names": ["hoelzer/clean"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["clean"], "list_contrib": ["hoelzer", "MarieLataretu"], "nb_contrib": 2, "codes": ["\nprocess minimap2_nano {\n  label 'minimap2'\n  \n  publishDir \"${params.output}/${name}/minimap2\", mode: 'copy', pattern: \"*.contamination.sorted.bam*\"\n\n  input: \n    tuple val(name), path(reads)\n    path db\n\n  output:\n    tuple val(name), path ('idxstats.tsv'), emit: idxstats\n    tuple val(name), val('clean'), path('*clean.fastq'), emit: cleaned_reads\n    tuple val(name), val('contamination'), path('*contamination.fastq'), emit: contaminated_reads\n    path '*.contamination.sorted.bam*'\n\n  script:\n  \"\"\"\n  PARAMS=\"-ax map-ont\"\n  if [[ ${params.reads_rna} != 'false' ]]; then\n    PARAMS=\"-ax splice -k14\"\n  fi\n\n  minimap2 \\$PARAMS -N 5 --secondary=no -t ${task.cpus} -o ${name}.sam ${db} ${reads}\n\n  samtools fastq -f 4 -0 ${reads.baseName}.clean.fastq ${name}.sam\n  samtools fastq -F 4 -0 ${reads.baseName}.contamination.fastq ${name}.sam\n\n  samtools view -b -F 2052 ${name}.sam | samtools sort -o ${name}.contamination.sorted.bam --threads ${task.cpus}\n  samtools index ${name}.contamination.sorted.bam\n  samtools idxstats  ${name}.contamination.sorted.bam > idxstats.tsv\n  \"\"\"\n}"], "list_proc": ["hoelzer/clean/minimap2_nano"], "list_wf_names": ["hoelzer/clean"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["clean"], "list_contrib": ["hoelzer", "MarieLataretu"], "nb_contrib": 2, "codes": ["\nprocess minimap2_illumina {\n  label 'minimap2'\n\n  publishDir \"${params.output}/${name}/minimap2\", mode: 'copy', pattern: \"*.contamination.sorted.bam*\"\n\n  input: \n    tuple val(name), path(reads)\n    path db\n    val mode\n\n  output:\n    tuple val(name), path ('idxstats.tsv'), emit: idxstats\n    tuple val(name), val('clean'), path('*clean.fastq'), emit: cleaned_reads\n    tuple val(name), val('contamination'), path('*contamination.fastq'), emit: contaminated_reads\n    path '*.contamination.sorted.bam*'\n\n  script:\n  if ( mode == 'paired' ) {\n    \"\"\"\n    minimap2 -ax sr -N 5 --secondary=no -t ${task.cpus} -o ${name}.sam ${db} ${reads[0]} ${reads[1]}\n    \n    # Use samtools -F 2 to discard only reads mapped in proper pair:\n    samtools fastq -F 2 -1 ${reads[0].baseName}.clean.fastq -2 ${reads[1].baseName}.clean.fastq ${name}.sam\n    samtools fastq -f 2 -1 ${reads[0].baseName}.contamination.fastq -2 ${reads[1].baseName}.contamination.fastq ${name}.sam\n\n    samtools view -b -f 2 -F 2048 ${name}.sam | samtools sort -o ${name}.contamination.sorted.bam --threads ${task.cpus}\n    samtools index ${name}.contamination.sorted.bam\n    samtools idxstats ${name}.contamination.sorted.bam > idxstats.tsv\n    \"\"\"\n  } else {\n    \"\"\"\n    minimap2 -ax sr -N 5 --secondary=no -t ${task.cpus} -o ${name}.sam ${db} ${reads}\n    \n    samtools fastq -f 4 -0 ${reads.baseName}.clean.fastq ${name}.sam\n    samtools fastq -F 4 -0 ${reads.baseName}.contamination.fastq ${name}.sam\n\n    samtools view -b -F 2052 ${name}.sam | samtools sort -o ${name}.contamination.sorted.bam --threads ${task.cpus}\n    samtools index ${name}.contamination.sorted.bam\n    samtools idxstats ${name}.contamination.sorted.bam > idxstats.tsv\n    \"\"\"\n  }\n}"], "list_proc": ["hoelzer/clean/minimap2_illumina"], "list_wf_names": ["hoelzer/clean"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["clean"], "list_contrib": ["hoelzer", "MarieLataretu"], "nb_contrib": 2, "codes": ["\nprocess concat_contamination {\n  label 'minimap2'                             \n  label 'smallTask'                  \n  \n  publishDir \"${params.output}/${name}/${tool}\", mode: 'copy', pattern: \"db.fa.gz\"\n  publishDir \"${params.output}/${name}/${tool}\", mode: 'copy', pattern: \"db.fa.fai\"\n  publishDir \"${params.output}/${name}/${tool}\", mode: 'copy', pattern: \"db.fa.gz.gzi\"\n\n  input:\n  val name\n  val tool\n  path '*'\n\n  output:\n  path 'db.fa.gz', emit: fa\n  path 'db.fa.fai'\n  path 'db.fa.gz.gzi'\n  \n  script:\n  \"\"\"\n  cat *.gz > db.fa.gz\n  samtools faidx db.fa.gz\n  mv db.fa.gz.fai db.fa.fai\n  \"\"\"\n}"], "list_proc": ["hoelzer/clean/concat_contamination"], "list_wf_names": ["hoelzer/clean"]}, {"nb_reuse": 1, "tools": ["dammit"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["dammit-nf"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process dammitGetDB {\n  if (params.full) {\n    if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/dammit-full/${params.busco}\", mode: 'copy', pattern: \"dbs\" }\n    else { storeDir \"nextflow-autodownload-databases/dammit-full/${params.busco}/\" }  \n  } else {\n    if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/dammit/${params.busco}\", mode: 'copy', pattern: \"dbs\" }\n    else { storeDir \"nextflow-autodownload-databases/dammit/${params.busco}/\" }  \n  }\n    label 'dammitDB'\n  input:\n    path(busco_db)\n  output:\n    path(\"dbs\", type: 'dir')\n  script:\n    if (params.full)\n    \"\"\"\n    BUSCO=\\$(echo ${params.busco} | awk 'BEGIN{FS=\"_\"};{print \\$1}')\n    dammit databases --install --database-dir \\${PWD}/dbs --busco-group \\${BUSCO} --full\n    # if the busco download fails use the busco db we downloaded before\n    tar -zxvf ${busco_db} -C dbs/busco2db/\n    \"\"\"\n    else\n    \"\"\"\n    BUSCO=\\$(echo ${params.busco} | awk 'BEGIN{FS=\"_\"};{print \\$1}')\n    dammit databases --install --database-dir \\${PWD}/dbs --busco-group \\${BUSCO}\n    # if the busco download fails use the busco db we downloaded before\n    tar -zxvf ${busco_db} -C dbs/busco2db/\n    \"\"\"\n}"], "list_proc": ["hoelzer/dammit-nf/dammitGetDB"], "list_wf_names": ["hoelzer/dammit-nf"]}, {"nb_reuse": 1, "tools": ["dammit"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["dammit-nf"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process dammit {\n    label 'dammit'  \n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"dammit\"\n\n  input:\n    tuple val(name), path(transcriptome)\n    path(dbs)\n\n  output:\n    tuple val(name), path(\"dammit\")\n\n  script:\n    if (params.full)\n    \"\"\"\n    BUSCO=\\$(echo ${params.busco} | awk 'BEGIN{FS=\"_\"};{print \\$1}')\n    dammit annotate ${transcriptome} --database-dir \\${PWD}/dbs --busco-group \\${BUSCO} -n ${name} -o dammit --n_threads ${task.cpus} --full \n    \"\"\"\n    else\n    \"\"\"\n    BUSCO=\\$(echo ${params.busco} | awk 'BEGIN{FS=\"_\"};{print \\$1}')\n    dammit annotate ${transcriptome} --database-dir \\${PWD}/dbs --busco-group \\${BUSCO} -n ${name} -o dammit --n_threads ${task.cpus} #--full \n    \"\"\"\n  }"], "list_proc": ["hoelzer/dammit-nf/dammit"], "list_wf_names": ["hoelzer/dammit-nf"]}, {"nb_reuse": 1, "tools": ["Diamond"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process diamond_download_db {\n        if (params.cloudProcess) { publishDir \"${params.databases}/diamond\", mode: 'copy', pattern: \"database_uniprot.dmnd\" }\n        else { storeDir 'nextflow-autodownload-databases/diamond' }  \n        label 'diamond' \n      output:\n        file(\"database_uniprot.dmnd\")\n      script:\n        \"\"\"\n        wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n        diamond makedb --in uniprot_sprot.fasta.gz -d database_uniprot\n        rm -f uniprot_sprot.fasta.gz\n        \"\"\"\n    }"], "list_proc": ["hoelzer/mgnify-lr/diamond_download_db"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess samtools {\n        label 'samtools'  \n    input:\n        tuple val(name), path(bam)\n    output:\n        tuple val(name), path(\"${name}.sorted.bam\"), path(\"${name}.sorted.bam.bai\")\n    script:\n        \"\"\"\n        samtools sort -@ ${task.cpus} ${bam} > ${name}.sorted.bam\n        samtools index -@ ${task.cpus} ${name}.sorted.bam\n        \"\"\"\n  }"], "list_proc": ["hoelzer/mgnify-lr/samtools"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process fastp {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}.fastp.R*.fastq.gz\"\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}.qc.html\"\n      label 'fastp'\n    input:\n      tuple val(name), file(sread)\n    output:\n      tuple val(name), file(\"${name}.fastp.R*.fastq.gz\")\n      tuple val(name), file(\"${name}.qc.html\") \n    script:\n      \"\"\"\n      fastp -w ${task.cpus} --in1 ${sread[0]} --in2 ${sread[1]} --out1 \"${name}.fastp.R1.fastq.gz\" \\\n        --out2 \"${name}.fastp.R2.fastq.gz\" --json \"${name}.qc.json\" --html \"${name}.qc.html\" ${params.fastp_additional_params}\n      \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/fastp"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["Diamond"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess diamond {\n      label 'diamond'\n    input:\n      tuple val(name), val(ASSEMBLY_STATUS), file(proteins)\n      file(database)\n    output:\n      tuple val(name), val(ASSEMBLY_STATUS), file(\"${name}.data\")\n    script:\n      \"\"\"\n      diamond blastp --threads ${task.cpus} --max-target-seqs 1 --db ${database} \\\n      --query ${proteins} --outfmt 6 qlen slen --out ${name}.data\n      \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/diamond"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["R2 platform", "studyStrap"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process ena_manifest {\n    label 'basics'\n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"manifest.txt\"\n    \n    input:\n    tuple val(name), file(assembly)\n    tuple val(name), file(flye_log)\n    tuple val(name), file(genome_size)\n    \n    output:\n    file(\"manifest.txt\")\n    \n    shell:\n    \"\"\"\n    MD5=\\$(md5sum ${assembly} | awk '{print \\$1}')\n    SIZE=\\$(cat !{genome_size})\n    COVERAGE=\\$(grep 'Mean coverage' !{flye_log} | awk '{print \\$3}')\n\n    FLYE_VERSION=2.5\n    RACON_VERSION=1.4.10\n    MEDAKA_VERSION=0.10.0\n\n    STUDY=${params.study}\n    SAMPLE=${params.sample}\n    RUN=${params.run}\n\n    touch manifest.txt\n    cat <<EOF >> manifest.txt\nSTUDY   \\${STUDY}_${workflow.scriptId}\nSAMPLE  \\${SAMPLE}\nRUN_REF \\${RUN} \nASSEMBLYNAME    \\${RUN}_\\${MD5}\nASSEMBLY_TYPE   primary metagenome \nCOVERAGE        \\${COVERAGE}\nPROGRAM         ${params.assemblerLong} v\\${FLYE_VERSION} \nPLATFORM        Oxford Nanopore Technologies MinION \nFASTA           ${params.output}/${name}/assembly/${assembly}\nDESCRIPTION     Reads < 500 nt removed prior assembly. Draft flye assembly polished with racon v\\${RACON_VERSION} and medaka v\\${MEDAKA_VERSION} (model: ${params.model}).\nEOF\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/ena_manifest"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["R2 platform", "studyStrap"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess ena_manifest_hybrid {\n    label 'basics'\n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"manifest.txt\"\n    \n    input:\n    tuple val(name), file(assembly)\n    \n    output:\n    file(\"manifest.txt\")\n    \n    shell:\n    \"\"\"\n    MD5=\\$(md5sum ${assembly} | awk '{print \\$1}')\n    COVERAGE=XXX\n\n    SPADES_VERSION=3.13.1\n\n    STUDY=${params.study}\n    SAMPLE=${params.sample}\n    RUN=${params.run}\n\n    touch manifest.txt\n    cat <<EOF >> manifest.txt\nSTUDY   \\${STUDY}_${workflow.scriptId}\nSAMPLE  \\${SAMPLE}\nRUN_REF \\${RUN} \nASSEMBLYNAME    \\$( echo \\${RUN} | sed 's/,/_/g')_\\${MD5}\nASSEMBLY_TYPE   primary metagenome \nCOVERAGE        \\${COVERAGE}\nPROGRAM         ${params.assemblerHybrid} v\\${SPADES_VERSION} \nPLATFORM        Oxford Nanopore Technologies MinION, Illumina MiSeq\nFASTA           ${params.output}/${name}/assembly/${assembly}\nDESCRIPTION     Reads were quality controlled with fastp prior assembly. Assembly done with SPAdes v\\${SPADES_VERSION}.\nEOF\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/ena_manifest_hybrid"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess bwa {\n    label \"bwa\"\n    input:\n        tuple val(name), path(reads), path(fasta)\n    output:\n        tuple val(name), path(\"${name}.bam\")\n    script:\n        \"\"\"\n        bwa index ${fasta}\n        bwa mem -t ${task.cpus} ${fasta} ${reads[0]} ${reads[1]} | samtools view -bS - > ${name}.bam\n        \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/bwa"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["TAG", "ACTION", "GEOsubmission", "themetagenomics", "MetHis", "project"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process ena_project_xml {\n    label 'basics'\n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*.xml\"\n    \n    input:\n    tuple val(name), file(assembly)\n    tuple val(name), file(flye_log)\n    tuple val(name), file(genome_size)\n    \n    output:\n    file(\"*.xml\")\n    \n    shell:\n    date = Date().format( 'yyyy-MM-dd' )\n    \"\"\"\n    MD5=\\$(md5sum ${assembly} | awk '{print \\$1}')\n    SIZE=\\$(cat !{genome_size})\n    COVERAGE=\\$(grep 'Mean coverage' !{flye_log} | awk '{print \\$3}')\n\n    FLYE_VERSION=2.5\n    RACON_VERSION=1.4.10\n    MEDAKA_VERSION=0.10.0\n\n    STUDY=${params.study}\n    SAMPLE=${params.sample}\n    RUN=${params.run}\n\n    touch project.xml\n    cat <<EOF >> project.xml\n<PROJECT_SET xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n    <PROJECT alias=\"\\${STUDY}_${workflow.scriptId}\">\n        <TITLE>Metagenomics assembly of study: \\${STUDY}, sample: \\${SAMPLE}, run: \\${RUN}.</TITLE>\n        <DESCRIPTION>This assembly was derived from the Oxford Nanopore Technologies MinION data set \\${RUN} and was assembled with ${params.assemblerLong} (v\\${FLYE_VERSION}) using the --meta and --plasmids options and an estimated genome size of \\${SIZE}. Before assembly, reads smaller 500 nt were filtered and the draft flye assembly was polished by one round of racon (v\\$RACON_VERSION) and medaka (v\\$MEDAKA_VERSION) using the ${params.model} model. </DESCRIPTION>\n        <SUBMISSION_PROJECT>\n            <SEQUENCING_PROJECT/>\n         </SUBMISSION_PROJECT>\n         <PROJECT_LINKS>\n            <PROJECT_LINK>\n                <XREF_LINK>\n                    <DB></DB>\n                    <ID></ID>\n                </XREF_LINK>\n            </PROJECT_LINK>\n        </PROJECT_LINKS>\n        <PROJECT_ATTRIBUTES>\n            <PROJECT_ATTRIBUTE>\n                <TAG>new_study_type</TAG>\n                <VALUE>Metagenomic assembly</VALUE>\n            </PROJECT_ATTRIBUTE>\n        </PROJECT_ATTRIBUTES>\n    </PROJECT>\n</PROJECT_SET>\nEOF\n\n    touch submission.xml\n    cat <<EOF >> submission.xml\n<SUBMISSION>\n   <ACTIONS>\n      <ACTION>\n         <ADD/>\n      </ACTION>\n      <ACTION>\n          <HOLD HoldUntilDate=\"${date}\"/>\n       </ACTION>\n   </ACTIONS>\n</SUBMISSION>\nEOF\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/ena_project_xml"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["TAG", "ACTION", "GEOsubmission", "themetagenomics", "MetHis", "project"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess ena_project_xml_hybrid {\n    label 'basics'\n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*.xml\"\n    \n    input:\n    tuple val(name), file(assembly)\n    \n    output:\n    file(\"*.xml\")\n    \n    shell:\n                                          \n    \"\"\"\n    MD5=\\$(md5sum ${assembly} | awk '{print \\$1}')\n    COVERAGE=XXXX\n\n    SPADES_VERSION=3.13.1\n\n    STUDY=${params.study}\n    SAMPLE=${params.sample}\n    RUN=${params.run}\n\n    touch project.xml\n    cat <<EOF >> project.xml\n<PROJECT_SET xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n    <PROJECT alias=\"\\${STUDY}_${workflow.scriptId}\">\n        <TITLE>Metagenomics assembly of study: \\${STUDY}, sample: \\${SAMPLE}, run: \\${RUN}.</TITLE>\n        <DESCRIPTION>This assembly was derived from hybrid data (Illumina, Oxford Nanopore Technologies MinION) data sets \\${RUN} and was assembled with ${params.assemblerHybrid} (v\\${SPADES_VERSION}) using the --meta and --nano options. Before assembly reads were filtered by fastp with default parameters.</DESCRIPTION>\n        <SUBMISSION_PROJECT>\n            <SEQUENCING_PROJECT/>\n         </SUBMISSION_PROJECT>\n         <PROJECT_LINKS>\n            <PROJECT_LINK>\n                <XREF_LINK>\n                    <DB></DB>\n                    <ID></ID>\n                </XREF_LINK>\n            </PROJECT_LINK>\n        </PROJECT_LINKS>\n        <PROJECT_ATTRIBUTES>\n            <PROJECT_ATTRIBUTE>\n                <TAG>new_study_type</TAG>\n                <VALUE>Metagenomic assembly</VALUE>\n            </PROJECT_ATTRIBUTE>\n        </PROJECT_ATTRIBUTES>\n    </PROJECT>\n</PROJECT_SET>\nEOF\n\n    touch submission.xml\n    cat <<EOF >> submission.xml\n<SUBMISSION>\n   <ACTIONS>\n      <ACTION>\n         <ADD/>\n      </ACTION>\n      <ACTION>\n          <HOLD HoldUntilDate=\"YYYY-MM-DD\"/>\n       </ACTION>\n   </ACTIONS>\n</SUBMISSION>\nEOF\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/ena_project_xml_hybrid"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 3, "tools": ["FastQC", "FeatureCounts", "Minimap2"], "nb_own": 2, "list_own": ["hoelzer", "hugovaysset"], "nb_wf": 2, "list_wf": ["ReproHackaton", "mgnify-lr"], "list_contrib": ["abeaude", "MorganePhilipp", "louisedem", "hugovaysset", "hoelzer"], "nb_contrib": 5, "codes": ["process minimap2_index_ont {\n  label 'minimap2'\n  if (params.cloudProcess) { \n      publishDir \"${params.databases}/minimap2/\", mode: 'copy', pattern: \"*.mmi\" \n  }\n  else { \n      storeDir \"${params.databases}/minimap2/\" \n  }  \n    input:\n  \t  tuple val(name), file(fasta) \n    output:\n      file(\"${name}.ont.mmi\") \n    script:\n      \"\"\"\n      minimap2 -x map-ont -t ${task.cpus} -d ${name}.ont.mmi ${fasta}\n      \"\"\"\n}", "\nprocess fastqc {\n                                                                           \n    publishDir \"results/fastqc_results\", mode: 'symlink'\n\n    input:\n    tuple val(SRAID), file(read1), file(read2) from fastq_files_to_qc\n\n    output:\n    tuple file(\"${SRAID}_1_fastqc.html\"), file(\"${SRAID}_2_fastqc.html\")\n    tuple file(\"${SRAID}_1_fastqc.zip\"), file(\"${SRAID}_2_fastqc.zip\") into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -f fastq -q --threads ${task.cpus} ${read1} ${read2}\n    \"\"\"\n\n}", "\nprocess countExon {\n                                                                                            \n                              \n                                                     \n\n    input:\n    file \"input.gtf\" from gtf_file\n    file bam from indexedBAM_2.flatten().filter(~/.*bam$/).collect() \n                                                                                              \n                                    \n\n\n    output:\n    file \"output_exon.counts\" into exoncounts\n    file \"output_exon.counts.summary\" into exoncounts_summary\n\n    script:\n    \"\"\"\n    featureCounts -T ${task.cpus} -f -s 0 -a input.gtf -o output_exon.counts ${bam}\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/minimap2_index_ont", "hugovaysset/ReproHackaton/fastqc", "hugovaysset/ReproHackaton/countExon"], "list_wf_names": ["hugovaysset/ReproHackaton", "hoelzer/mgnify-lr"]}, {"nb_reuse": 2, "tools": ["Minimap2", "MultiQC"], "nb_own": 2, "list_own": ["hoelzer", "hugovaysset"], "nb_wf": 2, "list_wf": ["ReproHackaton", "mgnify-lr"], "list_contrib": ["abeaude", "MorganePhilipp", "louisedem", "hugovaysset", "hoelzer"], "nb_contrib": 5, "codes": ["\nprocess minimap2_index_ill {\n  label 'minimap2'\n  if (params.cloudProcess) { \n      publishDir \"${params.databases}/minimap2/\", mode: 'copy', pattern: \"*.mmi\" \n  }\n  else { \n      storeDir \"${params.databases}/minimap2/\" \n  }  \n    input:\n  \t  tuple val(name), file(fasta) \n    output:\n      file(\"${name}.ill.mmi\") \n    script:\n      \"\"\"\n      minimap2 -x sr -t ${task.cpus} -d ${name}.ill.mmi ${fasta}\n      \"\"\"\n}", "\nprocess multi_qc{\n                                                                                         \n                                             \n                                                                             \n    publishDir \"results/multiqc_results\", mode:'symlink'\n\n    input:\n    file fastqc from fastqc_results.collect()\n    file star from star_stats.collect()\n    file 'outputs_gene.count.summary' from counts_summary\n    file 'outputs_exon.count.summary' from exoncounts_summary\n    file fastq_screen from fastq_screen_txt.collect()\n\n    output:\n    file \"multiqc_report.html\"\n    file \"multiqc_data\"\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/minimap2_index_ill", "hugovaysset/ReproHackaton/multi_qc"], "list_wf_names": ["hugovaysset/ReproHackaton", "hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess minimap2_index_assembly {\n  label 'minimap2'\n  if (params.cloudProcess) { \n      publishDir \"${params.databases}/minimap2/\", mode: 'copy', pattern: \"*.mmi\" \n  }\n  else { \n      storeDir \"${params.databases}/minimap2/\" \n  }  \n    input:\n  \t  tuple val(name), file(fasta) \n    output:\n      file(\"${name}.fna.mmi\") \n    script:\n      \"\"\"\n      minimap2 -x asm5 -t ${task.cpus} -d ${name}.fna.mmi ${fasta}\n      \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/minimap2_index_assembly"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Minimap2"], "nb_own": 2, "list_own": ["hoelzer", "rmoran7"], "nb_wf": 2, "list_wf": ["custom_sarek", "mgnify-lr"], "list_contrib": ["hoelzer", "rmoran7"], "nb_contrib": 2, "codes": ["\nprocess MapReads {\n\n    machineType 'mem3_ssd1_v2_x48'\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    bwa-mem2 mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t 48 -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess minimap2_to_polish {\n  label 'minimap2'\n    input:\n  \t  tuple val(name), file(read), file(assembly) \n    output:\n      tuple val(name), file(read), file(assembly), file(\"${name}.paf\") \n    script:\n      \"\"\"\n      minimap2 -x map-ont -t ${task.cpus} ${assembly} ${read} > ${name}.paf\n      \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/MapReads", "hoelzer/mgnify-lr/minimap2_to_polish"], "list_wf_names": ["rmoran7/custom_sarek", "hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess minimap2_clean_ont {\n  label 'minimap2'\n  publishDir \"${params.output}/${name}/decontamination/\", mode: 'copy', pattern: \"${name}.*.fastq.gz\"  \n\n  input: \n    tuple val(name), file(fastq)\n    file(db)\n\n  output:\n    tuple val(name), file(\"*.clean.fastq.gz\")\n    tuple val(name), file(\"*.contamination.fastq.gz\")\n\n  script:\n    \"\"\"\n    # remove spaces in read IDs to keep them in the later cleaned output\n    if [[ ${fastq} =~ \\\\.gz\\$ ]]; then\n      zcat ${fastq} | sed 's/ /DECONTAMINATE/g' > ${name}.id.fastq\n    else\n      sed 's/ /DECONTAMINATE/g' ${fastq} > ${name}.id.fastq\n    fi\n\n    minimap2 -a -t ${task.cpus} -o ${name}.sam ${db} ${name}.id.fastq\n    samtools fastq -f 4 -0 ${name}.clean.id.fastq ${name}.sam\n    samtools fastq -F 4 -0 ${name}.contamination.id.fastq ${name}.sam\n\n    sed 's/DECONTAMINATE/ /g' ${name}.clean.id.fastq | gzip > ${name}.clean.fastq.gz\n    sed 's/DECONTAMINATE/ /g' ${name}.contamination.id.fastq | gzip > ${name}.contamination.fastq.gz\n     \n    rm ${name}.sam ${name}.clean.id.fastq ${name}.contamination.id.fastq ${name}.id.fastq\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/minimap2_clean_ont"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess minimap2_clean_assembly {\n  label 'minimap2'\n  publishDir \"${params.output}/${name}/assembly/\", mode: 'copy', pattern: \"*.fasta\"  \n\n  input: \n    tuple val(name), file(fasta)\n    file(db)\n\n  output:\n    tuple val(name), file(\"*clean.fasta\")\n    tuple val(name), file(\"*contamination.fasta\")\n\n  script:\n    \"\"\"\n    # get the basename of the assembly to not overwrite output\n    BN=\\$(basename ${fasta} .fasta)\n\n    # remove spaces in fasta IDs to keep them in the later cleaned output\n    if [[ ${fasta} =~ \\\\.gz\\$ ]]; then\n      zcat ${fasta} | sed 's/ /DECONTAMINATE/g' > ${name}.id.fasta\n    else\n      sed 's/ /DECONTAMINATE/g' ${fasta} > ${name}.id.fasta\n    fi\n\n    minimap2 -a -t ${task.cpus} -o ${name}.sam ${db} ${name}.id.fasta\n    samtools fasta -f 4 -0 ${name}.clean.id.fasta ${name}.sam\n    samtools fasta -F 4 -0 ${name}.contamination.id.fasta ${name}.sam\n\n\n    sed 's/DECONTAMINATE/ /g' ${name}.clean.id.fasta > \\${BN}_clean.fasta\n    sed 's/DECONTAMINATE/ /g' ${name}.contamination.id.fasta > \\${BN}_contamination.fasta\n     \n    rm ${name}.sam ${name}.clean.id.fasta ${name}.contamination.id.fasta ${name}.id.fasta\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/minimap2_clean_assembly"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess minimap2_clean_ill {\n  label 'minimap2'\n  publishDir \"${params.output}/${name}/assembly/\", mode: 'copy', pattern: \"*.fastq.gz\"  \n\n  input: \n    tuple val(name), file(reads)\n    file(db)\n\n  output:\n    tuple val(name), file(\"*clean*.fastq.gz\")\n    tuple val(name), file(\"*contamination*.fastq.gz\")\n\n  script:\n    \"\"\"\n    # replace the space in the header to retain the full read IDs after mapping (the mapper would split the ID otherwise after the first space)\n    # this is working for ENA reads that have at the end of a read id '/1' or '/2'\n    EXAMPLE_ID=\\$(zcat ${reads[0]} | head -1)\n    if [[ \\$EXAMPLE_ID == */1 ]]; then \n      if [[ ${reads[0]} =~ \\\\.gz\\$ ]]; then\n        zcat ${reads[0]} | sed 's/ /DECONTAMINATE/g' > ${name}.R1.id.fastq\n      else\n       sed 's/ /DECONTAMINATE/g' ${reads[0]} > ${name}.R1.id.fastq\n     fi\n     if [[ ${reads[1]} =~ \\\\.gz\\$ ]]; then\n       zcat ${reads[1]} | sed 's/ /DECONTAMINATE/g' > ${name}.R2.id.fastq\n     else\n       sed 's/ /DECONTAMINATE/g' ${reads[1]} > ${name}.R2.id.fastq\n     fi\n    else\n      # this is for paried-end SRA reads that don't follow the ENA pattern\n      if [[ ${reads[0]} =~ \\\\.gz\\$ ]]; then\n        zcat ${reads[0]} > ${name}.R1.id.fastq\n        zcat ${reads[1]} > ${name}.R2.id.fastq\n      else\n        cp ${reads[0]} ${name}.R1.id.fastq\n        cp ${reads[1]} ${name}.R2.id.fastq\n      fi\n    fi\n\n    # Use samtools -F 2 to discard only reads mapped in proper pair:\n    minimap2 -a -t ${task.cpus} -o ${name}.sam ${db} ${name}.R1.id.fastq ${name}.R2.id.fastq\n    samtools fastq -F 2 -1 ${name}.clean.R1.id.fastq -2 ${name}.clean.R2.id.fastq ${name}.sam\n    samtools fastq -f 2 -1 ${name}.contamination.R1.id.fastq -2 ${name}.contamination.R2.id.fastq ${name}.sam\n    \n    # restore the original read IDs\n    sed 's/DECONTAMINATE/ /g' ${name}.clean.R1.id.fastq | awk 'BEGIN{LINE=0};{if(LINE % 4 == 0 || LINE == 0){print \\$0\"/1\"}else{print \\$0};LINE++;}' | gzip > ${name}.clean.R1.fastq.gz \n    sed 's/DECONTAMINATE/ /g' ${name}.clean.R2.id.fastq | awk 'BEGIN{LINE=0};{if(LINE % 4 == 0 || LINE == 0){print \\$0\"/2\"}else{print \\$0};LINE++;}' | gzip > ${name}.clean.R2.fastq.gz\n    sed 's/DECONTAMINATE/ /g' ${name}.contamination.R1.id.fastq | awk 'BEGIN{LINE=0};{if(LINE % 4 == 0 || LINE == 0){print \\$0\"/1\"}else{print \\$0};LINE++;}' | gzip > ${name}.contamination.R1.fastq.gz \n    sed 's/DECONTAMINATE/ /g' ${name}.contamination.R2.id.fastq | awk 'BEGIN{LINE=0};{if(LINE % 4 == 0 || LINE == 0){print \\$0\"/2\"}else{print \\$0};LINE++;}' | gzip > ${name}.contamination.R2.fastq.gz\n\n    # remove intermediate files\n    rm ${name}.R1.id.fastq ${name}.R2.id.fastq ${name}.clean.R1.id.fastq ${name}.clean.R2.id.fastq ${name}.contamination.R1.id.fastq ${name}.contamination.R2.id.fastq ${name}.sam\n\n    \"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/minimap2_clean_ill"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["Racon"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process racon {\n      label 'racon'\n\n    errorStrategy { task.exitStatus in 130..140 ? 'retry' : 'terminate' }\n    cpus { 24 }\n    memory { 60.GB * task.attempt }\n    clusterOptions { '-P bigmem' }\n    maxRetries 2\n\n   input:\n      tuple val(name), file(read), file(assembly), file(mapping) \n   output:\n   \ttuple val(name), file(read), file(\"${name}_racon.fasta\") \n   shell:\n      \"\"\"\n      racon -t ${task.cpus} ${read} ${mapping} ${assembly} > ${name}_racon.fasta\n      \"\"\"\n  }"], "list_proc": ["hoelzer/mgnify-lr/racon"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["pilon"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["mgnify-lr"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process pilon {\n  label 'pilon'\n  publishDir \"${params.output}/${name}/assembly/\", mode: 'copy', pattern: \"${name}_polished_${round}.fasta\"\n      input:\n        tuple val(name), path(assembly), path(bam), path(index) \n        val(round)\n      output:\n  \t    tuple val(name), file(\"${name}_polished_${round}.fasta\") \n      script:\n        \"\"\"\n        VALUE=\\$(echo ${task.memory} | awk '{print \\$2}')\n        if [[ \\$VALUE == \"GB\" ]]; then\n          MEM=\\$(echo ${task.memory} | sed 's/ GB//g')\n          VALUE=g\n        else\n          MEM=\\$(echo ${task.memory} | sed 's/ TB//g')\n          VALUE=t\n        fi\n        #only allow pilon to use 80% of the available memory\n        ADJUSTEDMEM=\\$(echo 0.8*\\$MEM | bc | awk 'BEGIN{FS=\".\"};{print \\$1}')\n        pilon -Xmx\\${ADJUSTEDMEM}\\${VALUE} --threads ${task.cpus} --genome ${assembly} --frags ${bam} --output ${name}_polished_${round}\n      \t\"\"\"\n}"], "list_proc": ["hoelzer/mgnify-lr/pilon"], "list_wf_names": ["hoelzer/mgnify-lr"]}, {"nb_reuse": 1, "tools": ["Filtlong"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nanovirus"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process filtlong {\n    label 'filtlong'\n  input:\n    tuple val(name), file(reads) \n  output:\n\t  tuple val(name), file(\"${name}_minlength_reduced.fastq\") \n  script:\n    \"\"\"\n    filtlong --min_length 100 ${reads} > ${name}_minlength_reduced.fastq\n    \"\"\"\n}"], "list_proc": ["hoelzer/nanovirus/filtlong"], "list_wf_names": ["hoelzer/nanovirus"]}, {"nb_reuse": 2, "tools": ["Kaiju", "Nursing"], "nb_own": 2, "list_own": ["hoelzer", "icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["nextflow-data-processing-utility-tools", "nanovirus"], "list_contrib": ["andricDu", "junjun-zhang", "lindaxiang", "lepsalex", "jaserud", "hoelzer"], "nb_contrib": 6, "codes": ["process kaiju {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.out\"\n      label 'kaiju'\n\n    input:\n      tuple val(name), file(fastq) \n      file(database) \n    \n    output:\n      tuple val(name), file(\"${name}.out\")\n      tuple val(name), file(\"${name}.out.krona\")\n    \n    shell:\n      if (params.fasta) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      }\n      if (params.nano) {\n      '''\n      kaiju -a greedy -e 5 -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      }\n}", "\nprocess songGetAnalysis {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}"], "list_proc": ["hoelzer/nanovirus/kaiju", "icgc-argo-workflows/nextflow-data-processing-utility-tools/songGetAnalysis"], "list_wf_names": ["hoelzer/nanovirus", "icgc-argo-workflows/nextflow-data-processing-utility-tools"]}, {"nb_reuse": 1, "tools": ["CANU"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nanovirus"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process canu {\n    label 'canu'  \n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.BIN*.canu.fasta.gz\"\n    errorStrategy{task.exitStatus=1 ?'ignore':'terminate'}\n  input:\n    tuple val(name), file(fastq), file(gsize)\n  output:\n    tuple val(name), file(\"${name}.BIN*.canu.fasta.gz\"), file(gsize)\n  script:\n    \"\"\"\n    GSIZE=\\$(cat ${gsize})\n    BIN=\\$(echo \"${gsize}\" | sed 's/${name}.//g' | sed 's/.gsize//g')\n    canu -p ${name} -d canu_results maxThreads=${task.cpus} maxMemory=16 genomeSize=\\${GSIZE} -correct corOutCoverage=400 stopOnLowCoverage=0 -nanopore-raw ${fastq}\n    mv canu_results/${name}.correctedReads.fasta.gz ${name}.\\${BIN}.canu.fasta.gz\n    \"\"\"\n  }"], "list_proc": ["hoelzer/nanovirus/canu"], "list_wf_names": ["hoelzer/nanovirus"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nanovirus"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process flye {\n    label 'flye'  \n    publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.flye.fasta\"\n  input:\n    tuple val(name), file(fastq), file(gsize)\n  output:\n    tuple val(name), file(read), file(\"${name}.flye.fasta\")\n    tuple val(name), file(\"${name}.flye.gfa\") \n  script:\n    \"\"\"\n    GSIZE=\\$(cat ${gsize})\n    flye --plasmids -g \\${GSIZE} --meta -t ${task.cpus} --nano-raw ${fastq} -o assembly\n    mv assembly/assembly.fasta ${name}.fly.fasta\n    mv assembly/assembly_graph.gfa ${name}.fly.gfa\n    \"\"\"\n  }"], "list_proc": ["hoelzer/nanovirus/flye"], "list_wf_names": ["hoelzer/nanovirus"]}, {"nb_reuse": 1, "tools": ["Racon"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nanovirus"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process racon {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_consensus.fasta\"\n      label 'racon'\n   input:\n      tuple val(name), file(read), file(assembly), file(mapping) \n   output:\n   \ttuple val(name), file(read), file(\"${name}_consensus.fasta\") \n   shell:\n      \"\"\"\n      racon --include-unpolished --quality-threshold=9 -t ${task.cpus} ${read} ${mapping} ${assembly} > ${name}_consensus.fasta\n      \"\"\"\n  }"], "list_proc": ["hoelzer/nanovirus/racon"], "list_wf_names": ["hoelzer/nanovirus"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nanovirus"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process minimap2 {\n  publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.paf\"\n  label 'minimap2'\n      input:\n  \t    tuple val(name), file(read), file(assembly) \n      output:\n        tuple val(name), file(read), file(assembly), file(\"${name}.paf\") \n      script:\n        \"\"\"\n      \tminimap2 -x map-ont -t ${task.cpus} ${assembly} ${read} > ${name}.paf\n        \"\"\"\n      }"], "list_proc": ["hoelzer/nanovirus/minimap2"], "list_wf_names": ["hoelzer/nanovirus"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nf_blast"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process blast {\n    label 'blast'\n    echo true\n                                                                                      \n\n    input:\n        set val(name), file(fasta) \n        file db\n    \n    output:\n\t    set val(name), file(\"${name}.blast\") \n    \n    script:\n    \"\"\"\n    ls -la\n    makeblastdb -in ${db} -dbtype nucl\n    blastn -task blastn -num_threads 4 -query ${fasta} -db ${db} -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" > ${name}.blast\n    \"\"\"\n}"], "list_proc": ["hoelzer/nf_blast/blast"], "list_wf_names": ["hoelzer/nf_blast"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["nf_example"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process SEARCH {\n\n                                         \n\n    input: \n    path(query)\n    path(target)\n\n    output:\n    path(\"${query.simpleName}\")\n\n    script:\n    \"\"\"\n    sourmash compute -k 31 ${query} ${target}\n    sourmash compare -k 31 *.sig -o ${query.simpleName}\n    \"\"\"\n}"], "list_proc": ["hoelzer/nf_example/SEARCH"], "list_wf_names": ["hoelzer/nf_example"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["treat"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess HISAT2 {\n  publishDir params.outdir, mode:'copy'\n\n  input:\n  set assembly_id, file(assembly)\n  set read_id, file(reads)\n  val threads\n\n  output:\n  set assembly_id, file(\"${assembly_id}.sorted.bam\")\n  \n  shell:\n  '''\n  hisat2-build !{assembly} !{assembly_id} \n  hisat2 -x !{assembly_id} -U !{reads} -p !{threads} | samtools view -bS | samtools sort -T tmp --threads !{threads} > !{assembly_id}.sorted.bam\n  ''' \n}"], "list_proc": ["hoelzer/treat/HISAT2"], "list_wf_names": ["hoelzer/treat"]}, {"nb_reuse": 1, "tools": ["transeq"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["virify"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process phanotate {\n      publishDir \"${params.output}/${name}/${params.phanotatedir}\", mode: 'copy', pattern: \"*.faa\"\n      label 'phanotate'\n\n    input:\n      tuple val(name), file(fasta) \n    \n    output:\n      tuple val(name), stdout, file(\"*.faa\")\n    \n    shell:\n    \"\"\"\n    # this can be removed as soon as a conda recipe is available\n    git clone https://github.com/deprekate/PHANOTATE.git\n    cd PHANOTATE \n    git clone https://github.com/deprekate/fastpath.git\n    make\n\n    BN=\\$(basename ${fasta} .fna)\n    phanotate.py -f fasta -o \\${BN}_phanotate.fna ../${fasta} > /dev/null\n    transeq -sequence \\${BN}_phanotate.fna -outseq \\${BN}_phanotate.faa > /dev/null\n    cp \\${BN}_phanotate.faa ../\n    printf \"\\$BN\"\n    \"\"\"\n}"], "list_proc": ["hoelzer/virify/phanotate"], "list_wf_names": ["hoelzer/virify"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["virify"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process blast {\n      publishDir \"${params.output}/${assembly_name}/${params.blastdir}/\", mode: 'copy', pattern: \"*.blast\"\n      publishDir \"${params.output}/${assembly_name}/${params.finaldir}/blast/\", mode: 'copy', pattern: \"*.filtered.blast\"\n      label 'blast'\n\n      errorStrategy 'retry'\n      maxRetries 1\n\n    input:\n      tuple val(assembly_name), val(confidence_set_name), file(fasta) \n      file(db)\n    \n    output:\n      tuple val(assembly_name), val(confidence_set_name), file(\"${confidence_set_name}.blast\"), file(\"${confidence_set_name}.filtered.blast\")\n    \n    shell:\n    if (task.attempt.toString() == '1')\n    \"\"\"\n      HEADER_BLAST=\"qseqid\\\\tsseqid\\\\tpident\\\\tlength\\\\tmismatch\\\\tgapopen\\\\tqstart\\\\tqend\\\\tqlen\\\\tsstart\\\\tsend\\\\tevalue\\\\tbitscore\\\\tslen\"\n      printf \"\\$HEADER_BLAST\\\\n\" > ${confidence_set_name}.blast\n\n      blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db IMG_VR_2018-07-01_4/IMGVR_all_nucleotides.fna -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" >> ${confidence_set_name}.blast\n      awk '{if(\\$4>0.8*\\$9){print \\$0}}' ${confidence_set_name}.blast >> ${confidence_set_name}.filtered.blast\n    \"\"\"\n    else if (task.attempt.toString() == '2')\n    \"\"\"\n      HEADER_BLAST=\"qseqid\\\\tsseqid\\\\tpident\\\\tlength\\\\tmismatch\\\\tgapopen\\\\tqstart\\\\tqend\\\\tqlen\\\\tsstart\\\\tsend\\\\tevalue\\\\tbitscore\\\\tslen\"\n      printf \"\\$HEADER_BLAST\\\\n\" > ${confidence_set_name}.blast\n\n      blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db ${db}/IMG_VR_2018-07-01_4/IMGVR_all_nucleotides.fna -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" >> ${confidence_set_name}.blast\n      awk '{if(\\$4>0.8*\\$9){print \\$0}}' ${confidence_set_name}.blast >> ${confidence_set_name}.filtered.blast\n    \"\"\"\n\n}"], "list_proc": ["hoelzer/virify/blast"], "list_wf_names": ["hoelzer/virify"]}, {"nb_reuse": 1, "tools": ["Kaiju"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["virify"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process kaiju {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.out\"\n      label 'kaiju'\n\n    input:\n      tuple val(name), file(fastq) \n      file(database) \n    \n    output:\n      tuple val(name), file(\"${name}.out\")\n      tuple val(name), file(\"${name}.out.krona\")\n    \n    shell:\n      if (params.illumina) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq[0]} -j !{fastq[1]} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      } \n      if (params.fasta) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      }\n}"], "list_proc": ["hoelzer/virify/kaiju"], "list_wf_names": ["hoelzer/virify"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["virify"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process fastp {\n    label 'fastp'  \n                                                                                                \n  input:\n    tuple val(name), file(reads)\n  output:\n    tuple val(name), file(\"${name}*.fastp.fastq.gz\")\n  script:\n    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} --thread ${task.cpus} -o ${name}.R1.fastp.fastq.gz -O ${name}.R2.fastp.fastq.gz\n    \"\"\"\n  }"], "list_proc": ["hoelzer/virify/fastp"], "list_wf_names": ["hoelzer/virify"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["virify"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process multiqc {\n    publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}_multiqc_report.html\"\n    label 'multiqc'  \n  input:\n    tuple val(name), file(fastqc)\n  output:\n    tuple val(name), file(\"${name}_multiqc_report.html\")\n  script:\n    \"\"\"\n    multiqc -i ${name} .\n    \"\"\"\n  }"], "list_proc": ["hoelzer/virify/multiqc"], "list_wf_names": ["hoelzer/virify"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["virify"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["process fastqc {\n    label 'fastqc'  \n  input:\n    tuple val(name), file(reads)\n  output:\n    tuple val(name), file(\"fastqc/${name}*fastqc*\")\n  script:\n    \"\"\"\n    mkdir fastqc\n    fastqc -t ${task.cpus} -o fastqc *.fastq.gz\n    \"\"\"\n  }"], "list_proc": ["hoelzer/virify/fastqc"], "list_wf_names": ["hoelzer/virify"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["wuhan"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess sketch {\n    input:\n    file(db)\n\n    output:\n    file(\"db.msh\")\n\n    \"\"\"\n    mash sketch -i -s 1000 -k 15 -S 42 -o db.msh ${db}\n    \"\"\"\n}"], "list_proc": ["hoelzer/wuhan/sketch"], "list_wf_names": ["hoelzer/wuhan"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["hoelzer"], "nb_wf": 1, "list_wf": ["wuhan"], "list_contrib": ["hoelzer"], "nb_contrib": 1, "codes": ["\nprocess screen {\n    publishDir \"${outdir}\", mode: \"copy\", pattern: \"${uid}.screen.csv\"\n    \n    input:\n    file(sketch)\n    tuple val(uid), file(reads)\n\n    output:\n                               \n    file(\"${uid}.screen.csv\")\n\n    \"\"\"\n    echo ${reads}\n    mash screen -w -p ${threads} -i 0 ${sketch} ${reads} > ${uid}.screen.csv\n    \"\"\"\n                                  \n                                                                     \n                                                                \n}"], "list_proc": ["hoelzer/wuhan/screen"], "list_wf_names": ["hoelzer/wuhan"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["hugovaysset"], "nb_wf": 1, "list_wf": ["ReproHackaton"], "list_contrib": ["MorganePhilipp", "abeaude", "louisedem", "hugovaysset"], "nb_contrib": 4, "codes": ["\nprocess makeGenomeIndex {\n                                                      \n                                                                  \n                                                                                                \n                                                                        \n\n    input:\n    file '*.fa.gz' from chrfa.collect()\n\n    output:\n    path ref into genome_idx\n\n    script:\n    \"\"\"\n    gunzip -c *.fa.gz > ref.fa \n    mkdir ref\n    STAR --runThreadN ${task.cpus} --runMode genomeGenerate --genomeDir ref/ --genomeFastaFiles ref.fa\n    \"\"\"\n}"], "list_proc": ["hugovaysset/ReproHackaton/makeGenomeIndex"], "list_wf_names": ["hugovaysset/ReproHackaton"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["hugovaysset"], "nb_wf": 1, "list_wf": ["ReproHackaton"], "list_contrib": ["MorganePhilipp", "abeaude", "louisedem", "hugovaysset"], "nb_contrib": 4, "codes": ["\nprocess mapFASTQ {\n                                                                                    \n                      \n\n    input:\n    path ref from genome_idx\n    tuple val(SRAID), file(\"read1.fa.gz\"), file(\"read2.fa.gz\") from fastq_files_to_map\n\n    output:\n    file \"${SRAID}.bam\" into bamfiles\n    file \"*Log.final.out\" into star_stats\n\n                                                                              \n                                              \n    script:\n    \"\"\"\n    STAR --genomeDir ${ref} \\\n        --runThreadN ${task.cpus} \\\n        --readFilesCommand zcat \\\n        --readFilesIn read1.fa.gz read2.fa.gz \\\n        --outSAMtype BAM SortedByCoordinate \\\n        --outSAMstrandField intronMotif \\\n        --outSAMunmapped None \\\n        --outFilterMismatchNmax 4 \\\n        --outFilterMultimapNmax 10 \\\n        --outStd BAM_SortedByCoordinate \\\n        --genomeLoad NoSharedMemory \\\n        --outFileNamePrefix ${SRAID} \\\n        --limitBAMsortRAM ${task.memory.toBytes()} > ${SRAID}.bam \n    \"\"\"\n}"], "list_proc": ["hugovaysset/ReproHackaton/mapFASTQ"], "list_wf_names": ["hugovaysset/ReproHackaton"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["hugovaysset"], "nb_wf": 1, "list_wf": ["ReproHackaton"], "list_contrib": ["MorganePhilipp", "abeaude", "louisedem", "hugovaysset"], "nb_contrib": 4, "codes": ["\nprocess indexBAM {\n                                                                                        \n                                                             \n\n    publishDir \"results/BAM_files\", mode: 'symlink' \n                                                                     \n    \n    input:\n    file bam from bamfiles\n\n    output:\n    tuple file(\"${bam}.bai\"), file(\"${bam}\") into indexedBAM_1, indexedBAM_2\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}"], "list_proc": ["hugovaysset/ReproHackaton/indexBAM"], "list_wf_names": ["hugovaysset/ReproHackaton"]}, {"nb_reuse": 2, "tools": ["FeatureCounts", "fastPHASE"], "nb_own": 2, "list_own": ["icbi-lab", "hugovaysset"], "nb_wf": 2, "list_wf": ["ReproHackaton", "nextNEOpi"], "list_contrib": ["abeaude", "cavei", "MorganePhilipp", "louisedem", "abyssum", "riederd", "hugovaysset"], "nb_contrib": 7, "codes": [" process fastp {\n\n        label 'nextNEOpiENV'\n\n        tag \"$meta.sampleName : $meta.sampleType\"\n\n        publishDir \"$params.outputDir/analyses/${meta.sampleName}/\",\n            mode: publishDirMode,\n            saveAs: {\n                filename ->\n                    if(filename.indexOf(\".json\") > 0) {\n                        return \"QC/fastp/$filename\"\n                    } else if(filename.indexOf(\"NO_FILE\") >= 0) {\n                        return null\n                    } else {\n                        return  \"01_preprocessing/$filename\"\n                    }\n            }\n\n        input:\n        tuple val(meta), path(reads) from reads_to_trim_ch\n\n        output:\n        tuple val(meta), path(\"*_trimmed_R{1,2}.fastq.gz\") into reads_trimmed_ch, fastqc_reads_trimmed_ch\n        tuple val(meta), path(\"*.json\") into ch_fastp           \n\n\n        script:\n        def reads_R1         = \"--in1 \" + reads[0]\n        def trimmed_reads_R1 = \"--out1 \" + meta.sampleName + \"_\" + meta.sampleType + \"_trimmed_R1.fastq.gz\"\n\n                               \n        def reads_R2         = \"\"\n        def trimmed_reads_R2 = \"\"\n        if(meta.libType == \"PE\") {\n            reads_R2          = \"--in2 \" + reads[1]\n            trimmed_reads_R2  = \"--out2 \" + meta.sampleName + \"_\" + meta.sampleType + \"_trimmed_R2.fastq.gz\"\n        }\n\n        def fastpAdapter = ''\n        def adapterSeqFile\n        def aseq = false\n        def aseqR2 = false\n        def afile = false\n\n        if (meta.sampleType.indexOf(\"DNA\") > 0) {\n            afile = params.adapterSeqFile\n            aseq = params.adapterSeq\n            aseqR2 = params.adapterSeqR2\n        } else {\n             afile = params.adapterSeqFileRNAseq\n             aseq = params.adapterSeqRNAseq\n             aseqR2 = params.adapterSeqR2RNAseq\n        }\n        if(afile != false) {\n            adapterSeqFile = Channel.fromPath(afile)\n            fastpAdapter = \"--adapter_fasta \" + adapterSeqFile\n        } else {\n            if(aseq != false) {\n                adapterSeq   = Channel.value(aseq)\n                fastpAdapter = \"--adapter_sequence \" + aseq.getVal()\n\n                if(aseqR2 != false && meta.libType == \"PE\") {\n                    adapterSeqR2   = Channel.value(aseqR2)\n                    fastpAdapter += \" --adapter_sequence_r2 \" + adapterSeqR2.getVal()\n                }\n            }\n        }\n\n        \"\"\"\n        fastp --thread ${task.cpus} \\\\\n            ${reads_R1} \\\\\n            ${reads_R2} \\\\\n            ${trimmed_reads_R1} \\\\\n            ${trimmed_reads_R2} \\\\\n            --json ${meta.sampleName}_${meta.sampleType}_fastp.json \\\\\n            ${fastpAdapter} \\\\\n            ${params.fastpOpts}\n        \"\"\"\n    }", "\nprocess countFeature {\n                                                                                        \n                                                                        \n\n    input:\n    file \"input.gtf\" from gtf_file\n    file bam from indexedBAM_1.flatten().filter(~/.*bam$/).collect() \n                                                                                         \n                                         \n\n\n    output:\n    file \"output_gene.counts\" into counts\n    file \"output_gene.counts.summary\" into counts_summary\n\n    script:\n    \"\"\"\n    featureCounts -T ${task.cpus} -t gene -g gene_id -s 0 -a input.gtf -o output_gene.counts ${bam}\n    \"\"\"\n}"], "list_proc": ["icbi-lab/nextNEOpi/fastp", "hugovaysset/ReproHackaton/countFeature"], "list_wf_names": ["hugovaysset/ReproHackaton", "icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["Cutadapt", "Picard", "MarkDuplicates (IP)", "FastQC"], "nb_own": 2, "list_own": ["nibscbioinformatics", "hukai916"], "nb_wf": 1, "list_wf": ["demo_nfcore_obsolete", "nf-core-viralevo"], "list_contrib": ["kaurravneet4123"], "nb_contrib": 1, "codes": ["\nprocess CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1'\n    } else {\n        container 'quay.io/biocontainers/cutadapt:3.4--py37h73a75cf_1'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $options.args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::picard=2.23.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.23.9--0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.23.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"*.version.txt\"                 , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.bam \\\\\n        METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt\n\n    echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d: > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["hukai916/demo_nfcore_obsolete/CUTADAPT"], "list_wf_names": ["hukai916/demo_nfcore_obsolete"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["hustal"], "nb_wf": 1, "list_wf": ["rnasequmi"], "list_contrib": ["hustal"], "nb_contrib": 1, "codes": [" process Check_Header {\n\n         publishDir \"${params.outdir}/Output_Files\", mode: 'copy'\n\n         input:\n\n         set sampleID, file (bam_files_with_RG) from  umi_bam_files\n\n         output:\n\n         set sampleID, file(\"${sampleID}_umi_RG.bam\") into umi_RG_bam_files\n         file \"*\"\n\n         script:\n\n         \"\"\"\n\n         samtools addreplacerg -r \"@RG\\tID:NGI-umi-project\\tSM:hs\" -o ${sampleID}_umi_RG.bam $bam_files_with_RG\n\n         \"\"\"\n\n }"], "list_proc": ["hustal/rnasequmi/Check_Header"], "list_wf_names": ["hustal/rnasequmi"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["hustal"], "nb_wf": 1, "list_wf": ["rnasequmi"], "list_contrib": ["hustal"], "nb_contrib": 1, "codes": [" process Mark_Duplicates_with_UMIs {\n\n         publishDir \"${params.outdir}/Output_Files\", mode: 'copy'\n\n         input:\n\n         set sampleID, file (bam_files) from umi_RG_bam_files\n\n         output:\n\n         file \"*\"\n\n\n         script:\n\n         \"\"\"\n\n         picard MarkDuplicates INPUT=$bam_files OUTPUT=${sampleID}_mark_dup.bam METRICS_FILE=${sampleID}_mark_dups_metrics.txt USE_JDK_DEFLATER=true USE_JDK_INFLATER=true BARCODE_TAG=RX ASSUME_SORTED=true TAG_DUPLICATE_SET_MEMBERS=true\n\n         \"\"\"\n\n\n }"], "list_proc": ["hustal/rnasequmi/Mark_Duplicates_with_UMIs"], "list_wf_names": ["hustal/rnasequmi"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": [" process bam2fastq {\n        label 'nextNEOpiENV'\n\n        tag \"$meta.sampleName : $meta.sampleType\"\n\n        publishDir \"${params.outputDir}/analyses/${meta.sampleName}/01_preprocessing\",\n            mode: publishDirMode\n\n        input:\n        tuple(\n            val(meta),\n            path(bam),\n            val(libType),\n            val(libOK)\n        ) from bam_ch\n            .combine(seqLibTypes_ok)\n\n\n        output:\n        tuple(\n            val(meta),\n            path(\"${prefix}_R*.fastq.gz\"),\n        ) into (\n            bam_fastq_ch\n        )\n\n        script:\n        prefix = meta.sampleName + \"_\" + meta.sampleType\n        meta.libType = libType\n        if (libType == \"PE\")\n            \"\"\"\n            samtools sort -@ ${task.cpus} -m ${params.STperThreadMem} -l 0 -n ${bam} | \\\\\n            samtools fastq \\\\\n                -@ ${task.cpus} \\\\\n                -c 5 \\\\\n                -1 ${prefix}_R1.fastq.gz \\\\\n                -2 ${prefix}_R2.fastq.gz \\\\\n                -0 /dev/null -s /dev/null \\\\\n                -n \\\\\n                /dev/stdin\n            \"\"\"\n        else if (libType == \"SE\")\n            \"\"\"\n            samtools fastq \\\\\n                -@ ${task.cpus} \\\\\n                -n \\\\\n                ${bam} | \\\\\n                bgzip -@ ${task.cpus} -c /dev/stdin > ${prefix}_R1.fastq.gz\n            \"\"\"\n    }"], "list_proc": ["icbi-lab/nextNEOpi/bam2fastq"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": ["\nprocess FastQC {\n\n    label 'nextNEOpiENV'\n\n    tag \"$meta.sampleName : $meta.sampleType\"\n\n    publishDir \"${params.outputDir}/analyses/${meta.sampleName}/QC/fastqc\",\n        mode: publishDirMode,\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n\n    input:\n    tuple val(meta), path(reads) from fastqc_reads_ch\n\n\n    output:\n    tuple val(meta), path(\"*_fastqc.zip\") into ch_fastqc           \n\n    script:\n    def reads_R1_ext = (reads[0].getExtension() == \"gz\") ? \"fastq.gz\" : reads[0].getExtension()\n    def reads_R1     = meta.sampleName + \"_\" + meta.sampleType + \"_R1.\" + reads_R1_ext\n\n                           \n    def reads_R2 = \"_missing_\"\n    if(meta.libType == \"PE\") {\n        def reads_R2_ext = (reads[1].getExtension() == \"gz\") ? \"fastq.gz\" : reads[1].getExtension()\n        reads_R2     = meta.sampleName + \"_\" + meta.sampleType + \"_R2.\" + reads_R2_ext\n    }\n    \"\"\"\n    if [ ! -e ${reads_R1} ]; then\n        ln -s ${reads[0]} ${reads_R1}\n    fi\n\n    if [ ${reads_R2} != \"_missing_\" ] && [ ! -e ${reads_R2} ]; then\n        ln -s ${reads[1]} ${reads_R2}\n    fi\n\n    fastqc --quiet --threads ${task.cpus} \\\\\n        ${reads_R1} ${reads_R2}\n    \"\"\"\n}"], "list_proc": ["icbi-lab/nextNEOpi/FastQC"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": [" process FastQC_trimmed {\n\n        label 'nextNEOpiENV'\n\n        tag \"$meta.sampleName : $meta.sampleType\"\n\n        publishDir \"${params.outputDir}/analyses/${meta.sampleName}/QC/fastqc\",\n            mode: publishDirMode,\n            saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n\n        input:\n        tuple val(meta), path(reads) from fastqc_reads_trimmed_ch\n\n\n        output:\n        tuple val(meta), path(\"*_fastqc.zip\") into ch_fastqc_trimmed           \n\n        script:\n        def reads_R1 = reads[0]\n        def reads_R2 = (meta.libType == \"PE\") ? reads[1] : \"\"\n        \"\"\"\n        fastqc --quiet --threads ${task.cpus} \\\\\n            ${reads_R1} ${reads_R2}\n        \"\"\"\n    }"], "list_proc": ["icbi-lab/nextNEOpi/FastQC_trimmed"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["vt"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": ["\nprocess gene_annotator {\n\n    tag \"${meta.sampleName}\"\n\n    label 'pVACtools'\n\n    input:\n    tuple(\n        val(meta),\n        path(vep_somatic_vcf_gz),\n        path(final_tpm),\n        path(RNA_bam),\n    ) from VEPvcf_out_ch2.RNA\n        .join(final_gene_expression_file, by: 0)\n        .join(star_bam_file, by: 0)\n\n    tuple(\n        path(RefFasta),\n        path(RefIdx),\n    ) from Channel.value(\n        [ reference.RefFasta,\n        reference.RefIdx ]\n    )\n\n    output:\n    tuple(\n        val(meta),\n        path(\"${meta.sampleName}_vep_somatic_gx.{vcf.gz,vcf.gz.tbi}\")\n    ) into (\n        vcf_vep_ex_gz,\n        gene_annotator_out_mixMHC2pred_ch0,\n        generate_protein_fasta_tumor_vcf_ch0\n    )\n\n    script:\n    \"\"\"\n    vcf-expression-annotator \\\\\n        -i GeneID \\\\\n        -e TPM \\\\\n        -s ${meta.sampleName}_tumor \\\\\n        ${vep_somatic_vcf_gz[0]} ${final_tpm} \\\\\n        custom gene \\\\\n        -o ./${meta.sampleName}_vep_somatic_gx_tmp.vcf\n    bgzip -f ${meta.sampleName}_vep_somatic_gx_tmp.vcf\n    tabix -p vcf ${meta.sampleName}_vep_somatic_gx_tmp.vcf.gz\n\n    vt decompose \\\\\n        -s ${meta.sampleName}_vep_somatic_gx_tmp.vcf.gz \\\\\n        -o ${meta.sampleName}_vep_somatic_gx_dec_tmp.vcf.gz\n\n    bam_readcount_helper.py \\\\\n        ${meta.sampleName}_vep_somatic_gx_dec_tmp.vcf.gz \\\\\n        ${meta.sampleName}_tumor \\\\\n        ${RefFasta} \\\\\n        ${RNA_bam[0]} \\\\\n        ./\n\n    vcf-readcount-annotator \\\\\n        -s ${meta.sampleName}_tumor \\\\\n        -t snv \\\\\n        -o ${meta.sampleName}_vep_somatic_gx_dec_snv_rc_tmp.vcf \\\\\n        ${meta.sampleName}_vep_somatic_gx_dec_tmp.vcf.gz \\\\\n        ${meta.sampleName}_tumor_bam_readcount_snv.tsv \\\\\n        RNA\n\n    vcf-readcount-annotator \\\\\n        -s ${meta.sampleName}_tumor \\\\\n        -t indel \\\\\n        -o ${meta.sampleName}_vep_somatic_gx.vcf \\\\\n        ${meta.sampleName}_vep_somatic_gx_dec_snv_rc_tmp.vcf \\\\\n        ${meta.sampleName}_tumor_bam_readcount_indel.tsv \\\\\n        RNA\n\n    bgzip -f ${meta.sampleName}_vep_somatic_gx.vcf\n    tabix -p vcf ${meta.sampleName}_vep_somatic_gx.vcf.gz\n    \"\"\"\n}"], "list_proc": ["icbi-lab/nextNEOpi/gene_annotator"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["BLASTP-ACC"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": ["\nprocess blast_epitopes {\n\n    label 'Blast'\n\n    tag \"${meta.sampleName}\"\n\n    cache 'lenient'\n\n    input:\n    tuple(\n        val(meta),\n        val(caller),\n        val(f_type),\n        val(mhc_class),\n        path(epitopes_fasta)\n    ) from make_epitopes_fasta_out_ch\n\n    path(blastdb) from Channel.value(reference.ProteinBlastDBdir)\n\n    output:\n    tuple(\n        val(meta),\n        val(caller),\n        val(f_type),\n        val(mhc_class),\n        path(outfile)\n    ) into blast_epitopes_out_ch\n\n    script:\n    outfile = epitopes_fasta.baseName + \"_blast.tsv\"\n    \"\"\"\n    blastp -task blastp-short \\\\\n        -db ${blastdb}/${params.references.ProteinBlastDBname} \\\\\n        -query ${epitopes_fasta} \\\\\n        -out ${outfile} \\\\\n        -outfmt \"6 qseqid sseqid qlen length nident qseq\" \\\\\n        -num_alignments 5 \\\\\n        -num_threads ${task.cpus} \\\\\n        -comp_based_stats 0 \\\\\n        -ungapped \\\\\n        -seg no\n    \"\"\"\n}"], "list_proc": ["icbi-lab/nextNEOpi/blast_epitopes"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["PatchD"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": [" process install_IGS {\n\n        tag 'install IGS'\n\n                       \n        cache false\n\n        output:\n        val(\"OK\") into igs_chck_ch\n        path(\".igs_install_ok.chck\")\n\n        script:\n        \"\"\"\n        mkdir -p ${igs_target} && \\\\\n        curl -sLk ${params.IGS_script_url} -o ${igs_target}/NeoAg_immunogenicity_predicition_GBM.R && \\\\\n        curl -sLk ${params.IGS_model_url} -o ${igs_target}/Final_gbm_model.rds && \\\\\n        patch -p0 ${igs_target}/NeoAg_immunogenicity_predicition_GBM.R ${baseDir}/assets/NeoAg_immunogenicity_predicition_GBM.patch && \\\\\n        chmod +x ${igs_target}/NeoAg_immunogenicity_predicition_GBM.R  && \\\\\n        echo \"OK\" > .igs_install_ok.chck && \\\\\n        cp -f .igs_install_ok.chck ${igs_chck_file}\n        \"\"\"\n    }"], "list_proc": ["icbi-lab/nextNEOpi/install_IGS"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["MiXCR"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": [" process mixcr {\n\n        label 'nextNEOpiENV'\n\n        tag \"${meta.sampleName} : ${meta.sampleType}\"\n\n        publishDir \"$params.outputDir/analyses/${meta.sampleName}/15_BCR_TCR\",\n            mode: publishDirMode\n\n        input:\n        tuple(\n            val(meta),\n            path(reads),\n            path(mixcr_chck_file)\n        ) from reads_mixcr_ch\n            .combine(mixcr_chck_ch)\n\n        output:\n        tuple(\n            val(meta),\n            path(\"${procSampleName}_mixcr.clonotypes.ALL.txt\"),\n        )\n\n        script:\n        def starting_material = (meta.sampleType == \"tumor_RNA\") ? \"rna\" : \"dna\"\n        procSampleName = meta.sampleName + \"_\" + meta.sampleType\n        \"\"\"\n        mixcr analyze shotgun \\\\\n            --threads ${task.cpus} \\\\\n            --species hs \\\\\n            --starting-material ${starting_material} \\\\\n            --only-productive \\\\\n            $reads \\\\\n            ${procSampleName}_mixcr\n        \"\"\"\n    }"], "list_proc": ["icbi-lab/nextNEOpi/mixcr"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["nextNEOpi"], "list_contrib": ["abyssum", "cavei", "riederd"], "nb_contrib": 3, "codes": ["\nprocess multiQC {\n\n    label 'nextNEOpiENV'\n\n    tag \"${meta.sampleName}\"\n\n    publishDir \"${params.outputDir}/analyses/${meta.sampleName}/QC\",\n        mode: publishDirMode\n\n   input:\n    tuple(\n        val(meta),\n        path(qcfiles)\n    ) from ch_fastqc\n        .mix(ch_fastp)\n        .mix(ch_fastqc_trimmed)\n        .mix(alignmentMetrics_ch)\n        .transpose()\n        .groupTuple()\n\n    output:\n    path(\"multiqc_data/*\")\n    path(\"multiqc_report.html\")\n\n    script:\n    def set_locale = \"\"\n    if(! params.enable_conda && workflow.containerEngine == 'singularity' ) {\n        set_locale = \"export LC_ALL=C.UTF-8; export LC_ALL=C.UTF-8\"\n    }\n    \"\"\"\n    ${set_locale}\n    multiqc .\n    \"\"\"\n\n}"], "list_proc": ["icbi-lab/nextNEOpi/multiQC"], "list_wf_names": ["icbi-lab/nextNEOpi"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": [" process makeSTARindex {\n            label 'high_memory'\n            tag \"$fasta\"\n            publishDir path: { params.save_reference ? \"${params.outdir}/reference_genome\" : params.outdir },\n                        saveAs: { params.save_reference ? it : null }, mode: \"$mode\"\n\n            input:\n                file fasta from fasta_star_idx\n                file gtf from gtf_star_idx\n\n            output:\n            file \"star\" into star_index\n\n            script:\n            def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n            \"\"\"\n            # unzip files if required\n            FASTA=${fasta}\n            GTF=${gtf}\n            if [[ \"${fasta}\" == *\".gz\"* ]]; then\n                gunzip -c ${fasta} > genome.fa\n                FASTA=genome.fa\n            fi\n            if [[ \"${gtf}\" == *\".gz\"* ]]; then\n                gunzip -c ${gtf} > annotation.gtf \n                GTF=annotation.gtf\n            fi\n\n            # make index\n            mkdir star\n            STAR \\\\\n                --runMode genomeGenerate \\\\\n                --runThreadN ${task.cpus} \\\\\n                --sjdbGTFfile \\$GTF \\\\\n                --genomeDir star/ \\\\\n                --genomeFastaFiles \\$FASTA \\\\\n                $avail_mem\n            \"\"\"\n        }"], "list_proc": ["icbi-lab/smartseq2_pipeline/makeSTARindex"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": [" process fastqc {\n        publishDir \"$outdir/fastqc/${sample}_fastqc\", mode: \"$mode\"\n        input:\n            set sample, file(in_fastq) from read_files_fastqc\n\n        output:\n            file(\"*.zip\") into fastqc_files\n\n        script:\n        \"\"\"\n        fastqc  \\\n        -t ${task.cpus} \\\n        ${in_fastq.get(0)} \\\n        ${in_fastq.get(1)}\n        \"\"\"\n    }"], "list_proc": ["icbi-lab/smartseq2_pipeline/fastqc"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": [" process STAR {\n        publishDir \"$outdir/STAR/${sample_fq}_STAR\", mode: \"$mode\"\n        input:\n            set sample_fq, file(in_fastq) from read_files_star\n            file \"star\" from star_index.collect()\n\n        output:\n            set sample_fq, file(\"${sample_fq}.Aligned.sortedByCoord.out.bam\") into bam_sort_filesgz\n            set sample_fq, file(\"${sample_fq}.Aligned.toTranscriptome.out.bam\") into bam_trans_filesgz\n            set sample_fq, file(\"${sample_fq}.Log.final.out\") into bam_mqc\n\n        script:\n        \"\"\"\n        TMP=\"\"\n        if [[ \"${in_fastq.get(0)}\" == *\".gz\"* ]]; then\n            TMP=\"--readFilesCommand zcat\"\n        fi\n        STAR --runThreadN ${task.cpus} --genomeDir star \\$TMP \\\n                --readFilesIn ${in_fastq.get(0)} ${in_fastq.get(1)} \\\n                --outSAMtype BAM SortedByCoordinate --limitBAMsortRAM 16000000000 --outSAMunmapped Within \\\n                --twopassMode Basic --outFilterMultimapNmax 1 --quantMode TranscriptomeSAM \\\n                --outFileNamePrefix \"${sample_fq}.\"\n        \"\"\"\n    }"], "list_proc": ["icbi-lab/smartseq2_pipeline/STAR"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": [" process featureCounts {\n            publishDir \"$outdir/featureCounts/$sample\", mode: \"$mode\"\n            input:\n                set sample, file(bsort) from bam_sort_filesgz\n                file anno_file from gtf_feature_counts.collect()\n\n            output:\n                file(\"*count.txt\") into count_files1\n                file(\"*count.txt\") into count_files2\n                file(\"*count.txt.summary\") into count_mqc\n\n            script:\n            \"\"\"\n            featureCounts -t exon -T ${task.cpus} \\\n            -g gene_name \\\n            -a ${anno_file} \\\n            -o ${sample}.count.txt \\\n            ${bsort}\n\n            \"\"\"\n        }"], "list_proc": ["icbi-lab/smartseq2_pipeline/featureCounts"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    publishDir \"$outdir/multiqc\", mode: \"$mode\"\n\n    input:\n    file ('fastqc/*') from fastqc_files.collect().ifEmpty([])\n    file ('star/*') from bam_mqc.collect().ifEmpty([])\n    file ('featureCounts/*') from count_mqc.collect().ifEmpty([])\n    file ('rsem/*') from rsem_mqc.collect().ifEmpty([])\n\n    output:\n    file \"multiqc_report.html\" into multiqc_report\n    file \"multiqc_data\"\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["icbi-lab/smartseq2_pipeline/multiqc"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 1, "tools": ["Tracer"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": [" process TraCeR{\n            publishDir \"$outdir/TraCeR\", mode: \"$mode\"\n\n            input:\n                set sample, file(in_tracer) from read_files_tracer\n\n            output:\n                file(\"*\") into tcr_files\n\n            script:\n            \"\"\"\n            tracer assemble -p ${task.cpus} -s ${species} \\\n            ${in_tracer.get(0)} ${in_tracer.get(1)} ${sample} .\n            \"\"\"\n        }"], "list_proc": ["icbi-lab/smartseq2_pipeline/TraCeR"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 1, "tools": ["Tracer"], "nb_own": 1, "list_own": ["icbi-lab"], "nb_wf": 1, "list_wf": ["smartseq2_pipeline"], "list_contrib": ["sandrocarollo", "grst"], "nb_contrib": 2, "codes": [" process TCR_summary{\n            publishDir \"$outdir/TraCeR\", mode: \"$mode\"\n\n            input:\n                file ('*') from tcr_files.collect()\n\n            output:\n                file(\"filtered_TCRAB_summary/*\")\n\n            script:\n            \"\"\"\n            tracer summarize -s ${species} .\n            \"\"\"\n        }"], "list_proc": ["icbi-lab/smartseq2_pipeline/TCR_summary"], "list_wf_names": ["icbi-lab/smartseq2_pipeline"]}, {"nb_reuse": 5, "tools": ["BCFtools", "CANU", "fastPHASE", "Nursing", "QUAST"], "nb_own": 3, "list_own": ["imendes93", "ikmb", "icgc-argo-workflows"], "nb_wf": 4, "list_wf": ["flye-assembly", "ReleaseTheKraken", "sv-nf", "dna-seq-processing-wfs"], "list_contrib": ["andricDu", "junjun-zhang", "hknahal", "lindaxiang", "lepsalex", "cimendes", "marchoeppner", "imendes93"], "nb_contrib": 8, "codes": ["\nprocess Quast {\n\n\tpublishDir \"${params.outdir}/Quast\", mode: 'copy'\n\n\tlabel 'quast' \n\n\twhen:\n\tparams.reference && params.gff\n\n\tinput:\n\tfile(assemblies) from AssemblyQuast.collect()\n\n\toutput:\n\tfile(quast_dir)\n\n\tscript:\n\tquast_dir = \"quast\"\n\t\"\"\"\n\t\tquast -o $quast_dir -r ${params.reference} --features ${params.gff} --threads ${task.cpus} $assemblies\n\t\"\"\"\n\n}", "\nprocess fastp {\n\n    tag { sample_id }\n\n    input:\n    set sample_id, file(fastq_pair) from IN_fastq_raw\n\n    output:\n    set sample_id, file(\"*trim_*.fq.gz\") into OUT_fastp\n\n    script:\n    \"\"\"\n    a=(${fastq_pair})\n\n    if ((\\${#a[@]} > 1));\n    then\n        fastp -i ${fastq_pair[0]} -o ${sample_id}_trim_1.fq.gz -I ${fastq_pair[1]} -O ${sample_id}_trim_2.fq.gz \n    else\n        fastp -i ${fastq_pair} -o ${sample_id}_trim_1.fq.gz \n    fi\n    \"\"\"\n}", "\nprocess bcftools {\n\n    publishDir \"results/vcf\"\n\n    input:\n    file vcf from IN_VCF_PROCESS_1\n\n    output:\n    file(\"stats*\") into OUT_BCF\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > stats.vchk\n    \"\"\"\n}", "\nprocess songGetAnalysis {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}", "\tprocess Canu {\n\n\t\tpublishDir \"${params.outdir}/${sample}/assembly/canu\", mode: 'copy'\n\n\t\tlabel 'canu'\n\n\t\tscratch true\n\n\t\tinput:\n\t\tset val(sample),file(reads) from grouped_movies_canu\n\n\t\toutput:\n\t\tfile(\"canu\")\n\t\tfile (report_canu)\n\t\tset val(sample),file(assembly) into (CanuAssembly, CanuAssemblyQuast)\n\n\t\tscript:\t\n\t\treport_canu = \"canu/canu.report\"\n\t\tassembly = sample + \".canu.contigs.fasta\"\n\n\t\tdef options = \"\"\n\t\tif (params.hifi) {\n        \t\toptions = \"-pacbio-hifi\"\n\t\t} else {\n\t\t\toptions = \"-pacbio\"\n\t        }\n\n\t\t\"\"\"\n\t\t\tcanu -p $sample -d canu \\\n\t\t\tgenomeSize=${params.genome_size} \\\n\t\t\tuseGrid=false \\\n\t\t\t$options *.fastq.gz\n\t\t\tcp canu/*.contigs.fasta $assembly\n\t\t\"\"\"\n\t}"], "list_proc": ["ikmb/flye-assembly/Quast", "imendes93/ReleaseTheKraken/fastp", "imendes93/sv-nf/bcftools", "icgc-argo-workflows/dna-seq-processing-wfs/songGetAnalysis", "ikmb/flye-assembly/Canu"], "list_wf_names": ["imendes93/sv-nf", "ikmb/flye-assembly", "icgc-argo-workflows/dna-seq-processing-wfs", "imendes93/ReleaseTheKraken"]}, {"nb_reuse": 4, "tools": ["FastQC", "BCFtools", "mosdepth", "Nursing"], "nb_own": 4, "list_own": ["imendes93", "ikmb", "imgag", "icgc-argo-workflows"], "nb_wf": 4, "list_wf": ["plasmIDent", "ngs-qc", "sv-nf", "dna-seq-processing-wfs"], "list_contrib": ["andricDu", "junjun-zhang", "hknahal", "lindaxiang", "lepsalex", "caspargross", "cimendes", "marchoeppner", "imendes93"], "nb_contrib": 9, "codes": ["\nprocess songManifest {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n        path upload\n    \n    output:\n        path \"out/manifest.txt\"\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"\n}", "\nprocess fastqc {\n\t\n\tlabel 'fastqc'\n\n\tpublishDir \"${params.outdir}/${project}/fastqc\", mode: 'copy' , overwrite: true\n\n\tscratch true\n\n\tstageOutMode 'rsync'\n\n\tinput:\n\tset val(project),path(fastq) from all_reads_by_project\n\n\toutput:\n\tset val(project), path(\"*.zip\") into fastqc_reports\n\tpath(\"*.html\")\n\t\n\tscript:\n\t\n\t\"\"\"\n\t\tfastqc -t 1 $fastq\t\n\t\"\"\"\n\n}", "\nprocess bcftools {\n\n    publishDir \"results/vcf\"\n\n    input:\n    file vcf from IN_VCF_PROCESS_1\n\n    output:\n    file(\"stats*\") into OUT_BCF\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > stats.vchk\n    \"\"\"\n}", "\nprocess mos_depth {\n                           \n    publishDir \"${params.outDir}/${id}/coverage\", mode: 'copy'\n    tag{id}\n\n    input:\n    set id, assembly, type, file(aln_lr), file(aln_lr_idx) from bam_cov\n\n    output:\n    file(\"${id}_cov_${type}.bed.gz\")\n    set id, file(\"${id}_cov_${type}.bed.gz\"), type into cov_bed\n\n    script:\n    \"\"\"\n    ${env}\n    mosdepth -t ${task.cpus} -F 4  -n -b ${params.covWindow} ${id} ${aln_lr} \n    mv ${id}.regions.bed.gz ${id}_cov_${type}.bed.gz\n    \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/dna-seq-processing-wfs/songManifest", "ikmb/ngs-qc/fastqc", "imendes93/sv-nf/bcftools", "imgag/plasmIDent/mos_depth"], "list_wf_names": ["imendes93/sv-nf", "ikmb/ngs-qc", "icgc-argo-workflows/dna-seq-processing-wfs", "imgag/plasmIDent"]}, {"nb_reuse": 4, "tools": ["blima", "SAMtools", "MultiQC", "BEDTools", "Nursing", "STAR", "mosdepth"], "nb_own": 4, "list_own": ["imgag", "ikmb", "inab", "icgc-argo-workflows"], "nb_wf": 4, "list_wf": ["plasmIDent", "RDConnect_RNASeq", "dna-seq-processing-wfs", "pacbio-preprocess"], "list_contrib": ["andricDu", "junjun-zhang", "hknahal", "lindaxiang", "lepsalex", "caspargross", "marchoeppner", "mbosio85"], "nb_contrib": 8, "codes": ["\nprocess songPublish {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n    \n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        val analysis_id, emit: analysis_id\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"\n}", "\nprocess Align {\n    tag \"Align\"\n    label 'STAR'\n    \n    publishDir params.outdir, mode:'copy'\n    \n    input:\n    file fq1 from fq1_ch\n    file fq2 from fq2_ch\n    file fasta_ref from ref_file_ch\n    file fai_ref from ref_fai_ch\n    file dict_ref from ref_dict_ch\n    file all_indexes from idxes_star.collect() \n     \n    output:\n    file(params.SAMPLEID + \"Aligned.toTranscriptome.out.bam\") into transcriptome_bam\n    file(params.SAMPLEID + \"Aligned.sortedByCoord.out.bam\") into aligned_bam\n    script:\n    \"\"\"\n    STAR --runThreadN $task.cpus \\\n     --outSAMunmapped Within \\\n     --genomeDir ./    \\\n     --readFilesIn $fq1 $fq2 \\\n     --outFileNamePrefix $params.SAMPLEID \\\n     --readFilesCommand zcat     \\\n     --quantMode TranscriptomeSAM \\\n     --outFilterType BySJout     \\\n     --outFilterMultimapNmax 20 \\\n     --alignSJoverhangMin 8     \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999 \\\n     --alignIntronMin 20     \\\n     --alignIntronMax 1000000 \\\n     --alignMatesGapMax 1000000     \\\n     --outTmpDir ./STAR_ \\\n     --outSAMtype BAM SortedByCoordinate     \\\n     --outSAMattrIHstart 0 \\\n     --outSAMattributes NH HI NM MD AS nM     \\\n     --outSAMattrRGline ID:$params.SAMPLEID SM:$params.SAMPLEID\n\n    \"\"\" \n}", "\nprocess get_software_versions {\n\n    publishDir \"${params.outdir}/Summary/versions\", mode: 'copy'\n\n    output:\n    file(\"v*.txt\")\n    file(yaml_file) into software_versions_yaml\n\n    script:\n    yaml_file = \"software_versions_mqc.yaml\"\n\n    \"\"\"\n    echo $workflow.manifest.version &> v_ikmb_pacbio-preprocess.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    bam2fasta --version &> v_bam2fasta.txt\n    lima --version &> v_lima.txt\n    extracthifi --version &> v_extracthifi.txt     \n    ccs --version &> v_ccs.txt\n    multiqc --version &> v_multiqc.txt\n    parse_versions.pl >  $yaml_file\n    \"\"\"\n}", "\nprocess find_ovlp_reads {\n                                                                        \n    tag{id + \":\" + contig_name}\n\n    input:\n    set id, lr, contig_name, length, seq, file(assembly), type, bam, bai from contigs.combine(bam_ovlp.filter{it[2] == 'padded'}, by : 0)\n\n    output:\n    set id, contig_name, length, file(\"reads.txt\"), file(\"ovlp.txt\"), file(\"cov_ovlp.txt\") optional true into circos_reads \n\n    script:\n    \"\"\"\n    ${env}\n    bedtools bamtobed -i ${bam} > reads.bed\n    echo -e ${contig_name}'\\\\t'\\$(expr ${params.seqPadding} - 10)'\\\\t'\\$(expr ${params.seqPadding} + 10) > breaks.bed\n    echo -e ${contig_name}'\\\\t'\\$(expr ${length} + ${params.seqPadding} - 10 )'\\\\t'\\$(expr ${length} + ${params.seqPadding} + 10) >> breaks.bed\n    samtools view -L breaks.bed -b ${bam} > region.bam\n    intersectBed -wa -a reads.bed -b breaks.bed > ovlp.bed\n    awk '{print \\$4}' ovlp.bed | sort | uniq -D | uniq > readID.txt\n    samtools view -H region.bam > ovlp.sam \n    samtools view region.bam | grep -f readID.txt >> ovlp.sam || true\n    samtools view -b ovlp.sam > ovlp.bam\n    samtools index ovlp.bam\n    \n    bedtools bamtobed -i ovlp.bam > ovlp_extracted.bed\n     \n    mosdepth -t ${task.cpus} -F 4 -n -b ${params.covWindow} ${contig_name} ovlp.bam\n    gunzip -c ${contig_name}.regions.bed.gz > cov_ovlp.bed\n    \n    03_prepare_bed.R ovlp_extracted.bed ${params.seqPadding} ovlp.txt FALSE ${contig_name} ${length}\n    03_prepare_bed.R cov_ovlp.bed ${params.seqPadding} cov_ovlp.txt TRUE\n    03_prepare_bed.R reads.bed ${params.seqPadding} reads.txt FALSE ${contig_name} ${length}\n    \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/dna-seq-processing-wfs/songPublish", "inab/RDConnect_RNASeq/Align", "ikmb/pacbio-preprocess/get_software_versions", "imgag/plasmIDent/find_ovlp_reads"], "list_wf_names": ["imgag/plasmIDent", "ikmb/pacbio-preprocess", "icgc-argo-workflows/dna-seq-processing-wfs", "inab/RDConnect_RNASeq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 1, "list_wf": ["icgc-argo-sv-copy-number"], "list_contrib": ["junjun-zhang", "jaesvi", "aferriz", "helrick", "alvinwt", "abenjak", "lDesiree", "kateeasoncrukci"], "nb_contrib": 8, "codes": ["\nprocess cram2bam {\n  container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:                                 \n    path input_cram\n    path reference\n    path reference_idx\n\n  output:                                  \n    path \"*.bam\", emit: output_bam\n    path \"*.bai\", emit: output_bai\n\n  shell:\n    '''\n    filename=`basename \"!{input_cram}\"`\n    fname=\"${filename%.*}\"\n    ext=\"${filename##*.}\"\n\n    if [ \"$ext\" != \"cram\" ]; then\n      echo \"Error: input CRAM file must have .cram extension.\"\n      exit 1\n    fi\n\n    samtools view -T !{reference} -b --threads !{params.cpus} -o ${fname}.bam !{input_cram}\n    samtools index ${fname}.bam\n    '''\n}"], "list_proc": ["icgc-argo-workflows/icgc-argo-sv-copy-number/cram2bam"], "list_wf_names": ["icgc-argo-workflows/icgc-argo-sv-copy-number"]}, {"nb_reuse": 4, "tools": ["blima", "SAMtools", "Minimap2", "MultiQC", "Circos", "Nursing"], "nb_own": 3, "list_own": ["ikmb", "imgag", "icgc-argo-workflows"], "nb_wf": 3, "list_wf": ["plasmIDent", "icgc-argo-sv-copy-number", "pacbio-preprocess"], "list_contrib": ["junjun-zhang", "jaesvi", "aferriz", "caspargross", "helrick", "marchoeppner", "alvinwt", "abenjak", "lDesiree", "kateeasoncrukci"], "nb_contrib": 10, "codes": ["\nprocess circos{\n                                                 \n    publishDir \"${params.outDir}/${id}/plots\", mode: 'copy'\n    tag{id + \":\" + contigID}\n\n    input:\n    set id, contigID, length, file(reads), file(ovlp), file(cov_ovlp), file(gc50), file(gc1000), file(gcskew50), file(gcskew1000), file(gcskewsum50), file(gcskewsum1000), file(cov), type, file(rgi), file(rgi_span), file(genes) from combined_data\n\n    output:\n    file(\"${id}_${contigID}_plasmid.*\")\n\n    script:\n    \"\"\"\n    ${env}\n    echo \"chr\t-\t${contigID}\t1\t0\t${length}\tchr1\tcolor=lblue\" > contig.txt\n    ln -s ${workflow.projectDir}/conf/circos//* .\n    circos\n    mv circos.png ${id}_${contigID}_plasmid.png\n    mv circos.svg ${id}_${contigID}_plasmid.svg\n    \"\"\"\n}", "\nprocess songGetAnalysis {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}", "\nprocess get_software_versions {\n\n    publishDir \"${params.outdir}/Summary/versions\", mode: 'copy'\n\n    output:\n    file(\"v*.txt\")\n    file(yaml_file) into software_versions_yaml\n\n    script:\n    yaml_file = \"software_versions_mqc.yaml\"\n\n    \"\"\"\n    echo $workflow.manifest.version &> v_ikmb_pacbio-preprocess.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    bam2fasta --version &> v_bam2fasta.txt\n    lima --version &> v_lima.txt\n    extracthifi --version &> v_extracthifi.txt     \n    ccs --version &> v_ccs.txt\n    multiqc --version &> v_multiqc.txt\n    parse_versions.pl >  $yaml_file\n    \"\"\"\n}", "\nprocess map_longreads {\n                                                    \n    publishDir \"${params.outDir}/${id}/alignment/\", mode: 'copy'\n    tag{id}\n\n    input:\n    set id, assembly, lr, type from to_mapping\n\n    output:\n    set id, assembly, type, file(\"${id}_${type}_lr.bam\"), file(\"${id}_${type}_lr.bam.bai\") into bam_lr\n\n    script:\n    \"\"\"\n    ${env}\n    minimap2 -Y -P -ax map-ont -t ${task.cpus} ${assembly} ${lr} \\\n    | samtools sort | samtools view -b -F 4 -o  ${id}_${type}_lr.bam \n    samtools index ${id}_${type}_lr.bam ${id}_${type}_lr.bam.bai\n    \"\"\"\n}"], "list_proc": ["imgag/plasmIDent/circos", "icgc-argo-workflows/icgc-argo-sv-copy-number/songGetAnalysis", "ikmb/pacbio-preprocess/get_software_versions", "imgag/plasmIDent/map_longreads"], "list_wf_names": ["imgag/plasmIDent", "ikmb/pacbio-preprocess", "icgc-argo-workflows/icgc-argo-sv-copy-number"]}, {"nb_reuse": 4, "tools": ["Rgin", "kraken2", "FastQC", "Nursing"], "nb_own": 4, "list_own": ["imendes93", "imgag", "independentdatalab", "icgc-argo-workflows"], "nb_wf": 4, "list_wf": ["plasmIDent", "nextflow-data-processing-utility-tools", "nf-riboseq", "ReleaseTheKraken"], "list_contrib": ["andricDu", "junjun-zhang", "lindaxiang", "lepsalex", "caspargross", "imendes93", "aneichyk", "jaserud"], "nb_contrib": 8, "codes": ["\nprocess identify_resistance_genes {\n                                                        \n    publishDir \"${params.outDir}/${id}/resistances\", mode: 'copy'\n    tag{id}\n\n    input:\n    set id, assembly, lr from samples_rgi\n    \n    output:\n    set id, file(\"${id}_rgi.txt\") into from_rgi\n\n    script:\n    \"\"\"\n    ${env}\n    rgi main -i ${assembly} -n ${task.cpus} -o ${id}_rgi\n    \"\"\"\n}", "\nprocess kraken2 {\n\n    tag { sample_id }\n\n    publishDir \"results/kraken/\"\n\n    input:\n    set sample_id, file(fastq_pair) from OUT_fastp\n\n    output:\n    set sample_id, file(\"*_kraken_report.txt\") into OUT_KRAKEN\n\n    script:\n    \"\"\"\n    a=(${fastq_pair})\n\n    if ((\\${#a[@]} > 1));\n    then\n        kraken2 --memory-mapping --threads $task.cpus --report ${sample_id}_kraken_report.txt --db minikraken2_v1_8GB --paired --gzip-compressed ${fastq_pair[0]} ${fastq_pair[1]}\n    else\n        kraken2 --memory-mapping --threads $task.cpus --report ${sample_id}_kraken_report.txt --db minikraken2_v1_8GB --single --gzip-compressed ${fastq_pair}\n    fi\n    \"\"\"\n}", "\nprocess fastqc{\n  \n  tag \"$sampleID\"\n  publishDir \"$params.outdir\", mode:\"$mode\"  \n  \n  input:\n    set sampleID, file(raw_fastq) from raw_fastq_to_fastqc\n\n  output:\n    file \"qc/fastq/*_fastqc.{zip, html}\" into fastqc_to_multiqc  \n\n  script:\n  \"\"\"\n    mkdir -p qc/fastq\n    fastqc ${params.fastqc_cmd_args} $raw_fastq -o qc/fastq\n  \"\"\"\n}", "\nprocess songSubmit {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    \n    tag \"${study_id}\"\n    label \"songSubmit\"\n    \n    input:\n        val study_id\n        path payload\n    \n    output:\n        stdout()\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        set -euxo pipefail\n        sing submit -f ${payload} | jq -er .analysisId | tr -d '\\\\n'\n        \"\"\"\n}"], "list_proc": ["imgag/plasmIDent/identify_resistance_genes", "imendes93/ReleaseTheKraken/kraken2", "independentdatalab/nf-riboseq/fastqc", "icgc-argo-workflows/nextflow-data-processing-utility-tools/songSubmit"], "list_wf_names": ["imgag/plasmIDent", "icgc-argo-workflows/nextflow-data-processing-utility-tools", "imendes93/ReleaseTheKraken", "independentdatalab/nf-riboseq"]}, {"nb_reuse": 3, "tools": ["STAR", "Cutadapt", "Nursing"], "nb_own": 3, "list_own": ["independentdatalab", "inab", "icgc-argo-workflows"], "nb_wf": 3, "list_wf": ["nextflow-data-processing-utility-tools", "nf-riboseq", "RDConnect_RNASeq"], "list_contrib": ["andricDu", "junjun-zhang", "lindaxiang", "lepsalex", "mbosio85", "aneichyk", "jaserud"], "nb_contrib": 7, "codes": ["\nprocess Align {\n    tag \"Align\"\n    label 'STAR'\n    \n    publishDir params.outdir, mode:'copy'\n    \n    input:\n    file fq1 from fq1_ch\n    file fq2 from fq2_ch\n    file fasta_ref from ref_file_ch\n    file fai_ref from ref_fai_ch\n    file dict_ref from ref_dict_ch\n    file all_indexes from idxes_star.collect() \n     \n    output:\n    file(params.SAMPLEID + \"Aligned.toTranscriptome.out.bam\") into transcriptome_bam\n    file(params.SAMPLEID + \"Aligned.sortedByCoord.out.bam\") into aligned_bam\n    script:\n    \"\"\"\n    STAR --runThreadN $task.cpus \\\n     --outSAMunmapped Within \\\n     --genomeDir ./    \\\n     --readFilesIn $fq1 $fq2 \\\n     --outFileNamePrefix $params.SAMPLEID \\\n     --readFilesCommand zcat     \\\n     --quantMode TranscriptomeSAM \\\n     --outFilterType BySJout     \\\n     --outFilterMultimapNmax 20 \\\n     --alignSJoverhangMin 8     \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999 \\\n     --alignIntronMin 20     \\\n     --alignIntronMax 1000000 \\\n     --alignMatesGapMax 1000000     \\\n     --outTmpDir ./STAR_ \\\n     --outSAMtype BAM SortedByCoordinate     \\\n     --outSAMattrIHstart 0 \\\n     --outSAMattributes NH HI NM MD AS nM     \\\n     --outSAMattrRGline ID:$params.SAMPLEID SM:$params.SAMPLEID\n\n    \"\"\" \n}", "\nprocess cutadapt3 {\n    \n  tag \"$sampleID\"\n\n  input:\n    set sampleID, file(raw_fastq) from raw_fastq_to_cutadapt\n\n  output:\n    set (\n      sampleID,\n      file(\"${sampleID}_trimmed3.fastq.gz\")\n    ) into trimmed_3_to_5\n    file \"${sampleID}.trim3.log\" into cutadapt3_to_multiqc  \n\n  script:\n  \"\"\"\n    cutadapt ${params.cutadapt3_cmd_args} \\\n      -o ${sampleID}_trimmed3.fastq.gz \\\n      $raw_fastq > ${sampleID}.trim3.log\n  \"\"\"\n}", "\nprocess songManifest {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n        path upload\n    \n    output:\n        path \"out/manifest.txt\"\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"\n}"], "list_proc": ["inab/RDConnect_RNASeq/Align", "independentdatalab/nf-riboseq/cutadapt3", "icgc-argo-workflows/nextflow-data-processing-utility-tools/songManifest"], "list_wf_names": ["icgc-argo-workflows/nextflow-data-processing-utility-tools", "inab/RDConnect_RNASeq", "independentdatalab/nf-riboseq"]}, {"nb_reuse": 3, "tools": ["FastQC", "Bowtie", "Nursing"], "nb_own": 2, "list_own": ["independentdatalab", "icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["nextflow-data-processing-utility-tools", "nf-riboseq"], "list_contrib": ["andricDu", "junjun-zhang", "lindaxiang", "lepsalex", "aneichyk", "jaserud"], "nb_contrib": 6, "codes": ["\nprocess songPublish {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n    \n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        val analysis_id, emit: analysis_id\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"\n}", "\nprocess trimmed_fastqc{\n  \n  tag \"$sampleID\"\n  publishDir \"$params.outdir\", mode:\"$mode\"  \n  \n  input:\n    set sampleID, file(trimmed_fastq) from trimmed_fastq_to_qc\n\n  output:\n    file \"qc/trimmed_fastq/*_fastqc.{zip, html}\" into trimmed_fastqc_to_multiqc  \n\n  script:\n  \"\"\"\n    mkdir -p qc/trimmed_fastq\n    fastqc ${params.fastqc_cmd_args} $trimmed_fastq -o qc/trimmed_fastq\n  \"\"\"\n}", "\nprocess bowtie2_index_ref{\n\n  publishDir \"$params.references.bowtie2_ref_dir\", mode:\"$mode\"  \n\n  input:\n      file rRNA_fasta from reference.rRNA_fasta\n      file tRNA_fasta from reference.tRNA_fasta\n      file cDNA_fasta from reference.cDNA_fasta\n\n  output:\n      file (\"**\")\n      val 1 into align_wait_4_index_2\n  \n  when: run_bowtie2_indexing\n\n  script:\n  \"\"\"\n    bowtie2-build ${params.bowtie2_index_cmd_args} $rRNA_fasta rRNA\n    bowtie2-build ${params.bowtie2_index_cmd_args} $tRNA_fasta tRNA\n    bowtie2-build ${params.bowtie2_index_cmd_args} $cDNA_fasta cDNA\n  \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/nextflow-data-processing-utility-tools/songPublish", "independentdatalab/nf-riboseq/trimmed_fastqc", "independentdatalab/nf-riboseq/bowtie2_index_ref"], "list_wf_names": ["icgc-argo-workflows/nextflow-data-processing-utility-tools", "independentdatalab/nf-riboseq"]}, {"nb_reuse": 3, "tools": ["Cutadapt", "FastQC", "Nursing"], "nb_own": 2, "list_own": ["independentdatalab", "icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["open-access-variant-filtering", "nf-riboseq"], "list_contrib": ["lindaxiang", "aneichyk"], "nb_contrib": 2, "codes": ["\nprocess cutadapt5 {\n    \n  tag \"$sampleID\"\n  publishDir \"$params.outdir/fastq/trimmed\", mode:\"$mode\"\n\n  input:\n    set sampleID, file(trimmed3_fastq) from trimmed_3_to_5\n\n  output:\n    set (\n      sampleID,\n      file(\"${sampleID}_trimmed.fastq.gz\")\n    ) into (trimmed_fastq_to_bowtie2, trimmed_fastq_to_qc)\n    file \"${sampleID}.trim.log\" into cutadapt5_to_multiqc  \n\n  script:\n  \"\"\"\n    cutadapt ${params.cutadapt5_cmd_args} \\\n      -o ${sampleID}_trimmed.fastq.gz \\\n      $trimmed3_fastq > ${sampleID}.trim.log\n  \"\"\"\n}", "\nprocess songSubmit {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    \n    tag \"${study_id}\"\n    label \"songSubmit\"\n    \n    input:\n        val study_id\n        path payload\n    \n    output:\n        stdout()\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        set -euxo pipefail\n        sing submit -f ${payload} | jq -er .analysisId | tr -d '\\\\n'\n        \"\"\"\n}", "\nprocess fastqc{\n  \n  tag \"$sampleID\"\n  publishDir \"$params.outdir\", mode:\"$mode\"  \n  \n  input:\n    set sampleID, file(raw_fastq) from raw_fastq_to_fastqc\n\n  output:\n    file \"qc/fastq/*_fastqc.{zip, html}\" into fastqc_to_multiqc  \n\n  script:\n  \"\"\"\n    mkdir -p qc/fastq\n    fastqc ${params.fastqc_cmd_args} $raw_fastq -o qc/fastq\n  \"\"\"\n}"], "list_proc": ["independentdatalab/nf-riboseq/cutadapt5", "icgc-argo-workflows/open-access-variant-filtering/songSubmit", "independentdatalab/nf-riboseq/fastqc"], "list_wf_names": ["icgc-argo-workflows/open-access-variant-filtering", "independentdatalab/nf-riboseq"]}, {"nb_reuse": 3, "tools": ["Circos", "mosdepth", "Nursing"], "nb_own": 2, "list_own": ["imgag", "icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["plasmIDent", "open-access-variant-filtering"], "list_contrib": ["lindaxiang", "caspargross"], "nb_contrib": 2, "codes": ["\nprocess mos_depth {\n                           \n    publishDir \"${params.outDir}/${id}/coverage\", mode: 'copy'\n    tag{id}\n\n    input:\n    set id, assembly, type, file(aln_lr), file(aln_lr_idx) from bam_cov\n\n    output:\n    file(\"${id}_cov_${type}.bed.gz\")\n    set id, file(\"${id}_cov_${type}.bed.gz\"), type into cov_bed\n\n    script:\n    \"\"\"\n    ${env}\n    mosdepth -t ${task.cpus} -F 4  -n -b ${params.covWindow} ${id} ${aln_lr} \n    mv ${id}.regions.bed.gz ${id}_cov_${type}.bed.gz\n    \"\"\"\n}", "\nprocess songPublish {\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n    \n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        val analysis_id, emit: analysis_id\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"\n}", "\nprocess circos{\n                                                 \n    publishDir \"${params.outDir}/${id}/plots\", mode: 'copy'\n    tag{id + \":\" + contigID}\n\n    input:\n    set id, contigID, length, file(reads), file(ovlp), file(cov_ovlp), file(gc50), file(gc1000), file(gcskew50), file(gcskew1000), file(gcskewsum50), file(gcskewsum1000), file(cov), type, file(rgi), file(rgi_span), file(genes) from combined_data\n\n    output:\n    file(\"${id}_${contigID}_plasmid.*\")\n\n    script:\n    \"\"\"\n    ${env}\n    echo \"chr\t-\t${contigID}\t1\t0\t${length}\tchr1\tcolor=lblue\" > contig.txt\n    ln -s ${workflow.projectDir}/conf/circos//* .\n    circos\n    mv circos.png ${id}_${contigID}_plasmid.png\n    mv circos.svg ${id}_${contigID}_plasmid.svg\n    \"\"\"\n}"], "list_proc": ["imgag/plasmIDent/mos_depth", "icgc-argo-workflows/open-access-variant-filtering/songPublish", "imgag/plasmIDent/circos"], "list_wf_names": ["imgag/plasmIDent", "icgc-argo-workflows/open-access-variant-filtering"]}, {"nb_reuse": 2, "tools": ["STAR", "Nursing"], "nb_own": 2, "list_own": ["inab", "icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["open-access-variant-filtering", "RDConnect_RNASeq"], "list_contrib": ["mbosio85", "lindaxiang"], "nb_contrib": 2, "codes": ["\nprocess Align {\n    tag \"Align\"\n    label 'STAR'\n    \n    publishDir params.outdir, mode:'copy'\n    \n    input:\n    file fq1 from fq1_ch\n    file fq2 from fq2_ch\n    file fasta_ref from ref_file_ch\n    file fai_ref from ref_fai_ch\n    file dict_ref from ref_dict_ch\n    file all_indexes from idxes_star.collect() \n     \n    output:\n    file(params.SAMPLEID + \"Aligned.toTranscriptome.out.bam\") into transcriptome_bam\n    file(params.SAMPLEID + \"Aligned.sortedByCoord.out.bam\") into aligned_bam\n    script:\n    \"\"\"\n    STAR --runThreadN $task.cpus \\\n     --outSAMunmapped Within \\\n     --genomeDir ./    \\\n     --readFilesIn $fq1 $fq2 \\\n     --outFileNamePrefix $params.SAMPLEID \\\n     --readFilesCommand zcat     \\\n     --quantMode TranscriptomeSAM \\\n     --outFilterType BySJout     \\\n     --outFilterMultimapNmax 20 \\\n     --alignSJoverhangMin 8     \\\n     --alignSJDBoverhangMin 1 \\\n     --outFilterMismatchNmax 999 \\\n     --alignIntronMin 20     \\\n     --alignIntronMax 1000000 \\\n     --alignMatesGapMax 1000000     \\\n     --outTmpDir ./STAR_ \\\n     --outSAMtype BAM SortedByCoordinate     \\\n     --outSAMattrIHstart 0 \\\n     --outSAMattributes NH HI NM MD AS nM     \\\n     --outSAMattrRGline ID:$params.SAMPLEID SM:$params.SAMPLEID\n\n    \"\"\" \n}", "\nprocess songManifest {\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n        path upload\n    \n    output:\n        path \"out/manifest.txt\"\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"\n}"], "list_proc": ["inab/RDConnect_RNASeq/Align", "icgc-argo-workflows/open-access-variant-filtering/songManifest"], "list_wf_names": ["icgc-argo-workflows/open-access-variant-filtering", "inab/RDConnect_RNASeq"]}, {"nb_reuse": 1, "tools": ["Nursing"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 1, "list_wf": ["open-access-variant-filtering"], "list_contrib": ["lindaxiang"], "nb_contrib": 1, "codes": ["\nprocess songGetAnalysis {\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/open-access-variant-filtering/songGetAnalysis"], "list_wf_names": ["icgc-argo-workflows/open-access-variant-filtering"]}, {"nb_reuse": 1, "tools": ["UniqueProt"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 1, "list_wf": ["open-access-variant-filtering"], "list_contrib": ["lindaxiang"], "nb_contrib": 1, "codes": ["\nprocess metadataParser {\n  container \"quay.io/icgc-argo/metadata-parser:metadata-parser.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path metadata_analysis\n\n  output:\n    env STUDY_ID, emit: study_id\n    env DONOR_ID, emit: donor_id\n    env EXP, emit: experimental_strategy\n    env PAIRED, emit: paired\n    env ANALYSIS_TOOLS, emit: analysis_tools\n\n  script:\n    \"\"\"\n    set -euxo pipefail\n    STUDY_ID=`cat ${metadata_analysis} | jq -er '.studyId' | tr -d '\\\\n'`\n    DONOR_ID=`cat ${metadata_analysis} | jq -er '.samples[0].donor.donorId' | tr -d '\\\\n'`\n    EXP=`cat ${metadata_analysis} | jq -er '.experiment | .experimental_strategy?  // .library_strategy' | tr -d '\\\\n'`\n    VARIABLE1=`cat ${metadata_analysis} | jq -r 'if ([.read_groups[]?] | length) >0 then [.read_groups[] | .is_paired_end] | all | tostring else null end' | tr -d '\\\\n'`\n    PAIRED=\\${VARIABLE1:-'null'}\n    VARIABLE2=`cat ${metadata_analysis} | jq -r '[.files[] | .info? | .analysis_tools[]?] | unique | join(\",\")' | tr -d '\\\\n'`\n    ANALYSIS_TOOLS=\\${VARIABLE2:-'null'}\n    \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/open-access-variant-filtering/metadataParser"], "list_wf_names": ["icgc-argo-workflows/open-access-variant-filtering"]}, {"nb_reuse": 2, "tools": ["Nursing"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["sanger-wgs-variant-calling", "sanger-wxs-variant-calling"], "list_contrib": ["lindaxiang", "junjun-zhang", "hknahal"], "nb_contrib": 3, "codes": ["\nprocess songGetAnalysis {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}", "\nprocess songGetAnalysis {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/sanger-wxs-variant-calling/songGetAnalysis", "icgc-argo-workflows/sanger-wgs-variant-calling/songGetAnalysis"], "list_wf_names": ["icgc-argo-workflows/sanger-wxs-variant-calling", "icgc-argo-workflows/sanger-wgs-variant-calling"]}, {"nb_reuse": 2, "tools": ["Nursing"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["sanger-wgs-variant-calling", "sanger-wxs-variant-calling"], "list_contrib": ["lindaxiang", "junjun-zhang", "hknahal"], "nb_contrib": 3, "codes": ["\nprocess songManifest {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n        path upload\n    \n    output:\n        path \"out/manifest.txt\"\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"\n}", "\nprocess songSubmit {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    \n    tag \"${study_id}\"\n    label \"songSubmit\"\n    \n    input:\n        val study_id\n        path payload\n    \n    output:\n        stdout()\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        set -euxo pipefail\n        sing submit -f ${payload} | jq -er .analysisId | tr -d '\\\\n'\n        \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/sanger-wxs-variant-calling/songManifest", "icgc-argo-workflows/sanger-wgs-variant-calling/songSubmit"], "list_wf_names": ["icgc-argo-workflows/sanger-wxs-variant-calling", "icgc-argo-workflows/sanger-wgs-variant-calling"]}, {"nb_reuse": 2, "tools": ["GATK", "Nursing"], "nb_own": 2, "list_own": ["ielis", "icgc-argo-workflows"], "nb_wf": 2, "list_wf": ["sanger-wgs-variant-calling", "nextflow-example"], "list_contrib": ["junjun-zhang", "tiagofilipe12", "hknahal", "lindaxiang", "ielis"], "nb_contrib": 5, "codes": ["\nprocess songManifest {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n        path upload\n    \n    output:\n        path \"out/manifest.txt\"\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"\n}", "\nprocess combineVcfs {\n  container 'broadinstitute/gatk:latest'\n\n  input:\n                               \n  file(ref_dict) from reference_genome_dict\n  file(ref_fai) from reference_genome_fai\n  file(ref) from reference_genome\n  file(vcf) from all_vcfs.collect()\n  file(vcf_tbi) from all_vcfs_tbi.collect()\n\n  output:\n  file(\"cohort.g.vcf.gz.*\") into someOutputChannel\n\n  script:\n  \"\"\"\n  NV='';\n  for k in ${vcf}; do\n    NV=\"\\$NV --variant \\$k\";\n  done;\n\n   gatk CombineGVCFs \\\n     -R ${ref} \\\n     \\$NV \\\n     -O cohort.g.vcf.gz\n  \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/sanger-wgs-variant-calling/songManifest", "ielis/nextflow-example/combineVcfs"], "list_wf_names": ["ielis/nextflow-example", "icgc-argo-workflows/sanger-wgs-variant-calling"]}, {"nb_reuse": 2, "tools": ["G-BLASTN", "Nursing"], "nb_own": 2, "list_own": ["icgc-argo-workflows", "iferres"], "nb_wf": 2, "list_wf": ["sanger-wgs-variant-calling", "CSI-nf"], "list_contrib": ["lindaxiang", "junjun-zhang", "iferres", "hknahal"], "nb_contrib": 4, "codes": ["\nprocess BLASTN {\n    label 'blastn'\n\n    input:\n    tuple val(genome_id),\n        val(contig_id),\n        val(spacer_id),\n        path(\"${genome_id}_${contig_id}_${spacer_id}.fasta\")\n\n    output:\n    tuple val(genome_id),\n        val(contig_id),\n        val(spacer_id),\n        path(\"${genome_id}_${contig_id}_${spacer_id}_vs_${db_name}.tab\")\n\n    script:\n    \"\"\"\n    blastn -task \"megablast\" \\\n        -evalue 1e-5 \\\n        -num_threads 1 \\\n        -db ${db_path}/${db_name} \\\n        -query ${genome_id}_${contig_id}_${spacer_id}.fasta \\\n        -out ${genome_id}_${contig_id}_${spacer_id}_vs_${db_name}.tab \\\n        -outfmt 6\n    \"\"\"\n}", "\nprocess songPublish {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n    \n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        val analysis_id, emit: analysis_id\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"\n}"], "list_proc": ["iferres/CSI-nf/BLASTN", "icgc-argo-workflows/sanger-wgs-variant-calling/songPublish"], "list_wf_names": ["iferres/CSI-nf", "icgc-argo-workflows/sanger-wgs-variant-calling"]}, {"nb_reuse": 1, "tools": ["Nursing"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 1, "list_wf": ["sanger-wxs-variant-calling"], "list_contrib": ["lindaxiang", "junjun-zhang", "hknahal"], "nb_contrib": 3, "codes": ["\nprocess songSubmit {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    \n    tag \"${study_id}\"\n    label \"songSubmit\"\n    \n    input:\n        val study_id\n        path payload\n    \n    output:\n        stdout()\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        set -euxo pipefail\n        sing submit -f ${payload} | jq -er .analysisId | tr -d '\\\\n'\n        \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/sanger-wxs-variant-calling/songSubmit"], "list_wf_names": ["icgc-argo-workflows/sanger-wxs-variant-calling"]}, {"nb_reuse": 1, "tools": ["Nursing"], "nb_own": 1, "list_own": ["icgc-argo-workflows"], "nb_wf": 1, "list_wf": ["sanger-wxs-variant-calling"], "list_contrib": ["lindaxiang", "junjun-zhang", "hknahal"], "nb_contrib": 3, "codes": ["\nprocess songPublish {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n    \n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        val analysis_id, emit: analysis_id\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"\n}"], "list_proc": ["icgc-argo-workflows/sanger-wxs-variant-calling/songPublish"], "list_wf_names": ["icgc-argo-workflows/sanger-wxs-variant-calling"]}, {"nb_reuse": 1, "tools": ["gff2ps"], "nb_own": 1, "list_own": ["iferres"], "nb_wf": 1, "list_wf": ["CSI-nf"], "list_contrib": ["iferres"], "nb_contrib": 1, "codes": ["\nprocess EXTRACT_SPACERS {\n    publishDir \"$params.outdir/SPACERS\", mode: 'copy'\n\n    input:\n    tuple val(genome_id), \n        val(contig_id), \n        path(\"${genome_id}_${contig_id}.gff\")\n\n    output:\n    tuple val(genome_id), \n        val(contig_id), \n        path(\"${genome_id}_${contig_id}_sp.gff\"), \n        emit: gff,\n        optional: true\n    tuple val(genome_id),\n        val(contig_id),\n        path(\"fasta_spacers/*.fasta\"), \n        emit: fasta,\n        optional: true\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    gff <- try(read.table(\"${genome_id}_${contig_id}.gff\", stringsAsFactors=FALSE))\n\n    if (class(gff)!=\"try-error\"){\n        spacers <- gff[gff\\$V3==\"CRISPRspacer\",,drop=FALSE]\n    \n        spl <- strsplit(spacers\\$V9, \";\")\n    \n        spacers\\$V10 <- sapply(spl, function(x){\n            gp <- grep(\"sequence=\", x, value = TRUE)\n            sq <- sub(\"sequence=\", \"\", gp, fixed = TRUE)\n            sq\n        })\n    \n        spacers\\$V11 <- sapply(spl, function(x) {\n            gp <- grep(\"ID=\", x, value=TRUE)\n            id <- sub(\"ID=\", \"\", gp, fixed = TRUE)\n            id\n        })\n \n        spacers\\$V12 <- paste(\"${genome_id}\", \"${contig_id}\", spacers\\$V11, sep = \"_\")\n\n        write.table(spacers, \n            file = \"${genome_id}_${contig_id}_sp.gff\", \n            quote = FALSE, \n            sep = \"\\\\t\", \n            row.names = FALSE, \n            col.names = FALSE)\n\n        fastas <- paste(paste0(\">\", spacers\\$V12), spacers\\$V10, sep = \"\\\\n\")\n        \n        dir.create(\"fasta_spacers\")\n        for (i in 1:length(fastas)){\n            fi <- paste0(\"fasta_spacers/\", spacers\\$V11[i], \".fasta\")\n            cat(fastas[i], file = fi)\n        }\n        \n    }\n    \"\"\"\n}"], "list_proc": ["iferres/CSI-nf/EXTRACT_SPACERS"], "list_wf_names": ["iferres/CSI-nf"]}, {"nb_reuse": 1, "tools": ["RANKS"], "nb_own": 1, "list_own": ["iferres"], "nb_wf": 1, "list_wf": ["CSI-nf"], "list_contrib": ["iferres"], "nb_contrib": 1, "codes": ["\nprocess COLLECT_RESULTS {\n    publishDir \"$params.outdir/LCA_RANKS\", mode: 'copy'\n\n    input:\n    path(\"*.tsv\")\n    \n    output:\n    path(\"ranks.tsv\")\n\n    script:\n    \"\"\"\n    echo -ne \"genome\\\\tcontig\\\\tspacer\\\\trank\\\\n\" > ranks\n    cat *.tsv >> ranks\n    mv ranks ranks.tsv\n    \"\"\"\n}"], "list_proc": ["iferres/CSI-nf/COLLECT_RESULTS"], "list_wf_names": ["iferres/CSI-nf"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["FLORA"], "list_contrib": ["cnoel-sebimer"], "nb_contrib": 1, "codes": ["process rrna_removal {\n\n    tag \"$id\"\n    label 'rrna_removal'\n    beforeScript '. /appli/bioinfo/bowtie2/2.4.1/env.sh'\n \n    publishDir \"${params.outdir}/${params.results_dirname}\", mode: 'copy', pattern : 'bowtie2.cmd', saveAs : { bowtie2_cmd -> \"cmd/${task.process}_complete.sh\" }\n\n    input:\n        tuple val(id), path(corrected_reads)\n\n    output:\n        tuple val(id), path(\"filtered_rrna_read_*.fq.{1,2}.gz\"), emit: filtered_rrna\n        path(\"bowtie2.cmd\"), emit: bowtie2_cmd\n\n    script:\n    \"\"\"\n    remove_rrna.sh ${task.cpus} ${params.rrna_db} ${corrected_reads[0]} ${corrected_reads[1]} filtered_rrna_read_${id}.fq.gz bowtie2.cmd &> bowtie2_${id}.log 2>&1\n    \"\"\"\n}"], "list_proc": ["ifremer-bioinformatics/FLORA/rrna_removal"], "list_wf_names": ["ifremer-bioinformatics/FLORA"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["pallor"], "list_contrib": ["alexcorm"], "nb_contrib": 1, "codes": ["\nprocess get_single_copy {\n  label 'busco_env'\n\n  publishDir \"${params.outdir}/${params.completness_dirname}\" , mode: 'copy', pattern : \"${genome_name}/short_summary*\"\n  publishDir \"${params.outdir}/${params.completness_dirname}\" , mode: 'copy', pattern : \"${genome_name}/run_*/*.tsv\" , saveAs : { full_table -> \"${genome_name}/full_table.tsv\" }\n  publishDir \"${params.outdir}/${params.completness_dirname}\" , mode: 'copy', pattern : \"*.faa\"\n\n  input:\n    set genome_name, file(fasta) from genomes\n\n  output:\n    file \"${genome_name}/short_summary*\"                                 \n    file \"${genome_name}/run_*/full_table.tsv\"                                \n    file \"${genome_name}.sg.faa\" into single_copy_proteins\n\n  shell:\n    \"\"\"\n    export AUGUSTUS_CONFIG_PATH=${params.augustus_config_path}\n    busco -c ${task.cpus} --force --offline -m genome -i ${fasta} -o ${genome_name} -l ${params.odb_path}/${params.odb_name} >& busco.log 2>&1\n    catSingleCopyBySpecie.py -f ${genome_name}/run_${params.odb_name}/busco_sequences/single_copy_busco_sequences/ -s ${genome_name} >& catSingleCopyBySpecie.log 2>&1\n    \"\"\"\n\n}"], "list_proc": ["ifremer-bioinformatics/pallor/get_single_copy"], "list_wf_names": ["ifremer-bioinformatics/pallor"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["pallor"], "list_contrib": ["alexcorm"], "nb_contrib": 1, "codes": ["\nprocess mafft {\n  label 'mafft_env'\n\n  publishDir \"${params.outdir}/${params.alignment_dirname}\", mode: 'copy'\n\n  input:\n    file faa from shared_single_copy_proteins.flatten()\n\n  output:\n    file \"*.mafft\" into aligned_shared_single_copy_proteins\n\n  shell:\n    \"\"\"\n    mafft --auto ${faa} > ${faa}.mafft\n    \"\"\"\n}"], "list_proc": ["ifremer-bioinformatics/pallor/mafft"], "list_wf_names": ["ifremer-bioinformatics/pallor"]}, {"nb_reuse": 1, "tools": ["gblocks"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["pallor"], "list_contrib": ["alexcorm"], "nb_contrib": 1, "codes": ["\nprocess gblocks {\n                                          \n  validExitStatus 1\n  label 'gblocks_env'\n\n  publishDir \"${params.outdir}/${params.cleaning_dirname}\", mode: 'copy'\n\n  input:\n    file aln from aligned_shared_single_copy_proteins\n\n  output:\n    file \"*-gb\" into cleaned_aligned_shared_single_copy_proteins\n\n  script:\n    \"\"\"\n    Gblocks ${aln} -t=p -p=n -b3=8 -b4=10 -b5=h\n    \"\"\"\n}"], "list_proc": ["ifremer-bioinformatics/pallor/gblocks"], "list_wf_names": ["ifremer-bioinformatics/pallor"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["samba"], "list_contrib": ["lleroi", "laure182", "cnoel-sebimer", "alexcorm"], "nb_contrib": 4, "codes": [" process q2_import {\n    \n    \tlabel 'qiime2_env1cpu'\n    \n    \tpublishDir \"${params.outdir}/${params.import_dirname}\", mode: 'copy', pattern: 'data.qz*'\n    \tpublishDir \"${params.outdir}/${params.report_dirname}\", mode: 'copy', pattern: '*_output'\n    \tpublishDir \"${params.outdir}/${params.report_dirname}/version\", mode: 'copy', pattern: 'v_*.txt'\n    \tpublishDir \"${params.outdir}/${params.report_dirname}\", mode: 'copy', pattern : 'completecmd', saveAs : { complete_cmd_import -> \"cmd/${task.process}_complete.sh\" }\n    \n    \tinput :\n    \t\tfile q2_manifest from manifest\n                file ready from ready_import\n    \n    \toutput :\n    \t\tfile 'data.qza' into imported_data\n    \t\tfile 'data.qzv' into imported_visu\n    \t\tfile 'import_output' into imported_output\n    \t\tfile 'completecmd' into complete_cmd_import\n                    file 'v_qiime2.txt' into qiime2_version\n    \n    \twhen :\n    \t\t!params.stats_only && !params.dada2merge\n    \n    \tscript :\n    \t\"\"\"\n    \tq2_import.sh ${params.singleEnd} ${q2_manifest} data.qza data.qzv import_output completecmd &> q2_import.log 2>&1\n    \tqiime --version|grep 'q2cli'|cut -d' ' -f3 > v_qiime2.txt\n    \t\"\"\"\n    }"], "list_proc": ["ifremer-bioinformatics/samba/q2_import"], "list_wf_names": ["ifremer-bioinformatics/samba"]}, {"nb_reuse": 1, "tools": ["QIIME", "BioMe"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["samba"], "list_contrib": ["lleroi", "laure182", "cnoel-sebimer", "alexcorm"], "nb_contrib": 4, "codes": [" process microDecon_step2 {\n    \n        \tlabel 'qiime2_env'\n    \n        \tpublishDir \"${params.outdir}/${params.microDecon_dirname}\", mode: 'copy', pattern: 'decontaminated_ASV_table.qza'\n        \tpublishDir \"${params.outdir}/${params.report_dirname}/microDecon\", mode: 'copy', pattern: 'decontaminated_ASV_table.qza'\n    \n        \tinput :\n        \t\tfile table4microDecon from decontam_table_step2\n    \n        \toutput :\n        \t\tfile 'decontaminated_ASV_table.qza' into decontam_table_qza, decontam_table_picrust2, decontam_table_ancom\n    \n        \twhen :\n        \t\t!params.stats_only && !params.dada2merge && params.microDecon_enable\n    \n        \tshell :\n        \t\"\"\"\n        \tbiom convert -i ${table4microDecon} -o decontaminated_ASV_table.biom --to-hdf5 --table-type=\"OTU table\" --process-obs-metadata taxonomy\n        \tqiime tools import --input-path decontaminated_ASV_table.biom --type 'FeatureTable[Frequency]' --input-format BIOMV210Format --output-path decontaminated_ASV_table.qza\n        \t\"\"\"\n        }"], "list_proc": ["ifremer-bioinformatics/samba/microDecon_step2"], "list_wf_names": ["ifremer-bioinformatics/samba"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["samba"], "list_contrib": ["lleroi", "laure182", "cnoel-sebimer", "alexcorm"], "nb_contrib": 4, "codes": [" process microDecon_step3 {\n    \n        \tlabel 'seqtk_env'\n    \n        \tpublishDir \"${params.outdir}/${params.microDecon_dirname}\", mode: 'copy', pattern: 'decontaminated_ASV_ID.txt'\n        \tpublishDir \"${params.outdir}/${params.microDecon_dirname}\", mode: 'copy', pattern: 'decontaminated_ASV.fasta'\n        \tpublishDir \"${params.outdir}/${params.report_dirname}/microDecon\", mode: 'copy', pattern: 'decontaminated_ASV.fasta'\n    \n        \tinput :\n        \t\tfile decontam_table from decontam_table_step3\n        \t\tfile dada2_output from decontam_output\n    \n        \toutput :\n        \t\tfile 'decontaminated_ASV_ID.txt' into decontam_ASV_ID\n        \t\tfile 'decontaminated_ASV.fasta' into decontam_ASV_fasta\n    \n        \twhen :\n        \t\t!params.stats_only && !params.dada2merge && params.microDecon_enable\n    \n        \tshell :\n        \t\"\"\"\n        \tcut -d \\$'\\t' -f1 ${decontam_table} | sed '1d' > decontaminated_ASV_ID.txt\n        \tseqtk subseq ${dada2_output}/sequences.fasta decontaminated_ASV_ID.txt > decontaminated_ASV.fasta\n        \t\"\"\"\n        }"], "list_proc": ["ifremer-bioinformatics/samba/microDecon_step3"], "list_wf_names": ["ifremer-bioinformatics/samba"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["samba"], "list_contrib": ["lleroi", "laure182", "cnoel-sebimer", "alexcorm"], "nb_contrib": 4, "codes": [" process microDecon_step4 {\n    \n        \tlabel 'qiime2_env'\n    \n        \tpublishDir \"${params.outdir}/${params.phylogeny_dirname}\", mode: 'copy', pattern: '*.qza'\n    \n        \tinput :\n        \t\tfile ASV_fasta from decontam_ASV_fasta\n    \n        \toutput :\n        \t\tfile 'decontam_seqs.qza' into decontam_seqs_qza, decontam_seqs_phylo, decontam_seqs_picrust2, decontam_seqs_ancom\n    \n        \twhen :\n        \t\t!params.stats_only && !params.dada2merge && params.microDecon_enable\n    \n        \tshell :\n        \t\"\"\"\n                qiime tools import --input-path ${ASV_fasta} --output-path decontam_seqs.qza --type 'FeatureData[Sequence]'\n        \t\"\"\"\n        }"], "list_proc": ["ifremer-bioinformatics/samba/microDecon_step4"], "list_wf_names": ["ifremer-bioinformatics/samba"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["samba"], "list_contrib": ["lleroi", "laure182", "cnoel-sebimer", "alexcorm"], "nb_contrib": 4, "codes": [" process lr_mapping {\n     label 'lr_mapping_env'\n   \n     publishDir \"${params.outdir}/${params.lr_mapping_dirname}\", mode: 'copy', pattern: '*.bam'\n     publishDir \"${params.outdir}/${params.report_dirname}/version\", mode: 'copy', pattern: 'v_*.txt'\n   \n     input:\n       set sample, file(fastq) from longreadsmanifest\n       file ready from ready_lr\n   \n     output:\n       file \"*.bam\" into lr_mapped\n       file 'lr_mapping.ok' into process_lr_mapping_report\n       file 'lr_samtools-sort.log' into process_lr_sort_report\n       file 'v_minimap2.txt' into v_minimap2_version\n   \n     shell:\n       \"\"\"\n       minimap2 -t ${task.cpus} -K 25M -ax ${params.lr_type} -L ${params.lr_tax_fna} ${fastq} | samtools view -h -F0xe00 | samtools sort -o ${sample}.bam -O bam - &> lr_minimap2.log 2>&1\n       touch lr_mapping.ok\n       samtools index ${sample}.bam &> lr_samtools-sort.log 2>&1\n       touch lr_samtools-sort.ok\n       minimap2 --version > v_minimap2.txt\n       \"\"\"\n   }"], "list_proc": ["ifremer-bioinformatics/samba/lr_mapping"], "list_wf_names": ["ifremer-bioinformatics/samba"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["ifremer-bioinformatics"], "nb_wf": 1, "list_wf": ["samba"], "list_contrib": ["lleroi", "laure182", "cnoel-sebimer", "alexcorm"], "nb_contrib": 4, "codes": [" process lr_getfasta {\n      label 'seqtk_env'\n      input : \n         file(fastq) from longreadstofasta\n\n      output :\n         file 'lr_sequences.fasta' into lr_sequences\n\n      shell :\n      \"\"\"\n         seqtk seq -a ${fastq} > 'lr_sequences.fasta'\n      \"\"\"\n   }"], "list_proc": ["ifremer-bioinformatics/samba/lr_getfasta"], "list_wf_names": ["ifremer-bioinformatics/samba"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["flye-assembly"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\tprocess runFastp {\n\n\t\tlabel 'fastp'\n\n\t\tinput:\n\t\tset val(sample),file(reads) from Reads\n\n\t\toutput:\n\t\tset val(sample),file(trimmed_reads) into ReadsFinal\n\n\t\tscript:\n\t\ttrimmed_reads = reads.getBaseName() + \".trimmed.fastq.gz\"\n\n\t\t\"\"\"\n\t\t\tfastp -i $reads -o $trimmed_reads --disable_adapter_trimming -f 30 -t 30 -Q -L -w ${task.cpus}\n\t\t\"\"\"\n\t}"], "list_proc": ["ikmb/flye-assembly/runFastp"], "list_wf_names": ["ikmb/flye-assembly"]}, {"nb_reuse": 1, "tools": ["IPA"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["flye-assembly"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\tprocess IPA {\n\n\t\tpublishDir \"${params.outdir}/${sample}/assembly/flye\", mode: 'copy'\n\n\t\tlabel 'ipa'\n\n\t\tscratch true\n\n\t\tinput:\n\t\tset val(sample),file(reads) from grouped_movies_ipa\n\n\t\toutput:\n\t\tset val(sample),file(assembly) into ( IpaAssembly, IpaAssemblyQuast)\n\n\t\tscript:\n\n\t\tassembly = sample + \".ipa.fa\"\n\n\t\t\"\"\"\n\t\t\tipa local --run-dir ipa_run -i $reads --nthreads ${task.cpus}\n\t\t\tcp ipa_run/assembly-results/final.p_ctg.fasta $assembly\n\t\t\"\"\"\n\n\t}"], "list_proc": ["ikmb/flye-assembly/IPA"], "list_wf_names": ["ikmb/flye-assembly"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["flye-assembly"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\tprocess Flye {\n\n\t\t\tpublishDir \"${params.outdir}/${sample}/assembly/flye\", mode: 'copy'\n\n\t\t\tlabel 'flye'\n\t\n\t\t\tscratch true\n\n\t\t\tinput:\n\t\t\tset val(sample),file(reads) from grouped_movies\n\n\t\t\toutput:\n\t\t\tset val(sample),file(assembly) into ( FlyeAssembly, FlyeAssemblyBusco )\n\t\t\tfile(assembly_renamed) into FlyeAssemblyQuast\n\t\t\tfile(\"${folder_name}/*\")\n\n\t\t\tscript:\n\t\t\tfolder_name = \"flye_assembly\"\n\t\t\trun_options = \"\"\n\t\t\tif (params.hifi) {\n\t\t\t\tfolder_name = \"flye_assembly_hifi\"\n\t\t\t}\n\t\t\tassembly = folder_name + \"/assembly.fasta\"\n\t\t\tassembly_info = folder_name + \"/assembly_info.txt\"\n\t\t\tassembly_renamed = sample + \".flye.assembly.fasta\"\n\n\t\t\tdef options = \"\"\n\t\t\tif (params.genome_size) {\n                \t\toptions = \"--genome-size ${params.genome_size} --asm-coverage 60\"\n\t\t        }\n\n\t\t\tif (params.hifi) {\n\t\t\t\toptions = options + \" --pacbio-hifi\"\n\t\t\t} else {\n\t\t\t\toptions = options + \" --pacbio-raw\"\n\t\t\t}\n\n\t\t\t\"\"\"\t\n\t\t\t\tflye $options $reads -i 2 --threads ${task.cpus} --out-dir $folder_name\n\t\t\t\tcp $assembly $assembly_renamed\n\t\t\t\"\"\"\n\n\t\t}"], "list_proc": ["ikmb/flye-assembly/Flye"], "list_wf_names": ["ikmb/flye-assembly"]}, {"nb_reuse": 1, "tools": ["KAT"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["flye-assembly"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\nprocess KatGCP {\n\n        publishDir \"${params.outdir}/${sample}/QC\", mode: 'copy'\n\n\tlabel 'kat'\n\n\twhen:\n\tparams.qc_kat\n\n\tinput:\n\tset val(sample),file(reads) from ReadsKat\n\n\toutput:\n\tfile(kat_dist) into KatDist\n\n\tscript:\n\tkat_dist = \"kat-gcp.mx.png\"\n\n\t\"\"\"\n\t\tkat gcp -t ${task.cpus} $reads\n\t\"\"\"\n\n}"], "list_proc": ["ikmb/flye-assembly/KatGCP"], "list_wf_names": ["ikmb/flye-assembly"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["flye-assembly"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\nprocess makeRef {\n\n\tlabel 'map'\n\n\twhen:\n\tparams.reference\n\n\tinput:\n\tfile(fasta) from Ref\n\n\toutput:\n\tset file(idx),file(fasta) into RefMap\n\n\tscript:\n\tidx = fasta.getBaseName() + \".idx\"\n\n\t\"\"\"\n\t\tminimap2 -d $idx $fasta\n\t\"\"\"\n}"], "list_proc": ["ikmb/flye-assembly/makeRef"], "list_wf_names": ["ikmb/flye-assembly"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["flye-assembly"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\nprocess Minimap {\n\n\tlabel 'map'\n\n\tpublishDir \"${params.outdir}/${sample}/Mapping\", mode: 'copy'\n\n\twhen:\n\tparams.reference \n\n\tinput:\n\tset val(sample),file(reads) from ReadsAlign\n\tset file(idx),file(fasta) from RefMap.collect()\n\n\toutput:\n\tset val(sample),file(bam),file(bai)\n\n\tscript:\n\tbam = reads.getBaseName() + \".bam\"\n\tbai = bam + \".bai\"\n\n\t\"\"\"\n\t\tminimap2 -ax map-pb $idx $reads | samtools sort -m 8G - | samtools view -bh -o $bam -\n\t\tsamtools index $bam\n\t\"\"\"\n\n}"], "list_proc": ["ikmb/flye-assembly/Minimap"], "list_wf_names": ["ikmb/flye-assembly"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["ngs-qc"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\nprocess multiqc_run {\n\n\tpublishDir \"${params.outdir}/MultiQC\", mode: 'copy', overwrite: true\n\n\tlabel 'multiqc'\n\n\tstageOutMode 'rsync'\n\n\tinput:\n\tfile(json) from stats_file\n\n\toutput:\n\tfile(multiqc)\n\n\tscript:\n\tmultiqc = \"multiqc_demux.html\"\t\n\t\"\"\"\n\t\tmultiqc -b \"Run ${run_dir}\" -n $multiqc .\n\t\"\"\"\n\n}"], "list_proc": ["ikmb/ngs-qc/multiqc_run"], "list_wf_names": ["ikmb/ngs-qc"]}, {"nb_reuse": 1, "tools": ["blima"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["pacbio-preprocess"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\tprocess PbDemux {\n\n\t\tpublishDir \"${params.outdir}/${sample}/Demux\", mode: 'copy'\n\n\t\tinput:\n\t\tset val(sample),file(bam),file(pbi) from HiFiBam\n\t\tfile(barcode_fa) from barcodes.collect()\n\n\t\toutput:\n\t\tset val(sample),file(\"*-*.bam\") into final_bams\n\t\tset val(sample),file(\"*.lima.*\") into lima_reports\n\t\t\n\t\tscript:\n\t\tdef options = \"\"\n\t\tif (params.hifi || params.extract-hifi) {\n\t\t\toptions = \"--ccs --min-score 80\"\n\t\t}\n\t\tdemux = bam.getBaseName() + \".demux.bam\"\n\t\t\"\"\"\n\t\t\tlima $bam $barcode_fa $demux --same $options --split-named\n\t\t\tpbindex $demux\n\t\t\"\"\"\n\n\t}"], "list_proc": ["ikmb/pacbio-preprocess/PbDemux"], "list_wf_names": ["ikmb/pacbio-preprocess"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ikmb"], "nb_wf": 1, "list_wf": ["vep"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\tprocess vcf_sites_only {\n\n\t\tlabel 'gatk'\n\n\t\tpublishDir \"${params.outdir}/VCFS_SITES_ONLY\", mode: 'copy'\n\n\t\tinput:\n\t\tset file(vcf),file(vcf_index) from vcfs\n\n\t\toutput:\n\t\tset file(vcf_sites),file(vcf_sites_index) into vcf_sites\n\n\t\tscript:\n\t\n\t\tvcf_sites = vcf.getBaseName() + \".sites.vcf.gz\"\n\t\tvcf_sites_index = vcf_sites + \".tbi\"\n\n\t\t\"\"\"\n\t\t\tgatk SelectVariants -V $vcf --sites-only-vcf-output -O $vcf_sites -OVI\n\t\t\"\"\"\n\t}"], "list_proc": ["ikmb/vep/vcf_sites_only"], "list_wf_names": ["ikmb/vep"]}, {"nb_reuse": 1, "tools": ["VGE"], "nb_own": 1, "list_own": ["imendes93"], "nb_wf": 1, "list_wf": ["sv-nf"], "list_contrib": ["imendes93", "cimendes"], "nb_contrib": 2, "codes": ["\nprocess index {\n\n    publishDir \"results/graph\"\n\n    input:\n    file(graph) from IN_INDEX_1\n    val kmer from IN_KMER\n\n    output:\n    file(\"*.xg\") into XG_FILE\n    file(\"*.gcsa\") into GCSA_FILE\n\n    script:\n    \"\"\"\n    vg index -x graph.xg ${graph}\n    vg index -g graph.gcsa -k ${kmer} ${graph}\n    \"\"\"\n}"], "list_proc": ["imendes93/sv-nf/index"], "list_wf_names": ["imendes93/sv-nf"]}, {"nb_reuse": 1, "tools": ["VGE"], "nb_own": 1, "list_own": ["imendes93"], "nb_wf": 1, "list_wf": ["sv-nf"], "list_contrib": ["imendes93", "cimendes"], "nb_contrib": 2, "codes": ["\nprocess view_map {\n\n    publishDir \"results/mapping\"\n\n    input: \n    file vg_graph from IN_INDEX_2\n    file gam from OUT_MAP_VIEW\n\n    output:\n    file(\"map.dot\") into MAP_VIEW\n\n    script:\n    \"\"\"\n    vg view -d ${vg_graph} -A ${gam} > map.dot\n    \"\"\"\n}"], "list_proc": ["imendes93/sv-nf/view_map"], "list_wf_names": ["imendes93/sv-nf"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["independentdatalab"], "nb_wf": 1, "list_wf": ["nf-riboseq"], "list_contrib": ["aneichyk"], "nb_contrib": 1, "codes": ["\nprocess bowtie2_index_ncRNA {\n  publishDir \"$params.references.bowtie2_ref_dir\", mode:\"$mode\"  \n\n  input:\n    file ncRNA_fasta from reference.ncRNA_fasta \n\n  output:\n    file (\"**\")\n  \n  when: run_bowtie2_indexing && ncRNA_fasta != 'NO_FILE'\n\n  script:\n  \"\"\"\n    bowtie2-build ${params.bowtie2_index_cmd_args} $ncRNA_fasta ncRNA\n  \"\"\"\n}"], "list_proc": ["independentdatalab/nf-riboseq/bowtie2_index_ncRNA"], "list_wf_names": ["independentdatalab/nf-riboseq"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["independentdatalab"], "nb_wf": 1, "list_wf": ["nf-riboseq"], "list_contrib": ["aneichyk"], "nb_contrib": 1, "codes": ["\nprocess bowtie2_align {\n  tag \"$sampleID\"\n  publishDir \"$params.outdir/qc\", mode:\"$mode\", pattern:\"bowtie2_logs\"  \n  \n  input:\n    set sampleID, file(trimmed_fastq) from trimmed_fastq_to_bowtie2\n    file bowtie2_ref from file(reference.bowtie2_ref_dir)\n    val flag from align_wait_4_index_1.mix(align_wait_4_index_2)\n\n  output:\n    set (\n      sampleID,\n      file (\"${sampleID}.sam\")\n    ) into sam_to_sort\n    file(\"bowtie2_logs/*\") into bowtie2_logs_to_multiqc\n\n  script:\n  \"\"\"\n    mkdir bowtie2_logs\n    gunzip -c ${trimmed_fastq} > unzipped.fq\n    bowtie2 ${params.bowtie2_contaminants_cmd_args} \\\n        -q unzipped.fq \\\n        --un no_rRNA.fq \\\n        -x ${bowtie2_ref}/rRNA > /dev/null 2> bowtie2_logs/${sampleID}_1_rRNA.log\n    bowtie2 ${params.bowtie2_contaminants_cmd_args} \\\n        -q no_rRNA.fq \\\n        --un=no_rRNA_tRNA.fq \\\n        -x ${bowtie2_ref}/tRNA > /dev/null 2> bowtie2_logs/${sampleID}_2_tRNA.log\n    if [[ $reference.ncRNA_fasta != \"NO_FILE\" ]]; then\n        echo \"Align to ncRNA\"\n        bowtie2 ${params.bowtie2_contaminants_cmd_args} \\\n            -q no_rRNA_tRNA.fq \\\n            --un=no_rRNA_tRNA_ncRNA.fq \\\n            -x ${bowtie2_ref}/ncRNA > /dev/null 2> bowtie2_logs/${sampleID}_3_ncRNA.log\n        bowtie2 ${params.bowtie2_cdna_cmd_args} \\\n            -q no_rRNA_tRNA_ncRNA.fq \\\n            --un=${sampleID}_unaligned.fq \\\n            -x ${bowtie2_ref}/cDNA --no-unal  > ${sampleID}.sam 2> bowtie2_logs/${sampleID}_4_cDNA.log\n    fi\n    if [[ $reference.ncRNA_fasta == \"NO_FILE\"  ]]; then\n        echo \"No ncRNA specified, align to cDNA\"\n        bowtie2 ${params.bowtie2_cdna_cmd_args} \\\n            -q no_rRNA_tRNA.fq \\\n            --un=${sampleID}_unaligned.fq \\\n            -x ${bowtie2_ref}/cDNA --no-unal  > ${sampleID}.sam 2> bowtie2_logs/${sampleID}_3_cDNA.log\n    fi\n  \"\"\"\n}"], "list_proc": ["independentdatalab/nf-riboseq/bowtie2_align"], "list_wf_names": ["independentdatalab/nf-riboseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["independentdatalab"], "nb_wf": 1, "list_wf": ["nf-riboseq"], "list_contrib": ["aneichyk"], "nb_contrib": 1, "codes": ["\nprocess samtools_sam_sort{\n  \n  tag \"$sampleID\"\n  \n  input:\n    set sampleID, file(sam) from sam_to_sort\n\n  output:\n    set (\n      sampleID,\n      file(\"${sampleID}.srtd.bam\"),\n    ) into bam_sorted_to_markdups\n\n  script:\n  \"\"\"\n    samtools view -F 16 -b -o ${sampleID}.bam $sam     \n    samtools sort ${sampleID}.bam -o ${sampleID}.srtd.bam -m 2GiB -@ 8\n  \"\"\"\n}"], "list_proc": ["independentdatalab/nf-riboseq/samtools_sam_sort"], "list_wf_names": ["independentdatalab/nf-riboseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["independentdatalab"], "nb_wf": 1, "list_wf": ["nf-riboseq"], "list_contrib": ["aneichyk"], "nb_contrib": 1, "codes": ["\nprocess samtools_bam_index{\n  \n  tag \"$sampleID\"\n  publishDir \"$params.outdir/bam/${sampleID}\", mode:\"$mode\" \n  \n  input:\n    set sampleID, file(bam) from final_bam_to_index\n\n  output:\n    set (\n      sampleID,\n      file(\"${bam}.bai\")\n    ) \n\n  script:\n  \"\"\"\n    samtools index ${bam} ${bam}.bai\n  \"\"\"\n}"], "list_proc": ["independentdatalab/nf-riboseq/samtools_bam_index"], "list_wf_names": ["independentdatalab/nf-riboseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["independentdatalab"], "nb_wf": 1, "list_wf": ["nf-riboseq"], "list_contrib": ["aneichyk"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n  publishDir \"$params.outdir/qc\", mode:\"$mode\"\n  \n  input:\n    file ('fastqc/*') from fastqc_to_multiqc.collect().ifEmpty([])\n    file ('cutadapt3/*') from cutadapt3_to_multiqc.collect().ifEmpty([])\n    file ('cutadapt5/*') from cutadapt5_to_multiqc.collect().ifEmpty([])\n    file ('trimmed_fastqc/*') from trimmed_fastqc_to_multiqc.collect().ifEmpty([])\n    file ('bowtie2/*') from bowtie2_logs_to_multiqc.collect().ifEmpty([])\n    file ('markDuplicates/*') from picard_markdups_to_multiqc.collect().ifEmpty([])\n\n  output:\n    file (\"**\")\n\n  script:  \n  \"\"\"   \n    multiqc .                                                                                                   \n  \"\"\"\n}"], "list_proc": ["independentdatalab/nf-riboseq/multiqc"], "list_wf_names": ["independentdatalab/nf-riboseq"]}, {"nb_reuse": 1, "tools": ["gmap_build"], "nb_own": 1, "list_own": ["isugifNF"], "nb_wf": 1, "list_wf": ["RNAseq"], "list_contrib": ["j23414"], "nb_contrib": 1, "codes": ["\nprocess gsnap_index {\n    tag \"${genome_gz.simpleName}\"\n    label 'gsnap'\n\n    publishDir \"${params.outdir}/03_GSNAP\", mode: 'copy'\n\n    input:\n    path(genome_gz)\n\n    output:\n    tuple val(\"${genome_gz.simpleName}\"), path(\"*\")\n\n    script:\n    \"\"\"\n    #! /usr/bin/env bash\n    gmap_build \\\n     --gunzip \\\n     -d ${genome_gz.simpleName} \\\n     -D gmapdb \\\n     ${genome_gz}\n    \"\"\"\n}"], "list_proc": ["isugifNF/RNAseq/gsnap_index"], "list_wf_names": ["isugifNF/RNAseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["isugifNF"], "nb_wf": 1, "list_wf": ["RNAseq"], "list_contrib": ["j23414"], "nb_contrib": 1, "codes": ["\nprocess gsnap_align {\n    tag \"${readname}\"\n    label 'gsnap'\n\n    publishDir \"${params.outdir}/03_GSNAP\", mode: 'copy'\n\n    input: tuple val(genome_name), path(gmap_dir),val(readname), path(read_pairs)\n\n    output: path(\"*\")\n\n    script:\n    \"\"\"\n    #! /usr/bin/env bash\n    PROC=\\$(((`nproc`-1)*3/4+1))\n    PROC2=\\$(((`nproc`-1)*1/4+1))\n    ${gsnap_app} \\\n     --gunzip \\\n     -d ${genome_name} \\\n     -D gmapdb/ \\\n     -N 1 -t \\$PROC -B 4 -m 5 \\\n     --input-buffer-size=1000000 \\\n     --output-buffer-size=1000000 \\\n     -A sam \\\n     ${read_pairs} |\n     samtools view --threads \\$PROC -bS - > ${readname}.bam\n    \"\"\"\n}"], "list_proc": ["isugifNF/RNAseq/gsnap_align"], "list_wf_names": ["isugifNF/RNAseq"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["isugifNF"], "nb_wf": 1, "list_wf": ["blast"], "list_contrib": ["j23414", "isugif"], "nb_contrib": 2, "codes": ["\nprocess software_check {\n  label 'software_check'\n\n  publishDir params.outdir\n\n  output:\n    path 'software_check.txt'\n\n  script:\n  \"\"\"\n  echo \"blastn -version\" > software_check.txt\n  blastn -version >> software_check.txt\n\n  echo \"\\nmakeblastdb -version\" >> software_check.txt\n  makeblastdb -version >> software_check.txt\n\n  \"\"\"\n}"], "list_proc": ["isugifNF/blast/software_check"], "list_wf_names": ["isugifNF/blast"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["isugifNF"], "nb_wf": 1, "list_wf": ["freebayes"], "list_contrib": ["j23414", "isugif"], "nb_contrib": 2, "codes": ["\nprocess createFastaIndex {\n\n  container = \"$samtools19_container\"\n\n  publishDir \"${params.outdir}\", mode: 'copy', pattern: '*fai'\n\n  input:\n  path genome from genome_samtools\n\n\n  output:\n  file(\"*.fai\") into freebayes_fai\n  file(\"*.fai\") into freebayes_fai2\n\n  script:\n\n  \"\"\"\nsamtools faidx ${genome} \n\n  \"\"\"\n\n}"], "list_proc": ["isugifNF/freebayes/createFastaIndex"], "list_wf_names": ["isugifNF/freebayes"]}, {"nb_reuse": 1, "tools": ["FreeBayes"], "nb_own": 1, "list_own": ["isugifNF"], "nb_wf": 1, "list_wf": ["freebayes"], "list_contrib": ["j23414", "isugif"], "nb_contrib": 2, "codes": ["\nprocess runFreebayes {\n\terrorStrategy 'retry' \n\tcontainer = \"$freebayes_container\"\n\n\t\n\toutput: \n\tfile('*.vcf') into window_vcf  \n\n\tinput:\n\tpath fai from freebayes_fai.val\n\t                                       \n\tpath genome from genome_freebayes.val\n \tval region from freebayes_regions.splitText(){it.trim()} \n\n\tscript:\n\n\t\"\"\"\n\tfreebayes --region ${region} ${params.options} --bam ${params.bam} --vcf ${params.vcf}\"_\"${region}\".vcf\" --fasta-reference ${genome}\n\t\n\t\"\"\"\t\t\n\t\n}"], "list_proc": ["isugifNF/freebayes/runFreebayes"], "list_wf_names": ["isugifNF/freebayes"]}, {"nb_reuse": 2, "tools": ["SAMtools", "EVE", "BWA"], "nb_own": 2, "list_own": ["j23414", "jambler24"], "nb_wf": 2, "list_wf": ["compare_workflows", "bac_pangenome"], "list_contrib": ["j23414", "jambler24"], "nb_contrib": 2, "codes": ["\nprocess Eve {\n  input: val baton_in\n    \n  output: stdout\n\n  script:\n  \"\"\"\n  #! /usr/bin/env bash\n  sleep 5                    # <= pause for a few seconds\n  echo \"$baton_in; Eve passes baton\"\n  \"\"\"\n}", "\nprocess read_mapping {\n  label 'high_memory'\n  input:\n    file forwardTrimmed\n    file reverseTrimmed\n    val sampleNumber\n    file genome from genome_file\n    file genome_bwa_amb\n    file genome_bwa_ann\n    file genome_bwa_bwt\n    file genome_bwa_pac\n    file genome_bwa_sa\n  output:\n    file \"sample_${sampleNumber}_sorted.bam\" into bamfiles\n    file \"sample_${sampleNumber}_sorted.bai\" into bamindexfiles\n    file \"sample_${sampleNumber}_sorted.bam\" into bam_rseqc\n    file \"sample_${sampleNumber}_sorted.bai\" into bamindexfiles_rseqc\n    file \"sample_${sampleNumber}_sorted.bam\" into bam_preseq\n    file \"sample_${sampleNumber}_sorted.bam\" into bam_forSubsamp\n    file \"sample_${sampleNumber}_sorted.bam\" into bam_skipSubsamp\n    file \"sample_${sampleNumber}_sorted.bam\" into bam_featurecounts\n  script:\n  \"\"\"\n  bwa mem $genome $forwardTrimmed $reverseTrimmed | /tools/samtools-1.10/samtools sort -O BAM -o sample_${sampleNumber}_sorted.bam\n  /tools/samtools-1.10/samtools index sample_${sampleNumber}_sorted.bam sample_${sampleNumber}_sorted.bai\n  \"\"\"\n}"], "list_proc": ["j23414/compare_workflows/Eve", "jambler24/bac_pangenome/read_mapping"], "list_wf_names": ["jambler24/bac_pangenome", "j23414/compare_workflows"]}, {"nb_reuse": 1, "tools": ["EVE"], "nb_own": 1, "list_own": ["j23414"], "nb_wf": 1, "list_wf": ["desc_workflows"], "list_contrib": ["j23414"], "nb_contrib": 1, "codes": ["\nprocess Eve {\n  input: val baton_in from Dave_out\n    \n  output: stdout Eve_out\n\n  script:\n  \"\"\"\n  #! /usr/bin/env bash\n  sleep 5                    # <= pause for a few seconds\n  echo \"$baton_in; Eve passes baton\"\n  \"\"\"\n}"], "list_proc": ["j23414/desc_workflows/Eve"], "list_wf_names": ["j23414/desc_workflows"]}, {"nb_reuse": 1, "tools": ["IMPCdata"], "nb_own": 1, "list_own": ["j23414"], "nb_wf": 1, "list_wf": ["wgcna_nf"], "list_contrib": ["j23414"], "nb_contrib": 1, "codes": ["\nprocess plot_expression {\n  tag \"${in_RData.fileName}\"\n  input:\n  path in_RData\n                               \n                              \n\n  output:\n  path \"*\"\n\n  script:\n  \"\"\"\n  #! /usr/bin/env Rscript\n  library(magrittr)\n  library(ggplot2)\n\n  load(\\\"$in_RData\\\")\n  cdata <- data %>%\n    tidyr::pivot_longer(., cols = starts_with(\"F2\"))\n\n  p <- cdata %>% ggplot(., aes(x=name, y= value, group=substanceBXH)) +\n    geom_line(alpha=0.5) +\n    theme_bw() +\n    theme(\n      axis.text.x = element_text(angle=90, hjust=0.5)\n      )+\n    labs(\n      x=\"treatment\",\n      y=\"expression\"\n      )\n  ggsave(\"expression.png\", plot=p, height=3, width=9)\n  \"\"\"\n}"], "list_proc": ["j23414/wgcna_nf/plot_expression"], "list_wf_names": ["j23414/wgcna_nf"]}, {"nb_reuse": 1, "tools": ["OBCOL", "PowerShell"], "nb_own": 1, "list_own": ["j23414"], "nb_wf": 1, "list_wf": ["wgcna_nf"], "list_contrib": ["j23414"], "nb_contrib": 1, "codes": ["\nprocess pick_soft_threshold {\n  tag \"${in_RData.fileName}\"\n  input:\n  path in_RData\n\n  output:\n  path \"*\"\n\n  script:\n  \"\"\"\n  #! /usr/bin/env Rscript\n  library(WGCNA)\n  allowWGCNAThreads()\n\n  load('$in_RData')\n\n  powers = c(c(1:10), seq(from=12, to=20, by=2))\n  sft = pickSoftThreshold(\n    inputMatrix,\n    powerVector = powers,\n    verbose = 5\n    )\n\n  # Plot the results:\n  png(\"softthreshold.png\", width=600, height=300)\n  par(mfrow = c(1,2));\n  cex1 = 0.9;\n  plot(\n    sft\\$fitIndices[, 1],\n    -sign(sft\\$fitIndices[, 3]) * sft\\$fitIndices[, 2],\n    xlab = \"Soft Threshold (power)\",\n    ylab = \"Scale Free Topology Model Fit, signed R^2\",\n    main = paste(\"Scale independence\")\n  )\n  text(\n    sft\\$fitIndices[, 1],\n    -sign(sft\\$fitIndices[, 3]) * sft\\$fitIndices[, 2],\n    labels = powers,\n    cex = cex1,\n    col = \"red\"\n  )\n  abline(h = 0.90, col = \"red\")\n  plot(\n    sft\\$fitIndices[, 1],\n    sft\\$fitIndices[, 5],\n    xlab = \"Soft Threshold (power)\",\n    ylab = \"Mean Connectivity\", type = \"n\", main = paste(\"Mean connectivity\")\n  )\n  text(\n    sft\\$fitIndices[, 1],\n    sft\\$fitIndices[, 5],\n    labels = powers,\n    cex = cex1,\n    col = \"red\"\n    )\n  dev.off()\n  \"\"\"\n}"], "list_proc": ["j23414/wgcna_nf/pick_soft_threshold"], "list_wf_names": ["j23414/wgcna_nf"]}, {"nb_reuse": 1, "tools": ["CPModule"], "nb_own": 1, "list_own": ["j23414"], "nb_wf": 1, "list_wf": ["wgcna_nf"], "list_contrib": ["j23414"], "nb_contrib": 1, "codes": ["\nprocess wgcna_network {\n  tag \"${in_RData.fileName}\"\n  input:\n  path in_RData\n\n  output:\n  tuple path(\"*TOM*\"), path(\"*.png\"), path(\"*clusters.RData\")\n\n  script:\n  \"\"\"\n  #! /usr/bin/env Rscript\n  load('$in_RData')\n\n  library(WGCNA)\n  allowWGCNAThreads()\n\n  netwk = blockwiseModules(\n    inputMatrix,\n    power = $params.power,\n    TOMType = \"unsigned\",\n    minModuleSize = 30,\n    reassignThreshold = 0,\n    mergeCutHeight = 0.25,\n    numericLabels = TRUE,\n    pamRespectsDendro = FALSE,\n    saveTOMs = TRUE,\n    saveTOMFileBase = \"${in_RData.simpleName}TOM\",\n    verbose = 3)\n\n  mergedColors = labels2colors(netwk\\$colors)\n  png(\"wgcna_modules.png\", width=800, height=300)\n  plotDendroAndColors(\n    netwk\\$dendrograms[[1]],\n    mergedColors[netwk\\$blockGenes[[1]]],\n    \"Module colors\",\n    dendroLabels = FALSE, hang = 0.03,\n    addGuide = TRUE, guideHang = 0.05)\n  dev.off()\n\n  save(mergedColors, file = '${in_RData.simpleName}_clusters.RData')\n  \"\"\"\n}"], "list_proc": ["j23414/wgcna_nf/wgcna_network"], "list_wf_names": ["j23414/wgcna_nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard", "preseq", "MultiQC"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess get_software_versions {\n\n    output:\n      file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo \\$(bamtools --version 2>&1) > v_bamtools.txt\n    samtools --version > v_samtools.txt\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    preseq &> v_preseq.txt\n    multiqc --version > v_multiqc.txt\n    trim_galore --version > v_trim_galore.txt\n    picard MarkDuplicates --version &> v_picard.txt  || true\n    echo \\$(bwa 2>&1) > v_bwa.txt\n    #scrape_software_versions.py > software_versions_mqc.yaml\n    echo \"not yet\" > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/get_software_versions"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process convertGFFtoGTF {\n      tag \"$gff\"\n\n      input:\n        file gff from gffFile\n\n      output:\n        file \"${gff.baseName}.gtf\" into gtf_makeSTARindex, gtf_makeBED12, gtf_star, gtf_dupradar, gtf_featureCounts\n\n      script:\n      \"\"\"\n      gffread $gff -T -o ${gff.baseName}.gtf\n      \"\"\"\n  }"], "list_proc": ["jambler24/bac_pangenome/convertGFFtoGTF"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 2, "tools": ["STAR", "gffread"], "nb_own": 2, "list_own": ["vib-singlecell-nf", "jambler24"], "nb_wf": 2, "list_wf": ["vsn-pipelines", "bac_pangenome"], "list_contrib": ["dependabot[bot]", "jambler24", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 6, "codes": [" process convertGTFtoGFF {\n\n  input:\n    file gtf from gtfFile\n\n  output:\n\n    file \"${gtf.baseName}.gtf\" into gtf_makeSTARindex, gtf_makeBED12, gtf_star, gtf_dupradar, gtf_featureCounts\n    file \"${gtf.baseName}.gff\" into snpeff_gff_old_to_test_deleting\n\n  script:\n  \"\"\"\n  gffread $gtf -o ${gtf.baseName}.gff\n  \"\"\"\n\n  }", "\nprocess SC__STAR__BUILD_INDEX {\n\n    container params.tools.star.container\n    label 'compute_resources__star_build_genome'\n\n    input:\n        file(annotation)\n        file(genome)\n\n    output:\n        file(\"STAR_index\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.star.build_genome)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        mkdir STAR_index\n        STAR \\\n            --runThreadN ${task.cpus} \\\n            --runMode genomeGenerate \\\n            --genomeDir STAR_index \\\n            --genomeFastaFiles ${genome} \\\n            --sjdbGTFfile ${annotation} \\\n            --sjdbOverhang ${processParams.sjdbOverhang} \\\n            --genomeSAindexNbases ${processParams.genomeSAindexNbases} # Suggested by STAR (default: 14), otherwise keeps on hanging\n        \"\"\"\n\n}"], "list_proc": ["jambler24/bac_pangenome/convertGTFtoGFF", "vib-singlecell-nf/vsn-pipelines/SC__STAR__BUILD_INDEX"], "list_wf_names": ["jambler24/bac_pangenome", "vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess prepare_genome_samtools {\n  tag \"$genome.baseName\"\n\n  input:\n      file genome from genome_file\n\n  output:\n      file \"${genome}.fai\" into genome_index_ch\n\n  script:\n  \"\"\"\n  samtools faidx ${genome}\n  \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/prepare_genome_samtools"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess prepare_genome_picard {\n  tag \"$genome.baseName\"\n\n  input:\n      file genome from genome_file\n  output:\n      file \"${genome.baseName}.dict\" into genome_dict_ch\n\n  script:\n  \"\"\"\n  #picard -XX:ParallelGCThreads=5 -Xmx16G -Xms16G CreateSequenceDictionary R=$genome O=${genome.baseName}.dict\n  picard CreateSequenceDictionary R=$genome O=${genome.baseName}.dict\n  \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/prepare_genome_picard"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess prepare_genome_bwa {\n  tag \"$genome.baseName\"\n\n  input:\n      file genome from genome_file\n  output:\n      file \"${genome}.amb\" into genome_bwa_amb\n      file \"${genome}.ann\" into genome_bwa_ann\n      file \"${genome}.bwt\" into genome_bwa_bwt\n      file \"${genome}.pac\" into genome_bwa_pac\n      file \"${genome}.sa\" into genome_bwa_sa\n\n  script:\n  \"\"\"\n  bwa index $genome\n  \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/prepare_genome_bwa"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n        set number, file(R1), file(R2) from newSampleChannelFastQC\n\n    output:\n        file \"*_fastqc.{zip,html}\" into fastqc_results\n        file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $R1\n    fastqc -q $R2\n    \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/fastqc"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess mark_duplicates {\n  label 'high_memory'\n  tag \"${sample_bam.baseName}\"\n\n  input:\n    file sample_bam from bamfiles\n  output:\n    file \"${sample_bam.baseName}_dedup.bam\" into dedup_bamfiles\n    file \"${sample_bam.baseName}_dedup.bam\" into bam_md\n    file \"${sample_bam.baseName}_dedup.bam.bai\"\n    file \"${sample_bam.baseName}.txt\" into picard_results\n  script:\n    \"\"\"\n    picard MarkDuplicates INPUT=$sample_bam OUTPUT=${sample_bam.baseName}_dedup.bam METRICS_FILE=${sample_bam.baseName}.txt ASSUME_SORTED=true REMOVE_DUPLICATES=false\n    samtools index ${sample_bam.baseName}_dedup.bam\n    \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/mark_duplicates"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["Unicycler", "Bandage"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess unicycler {\n    label 'high_memory'\n    tag \"$sample_id\"\n    publishDir \"${params.outdir}/${sample_id}_assembly/unicycler\", mode: 'copy'\n\n    input:\n        set sample_id, file(fq1), file(fq2) from unicycler_read_pairs\n\n    output:\n        set sample_id, file(\"${sample_id}_assembly.fasta\") into (prokka_ch, quast_ch, dfast_ch)\n        file(\"${sample_id}_assembly.fasta\") into (ch_assembly_nanopolish_unicycler,ch_assembly_medaka_unicycler)\n        file(\"${sample_id}_assembly.gfa\")\n        file(\"${sample_id}_assembly.png\")\n        file(\"${sample_id}_unicycler.log\")\n\n    script:\n    \"\"\"\n    unicycler -1 $fq1 -2 $fq2 --threads ${task.cpus} --pilon_path /usr/local/bin/pilon-1.23.jar --keep 0 -o .\n    mv unicycler.log ${sample_id}_unicycler.log\n    # rename so that quast can use the name\n    mv assembly.gfa ${sample_id}_assembly.gfa\n    mv assembly.fasta ${sample_id}_assembly.fasta\n    Bandage image ${sample_id}_assembly.gfa ${sample_id}_assembly.png\n    \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/unicycler"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess prokka {\n    label 'high_memory'\n    tag \"$sample_id\"\n    publishDir \"${params.outdir}/${sample_id}_assembly/prokka\", mode: 'copy'\n\n    input:\n        set sample_id, file(fasta) from prokka_ch\n\n    output:\n        file(\"${sample_id}_annotation/sample_${sample_id}.gff\") into gff\n                                                                                                          \n                                                                                                  \n                       \n                \"${sample_id}_annotation/*txt\"                     \n\n   script:\n   \"\"\"\n   prokka --cpus ${task.cpus} --prefix sample_${sample_id} --outdir ${sample_id}_annotation ${fasta}\n   \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/prokka"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["Roary"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess roary {\n    publishDir \"${params.outdir}/roary\", mode: 'copy'\n\n    input:\n        file gff from gff.collect()\n\n    output:\n        file(\"*\") into roary\n        file(\"pan_genome_reference.fa\") into pan_genome\n        file(\"gene_presence_absence.csv\") into mGWAS_GPA\n\n    script:\n    \"\"\"\n    roary -e -n -v -r $gff\n    \"\"\"\n}"], "list_proc": ["jambler24/bac_pangenome/roary"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["Scoary"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bac_pangenome"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process scoary {\n\n    input:\n        file traits_file\n        file mGWAS_GPA\n\n    output:\n        file(\"*\") into scoary\n\n    script:\n    \"\"\"\n    scoary -g $mGWAS_GPA -t $traits_file\n    \"\"\"\n    }"], "list_proc": ["jambler24/bac_pangenome/scoary"], "list_wf_names": ["jambler24/bac_pangenome"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bacterial_transcriptomics"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process convertGFFtoGTF {\n      tag \"$gff\"\n\n      input:\n      file gff from gffFile\n\n      output:\n      file \"${gff.baseName}.gtf\" into gtf_makeSTARindex, gtf_makeBED12, gtf_star, gtf_dupradar, gtf_featureCounts\n      file \"${gff.baseName}.gff3\" into snpeff_gff\n\n      script:\n      \"\"\"\n      gffread $gff -T -o ${gff.baseName}.gtf\n      \"\"\"\n  }"], "list_proc": ["jambler24/bacterial_transcriptomics/convertGFFtoGTF"], "list_wf_names": ["jambler24/bacterial_transcriptomics"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["jambler24"], "nb_wf": 1, "list_wf": ["bacterial_transcriptomics"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process convertGTFtoGFF {\n\n  input:\n  file gtf from gtfFile\n\n  output:\n\n  file \"${gtf.baseName}.gtf\" into gtf_makeSTARindex, gtf_makeBED12, gtf_star\n  file \"${gtf.baseName}.gff\" into snpeff_gff\n  file \"${gtf.baseName}.gtf\" into gtf_featureCounts\n  file \"${gtf.baseName}.gtf\" into gtf_dupradar\n  script:\n  \"\"\"\n  gffread $gtf -o ${gtf.baseName}.gff\n  \"\"\"\n\n  }"], "list_proc": ["jambler24/bacterial_transcriptomics/convertGTFtoGFF"], "list_wf_names": ["jambler24/bacterial_transcriptomics"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n\tcontainer 'nfcore/rnaseq:1.4.2'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 50\n\tlabel 'small'\n\n\tinput:\n\ttuple val(sampleID), path(fastq1), path(fastq2)\n\n\toutput:\n    \tfile \"*_fastqc.{zip,html}\"\n\n\n\t\"\"\"\n        fastqc --quiet --threads 3 ${fastq1} ${fastq2}\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/fastqc"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess bwa_index {\n\tcontainer \"fredhutch/bwa:0.7.17\"\n\terrorStrategy 'retry'\n\tmaxRetries 30\n\tlabel 'small'\n\n\tinput:\n        file combined_reference\n\n     \toutput:\n\t    file \"*.{amb,ann,bwt,pac,sa,alt}\"\n\n        script:\n        \"\"\"\n        bwa index $combined_reference\n        \"\"\"\t\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/bwa_index"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess bwa_mem {\n\tcontainer \"fredhutch/bwa:0.7.17\"\n  \terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n  \tmaxRetries 50\n\tlabel 'medium'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patient), file(R1), file(R2)\n\tfile ref \n\tfile bwa_ind\n\n\toutput:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}.sam\")\n\n\t\"\"\"\n\tbwa mem -R '@RG\\\\tID:${params.run}\\\\tLB:hg38\\\\tPL:Illumina\\\\tPU:barcode\\\\tSM:${params.run}' -t 3  ${ref} ${R1} ${R2} > ${sampleID}.sam\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/bwa_mem"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess sam2fastq {\n\tcontainer 'broadinstitute/gatk:4.1.4.1'\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\tlabel 'small'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patient), file(sam_file)\n\n\toutput:\n        tuple val(\"$sampleID\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}_R1.fq.gz\"), file(\"${sampleID}_R2.fq.gz\")\n\n        \"\"\"\n\tgatk SamToFastq -I ${sam_file} \\\n\t\t\t-F ${sampleID}_R1.fq.gz\\\n\t\t\t-F2 ${sampleID}_R2.fq.gz \\\n\t\t\t-FU ${sampleID}_unpaired.fq.gz\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/sam2fastq"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 2, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["jamez-eh", "javaidm"], "nb_wf": 2, "list_wf": ["nf-fh-pcp-wes-align", "layer_lab_somatic_sv"], "list_contrib": ["jamez-eh", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess sam_to_bam {\n\tcontainer \"fredhutch/bwa:0.7.17-samtools-1.10\"\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\tlabel 'small'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patient), file(sam_file) \n\t\n\toutput:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}.bam\") \n\n\t\"\"\"\n\tsamtools view -bhS ${sam_file} > ${sampleID}.bam\n        \"\"\"\n}", "\nprocess DenoiseReadCounts {\n    echo true\n    tag \"$sample\"\n    \n    publishDir \"${OUT_DIR}/misc/denoised_read_counts\", mode: 'copy'\n    \n    input:\n    file(sample_counts_hdf5)\n    file(read_count_somatic_pon)\n\n    output:\n    file(std_copy_ratio)\n    file(denoised_copy_ratio)\n\n    script:\n    sample = sample_counts_hdf5.simpleName\n    std_copy_ratio = \"${sample}.standardizedCR.tsv\"\n    denoised_copy_ratio = \"${sample}.denoisedCR.tsv\"\n    \n    \"\"\"\n    \n    gatk DenoiseReadCounts \\\n        -I $sample_counts_hdf5 \\\n        --count-panel-of-normals $read_count_somatic_pon \\\n        --standardized-copy-ratios $std_copy_ratio \\\n        --denoised-copy-ratios $denoised_copy_ratio\n    \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/sam_to_bam", "javaidm/layer_lab_somatic_sv/DenoiseReadCounts"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align", "javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess bam_filtering {\n\tcontainer \"fredhutch/bwa:0.7.17-samtools-1.10\"\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\tlabel 'small'\n\n        input:\n        tuple val(sampleID), val(kitID), val(type), val(patient), file(bam_file) \n\n        output:\n        tuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}_q.bam\")\n\n        \"\"\"\n\tsamtools view -bhS -q 20 ${bam_file} > ${sampleID}_q.bam\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/bam_filtering"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess picard_clean {\n\tcontainer 'broadinstitute/gatk:4.1.4.1'\n\terrorStrategy 'ignore'\n\tlabel 'medium'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patient), file(bam_q_file)\n\n\toutput:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}_q_clean.bam\")\n\n\t\"\"\"\n\tgatk --java-options \"-Xmx30G\" CleanSam -I ${bam_q_file} -O ${sampleID}_q_clean.bam\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/picard_clean"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess sam_sort {\n\tcontainer \"fredhutch/bwa:0.7.17-samtools-1.10\"\n\terrorStrategy 'ignore'\n\tlabel 'medium'\n\n        input:\n        tuple val(sampleID), val(kitID), val(type), val(patient), file(bam_q_clean_file)\n\n        output:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}_q_clean_sorted.bam\") \n\n        \"\"\"\n\tsamtools sort ${bam_q_clean_file} -o ${sampleID}_q_clean_sorted.bam\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/sam_sort"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess picard_duplicates {\n\tcontainer 'broadinstitute/gatk:4.1.7.0'\t\t\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\tlabel 'large'\n\n\tinput:\n        tuple val(sampleID), val(kitID), val(type), val(patient), file(bam_q_clean_sorted_file)\n\n        output:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}_q_clean_sorted_rmdp.bam\")\n\n        \"\"\"\n\tmkdir temporary\n        gatk MarkDuplicates -I ${bam_q_clean_sorted_file} -O ${sampleID}_q_clean_sorted_rmdp.bam -METRICS_FILE {sampleID}_metrics.txt  -REMOVE_DUPLICATES true -VALIDATION_STRINGENCY STRICT -ASSUME_SORTED true -CREATE_INDEX true --SORTING_COLLECTION_SIZE_RATIO 0.20 --TMP_DIR temporary\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/picard_duplicates"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess gatk_baserecalibrator {\n\tcontainer 'broadinstitute/gatk:4.1.7.0'\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n\n        input:\n        tuple val(sampleID), val(kitID), val(type), val(patient), file(sorted_bam)\n        file ref\n        file rear\n\tfile reference_index\n\tfile reference_dict\n\tfile rear_index\n\n\n        output:\n        tuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"),  file(\"${sampleID}_recal_data.table\") \n\n        \"\"\"\n        gatk BaseRecalibrator -R ${ref} -known-sites ${rear} -I ${sorted_bam} -O ${sampleID}_recal_data.table --java-options -Xmx8g \n  \n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/gatk_baserecalibrator"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-align"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess gatk_printreads{\n        container 'broadinstitute/gatk:4.1.4.1'\n        publishDir \"$params.output_folder/${sampleID}\"\n\n\n        input:\n        tuple val(sampleID), val(kitID), val(type), val(patient), file(precal_data), file(bam_file)\n        file reference\n        file reference_index\n        file reference_dict\n\n        output:\n        tuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patient}\"), file(\"${sampleID}_bqsr.bam\")\n\n        \"\"\"\n\techo ${kitID}\n\techo ${type}\n\techo ${sampleID}\n\techo ${precal_data}\n\techo ${bam_file}\n        gatk --java-options \"-Xmx30G\" ApplyBQSR -R ${reference} -I ${bam_file} -bqsr-recal-file ${precal_data} -O ${sampleID}_bqsr.bam\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-align/gatk_printreads"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-align"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess DownloadData {\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n\n                output:\n                path funcotator_dataSource\n\n                \"\"\"\n                gatk FuncotatorDataSourceDownloader --somatic --validate-integrity --extract-after-download\n                rm funcotator_dataSource*gz\n                mv funcotator_dataSource* funcotator_dataSource\n                \"\"\"\n\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/DownloadData"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["SAMtools", "GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess haplotypecaller {\n\tcontainer 'broadinstitute/gatk:4.1.4.1'\nerrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        set val(sampleID), file(bqsr_bam)\n        file reference\n        file reference_index\n        file reference_dict\n\n\n        output:\n        tuple val(\"${sampleID}\"), file(\"${sampleID}_unfiltered_germ.vcf\")\n  \n        \"\"\"\n\tsamtools index ${bqsr_bam}\n        gatk HaplotypeCaller \\\n\t-R ${reference} \\\n\t-I ${bqsr_bam} \\\n\t-O ${sampleID}_unfiltered_germ.vcf\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/haplotypecaller"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess Funcotator {\n\tcontainer 'broadinstitute/gatk:4.1.5.0'\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n\tinput:\n\ttuple sampleID, file(variants)\n\tfile reference\n\tfile reference_index\n\tfile reference_dict\n\tpath data_source\n\n\toutput:\n\ttuple val(\"${sampleID}\"), file (\"${sampleID}_func.maf\")\n\n\t\"\"\"\n\tfor F in *.vcf.gz ; do   tabix -f -p vcf \\${F}  ; done\n\tgatk --java-options -Xmx20g Funcotator \\\n   \t-R ${reference} \\\n   \t-V ${variants} \\\n  \t-O ${sampleID}_func.maf \\\n   \t--output-file-format MAF \\\n   \t--data-sources-path ${data_source} \\\n   \t--ref-version hg38 \\\n\t--remove-filtered-variants true\n\n   \t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/Funcotator"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess samtoolsRemoveSecondary {\n        container \"fredhutch/bwa:0.7.17\"\n                                                                                          \n                        \n\tlabel 'small'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patientID), file(bam_file)\n\n\toutput:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), val(\"${patientID}\"), file(\"${sampleID}_primary.bam\")\n\n\n\t\"\"\"\n\tsamtools view -bh -f 0 -F 256 ${bam_file} > ${sampleID}_primary.bam\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/samtoolsRemoveSecondary"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess mutect2_normal_only {\n        container 'broadinstitute/gatk:4.1.7.0'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 10\n\tlabel 'medium'\n\t\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patientID), file(bam_file), file(bam_index)\n\tfile reference\n        file reference_dict\n        file reference_index\n\n\toutput:\n\ttuple val(\"${sampleID}\"), val(\"${kitID}\"), val(\"${type}\"), file(\"${sampleID}.vcf.gz\")\n\t\n\t\"\"\"\n\tgatk Mutect2 --java-options \"-Xmx30G\" \\\n\t-R ${reference} \\\n\t-I ${bam_file} \\\n\t-O ${sampleID}.vcf.gz\n\n\t\"\"\"\n\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/mutect2_normal_only"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess CreateSomaticPanelOfNormals {\n        container 'broadinstitute/gatk:4.1.7.0'\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\tlabel 'medium'\n\n\tinput:\n\ttuple val(kitID), path(db)\n        file reference\n        file reference_dict\n        file reference_index\n\n\toutput:\n\ttuple val(\"${kitID}\"), file(\"${kitID}.vcf.gz\"), file(\"${kitID}.vcf.gz.tbi\")\n\t\n\t\"\"\"\n\tgatk CreateSomaticPanelOfNormals -R ${reference} -V gendb://${db} -O ${kitID}.vcf.gz\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/CreateSomaticPanelOfNormals"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess\tmutect2_tumor_only {\n\tcontainer 'broadinstitute/gatk:4.1.7.0'\n\tlabel 'medium'\n\n\n\n\tinput:\n\ttuple val(kitID), path(normals), path(normal_index), val(sampleID), file(bam_file), file(bam_index)\t\n\tfile reference\n\tfile reference_dict\n\tfile reference_index\n\tfile common_variants\n\tfile common_variants_index\n\n\n\toutput:\n\ttuple val(\"${sampleID}\"), file(\"${sampleID}.vcf.gz\"),  file(\"${sampleID}.vcf.gz.tbi\"), file(\"${sampleID}.vcf.gz.stats\")\n\t\"\"\"\n  \tgatk  --java-options \"-Xmx30G\" Mutect2 \\\n  \t     -R ${reference} \\\n  \t     -I ${bam_file} \\\n\t     --panel-of-normals ${normals} \\\n\t     --germline-resource ${common_variants} \\\n\t     --min-base-quality-score ${params.min_base_quality_score} \\\n\t     --pcr-indel-model ${params.pcr_indel_model} \\\n\t     --callable-depth ${params.callable_depth} \\\n\t     --minimum-allele-fraction ${params.minimum_allele_fraction} \\\n\t     --base-quality-score-threshold ${params.base_quality_score_threshold} \\\n    \t     -O ${sampleID}.vcf.gz\n\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/mutect2_tumor_only"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["SAMtools", "GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess mutect2_matched_normal {\n        container 'broadinstitute/gatk:4.1.7.0'\n                                                                                  \n                \n\tlabel 'medium'\n\n        input:\n        tuple val(kitID), path(normals), path(normal_index), val(sampleID), file(bam_file), file(bam_index), val(normal_sampleID), file(normal_bam_file), file(normal_index)\n        file reference\n        file reference_dict\n        file reference_index\n        file common_variants\n        file common_variants_index\n\n\n        output:\n        tuple val(\"${sampleID}\"), file(\"${sampleID}.vcf.gz\"),  file(\"${sampleID}.vcf.gz.tbi\"), file(\"${sampleID}.vcf.gz.stats\")\n        \"\"\"\n\n\tgatk AddOrReplaceReadGroups \\\n       \t-I ${bam_file} \\\n       \t-O ${sampleID}.bam \\\n       \t--RGID none \\\n       \t--RGLB none \\\n       \t--RGPL none \\\n       \t--RGPU none \\\n       \t--RGSM ${sampleID}\n\n        gatk AddOrReplaceReadGroups \\\n        -I ${normal_bam_file} \\\n        -O ${normal_sampleID}.bam \\\n        --RGID none \\\n        --RGLB none \\\n        --RGPL none \\\n        --RGPU none \\\n        --RGSM ${normal_sampleID}\n\t\n\techo *\n       samtools index ${sampleID}.bam\n       samtools index ${normal_sampleID}.bam\n\n        gatk  --java-options \"-Xmx30G\" Mutect2 \\\n             -R ${reference} \\\n             -I ${sampleID}.bam \\\n\t     -I ${normal_sampleID}.bam \\\n\t     -normal ${normal_sampleID} \\\n             --panel-of-normals ${normals} \\\n             --germline-resource ${common_variants} \\\n             --min-base-quality-score ${params.min_base_quality_score} \\\n             --pcr-indel-model ${params.pcr_indel_model} \\\n             --callable-depth ${params.callable_depth} \\\n             --minimum-allele-fraction ${params.minimum_allele_fraction} \\\n             --base-quality-score-threshold ${params.base_quality_score_threshold} \\\n             -O ${sampleID}.vcf.gz\n\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/mutect2_matched_normal"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess FilterMutectCalls {\n        container 'broadinstitute/gatk:4.1.7.0'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n\tmaxRetries 100\n\tlabel 'medium'\n\n        input:\n        tuple val(sampleID), file(vcf),  file(vcf_index), file(stats)\n        file reference\n        file reference_dict\n        file reference_index\n\n        output:\n\ttuple val(\"${sampleID}\"), file(\"${sampleID}_filtered.vcf.gz\")\n\n\t\"\"\"\n\tgatk FilterMutectCalls \\\n\t     -R ${reference} \\\n\t     -V ${vcf} \\\n\t     -O ${sampleID}_filtered.vcf.gz\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/FilterMutectCalls"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess BedToIntervalList {\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        tuple val(kitID), file(capture_bed)\n        file reference\n        file reference_dict\n\n        output:\n\t\n        tuple val(\"${kitID}\"), file(\"${kitID}.interval_list\")\n\n        \"\"\"\n        gatk BedToIntervalList \\\n            -I ${capture_bed} \\\n\t    -O ${kitID}.interval_list \\\n\t    -SD ${reference}\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/BedToIntervalList"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess PreprocessIntervals {\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        tuple val(kitID), file(capture_intervals)\n        file reference\n\tfile reference_dict\n\tfile reference_index\n\n        output:\n        tuple val(\"${kitID}\"), file(\"${kitID}.interval_list\")\n\n        \"\"\"\n\tgatk PreprocessIntervals \\\n\t    -L ${capture_intervals} \\\n    \t    -R ${reference} \\\n    \t    --bin-length 0 \\\n    \t    --interval-merging-rule OVERLAPPING_ONLY \\\n    \t    -O ${kitID}_processed.interval_list\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/PreprocessIntervals"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess samtoolsIndex {\n\tcontainer \"fredhutch/bwa:0.7.17\"\t\n\tlabel 'small'\n\n\tinput:\n\ttuple val(sampleID), val(kitID), val(type), val(patientID),  file(bam)\t\n\t\n\toutput:\n\ttuple val(\"${kitID}\"), val(\"${patientID}\"), val(\"${type}\"), val(\"${sampleID}\"), file(\"${bam}\"), file(\"${bam}.bai\")\n\n\n\t\"\"\"\n\tsamtools index ${bam}\n\t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/samtoolsIndex"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess CollectReadCounts {\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        tuple val(patientID), val(type), val(sampleID), file(bam), file(bam_index), val(kitID), file(capture_intervals)\n        file reference\n                              \n                             \n\n        output:\n        tuple val(\"${sampleID}\"), val(\"${type}\"), val(\"${kitID}\"), val(\"${patientID}\"), file(\"${sampleID}.counts.hdf5\")\n\n        \"\"\"\n\tgatk CollectReadCounts \\\n    \t     -I ${bam} \\\n    \t     -L  ${capture_intervals} \\\n    \t     --interval-merging-rule OVERLAPPING_ONLY \\\n    \t     -O ${sampleID}.counts.hdf5\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/CollectReadCounts"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess DenoiseReadCounts{\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n\ttuple val(kitID), file(pon), val(sampleID), val(patientID), file(counts)\n\t\n        output:\n\ttuple val(\"${sampleID}\"), val(\"${patientID}\"), file(\"${sampleID}.standardizedCR.tsv\"), file(\"${sampleID}.denoisedCR.tsv\")\n\n\t\"\"\"\n\tgatk --java-options \"-Xmx12g\" DenoiseReadCounts \\\n\t    -I ${counts} \\\n    \t    --count-panel-of-normals ${pon} \\\n    \t    --standardized-copy-ratios ${sampleID}.standardizedCR.tsv  --denoised-copy-ratios ${sampleID}.denoisedCR.tsv\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/DenoiseReadCounts"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess CreateSequenceDictionary {\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        file reference\n\n\n        output:\n\tfile(\"${reference}.dict\")\n\n        \"\"\"\n\tgatk CreateSequenceDictionary -R ${reference} -O ${reference}.dict\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/CreateSequenceDictionary"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess PlotDenoisedCopyRatios{\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n\tinput:\n\ttuple val(sampleID), file(standard), file(denoised)\n\tfile dict\n\t\n\toutput:\n\ttuple val(\"${sampleID}\"), path(\"plots\")\n\n\n\t\"\"\"\n     mkdir plots\n     gatk PlotDenoisedCopyRatios \\\n          --standardized-copy-ratios ${standard} \\\n          --denoised-copy-ratios ${denoised} \\\n          --sequence-dictionary ${dict} \\\n          --minimum-contig-length 46709983 \\\n    \t  --output ./plots \\\n    \t  --output-prefix ${sampleID}\n\t\"\"\"\n\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/PlotDenoisedCopyRatios"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess CollectAllelicCounts {\n\tcontainer 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\t\n\t\n\tinput:\n        tuple val(patientID), val(type), val(sampleID), file(bam), file(bam_index), val(kitID), file(capture_intervals)\n\tfile reference\n\tfile reference_index\n\tfile reference_dict\n\n\toutput:\n\ttuple val(\"${patientID}\"), val(\"${type}\"), val(\"${kitID}\"), val(\"${sampleID}\"), file(\"${sampleID}.allelicCounts.tsv\")\n\n\t\"\"\"\n\n\tgatk CollectAllelicCounts \\\n          -I ${bam} \\\n          -R ${reference} \\\n          -L ${capture_intervals} \\\n          -O ${sampleID}.allelicCounts.tsv\n\n\t \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/CollectAllelicCounts"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess ModelSegments{\n\tlabel 'large'\n        container 'broadinstitute/gatk:4.1.4.1'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        tuple val(sampleID), file(standard), file(denoised), file(allelic)\n  \n        output:\n        tuple val(\"${sampleID}\"), path(\"${sampleID}_modelSeg\")\n\n\n        \"\"\"\n\tmkdir ${sampleID}_modelSeg\n\n\tgatk --java-options \"-Xmx30G\" ModelSegments \\\n          --denoised-copy-ratios ${denoised} \\\n          --output-prefix ${sampleID} \\\n\t  --allelic-counts ${allelic} \\\n          -O ${sampleID}_modelSeg\n\n        \"\"\"\n\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/ModelSegments"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess ModelSegmentsMatched {\n\tcontainer 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'large'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        tuple val(sampleID), file(standard), file(denoised), file(allelic), file(normal_allelic)\n\n\n        output:\n        tuple val(\"${sampleID}\"), path(\"${sampleID}_modelSeg\")\n\n\n        \"\"\"\n        mkdir ${sampleID}_modelSeg\n\n        gatk --java-options \"-Xmx42G\" ModelSegments \\\n          --denoised-copy-ratios ${denoised} \\\n          --output-prefix ${sampleID} \\\n\t  --normal-allelic-counts ${normal_allelic} \\\n          --allelic-counts ${allelic} \\\n          -O ${sampleID}_modelSeg\n\n        \"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/ModelSegmentsMatched"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess CallCopyRatioSegments {\n\tcontainer 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n\tinput:\n\ttuple val(sampleID), path(modelSeg)\n\n\toutput:\n\ttuple val(\"${sampleID}\"), file(\"${sampleID}.called.seg\")\n\t\n\t\"\"\"\n\techo ${modelSeg}/*\n\tgatk CallCopyRatioSegments \\\n          -I ${modelSeg}/${sampleID}.cr.seg \\\n          -O ${sampleID}.called.seg \\\n\t  --neutral-segment-copy-ratio-upper-bound 1.4 \\\n\t  --neutral-segment-copy-ratio-lower-bound 0.6\n\n \t\"\"\"\n\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/CallCopyRatioSegments"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess PlotModeledSegments {\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n        tuple val(sampleID), path(modelSeg), path(denoised)\n        path contig_dict\n\n        output:\n        tuple val(\"${sampleID}\"), path(\"${sampleID}_PlotModeledSegments\")\n\n        \"\"\"\n        gatk PlotModeledSegments \\\n          --denoised-copy-ratios ${denoised} \\\n          --allelic-counts ${modelSeg}/${sampleID}.hets.tsv \\\n          --segments ${modelSeg}/${sampleID}.modelFinal.seg \\\n          --sequence-dictionary ${contig_dict} \\\n          --output-prefix ${sampleID} \\\n          -O ${sampleID}_PlotModeledSegments\n\n        \"\"\"\n\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/PlotModeledSegments"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["jamez-eh"], "nb_wf": 1, "list_wf": ["nf-fh-pcp-wes-mutect2"], "list_contrib": ["jamez-eh"], "nb_contrib": 1, "codes": ["\nprocess FuncotateSegments{\n        container 'broadinstitute/gatk:4.1.4.1'\n\tlabel 'small'\n        errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n        maxRetries 100\n\n        input:\n\ttuple val(sampleID), path(modelSeg)\n        path reference\n        path reference_dict\n        path reference_index\n        path data_source\n\n        output:\n        tuple val(\"${sampleID}\"), path(\"${sampleID}_func.seg\")\n\n        \"\"\"\n\n        gatk FuncotateSegments \\\n        --data-sources-path ${data_source} \\\n        --ref-version hg38 \\\n        --output-file-format SEG \\\n        -R ${reference} \\\n        --segments ${modelSeg}/${sampleID}.cr.seg \\\n        -O ${sampleID}_func.seg\n\n     \t\"\"\"\n}"], "list_proc": ["jamez-eh/nf-fh-pcp-wes-mutect2/FuncotateSegments"], "list_wf_names": ["jamez-eh/nf-fh-pcp-wes-mutect2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_dna_seq_vc"], "list_contrib": ["ryanlayer", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess VariantEval{\n    echo true\n    publishDir \"${OUT_DIR}/variant_eval/\", mode: 'copy', overwrite: false\n\n    input:\n    file(cohort_vcf)\n    file(vcf_index)\n\n    output:\n    file \"$out_file\"\n\n    script:\n    out_file = \"cohort.eval.grp\"    \n    script:\n    \"\"\" \n    gatk VariantEval --eval $cohort_vcf --comp $DBSNP -R $REF_FASTA --output $out_file\n    \"\"\"  \n}"], "list_proc": ["javaidm/layer_lab_dna_seq_vc/VariantEval"], "list_wf_names": ["javaidm/layer_lab_dna_seq_vc"]}, {"nb_reuse": 2, "tools": ["Bowtie", "MultiQC"], "nb_own": 2, "list_own": ["jermth", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_dna_seq_vc", "hls-workshop"], "list_contrib": ["ryanlayer", "jermth", "javaidm"], "nb_contrib": 3, "codes": ["\nprocess RunMultiQC {\n    publishDir \"${OUT_DIR}/multiqc\", mode: 'copy', overwrite: false\n\n    input:\n    file (fastqc:'fastqc/*')\n    file ('gatk_base_recalibration/*')\n    file ('gatk_variant_eval/*')\n    \n    output:\n    file '*multiqc_report.html'\n    file '*_data'\n    file '.command.err'\n    val prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = CUSTOM_RUN_NAME ? \"--title \\\"$CUSTOM_RUN_NAME\\\"\" : ''\n    rfilename = CUSTOM_RUN_NAME ? \"--filename \" + CUSTOM_RUN_NAME.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename  . 2>&1\n    \n    \"\"\"\n}", "\nprocess runBowtie2 {\n    \n    publishDir params.outdir, mode:'copy'\n\n    input:\n    file bowtie2_index from bowtie2_index_location\n    set pair_id, file(read1), file(read2) from read_pairs_bowtie2_ch\n     \n    output:\n    file (\"${pair_id}.${params.prefix}.sam\") into bowtie2_sam_output\n       \n    \"\"\"\n    bowtie2 -t -x ${bowtie2_index_location} -p ${task.cpus} -U ${read1},${read2} -u ${number_of_reads} -S ${pair_id}.${params.prefix}.sam\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_dna_seq_vc/RunMultiQC", "jermth/hls-workshop/runBowtie2"], "list_wf_names": ["jermth/hls-workshop", "javaidm/layer_lab_dna_seq_vc"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA", "SAMBLASTER"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess MapReads {\n    echo true\n    tag \"$sample\"\n    publishDir \"${OUT_DIR}/align/crams/\" , mode: 'copy', overwrite: false\n    input:\n    tuple val(sample), file(reads)\n\n    output:\n    file(out_file)\n    file(\"${out_file}.crai\")\n\n    script:\n    r1 = reads[0]\n    r2 = reads[1]\n    \n    script:\n    out_file = \"${sample}.cram\"\n   \"\"\"\n   bwa mem \\\n    -t $_THREADS -R \"@RG\\tID:$sample\\tSM:$sample\\tPL:$_PLATFORM\\tPU:$sample\\tLB:$sample\" $ref_fasta $r1 $r2 \\\n    | samblaster \\\n    | samtools sort --output-fmt-option seqs_per_slice=4000 -O CRAM --reference $ref_fasta -m 18G -@ 6 /dev/stdin -o $out_file \\\n    && samtools index $out_file\n    \n   \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/MapReads"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess CramToBam{\n    echo true\n    tag \"$sample\"\n    publishDir \"${OUT_DIR}/align/bams\" , mode: 'copy', overwrite: false\n    input:\n    file(cram)\n\n    output:\n    file(out_file)\n    file(\"${out_file}.bai\")\n\n    script:\n    sample = \"${cram.simpleName}\"\n    out_file = \"${sample}.bam\"\n    \n    script:\n   \"\"\"\n   samtools view -T $ref_fasta -b -o $out_file $cram \\\n   && samtools index $out_file\n   \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/CramToBam"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess PreprocessIntervals {\n    publishDir \"${OUT_DIR}/misc/preprocessed_intervals\", mode: 'copy'\n                  \n    input:\n                           \n    \n    output:\n    file(out_file)\n    script:\n    out_file = 'preprocessed.interval_list'\n    \"\"\"\n    gatk PreprocessIntervals \\\n        -L $intervals_list \\\n        -R $ref_fasta \\\n        --bin-length 0 \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O $out_file\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/PreprocessIntervals"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess CollectReadCounts {\n    echo true\n    tag \"$sample\"\n                  \n    publishDir \"${OUT_DIR}/misc/read_counts\", mode: 'copy'\n\n    input:\n    file(cram)\n    file(cram_index)\n    file(preprocessed_intervals)\n\n    output:\n    file(out_file)\n\n    script:\n                                \n                                      \n    sample = cram.simpleName\n    out_file = \"${sample}.counts.hdf5\"\n    \n    \n    \"\"\"\n    gatk CollectReadCounts \\\n        -I $cram \\\n        -R $ref_fasta \\\n        -L $preprocessed_intervals \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O $out_file\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/CollectReadCounts"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess CreateReadCountSomaticPON {\n    echo true\n                    \n    \n    publishDir \"${OUT_DIR}/misc/read_count_somatic_pon\", mode: 'copy'\n    \n    input:\n                              'all_read_counts/*' \n    file(read_count_hdf5s)\n    \n                      \n\n    output:\n    file(out_file)\n\n    script:\n                                    \n    out_file = \"cnv.pon.hdf5\"\n    params_str = ''\n\n                                  \n    read_count_hdf5s.each{\n        sample = it.simpleName\n                                                                \n        if (LLabUtils.sampleInList(sample, list_of_controls)){\n          params_str = \"${params_str} -I ${it}\"\n        }\n    }\n\n    \n    \"\"\"\n    gatk CreateReadCountPanelOfNormals \\\n        $params_str \\\n        -O $out_file\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/CreateReadCountSomaticPON"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess PlotDenoisedCopyRatios {\n    echo true\n    tag \"$sample\"\n    \n    publishDir \"${OUT_DIR}/misc/denoisedCR_plots\", mode: 'copy'\n    \n    input:\n    file(std_copy_ratio)\n    file(denoised_copy_ratio)\n\n    output:\n    file(out_dir)\n\n    script:\n    sample = std_copy_ratio.simpleName\n    out_dir = \"${sample}\"\n    \n    \"\"\"\n    mkdir $out_dir\n    gatk PlotDenoisedCopyRatios \\\n        --standardized-copy-ratios $std_copy_ratio \\\n        --denoised-copy-ratios $denoised_copy_ratio \\\n        --sequence-dictionary $ref_dict \\\n        --output-prefix $sample \\\n        -O $out_dir\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/PlotDenoisedCopyRatios"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess RunModelSegments {\n    echo true\n    tag \"$sample\"\n    \n    publishDir \"${OUT_DIR}/misc/modeled_segments\", mode: 'copy', overwrite: true\n    \n    input:\n    file(case_denoisedCR)\n\n    output:\n    file(out_dir)\n    file(\"${out_dir}/${sample}.cr.seg\")\n    file(\"${out_dir}/${sample}.modelFinal.seg\")\n\n    script:\n    sample = case_denoisedCR.simpleName\n    out_dir = \"${sample}_modeled_segments\"\n    \"\"\"\n    mkdir $out_dir\n    gatk ModelSegments \\\n        --denoised-copy-ratios $case_denoisedCR \\\n        --output-prefix $sample \\\n        -O $out_dir\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/RunModelSegments"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess PlotModeledSegments {\n    echo true\n    tag \"$sample\"\n    \n    publishDir \"${OUT_DIR}/misc/modeled_segments_plots\", mode: 'copy', overwrite: true\n    \n    input:\n    file(\"denoised_crs/*\")\n    file(model_final_seg)\n\n    output:\n    file(out_dir)\n\n    script:\n    sample = model_final_seg.simpleName\n    sample_denoised_cr = \"denoised_crs/${sample}.denoisedCR.tsv\"\n    out_dir = \"${sample}\"\n    \n    \"\"\"\n    mkdir $out_dir\n    gatk PlotModeledSegments \\\n        --denoised-copy-ratios $sample_denoised_cr \\\n        --segments $model_final_seg \\\n        --sequence-dictionary $ref_dict \\\n        --output-prefix $sample \\\n        -O $out_dir\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/PlotModeledSegments"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_somatic_sv"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess CallCopyRatioSegments {\n    echo true\n    tag \"$sample\"\n    \n    publishDir \"$publish_dir\", mode: 'copy', overwrite: true\n    \n    input:\n    file(cr_seg)\n    \n    output:\n    file(out_file)\n                        \n\n    script:\n    sample = cr_seg.simpleName\n    publish_dir = \"${OUT_DIR}/misc/called_cr_segments\"\n    out_file = \"${sample}.called.seg\"\n    \n    \"\"\"\n    gatk CallCopyRatioSegments \\\n        -I $cr_seg \\\n        -O $out_file\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_somatic_sv/CallCopyRatioSegments"], "list_wf_names": ["javaidm/layer_lab_somatic_sv"]}, {"nb_reuse": 3, "tools": ["BCFtools", "Arriba", "BWA", "SAMtools", "FreeBayes", "MultiQC", "TIDDIT", "FastQC", "STAR", "QualiMap", "GATK", "VCFtools"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["process arriba {\n\n\ttag { \"Arriba - ${filename}\" } \n    publishDir \"${outdir}/${group}/${filename}/Arriba\", mode: 'copy'\n    label 'process_arriba'\n\n    input:\n\ttuple val(filename), val(group), val(sample), val(path), file(reads)\n\tval assembly\n\tval gtf\n\tval blacklist\n\tval knownfus\n\tval proteindom\n\tval staridx\n\tval outdir\n    \t \n    output:\n\tfile \"${filename}.arriba.fusions.tsv\"\n\tfile \"${filename}.arriba.fusions.discarded.tsv\"\n\n    script:\n    \"\"\"\n\tSTAR \\\n\t\t--genomeDir ${staridx} \\\n\t\t--genomeLoad NoSharedMemory \\\n\t\t--readFilesIn ${reads} \\\n\t\t--runThreadN ${task.cpus} \\\n\t\t--readFilesCommand zcat \\\n\t\t--outStd BAM_Unsorted \\\n\t\t--outSAMtype BAM Unsorted \\\n\t\t--outSAMunmapped Within \\\n\t\t--outBAMcompression 0 \\\n\t\t--outFilterMultimapNmax 50 \\\n\t\t--peOverlapNbasesMin 10 \\\n\t\t--alignSplicedMateMapLminOverLmate 0.5 \\\n\t\t--alignSJstitchMismatchNmax 5 -1 5 5 \\\n\t\t--chimSegmentMin 10 \\\n\t\t--chimOutType WithinBAM HardClip \\\n\t\t--chimJunctionOverhangMin 10 \\\n\t\t--chimScoreDropMax 30 \\\n\t\t--chimScoreJunctionNonGTAG 0 \\\n\t\t--chimScoreSeparation 1 \\\n\t\t--chimSegmentReadGapMax 3 \\\n\t\t--chimMultimapNmax 50 |\n\tarriba \\\n    \t-x /dev/stdin \\\n\t    -o ${filename}.arriba.fusions.tsv \\\n\t    -O ${filename}.arriba.fusions.discarded.tsv \\\n\t\t-a ${assembly} \\\n\t\t-g ${gtf} \\\n\t\t-b ${blacklist} \\\n\t\t-k ${knownfus} \\\n\t\t-p ${proteindom}\n    \"\"\"\n}", "\nprocess PreprocessIntervals {\n    label 'container_llab'\n    label 'cpus_8'\n    \n    input:    \n        file(intervalBed)\n        file(fasta)\n        file(fasta_fai)\n        file(dict)\n    \n    output:\n                                                                                    \n        file(\"preprocessed_intervals.interval_list\")\n\n                                                                             \n    when: ('gatk_cnv_somatic' in tools)\n    \n    script:\n    intervals_options = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    padding_options =  params.no_intervals ? \"--padding 0\" : \"--padding 250\"\n    bin_options =  params.no_intervals ? \"--bin-length 1000\" : \"--bin-length 0\"\n\n    \"\"\"\n    init.sh\n    gatk PreprocessIntervals \\\n        ${intervals_options} \\\n        ${padding_options} \\\n        ${bin_options} \\\n        -R ${fasta} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O preprocessed_intervals.interval_list\n    \"\"\"\n}", "\nprocess GetSoftwareVersions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode\n\n    output:\n                                                                       \n        file 'software_versions_mqc.yaml'\n\n    when: !('versions' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    bcftools version > v_bcftools.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    configManta.py --version > v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version > v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    echo \"SNPEFF version\"\\$(snpEff -h 2>&1) > v_snpeff.txt\n    fastqc --version > v_fastqc.txt 2>&1 || true\n    freebayes --version > v_freebayes.txt 2>&1 || true\n    gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt  || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/arriba", "ryanlayerlab/layer_lab_caw/PreprocessIntervals", "javaidm/layer_lab_vc/GetSoftwareVersions"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "sagc-bioinformatics/modules"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FilterMutect2TNCalls {\n    label 'container_llab'\n    label 'cpus_1'\n\n    tag {idSampleTN}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTN}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                \n                                                                              \n                                                  \n                                                        \n        tuple idPatient, \n            idSampleTN, \n            file(unfiltered), file(unfilteredIndex),\n            file(\"${idSampleTN}.vcf.gz.stats\")\n                                                        \n        \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n                                            \n        \n    output:\n        tuple val(\"Mutect2\"), idPatient, idSampleTN,\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz\"),\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz.tbi\"),\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz.filteringStats.tsv\")\n\n                                             \n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --stats ${idSampleTN}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O filtered_mutect2_${idSampleTN}.vcf.gz\n    \"\"\"\n}", "\nprocess BuildFastaGzFai {\n    tag \"${fasta}.gz.fai\"\n                                              \n\n    input:\n    file(fasta)\n    file(fastagz)\n\n    output:\n    file(\"${fasta}.gz.fai\")\n    when: !(params.fasta_gz_fai)\n    script:\n    \"\"\"\n    init.sh\n    samtools faidx $fastagz\n    \"\"\"\n  }", "\nprocess FilterIntervals {\n    label 'container_llab'\n    label 'cpus_16'\n    echo true\n    publishDir \"${params.outdir}/Preprocessing/gatk_gcnv/\", mode: params.publish_dir_mode\n    input:\n        file(preprocessed_intervals)\n        file(annotated_intervals)\n        file(\"cvg/*\")\n\n    output:\n        file(\"cohort.gc.filtered.interval_list\")\n        \n\n    when: ('gatk_gcnv_cohort_mode' in tools)\n\n    script:\n    \"\"\"\n    init.sh\n    #cvg_opts=''\n    for x in `ls cvg/*.tsv`\n    do\n        echo -n \"-I \\$x \" >> args.list\n    done\n    \n    #echo \"my cvg_opts: \\$cvg_opts\"\n    \n    gatk FilterIntervals \\\n        -L ${preprocessed_intervals} \\\n        --annotated-intervals ${annotated_intervals} \\\n        --arguments_file args.list \\\n        -imr OVERLAPPING_ONLY \\\n        -O cohort.gc.filtered.interval_list\n\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/FilterMutect2TNCalls", "javaidm/layer_lab_vc/BuildFastaGzFai", "ryanlayerlab/layer_lab_chco/FilterIntervals"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 5, "tools": ["BWA", "FREEC", "FastQC", "GATK", "VCFtools"], "nb_own": 4, "list_own": ["nibscbioinformatics", "lifebit-ai", "ryanlayerlab", "javaidm"], "nb_wf": 4, "list_wf": ["GenomeChronicler-Sarek-nf", "layer_lab_vc", "layer_lab_chco", "nf-core-bagobugs", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "MGordon09", "javaidm", "cgpu"], "nb_contrib": 4, "codes": ["\nprocess DetermineGermlineContigPloidyCaseMode {\n                             \n    label 'container_gatk'\n    label 'cpus_32'\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/GATK_gcnv/\", mode: params.publish_dir_mode\n    input:\n        tuple idPatient, idSample, file(sample_cvg)\n        file(ploidy_model)\n\n    output:\n        tuple idPatient, idSample, file(\"ploidy-case-calls\")\n\n    when: ( 'gatk_gcnv' in tools )\n\n    script:\n     \n    \"\"\"\n    init.sh\n    gatk DetermineGermlineContigPloidy \\\n    --model ${ploidy_model} \\\n        -I ${sample_cvg} \\\n        -O . \\\n        --output-prefix ploidy-case \\\n        --verbosity DEBUG\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\")\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.*\")\n\n    when: !(params.bwa_index) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    init.sh\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess BaseRecalibrator {\n    label 'container_llab'\n                     \n    label 'cpus_8'\n                  \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                       \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta) \n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex) \n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.table\")\n                                                          \n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                            \n                                         \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_2'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/controlFREEC\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chrDir\n        file(chrLength) from ch_chrLength\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup.gz_CNVs\"), file(\"${idSampleTumor}.pileup.gz_ratio.txt\"), file(\"${idSampleTumor}.pileup.gz_normal_CNVs\"), file(\"${idSampleTumor}.pileup.gz_normal_ratio.txt\"), file(\"${idSampleTumor}.pileup.gz_BAF.txt\"), file(\"${idSampleNormal}.pileup.gz_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup.gz*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"coefficientOfVariation = 0.05\" >> ${config}\n    echo \"contaminationAdjustment = TRUE\" >> ${config}\n    echo \"forceGCcontentNormalization = 0\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = 20\" >> ${config}\n    echo \"ploidy = 2,3,4\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"window = 50000\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/DetermineGermlineContigPloidyCaseMode", "javaidm/layer_lab_vc/Vcftools", "javaidm/layer_lab_vc/BuildBWAindexes", "ryanlayerlab/layer_lab_caw/BaseRecalibrator", "lifebit-ai/GenomeChronicler-Sarek-nf/ControlFREEC"], "list_wf_names": ["ryanlayerlab/layer_lab_chco", "lifebit-ai/GenomeChronicler-Sarek-nf", "ryanlayerlab/layer_lab_caw", "javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", \n    mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), \n        file(\"${idSample}_${idRun}_R2.fastq.gz\")\n\n    output:\n        file(\"*.{html,zip}\")\n\n    when: !('fastqc' in skipQC) && (step == 'mapping')\n                                                                                                         \n    \n    script:\n    \"\"\"\n    init.sh\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/FastQCFQ"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 3, "tools": ["SAMtools", "BEDTools", "BWA", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MapReads {\n    label 'cpus_max'\n\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(inputFile1), file(inputFile2)\n        file(fasta) \n        file(fastaFai)\n        file(bwaIndex) \n\n    output:\n                                                                             \n                                                                                         \n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\"), emit : bam_mapped\n        tuple idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\"), emit : bam_mapped_BamQC\n    \n                                                                                \n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n        init.sh\n        ${convertToFastq}\n        bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess PreprocessIntervals {\n    label 'container_llab'\n    label 'cpus_8'\n    \n    input:    \n        file(intervalBed)\n        file(fasta)\n        file(fasta_fai)\n        file(dict)\n    \n    output:\n                                                                                    \n        file(\"preprocessed_intervals.interval_list\")\n\n                                                                             \n    when: ('gatk_cnv_somatic' in tools ||\n           'gatk_cnv_germline_cohort_mode' in tools ||\n           'gatk_cnv_germline' in tools\n           )\n    \n    script:\n    intervals_options = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    padding_options =  params.no_intervals ? \"--padding 0\" : \"--padding 250\"\n    bin_options =  params.no_intervals ? \"--bin-length 1000\" : \"--bin-length 0\"\n\n    \"\"\"\n    init.sh\n    gatk PreprocessIntervals \\\n        ${intervals_options} \\\n        ${padding_options} \\\n        ${bin_options} \\\n        -R ${fasta} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O preprocessed_intervals.interval_list\n    \"\"\"\n}", "process manta_to_bed{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n                                                                                                                 \n\n    input:\n    tuple caller, idPatient, idSample, file(vcfgz), file(vcfgztbi)\n    file(exon_file)\n\n    output:\n    tuple val('Manta'), idPatient, idSample, file(\"${idSample}.bed\")\n\n    when: ! ('chco_qc' in _skip_qc)  && params.bait_bed\n\n    script:\n    \"\"\"\n    bedtools intersect -wb -b Manta_${idSample}.tumorSV.vcf.gz -a $exon_file > temp.tsv\n    manta_to_bed.py $idSample ${idSample}.bed temp.tsv\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/MapReads", "ryanlayerlab/layer_lab_chco/PreprocessIntervals", "ryanlayerlab/layer_lab_chco/manta_to_bed"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 4, "tools": ["SAMtools", "Sambamba", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CombineGVCFs {\n    label 'container_llab'\n    label 'cpus_32'\n    publishDir \"${params.outdir}/VariantCalling/CombinedGVCF\", mode: params.publish_dir_mode\n    input:\n                                                                                                             \n        file(gvcfs)\n        file(tbis)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n\n    output:\n       tuple file('cohort.g.vcf.gz'), file('cohort.g.vcf.gz.tbi')\n                                     \n    \n    when: 'joint_genotype' in tools\n\n    script:\n    vcfs_str = ''\n    gvcfs.each{vcfs_str += \"--variant ${it} \"}\n\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        CombineGVCFs \\\n        -R ${fasta} \\\n        ${vcfs_str} \\\n        --create-output-variant-index \\\n        -O cohort.g.vcf.gz\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    echo true\n    label 'container_llab'\n    label 'cpus_16'\n    tag {intervalBed.baseName}\n    input:\n                                                                                                             \n                                                                                           \n        tuple file(cohort_gvcf), file(tbi), file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n    file(\"${out_file_bn}.vcf\") \n    file(\"${out_file_bn}.vcf.idx\")\n\n    when: 'joint_genotype' in tools\n\n    script:\n    out_file_bn = intervalBed.baseName \n                                                                                   \n    \"\"\"\n    init.sh\n    #echo \"cohort_gvcf: ${cohort_gvcf}\"\n    #echo \"tbi: ${tbi}\"\n    #echo \"intervalBed: ${intervalBed}\"\n    #echo \"fasta: ${fasta}\"\n    #echo \"fastaFai: ${fastaFai}\"\n    #echo \"dict: ${dict}\"\n    #echo \"dbsnp: ${dbsnp}\"\n    #echo \"dbsnpIndex: ${dbsnpIndex}\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${cohort_gvcf} \\\n        --create-output-variant-index \\\n        -O \"${out_file_bn}.vcf\"\n    \"\"\"\n}", "\nprocess FilterBamRead1 {\n    label 'cpus_32'\n\n    tag {idPatient + \"-\" + idSample + \"_\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\")\n        \n    output:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_filtered_r1.bam\"), emit: filtered_reads\n\n    when: params.filter_bams\n    script:\n    if( params.remove_supplementary_reads)\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(first_of_pair and mapping_quality >=${params.bam_mapping_q} \\\n                and not ([XA] != null or [SA] != null)) \\\n                or second_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r1.bam\"\n        \"\"\"\n\n    else\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(first_of_pair and mapping_quality >=${params.bam_mapping_q}) \\\n                or second_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r1.bam\"\n        \"\"\"\n}", "\nprocess FilterMutect2TNCalls {\n    label 'cpus_1'\n\n    tag {idSampleTN}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTN}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                \n                                                                              \n                                                  \n                                                        \n        tuple idPatient, \n            idSampleTN, \n            file(unfiltered), file(unfilteredIndex),\n            file(\"${idSampleTN}.vcf.gz.stats\")\n                                                        \n        \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n                                            \n        \n    output:\n        tuple val(\"Mutect2\"), idPatient, idSampleTN,\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz\"),\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz.tbi\"),\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz.filteringStats.tsv\")\n\n                                             \n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --stats ${idSampleTN}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O filtered_mutect2_${idSampleTN}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CombineGVCFs", "ryanlayerlab/layer_lab_chco/GenotypeGVCFs", "javaidm/layer_lab_vc/FilterBamRead1", "javaidm/layer_lab_vc/FilterMutect2TNCalls"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Sambamba", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess GenotypeGVCFs {\n    echo true\n    label 'container_llab'\n    label 'cpus_16'\n    tag {intervalBed.baseName}\n    input:\n                                                                                                             \n                                                                                           \n        tuple file(cohort_gvcf), file(tbi), file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n    file(\"${out_file_bn}.vcf\") \n    file(\"${out_file_bn}.vcf.idx\")\n\n    when: 'joint_genotype' in tools\n\n    script:\n    out_file_bn = intervalBed.baseName \n                                                                                   \n    \"\"\"\n    init.sh\n    #echo \"cohort_gvcf: ${cohort_gvcf}\"\n    #echo \"tbi: ${tbi}\"\n    #echo \"intervalBed: ${intervalBed}\"\n    #echo \"fasta: ${fasta}\"\n    #echo \"fastaFai: ${fastaFai}\"\n    #echo \"dict: ${dict}\"\n    #echo \"dbsnp: ${dbsnp}\"\n    #echo \"dbsnpIndex: ${dbsnpIndex}\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${cohort_gvcf} \\\n        --create-output-variant-index \\\n        -O \"${out_file_bn}.vcf\"\n    \"\"\"\n}", "\nprocess SelectVariants {\n                \n    label 'container_llab'\n    label 'cpus_4'\n    tag {idSample}\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}/HC_jointly_genotyped_vcf\", mode: params.publish_dir_mode\n    input:\n                                                                                                                                 \n        tuple file (cohort_vcf), file (tbi), idSample\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n\n    output:\n                                                                                                                                                         \n    \n    tuple val(\"HaplotypeCaller_Jointly_Genotyped\"), val('patient id placeholder') ,idSample, file(\"${idSample}.vcf.gz\"), file(\"${idSample}.vcf.gz.tbi\")\n    tuple file(\"${idSample}.vcf.gz\"), file(\"${idSample}.vcf.gz.tbi\")\n    when: ('joint_genotype' in tools )\n\n    script:\n                       \n                                              \n                                                                                   \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n            SelectVariants \\\n            -R ${fasta} \\\n            -V ${cohort_vcf} \\\n            -O ${idSample}.vcf.gz \\\n            -sn ${idSample} \\\n            --create-output-variant-index\n    \n    \"\"\"\n}", "\nprocess FilterBamRead2 {\n    label 'cpus_32'\n\n    tag {idPatient + \"-\" + idSample + \"_\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\")\n        \n    output:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_filtered_r2.bam\"), emit: filtered_reads\n\n    when: params.filter_bams\n    script:\n    if( params.remove_supplementary_reads)\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(second_of_pair and mapping_quality >=${params.bam_mapping_q} \\\n                and not ([XA] != null or [SA] != null)) \\\n                or first_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r2.bam\"\n        \"\"\"\n    else\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(second_of_pair and mapping_quality >=${params.bam_mapping_q}) \\\n                or first_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r2.bam\"\n        \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/GenotypeGVCFs", "ryanlayerlab/layer_lab_chco/SelectVariants", "javaidm/layer_lab_vc/FilterBamRead2"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess count_reads {\n    label 'container_llab'\n    label 'cpus_1'\n    publishDir \"${params.outdir}/CNV_Plotting/${idSample}/ReadCounts\", mode: params.publish_dir_mode \n    \n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n \n    output:\n        tuple idPatient, idSample, file(\"${idSample}.num_reads.txt\")\n    \n    script:\n    \"\"\"\n    samtools view -c -F 260 $bam > ${idSample}.num_reads.txt\n    \"\"\"\n\n}", "\nprocess MergeFilteredBamReads {\n    label 'cpus_32'\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, file(partial_filtered_bams)\n\n    output:\n        tuple idPatient, idSample, idRun,  file(out_bam), file(\"${out_bam}.bai\"), emit: filtered_bam\n\n    when: (params.filter_bams)\n\n    script:\n     out_bam = \"${idSample}_${idRun}_filtered.bam\"\n                                  \n                            \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} -n -c -p merged.bam ${partial_filtered_bams}\n    samtools sort --threads ${task.cpus} merged.bam -o ${idSample}_${idRun}_filtered.bam\n    #samtools index  --threads ${task.cpus} ${idSample}_${idRun}_filtered.bam\n    samtools index   ${idSample}_${idRun}_filtered.bam\n    # cleaning\n    rm -f merged.bam\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/count_reads", "javaidm/layer_lab_vc/MergeFilteredBamReads"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess DetermineGermlineContigPloidyCaseMode {\n                             \n    label 'container_gatk'\n    label 'cpus_32'\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/GATK_gcnv/\", mode: params.publish_dir_mode\n    input:\n        tuple idPatient, idSample, file(sample_cvg)\n        file(ploidy_model)\n\n    output:\n        tuple idPatient, idSample, file(\"ploidy-case-calls\")\n\n    when: ( 'gatk_gcnv' in tools )\n\n    script:\n     \n    \"\"\"\n    init.sh\n    gatk DetermineGermlineContigPloidy \\\n    --model ${ploidy_model} \\\n        -I ${sample_cvg} \\\n        -O . \\\n        --output-prefix ploidy-case \\\n        --verbosity DEBUG\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_16'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, out_suffix, file(bams)\n                                                                    \n\n    output:\n        tuple idPatient, idSample,  file(\"${idSample}${out_suffix}.bam\")\n\n    script:\n                                                          \n                                        \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} \"${idSample}${out_suffix}.bam\" ${bams}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/DetermineGermlineContigPloidyCaseMode", "ryanlayerlab/layer_lab_chco/HaplotypeCaller", "javaidm/layer_lab_vc/MergeBamMapped"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "BWA", "mosdepth"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess mosdepth {\n                                                       \n                            \n    label 'cpus_1'\n\n                     \n    publishDir \"${params.outdir}/CNV_Plotting/${idSample}/Mosdepth\", mode: params.publish_dir_mode\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.per-base.bed.gz\"), file(\"${idSample}.per-base.bed.gz.tbi\")\n\n\n                                       \n\n    script:\n    \"\"\"\n    mosdepth $idSample $bam\n    tabix -p bed ${idSample}.per-base.bed.gz\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.*\")\n\n    when: !(params.bwa_index) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    init.sh\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Bams/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        tuple idPatient, idSample, file(bam), file(\"${bam.baseName}.bai\")\n\n                                \n\n    script:\n    \"\"\"\n    init.sh\n    samtools index ${bam}\n    mv ${bam}.bai ${bam.baseName}.bai\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/mosdepth", "ryanlayerlab/layer_lab_chco/BuildBWAindexes", "javaidm/layer_lab_vc/IndexBamFile"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools", "SAMBLASTER"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MarkDuplicates {\n    label 'cpus_max'\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bai\")\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bai\"), emit: marked_bams\n                                           \n\n                                                                     \n\n    script:\n    \"\"\"\n    init.sh\n    samtools sort -n --threads ${task.cpus}  -O SAM  ${idSample}.bam | \\\n        samblaster -M --ignoreUnmated| \\\n        samtools sort --threads ${task.cpus}  -O BAM > ${idSample}.md.bam\n\n    samtools index ${idSample}.md.bam && \\\n        mv ${idSample}.md.bam.bai ${idSample}.md.bai\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/MarkDuplicates"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess BaseRecalibrator {\n                     \n    label 'cpus_8'\n                  \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                       \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta) \n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex) \n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.table\")\n                                                          \n\n    when: params.known_indels  && step != 'variantcalling' &&\n        ('haplotypecaller' in tools || \n        'mutect2' in tools ||\n        'mutect2_single' in tools ||\n        'gen_somatic_pon' in tools)\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                            \n                                         \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/BaseRecalibrator"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n    echo true\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publish_dir_mode, overwrite: false\n\n    input:\n        tuple idPatient, idSample, file(recal)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.table\"), emit: recal_table\n                                                     \n\n    when: params.known_indels  && step != 'variantcalling' &&\n        ('haplotypecaller' in tools || \n            'mutect2' in tools ||\n            'mutect2_single' in tools ||\n            'gen_somatic_pon' in tools)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/GatherBQSRReports"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n                      \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                        \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(fasta)\n        file(fastaFai) \n        file(dict)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\")\n\n    when: params.known_indels  && step != 'variantcalling' &&\n        ('haplotypecaller' in tools || \n        'mutect2' in tools ||\n        'mutect2_single' in tools ||\n        'gen_somatic_pon' in tools)\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/ApplyBQSR"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 3, "list_own": ["s-andrews", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["nextflow_pipelines", "layer_lab_vc", "layer_lab_chco"], "list_contrib": ["FelixKrueger", "s-andrews", "javaidm", "MSBradshaw", "laurabiggins"], "nb_contrib": 5, "codes": ["\nprocess SAMTOOLS_INDEX{\t\n    \n\ttag \"$bam\"                                                            \n\tlabel 'bigMem'        \n\n\tinput:\n\t\tpath(bam)\n\t\tval (outputdir)\n\t\tval (samtools_index_args)\n\t\tval (verbose)\n\n\toutput:\n\t\tpath \"*.bai\",     emit: bai\n    \t\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n    script:\n\t\tsamtools_index_options = samtools_index_args\n\t\t\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] SAMTOOLS INDEX ARGS: \" + samtools_index_args)\n\t\t}\n\t\t\n\t\t\"\"\"\n\t\tmodule load samtools\n\t\tsamtools index $samtools_index_options $bam\n\t\t\"\"\"\n\t\t\n\t\n}", "\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bai\"), emit: bam_recal\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), emit: bam_recal_qc\n                                                   \n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    mv ${idSample}.recal.bam.bai ${idSample}.recal.bai\n    \"\"\"\n}", "\nprocess PreprocessIntervals {\n    label 'container_llab'\n    label 'cpus_8'\n    \n    input:    \n        file(intervalBed)\n        file(fasta)\n        file(fasta_fai)\n        file(dict)\n    \n    output:\n                                                                                    \n        file(\"preprocessed_intervals.interval_list\")\n\n                                                                             \n    when: ('gatk_cnv_somatic' in tools ||\n           'gatk_cnv_germline_cohort_mode' in tools ||\n           'gatk_cnv_germline' in tools\n           )\n    \n    script:\n    intervals_options = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    padding_options =  params.no_intervals ? \"--padding 0\" : \"--padding 250\"\n    bin_options =  params.no_intervals ? \"--bin-length 1000\" : \"--bin-length 0\"\n\n    \"\"\"\n    init.sh\n    gatk PreprocessIntervals \\\n        ${intervals_options} \\\n        ${padding_options} \\\n        ${bin_options} \\\n        -R ${fasta} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O preprocessed_intervals.interval_list\n    \"\"\"\n}"], "list_proc": ["s-andrews/nextflow_pipelines/SAMTOOLS_INDEX", "javaidm/layer_lab_vc/MergeBamRecal", "ryanlayerlab/layer_lab_chco/PreprocessIntervals"], "list_wf_names": ["javaidm/layer_lab_vc", "s-andrews/nextflow_pipelines", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess BamRecalOnTarget {\n    label 'cpus_32'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/RecalibratedOnTarget\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\")\n        file(paddedTargetBed)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.on_target.bam\"), file(\"${idSample}.recal.on_target.bai\"), emit: bam_recal_on_target\n                                                                                                           \n        \n    when: params.padded_target_bed && !('on_target_assessment' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    bedtools intersect -a ${idSample}.recal.bam -b ${paddedTargetBed} > ${idSample}.recal.on_target.bam\n    samtools index ${idSample}.recal.on_target.bam\n    mv ${idSample}.recal.on_target.bam.bai ${idSample}.recal.on_target.bai\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/BamRecalOnTarget"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 3, "tools": ["SAMtools", "HISAT2", "GATK"], "nb_own": 3, "list_own": ["s-andrews", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["nextflow_pipelines", "layer_lab_vc", "layer_lab_chco"], "list_contrib": ["FelixKrueger", "s-andrews", "javaidm", "MSBradshaw", "laurabiggins"], "nb_contrib": 5, "codes": ["\nprocess HISAT2 {\n\t\n\ttag \"$name\"                                                        \n\n\tlabel 'bigMem'\n\tlabel 'multiCore'\n\n    input:\n\t    tuple val(name), path(reads)\n\t\tval (outputdir)\n\t\tval (hisat2_args)\n\t\tval (verbose)\n\n\toutput:\n\t    path \"*bam\",       emit: bam\n\t\tpath \"*stats.txt\", emit: stats\n\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n    script:\n\t\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] HISAT2 ARGS: \" + hisat2_args)\n\t\t}\n\t\n\t\tcores = 8\n\t\treadString = \"\"\n\t\thisat_options = hisat2_args\n\n\t\t                     \n\t\thisat_options = hisat_options + \" --no-unal --no-softclip --new-summary\"\n\n\t\tif (reads instanceof List) {\n\t\t\treadString = \"-1 \"+reads[0]+\" -2 \"+reads[1]\n\t\t\thisat_options = hisat_options + \" --no-mixed --no-discordant\"\n\t\t}\n\t\telse {\n\t\t\treadString = \"-U \"+reads\n\t\t}\n\t\tindex = params.genome[\"hisat2\"]\n\t\t\n\t\t                                                                                                                 \n\t\tsplices = \" --known-splicesite-infile \" + params.genome[\"hisat2_splices\"]\n\t\thisat_name = name + \"_\" + params.genome[\"name\"]\n\n\t\t\"\"\"\n\t\tmodule load hisat2\n\t\tmodule load samtools\n\t\thisat2 -p ${cores} ${hisat_options} -x ${index} ${splices} ${readString}  2>${hisat_name}_hisat2_stats.txt | samtools view -bS -F 4 -F 8 -F 256 -> ${hisat_name}_hisat2.bam\n\t\t\"\"\"\n\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\")\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\"), emit: bam_recal\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), emit: bam_recal_qc\n        \n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    init.sh\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess FilterIntervals {\n    label 'container_llab'\n    label 'cpus_16'\n    echo true\n    publishDir \"${params.outdir}/Preprocessing/gatk_gcnv/\", mode: params.publish_dir_mode\n    input:\n        file(preprocessed_intervals)\n        file(annotated_intervals)\n        file(\"cvg/*\")\n\n    output:\n        file(\"cohort.gc.filtered.interval_list\")\n        \n\n    when: ('gatk_gcnv_cohort_mode' in tools)\n\n    script:\n    \"\"\"\n    init.sh\n    #cvg_opts=''\n    for x in `ls cvg/*.tsv`\n    do\n        echo -n \"-I \\$x \" >> args.list\n    done\n    \n    #echo \"my cvg_opts: \\$cvg_opts\"\n    \n    gatk FilterIntervals \\\n        -L ${preprocessed_intervals} \\\n        --annotated-intervals ${annotated_intervals} \\\n        --arguments_file args.list \\\n        -imr OVERLAPPING_ONLY \\\n        -O cohort.gc.filtered.interval_list\n\n    \"\"\"\n}"], "list_proc": ["s-andrews/nextflow_pipelines/HISAT2", "javaidm/layer_lab_vc/IndexBamRecal", "ryanlayerlab/layer_lab_chco/FilterIntervals"], "list_wf_names": ["javaidm/layer_lab_vc", "s-andrews/nextflow_pipelines", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FeatureCounts"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["process featureCounts {\n\n    tag { \"FeatureCounts\" }\n    publishDir \"${outdir}/featureCounts\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    file gtf\n    file bams\n    file bais\n    val outdir\n    val opt_args\n\n    output:\n    file \"*\"\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    featureCounts \\\n        ${usr_args} \\\n        -T ${task.cpus} \\\n        -a ${gtf} \\\n        -o counts.txt \\\n        ${bams}\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.fai\")\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        file (\"${bam}.samtools.stats.out\")\n\n    when: !('samtools' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/featureCounts", "ryanlayerlab/layer_lab_chco/BuildFastaFai", "javaidm/layer_lab_vc/SamtoolsStats"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco", "sagc-bioinformatics/modules"]}, {"nb_reuse": 3, "tools": ["SAMtools", "QualiMap", "BWA"], "nb_own": 3, "list_own": ["salvadorlab", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "mbovpan", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "noahaus", "javaidm"], "nb_contrib": 3, "codes": ["\nprocess BamQC {\n                         \n    label 'cpus_16'\n                  \n\n    tag {idPatient + \"-\" + idSample}\n\n                                                                                             \n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam) \n        file(targetBED)\n\n    output:\n        file(\"${bam.baseName}\")\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    init.sh\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess BuildFastaGzFai {\n    label 'container_llab'\n    tag \"${fasta}.gz.fai\"\n                                              \n\n    input:\n    file(fasta)\n    file(fastagz)\n\n    output:\n    file(\"${fasta}.gz.fai\")\n    when: !(params.fasta_gz_fai)\n    script:\n    \"\"\"\n    init.sh\n    samtools faidx $fastagz\n    \"\"\"\n  }", " process read_map {\n    publishDir = output \n\n    cpus threads\n    \n    conda \"$workflow.projectDir/envs/samtools.yaml\"\n   \n    input:\n    tuple file(trim1), file (trim2) from fastp_reads2 \n\n    output:\n    file(\"${trim1.baseName - ~/_trimmed_R*/}.bam\")  into map_ch \n\n    script:\n    \"\"\"\n    bwa mem -t ${task.cpus}-M -R \"@RG\\\\tID:${trim1.baseName - ~/_trimmed_R*/}\\\\tSM:${trim1.baseName - ~/_trimmed_R*/}\\\\tPL:ILLUMINA\\\\tPI:250\" ${ref} ${trim1} ${trim2} | samtools view -Sb | samtools sort -o ${trim1.baseName - ~/_trimmed_R*/}.bam\n    \"\"\"\n    }"], "list_proc": ["javaidm/layer_lab_vc/BamQC", "ryanlayerlab/layer_lab_chco/BuildFastaGzFai", "salvadorlab/mbovpan/read_map"], "list_wf_names": ["salvadorlab/mbovpan", "javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["Sambamba", "GATK", "somalier"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess CollectAlignmentSummaryMetrics{\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/alignment_summary/\", mode: params.publish_dir_mode\n    \n    input:\n    tuple idPatient, idSample, file(bam) \n    file(fasta) \n    file(fastaFai)\n    file(dict)\n\n    output:\n    file(\"${bam.baseName}_alignment_metrics.txt\")\n    \n    when: ! ('alignment_summary' in skipQC)\n    \n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx32G CollectAlignmentSummaryMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}_alignment_metrics.txt \\\n    -R ${fasta}\n    \"\"\"\n}", "\nprocess SomalierRelate {\n    label 'container_llab'\n    label 'cpus_16'\n                                       \n\n    publishDir \"${params.outdir}/Somalier/relate/\", mode: params.publish_dir_mode\n\n    input:\n       file(pedigree)\n       file(\"somalier_extracted/*\")\n                   \n\n    output:\n        tuple file(\"somalier.html\"), file(\"somalier.pairs.tsv\"), file(\"somalier.samples.tsv\")\n\n    when: 'somalier' in  tools\n\n    script:\n    \"\"\"\n    init.sh\n    #mkdir somalier_extracted\n    #mv *.somalier somalier_extracted/\n    somalier relate --ped ${pedigree} somalier_extracted/*.somalier\n    \"\"\"\n}", "process markDupSambamba {\n    tag { \"Sambamba MarkDups - ${sample_id}\" }\n    publishDir \"${params.outdir}/markDuplicates\", mode: 'copy'\n    label 'process_medium'\n\n    input:\n    tuple val(sample_id),\n        file(bam),\n        file(bai)\n\n    output:\n    path \"${sample_id}.markdup.bam\", emit: bam\n    path \"${sample_id}.markdup.bam.bai\", emit: bai\n\n    script:\n    \"\"\"\n    sambamba markdup \\\n        --tmpdir=\\${PWD} \\\n        -t ${task.cpus} \\\n        ${bam} ${sample_id}.markdup.bam\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/CollectAlignmentSummaryMetrics", "ryanlayerlab/layer_lab_chco/SomalierRelate", "sagc-bioinformatics/modules/markDupSambamba"], "list_wf_names": ["javaidm/layer_lab_vc", "sagc-bioinformatics/modules", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["FusionCatcher", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess CollectInsertSizeMetrics{\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/insert_size_metrics/\", mode: params.publish_dir_mode\n    \n    input:\n    tuple idPatient, idSample, file(bam)\n\n    output:\n    file(\"${bam.baseName}_insert_size_metrics.txt\")\n    file(\"${bam.baseName}_insert_size_histogram.pdf\")\n    \n    \n    when: !('insert_size_metrics' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx32G CollectInsertSizeMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}_insert_size_metrics.txt \\\n    -H ${bam.baseName}_insert_size_histogram.pdf \n    \"\"\"\n}", "process exonCoverage{\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/Reports/${idSample}/exonCoverage/\", mode: params.publish_dir_mode\n    publishDir \"${params.outdir}/QC/${idSample}/exonCoverage\", mode: params.publish_dir_mode\n\n    input:\n    tuple idPatient, idSample, file(bam), file(bai)\n                                           \n    file(fasta)\n    file(fastaFai)\n    file(dict)\n    file(targetBED)\n    file(baitBED)\n    val outname\n\n\n    output:\n    path \"${idSample}.*\", emit: files\n    file(\"target.interval_list\")\n\n    when: ! ('chco_qc' in _skip_qc)  && params.bait_bed\n\n    script:\n    \"\"\"\n    init.sh\n    gatk BedToIntervalList -I ${targetBED} -O target.interval_list -SD ${dict}\n    gatk BedToIntervalList -I ${baitBED} -O bait.interval_list -SD ${dict}\n    gatk --java-options -Xmx32G CollectHsMetrics --VALIDATION_STRINGENCY SILENT \\\n    -I ${bam} \\\n    -O ${idSample}.${outname}.hs_metrics.txt \\\n    -TI target.interval_list \\\n    -BI bait.interval_list \\\n    --PER_BASE_COVERAGE ${bam.baseName}.per_base_coverage.txt \\\n    -R ${fasta}\n    exonCoverage.py target.interval_list ${bam.baseName}.per_base_coverage.txt ${bam.baseName}_per_exon_coverage.txt\n    \"\"\"\n}", "\nprocess fusioncatcher_v099 {\n\n    tag { \"fusioncatcher - ${sample_id}\" } \n    publishDir \"${outdir}/${sampleProject}/fusioncatcher_v099\", mode: 'copy'\n    label 'process_fusioncatcher'\n\n    input:\n    tuple val(sample_id), file(reads)\n\tpath data_dir\n     \n    output:\n    tuple val(sample_id), val(outdir)\n    file(\"${sample_id}_fusioncatcher.txt\")\n\n    script:\n    \"\"\"\n\tfusioncatcher \\\\\n        -d ${data_dir} \\\\\n        --threads ${task.cpus} \\\\\n        --i \"${reads[0]},${reads[1]}\" \\\\\n        -o ${outdir} \\\\\n        --skip-blat \n\t\t\t\n\tmv final-list_candidate-fusion-genes.txt ${sample_id}_fusioncatcherv099.txt\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/CollectInsertSizeMetrics", "ryanlayerlab/layer_lab_chco/exonCoverage", "sagc-bioinformatics/modules/fusioncatcher_v099"], "list_wf_names": ["javaidm/layer_lab_vc", "sagc-bioinformatics/modules", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["fastPHASE", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["process umiadd {\n\n    tag { \"Fastp UMI-ADD - ${sample_id}\" }\n    publishDir \"${outdir}/${sampleProject}/umi_add\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    tuple val(sample_id), file(reads), file(UMI)\n    val outdir\n    val sampleProject\n\n    output:\n    tuple val(sample_id), file(\"${sample_id}_U{1,2}.fastq.gz\"), emit: reads\n    file(\"${sample_id}.{html,json}\")\n\n    script:\n    \"\"\"\n    fastp \\\n        ${usr_args} \\\n        -i ${reads[0]}  \\\n        -I ${UMI}  \\\n        -o ${sample_id}_U1.fastq.gz \\\n        -O ${sample_id}_umi1.fastq.gz \\\n        --json ${sample_id}_U1.json \\\n        --html ${sample_id}_U1.html \\\n        --umi --umi_loc=read2 --umi_len=8 \\\n        -G -Q -A -L -w 1 -u 100 -n 8 -Y 100\n\n    fastp \\\n        ${usr_args} \\\n        -i ${reads[1]}  \\\n        -I ${UMI}  \\\n        -o ${sample_id}_U2.fastq.gz \\\n        -O ${sample_id}_umi2.fastq.gz \\\n        --json ${sample_id}_U2.json \\\n        --html ${sample_id}_U2.html \\\n        --umi --umi_loc=read2 --umi_len=8 \\\n        -G -Q -A -L -w 1 -u 100 -n 8 -Y 100\n    \"\"\"\n}", "\nprocess CollectHsMetrics{\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/hs_metrics/\", mode: params.publish_dir_mode\n    \n    input:\n                                                      \n    tuple idPatient, idSample, file(bam)\n    file(fasta) \n    file(fastaFai)\n    file(dict)\n    file(targetBED)\n    file(baitBED)\n                          \n\n    output:\n    file(\"${bam.baseName}.txt\")\n                                                   \n    \n    \n    when: !('hs_metrics' in skipQC) && params.bait_bed\n    script:\n    \"\"\"\n    init.sh\n    gatk BedToIntervalList -I ${targetBED} -O target.interval_list -SD ${dict}\n    gatk BedToIntervalList -I ${baitBED} -O bait.interval_list -SD ${dict}\n\n    gatk --java-options -Xmx32G CollectHsMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}.txt \\\n    -TI target.interval_list \\\n    -BI bait.interval_list \\\n    -R ${fasta}\n    \"\"\"\n}", "\nprocess insertSize{\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/insertSize/\", mode: params.publish_dir_mode\n    publishDir \"${params.outdir}/QC/${idSample}/insertSize\", mode: params.publish_dir_mode\n\n    input:\n    tuple idPatient, idSample, file(bam), file(bai)\n\n\n    output:\n    path \"${idSample}_insert_size_metrics.txt\", emit: files\n    file(\"${idSample}_insert_size_histogram.pdf\")\n\n    when: ! ('chco_qc' in _skip_qc)\n\n    script:\n    \"\"\"\n        gatk --java-options -Xmx32G CollectInsertSizeMetrics \\\n        -I $bam \\\n        -O ${idSample}_insert_size_metrics.txt \\\n        -H ${idSample}_insert_size_histogram.pdf \\\n        -M 0.5\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/umiadd", "javaidm/layer_lab_vc/CollectHsMetrics", "ryanlayerlab/layer_lab_chco/insertSize"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco", "sagc-bioinformatics/modules"]}, {"nb_reuse": 3, "tools": ["GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/HaplotypeCaller", "javaidm/layer_lab_vc/HaplotypeCaller", "ryanlayerlab/layer_lab_caw/HaplotypeCaller"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["GATK", "QualiMap", "somalier"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SomalierExtraction {\n    label 'container_llab'\n    label 'cpus_8'\n    tag {idSample}\n    \n    publishDir \"${params.outdir}/Somalier_extracted/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta)\n        file(fasta_fai)\n        file(somalier_sites)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.somalier\")\n\n    when: params.somalier_sites\n\n    script:\n    \"\"\"\n    init.sh\n    somalier extract  --sites ${somalier_sites} -f ${fasta}  ${bam}\n    \"\"\"\n}", "\nprocess IndividuallyGentoypeGVCF{\n    label 'cpus_8'\n    tag {idSample + \"-\" + gvcf.baseName}\n                      \n                                                                                                                            \n    input:\n        tuple idPatient, idSample, file(intervalsBed), file(gvcf)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n    output:\n                                                                                                                                              \n        tuple idPatient, idSample, file(out_file), emit: vcf_HaplotypeCaller\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                       \n                                \n                                      \n    prefix=\"${gvcf.fileName}\" - \".g.vcf\"\n                                      \n                                          \n    out_file=\"${prefix}.vcf\"\n    \"\"\"\n    init.sh\n    bgzip  ${gvcf}\n    tabix  ${gvcf}.gz\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalsBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf}.gz \\\n        -O \"${out_file}\"\n    \"\"\"\n}", "\nprocess BamQC {\n                         \n    label 'container_llab'\n    label 'cpus_16'\n                  \n\n    tag {idPatient + \"-\" + idSample}\n\n                                                                                             \n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam) \n        file(targetBED)\n\n    output:\n        file(\"${bam.baseName}\")\n\n                                 \n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    init.sh\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/SomalierExtraction", "javaidm/layer_lab_vc/IndividuallyGentoypeGVCF", "ryanlayerlab/layer_lab_chco/BamQC"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SelectVariants {\n    label 'container_llab'\n    label 'cpus_8'\n    tag {interval_bed.baseName}\n    input:\n                                                                                                                                 \n        tuple val(caller), val(id_patient), val(id_sample), file(interval_bed), file (vcf), file (vcf_idx)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n\n    output:\n                                                                                                                                  \n                                                                                                                                                                                                                 \n    tuple val(\"HaplotypeCaller_Jointly_Genotyped\"), id_patient, id_sample, file(\"${interval_bed.baseName}_${id_sample}.vcf\"), emit: vcf_SelectVariants\n    \n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n            SelectVariants \\\n            -R ${fasta} \\\n            -L ${interval_bed} \\\n            -V ${vcf} \\\n            -O ${interval_bed.baseName}_${id_sample}.vcf \\\n            -sn ${id_sample}\n    \"\"\"\n}", "\nprocess GenomicsDBImport {\n    label 'cpus_16'\n                \n    tag{interval_name}\n                                                                               \n\n    input:\n                                                                                                           \n    tuple val(interval_name), file(interval_bed), val(patientSampleIdMap), file(gvcfs)\n    \n    output:\n    tuple val(interval_name), file(interval_bed), val(patientSampleIdMap), file (\"${interval_name}.gdb\")\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    sample_map=\"cohort_samples.map\"\n    interval_name_with_underscore=\"${interval_name}_\"\n                \n    \"\"\"\n    init.sh\n    for x in *.g.vcf\n    do\n        bgzip \\$x\n        tabix \\${x}.gz\n    done\n\n    for x in *.g.vcf.gz\n    do\n        \n        base_name=`basename \\$x .g.vcf.gz`\n        sample=\\${base_name#$interval_name_with_underscore}\n        echo \"\\${sample}\\t\\${x}\" >> ${sample_map}\n    done\n    \n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    GenomicsDBImport \\\n    --genomicsdb-workspace-path ${interval_name}.gdb \\\n    -L $interval_bed \\\n    --sample-name-map ${sample_map} \\\n    --reader-threads ${task.cpus}\n\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/SelectVariants", "javaidm/layer_lab_vc/GenomicsDBImport"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["GATK", "MultiQC"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MultiQC {\n    label 'container_sarek'\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n    input:\n        file (multiqcConfig) \n        file (versions) \n        file ('bamQC/*') \n        file ('FastQC/*') \n        file ('BCFToolsStats/*') \n        file ('VCFTools/*')\n                 'MarkDuplicates/*'  \n        file ('SamToolsStats/*') \n        file ('CollectAlignmentSummary/*')\n        file ('CollectInsertSizeMetrics/*')\n        file ('CollectHsMetrics/*')\n                 'snpEff/*'  \n\n    output:\n        tuple file(\"*multiqc_report.html\"), file(\"*multiqc_data\") \n\n    script:\n    \"\"\"\n    multiqc -f -v .\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    label 'cpus_8'\n    tag {interval_bed.baseName}\n    input:\n                                                                                                             \n        tuple val(interval_name), file(interval_bed), val(patientSampleIdMap), file(gdb)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n                                                                                                                                                             \n    tuple val(\"HaplotypeCaller\"),  val(patientSampleIdMap), file(interval_bed), file(\"${interval_name}.vcf\"), file(\"${interval_name}.vcf.idx\"), emit: vcf_GenotypeGVCFs\n                                                                                                                                                                                                                                   \n    \n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${interval_bed} \\\n        -D ${dbsnp} \\\n        -V gendb://${gdb} \\\n        --create-output-variant-index \\\n        -O \"${interval_name}.vcf\"\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/MultiQC", "javaidm/layer_lab_vc/GenotypeGVCFs"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SelectVariants {\n    label 'cpus_8'\n    tag {interval_bed.baseName}\n    input:\n                                                                                                                                 \n        tuple val(caller), val(id_patient), val(id_sample), file(interval_bed), file (vcf), file (vcf_idx)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n\n    output:\n                                                                                                                                  \n                                                                                                                                                                                                                 \n    tuple val(\"HaplotypeCaller_Jointly_Genotyped\"), id_patient, id_sample, file(\"${interval_bed.baseName}_${id_sample}.vcf\"), emit: vcf_SelectVariants\n    \n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n            SelectVariants \\\n            -R ${fasta} \\\n            -L ${interval_bed} \\\n            -V ${vcf} \\\n            -O ${interval_bed.baseName}_${id_sample}.vcf \\\n            -sn ${id_sample}\n    \"\"\"\n}", "\nprocess BuildFastaGzFai {\n    label 'container_llab'\n    tag \"${fasta}.gz.fai\"\n                                              \n\n    input:\n    file(fasta)\n    file(fastagz)\n\n    output:\n    file(\"${fasta}.gz.fai\")\n    when: !(params.fasta_gz_fai)\n    script:\n    \"\"\"\n    init.sh\n    samtools faidx $fastagz\n    \"\"\"\n  }"], "list_proc": ["javaidm/layer_lab_vc/SelectVariants", "ryanlayerlab/layer_lab_caw/BuildFastaGzFai"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Mpileup {\n    label 'memory_singleCPU_2_task'\n    tag {idSample + \"-\" + intervalBed.baseName}\n    \n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup.gz\" ? \"VariantCalling/${idSample}/mpileup/${it}\" : '' }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.pileup.gz\")\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n    \"\"\"\n    init.sh\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} \\\n    | bgzip --threads ${task.cpus} -c > ${prefix}${idSample}.pileup.gz\n    \"\"\"\n}", "\nprocess BuildDict {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta.baseName}.dict\")\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_16'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, out_suffix, file(bams)\n                                                                    \n\n    output:\n        tuple idPatient, idSample,  file(\"${idSample}${out_suffix}.bam\")\n\n    script:\n                                                          \n                                        \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} \"${idSample}${out_suffix}.bam\" ${bams}\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/Mpileup", "ryanlayerlab/layer_lab_caw/BuildDict", "ryanlayerlab/layer_lab_chco/MergeBamMapped"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["BCFtools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess ModelSegments {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}\", mode: params.publish_dir_mode\n    \n    input:\n         tuple idPatient, idSample, file(denoised_copy_ratio)\n\n    output:\n        tuple idPatient, idSample, file(\"${out_dir}/${idSample}.cr.seg\"), file(\"${out_dir}/${idSample}.modelFinal.seg\"), emit: 'modeled_seg'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    out_dir = \"ModeledSegments\"\n\n    \"\"\"\n    init.sh\n    mkdir $out_dir\n    gatk ModelSegments \\\n        --denoised-copy-ratios ${denoised_copy_ratio} \\\n        --output-prefix ${idSample} \\\n        -O ${out_dir}\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n                                                   \n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n                                                                 \n        file (\"*.bcf.tools.stats.out\")\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    init.sh\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/ModelSegments", "javaidm/layer_lab_vc/BcftoolsStats"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SnpEff {\n    tag {\"${idSample} - ${variantCaller} - ${vcf}\"}\n                  \n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: {\n        if (it == \"${reducedVCF}_snpEff.ann.vcf\") null\n        else \"Reports/${idSample}/snpEff/${variantCaller}/${it}\"\n    }\n\n    input:\n        tuple variantCaller, idSample, file(vcf) \n        file(dataDir)\n                        \n        val snpeffDb\n\n    output:\n        tuple file(\"${reducedVCF}_snpEff.txt\"), file(\"${reducedVCF}_snpEff.html\"), file(\"${reducedVCF}_snpEff.csv\"), emit:snpEff_report\n        tuple variantCaller, idSample, file(\"${reducedVCF}_snpEff.ann.vcf\"), emit: snpEff_vcf\n\n    when: 'snpeff' in tools || 'merge' in tools\n\n    script:\n    reducedVCF = reduceVCF(vcf.fileName)\n    cache = (params.snpEff_cache && params.annotation_cache) ? \"-dataDir \\${PWD}/${dataDir}\" : \"\"\n                                                                                            \n    \"\"\"\n    init.sh\n    snpEff -Xmx${task.memory.toGiga()}g \\\n        ${snpeffDb} \\\n        -csvStats ${reducedVCF}_snpEff.csv \\\n        -nodownload \\\n        ${cache} \\\n        -canon \\\n        -v \\\n        ${vcf} \\\n        > ${reducedVCF}_snpEff.ann.vcf\n\n    mv snpEff_summary.html ${reducedVCF}_snpEff.html\n    mv ${reducedVCF}_snpEff.genes.txt ${reducedVCF}_snpEff.txt\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/SnpEff"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 3, "tools": ["BEDTools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess Mutect2Single{\n    tag {idSample + \"-\" + intervalBed.baseName}\n    label 'cpus_16'\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n\n    output:\n        tuple idPatient, idSample, file(out_vcf), emit: vcf\n        tuple idPatient, idSample, file(out_stats) , emit: stats\n    \n    when: 'mutect2_single' in tools\n    \n    script:\n    out_vcf = \"${intervalBed.baseName}_${idSample}.vcf\"\n    out_stats = \"${intervalBed.baseName}_${idSample}.vcf.stats\"\n    \"\"\"\n    # max-mnp-distance is set to 0 to avoid a bug in \n    # next process GenomicsDbImport\n    # See https://gatk.broadinstitute.org/hc/en-us/articles/360046224491-CreateSomaticPanelOfNormals-BETA-\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta} \\\n      -I ${bam}  \\\n      -max-mnp-distance 0 \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      -O ${out_vcf}\n    \"\"\"\n}", "\nprocess savvy_to_bed{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/VariantCalling/SavvyIntermediates/\", mode: params.publish_dir_mode\n\n    input:\n    file(SavvycnvResults)\n    file(exon_file)\n\n    output:\n    file(\"*savvy.bed\")\n\n    script:\n    \"\"\"\n    # the file is hard coded for now, change before going live to $SavvycnvResults/cnv_list.csv /scratch/Shares/CHCO/workspace/cna_positive_wes/results/savvycnv/VariantCalling/SavvycnvResults/cnv_list.csv\n    # remove chr from chromosome\n    cat $SavvycnvResults/cnv_list.csv | sed 's/chr//g' | bedtools intersect -wb -a $exon_file -b stdin > savvy-temp.tsv\n    savvy_to_bed.py savvy-temp.tsv \n    ls\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/HaplotypeCaller", "javaidm/layer_lab_vc/Mutect2Single", "ryanlayerlab/layer_lab_chco/savvy_to_bed"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["GATK", "BWA", "BEDTools"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess combine_callers{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/AllCNVCallers/\", mode: params.publish_dir_mode\n\n    input:\n    file(savvy_beds)\n    tuple caller, idPatient, idSample, file(cnvkit_bed)\n                                      \n\n    output:\n    tuple idSample, file(\"${idSample}.multi_caller.bed\"), file(\"${idSample}.log\")\n\n    script:\n    \"\"\"\n    touch ${idSample}.log\n    if [[ \"$savvy_beds\" != \"not_used\"  ]];\n    then    \n        FILE=\"${idSample}.coverageBinner_savvy.bed\"\n        if test -f \"\\$FILE\"; then\n            cat ${idSample}.coverageBinner_savvy.bed >> temp.bed\n        else\n            echo \"# No calls from Savvy for $idSample\" >> ${idSample}.log\n        fi\n    fi \n\n    if [ \"$caller\" != \"cnvkit_not_used\" ];\n    then\n        if [[ \\$(wc -l <$cnvkit_bed) -ge 2 ]]\n        then\n            echo \"\"\n        else\n            echo \"# No calls from CNVKit for $idSample\" >> ${idSample}.log    \n        fi\n        echo \"Adding CNVKit bed\"\n        cat $cnvkit_bed >> temp.bed\n    else\n        echo \"Skipping CNVKit bed addition\"\n    fi\n\n    if test -f \"temp.bed\"; then\n        #cnvkit_file=\\$(find )\n\n        grep -v BND temp.bed | awk '(\\$2 <= \\$3)' >  filtered_temp.bed\n        cat filtered_temp.bed | sort -k1,1V -k2,2n -k3,3n > tripple_sorted.bed\n        bedtools cluster -i tripple_sorted.bed > clustered_test.bed\n        agg_cluster.py clustered_test.bed > ${idSample}.multi_caller.bed\n    else\n        touch ${idSample}.multi_caller.bed\n    fi\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.*\")\n\n    when: !(params.bwa_index) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    init.sh\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess MergeMutect2SingleStats {\n    label 'cpus_16'\n    tag {idSample}\n\n                                                                                                                              \n\n    input:\n        tuple idPatient, idSample, file(statsFiles)                         \n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.vcf.gz.stats\")\n\n    when: 'mutect2_single' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/combine_callers", "ryanlayerlab/layer_lab_chco/BuildBWAindexes", "javaidm/layer_lab_vc/MergeMutect2SingleStats"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FilterMutect2SingleCalls {\n    label 'cpus_1'\n\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/mutect2_single_filtered\", mode: params.publish_dir_mode\n\n    input:\n        tuple   idPatient, \n                idSample, \n                file(unfiltered), file(unfilteredIndex),\n                file(\"${idSample}.vcf.gz.stats\")\n        \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n                                            \n        \n    output:\n        tuple val(\"Mutect2Single\"), idPatient, idSample,\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz\"),\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz.tbi\"),\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz.filteringStats.tsv\")\n\n                                             \n     when: 'mutect2_single' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --stats ${idSample}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O mutect2_single_filtered_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.fai\")\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    samtools faidx ${fasta}\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/FilterMutect2SingleCalls", "ryanlayerlab/layer_lab_chco/HaplotypeCaller", "ryanlayerlab/layer_lab_chco/BuildFastaFai"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["GATK", "BEDTools"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess merge_all_allele_counts {\n    label 'container_llab'\n    label 'cpus_1'\n\n    publishDir \"${params.outdir}/CNV_Plotting/merge_all_allele_counts\", mode: params.publish_dir_mode\n\n    input:\n    file(agged_allele_counts)                                 \n    file(probes)\n\n    output:\n    tuple file(\"ab.sorted.tsv.gz\"), file(\"ab.sorted.tsv.gz.tbi\")\n    \n    script:\n    \"\"\"\n    cat *.agg.allele_count.bed > all_samples.aggregate_probe_allele_counts.txt\n    create_all_sample_allele_count_bed.py all_samples.aggregate_probe_allele_counts.txt $probes> ab.bed\n    head -1 ab.bed > ab.header.tsv\n    tail -n +2 ab.bed > ab.tsv\n    bedtools sort -i ab.tsv > ab.sorted.tsv\n    bgzip ab.sorted.tsv\n    tabix -p bed ab.sorted.tsv.gz \n    \"\"\"\n}", "\nprocess SomaticPonGenomicsDBImport {\n    label 'cpus_32'\n\n    publishDir \"${params.outdir}/Preprocessing/Somatic_pon_db\", mode: params.publish_dir_mode\n\n    input:\n    file(\"vcfs/*\")\n    file(targetBED)\n\n    output:\n    file(\"somatic_pon.gdb\")\n\n    when: 'gen_somatic_pon' in tools\n\n    script:\n    sample_map=\"cohort_samples.map\"\n    \n                \n    \"\"\"\n    init.sh\n    vcfs=' '\n    for x in `ls vcfs/*.vcf.gz`\n    do\n        base_name=`basename \\${x}`\n        without_ext=\\${base_name%.vcf.gz}\n        sample_name=\\${without_ext##*_}\n        echo \"\\${sample_name}\\t\\$x\" >> $sample_map \n    done\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    GenomicsDBImport  \\\n    --genomicsdb-workspace-path somatic_pon.gdb \\\n    -L ${targetBED} \\\n    --sample-name-map $sample_map \\\n    --merge-input-intervals \\\n    --reader-threads ${task.cpus}\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/merge_all_allele_counts", "javaidm/layer_lab_vc/SomaticPonGenomicsDBImport", "ryanlayerlab/layer_lab_caw/HaplotypeCaller"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["GATK", "somalier"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SomalierExtraction {\n    label 'container_llab'\n    label 'cpus_8'\n    tag {idSample}\n    \n    publishDir \"${params.outdir}/Somalier_extracted/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta)\n        file(fasta_fai)\n        file(somalier_sites)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.somalier\")\n\n    when: params.somalier_sites\n\n    script:\n    \"\"\"\n    init.sh\n    somalier extract  --sites ${somalier_sites} -f ${fasta}  ${bam}\n    \"\"\"\n}", "\nprocess CreateSomaticPON{\n    label 'cpus_max'\n                         \n     publishDir \"${params.outdir}/Preprocessing/Somatic_pon\", mode: params.publish_dir_mode\n\n    input:\n    file(pon) \n    file(fasta)\n    file(fastaFai)\n    file(dict)\n    file(germlineResource)\n    file(germlineResourceIndex)\n    \n    output:\n    tuple file(out_file), file (\"${out_file}.tbi\")\n\n    when: 'gen_somatic_pon' in tools\n\n    script:\n    args_file = \"normals_for_pon_vcf.args\"\n    out_file = \"somatic_pon.vcf.gz\" \n    pon_db = \"gendb://${pon}\"\n    \n    \"\"\"\n    init.sh\n     gatk --java-options -Xmx${task.memory.toGiga()}g \\\n     CreateSomaticPanelOfNormals -R ${fasta} \\\n     --germline-resource ${germlineResource} \\\n    -V ${pon_db} \\\n    -O ${out_file}\n    \"\"\"\n}", "\nprocess DenoiseReadCounts {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DenoisedReadCounts/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file( \"${idSample}.counts.hdf5\")\n        file(read_count_somatic_pon)\n\n    output:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio), emit: 'denoised_cr'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    std_copy_ratio = \"${idSample}.standardizedCR.tsv\"\n    denoised_copy_ratio = \"${idSample}.denoisedCR.tsv\"\n    pon_option = params.read_count_pon ? \"--count-panel-of-normals ${read_count_somatic_pon}\" : \"\"\n    \"\"\"\n    init.sh\n    gatk DenoiseReadCounts \\\n        -I ${idSample}.counts.hdf5 \\\n        ${pon_option} \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/SomalierExtraction", "javaidm/layer_lab_vc/CreateSomaticPON", "ryanlayerlab/layer_lab_chco/DenoiseReadCounts"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeMutect2SingleStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSample}\n\n                                                                                                                              \n\n    input:\n        tuple idPatient, idSample, file(statsFiles)                         \n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.vcf.gz.stats\")\n\n    when: 'mutect2_single' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess Mutect2TN{\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_2'\n\n    input:\n        tuple idPatient, \n            idSampleNormal, file(bamNormal), file(baiNormal),\n            idSampleTumor, file(bamTumor), file(baiTumor), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n        file(ponSomatic)\n        file(ponSomaticIndex)\n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\"), emit: vcf\n        \n        tuple idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), emit: stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon_somatic ? \"--panel-of-normals ${ponSomatic}\" : \"\"\n                \n    \"\"\"\n    init.sh\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess GermlineCNVCallerCohortMode {\n                             \n    label 'container_gatk'\n    label 'cpus_32'\n    publishDir \"${params.outdir}/Preprocessing/GATK_gcnv/\", mode: params.publish_dir_mode\n    input:\n        file(annotated_intervals)\n        file(filtered_intervals)\n        file(ploidy_calls)\n        file(\"cvg/*\")\n\n    output:\n        file(\"gcnv_model\")\n\n    when:  'gatk_gcnv_cohort_mode' in tools \n\n    script:\n     \n    \"\"\"\n    init.sh\n    for x in `ls cvg/*.tsv`\n    do\n        echo -n \"-I \\$x \" >> args.list\n    done\n    gatk GermlineCNVCaller \\\n        --run-mode COHORT \\\n        -L ${filtered_intervals} \\\n        --arguments_file args.list \\\n        --contig-ploidy-calls ${ploidy_calls} \\\n        --annotated-intervals ${annotated_intervals} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        --output gcnv_model \\\n        --output-prefix gcnv_model \\\n        --verbosity DEBUG\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MergeMutect2SingleStats", "javaidm/layer_lab_vc/Mutect2TN", "ryanlayerlab/layer_lab_chco/GermlineCNVCallerCohortMode"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "BWA", "GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeMutect2TNStats {\n    label 'cpus_16'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                                                                                                     \n        tuple idPatient, idSampleTumor, idSampleNormal, file(statsFiles)                         \n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\")\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(inputFile1), file(inputFile2)\n        file(fasta) \n        file(fastaFai)\n        file(bwaIndex) \n\n    output:\n                                                                             \n                                                                                         \n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\"), emit : bam_mapped\n                                                                                                                  \n    \n                                                                                \n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = status_map[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n        init.sh\n        ${convertToFastq}\n        bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess SamtoolsStats {\n    label 'container_llab'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        file (\"${bam}.samtools.stats.out\")\n\n                                      \n\n    script:\n    \"\"\"\n    init.sh\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/MergeMutect2TNStats", "ryanlayerlab/layer_lab_chco/MapReads", "ryanlayerlab/layer_lab_chco/SamtoolsStats"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Mutect2Single{\n    label 'container_llab'\n    tag {idSample + \"-\" + intervalBed.baseName}\n    label 'cpus_16'\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n\n    output:\n        tuple idPatient, idSample, file(out_vcf), emit: vcf\n        tuple idPatient, idSample, file(out_stats) , emit: stats\n    \n    when: 'mutect2_single' in tools\n    \n    script:\n    out_vcf = \"${intervalBed.baseName}_${idSample}.vcf\"\n    out_stats = \"${intervalBed.baseName}_${idSample}.vcf.stats\"\n    \"\"\"\n    # max-mnp-distance is set to 0 to avoid a bug in \n    # next process GenomicsDbImport\n    # See https://gatk.broadinstitute.org/hc/en-us/articles/360046224491-CreateSomaticPanelOfNormals-BETA-\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta} \\\n      -I ${bam}  \\\n      -max-mnp-distance 0 \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      -O ${out_vcf}\n    \"\"\"\n}", "\nprocess PreprocessIntervals {\n\n    label 'cpus_8'\n    \n    input:    \n        file(intervalBed)\n        file(fasta)\n        file(fasta_fai)\n        file(dict)\n    \n    output:\n                                                                                    \n        file(\"preprocessed_intervals.interval_list\")\n\n    when: ('gatkcnv' in tools) || ('gen_read_count_pon' in tools)\n    \n    script:\n    intervals_options = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    padding_options =  params.no_intervals ? \"--padding 0\" : \"--padding 250\"\n    bin_options =  params.no_intervals ? \"--bin-length 1000\" : \"--bin-length 0\"\n\n    \"\"\"\n    init.sh\n    gatk PreprocessIntervals \\\n        ${intervals_options} \\\n        ${padding_options} \\\n        ${bin_options} \\\n        -R ${fasta} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O preprocessed_intervals.interval_list\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/Mutect2Single", "javaidm/layer_lab_vc/PreprocessIntervals"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 2, "list_own": ["ryanlayerlab", "javaidm"], "nb_wf": 2, "list_wf": ["layer_lab_vc", "layer_lab_caw"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FilterMutect2SingleCalls {\n    label 'container_llab'\n    label 'cpus_1'\n\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/mutect2_single_filtered\", mode: params.publish_dir_mode\n\n    input:\n        tuple   idPatient, \n                idSample, \n                file(unfiltered), file(unfilteredIndex),\n                file(\"${idSample}.vcf.gz.stats\")\n        \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n                                            \n        \n    output:\n        tuple val(\"Mutect2Single\"), idPatient, idSample,\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz\"),\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz.tbi\"),\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz.filteringStats.tsv\")\n\n                                             \n     when: 'mutect2_single' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --stats ${idSample}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O mutect2_single_filtered_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess CollectReadCounts {\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(preprocessed_intervals)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.counts.hdf5\"), emit: 'sample_read_counts'\n\n    when: ('gatkcnv' in tools) || ('gen_read_count_pon' in tools)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk CollectReadCounts \\\n        -I ${bam} \\\n        -L ${preprocessed_intervals} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O ${idSample}.counts.hdf5\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/FilterMutect2SingleCalls", "javaidm/layer_lab_vc/CollectReadCounts"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 3, "tools": ["MultiQC", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess CreateReadCountPon {\n                \n    tag \"ReadCountPon\"\n    \n    publishDir \"${params.outdir}/Preprocessing/ReadCountPon/\", \n    mode: params.publish_dir_mode\n\n    \n    input:\n    file(read_count_hdf5s)\n    \n                      \n\n    output:\n    file(out_file)\n\n    script:\n    when:'gen_read_count_pon' in tools\n                                    \n    out_file = \"read_count_pon.hdf5\"\n    params_str = ''\n                                  \n    read_count_hdf5s.each{\n        params_str = \"${params_str} -I ${it}\"\n    }\n\n    \n    \"\"\"\n    init.sh\n    gatk CreateReadCountPanelOfNormals \\\n        $params_str \\\n        -O $out_file\n    \"\"\"\n}", "\nprocess Mutect2Single{\n    label 'container_llab'\n    tag {idSample + \"-\" + intervalBed.baseName}\n    label 'cpus_16'\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n\n    output:\n        tuple idPatient, idSample, file(out_vcf), emit: vcf\n        tuple idPatient, idSample, file(out_stats) , emit: stats\n    \n    when: 'mutect2_single' in tools\n    \n    script:\n    out_vcf = \"${intervalBed.baseName}_${idSample}.vcf\"\n    out_stats = \"${intervalBed.baseName}_${idSample}.vcf.stats\"\n    \"\"\"\n    # max-mnp-distance is set to 0 to avoid a bug in \n    # next process GenomicsDbImport\n    # See https://gatk.broadinstitute.org/hc/en-us/articles/360046224491-CreateSomaticPanelOfNormals-BETA-\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta} \\\n      -I ${bam}  \\\n      -max-mnp-distance 0 \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      -O ${out_vcf}\n    \"\"\"\n}", "process multiqc {\n\n    tag { \"MultiQC\" } \n    publishDir \"${outdir}/${sampleProject}/QC-results\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    file fastqc_in\n    file bcl_stats\n    file bbduck_stats\n    file kraken2_stats\n    val outdir\n    val sampleProject\n                    \n\n    output:\n    path \"multiqc_report.html\", emit: multiqc_report\n    path \"multiqc_data.zip\", emit: multiqc_data\n\n    script:\n                                 \n\n    \"\"\"\n    if [[ ${bcl_stats} != 'false' ]]; then\n        cp Stats/Stats.json .\n    fi\n    \n    multiqc \\\n        --config ${projectDir}/conf/multiqc_config.yaml \\\n        .\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/CreateReadCountPon", "ryanlayerlab/layer_lab_chco/Mutect2Single", "sagc-bioinformatics/modules/multiqc"], "list_wf_names": ["javaidm/layer_lab_vc", "sagc-bioinformatics/modules", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Sambamba", "BWA", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess DenoiseReadCounts {\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DenoisedReadCounts/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file( \"${idSample}.counts.hdf5\")\n        file(read_count_somatic_pon)\n\n    output:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio), emit: 'denoised_cr'\n\n    when: 'gatkcnv' in tools\n\n    script:\n    std_copy_ratio = \"${idSample}.standardizedCR.tsv\"\n    denoised_copy_ratio = \"${idSample}.denoisedCR.tsv\"\n    pon_option = params.read_count_pon ? \"--count-panel-of-normals ${read_count_somatic_pon}\" : \"\"\n    \"\"\"\n    init.sh\n    gatk DenoiseReadCounts \\\n        -I ${idSample}.counts.hdf5 \\\n        ${pon_option} \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio}\n    \"\"\"\n}", "\nprocess FilterMutect2SingleCalls {\n    label 'container_llab'\n    label 'cpus_1'\n\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/mutect2_single_filtered\", mode: params.publish_dir_mode\n\n    input:\n        tuple   idPatient, \n                idSample, \n                file(unfiltered), file(unfilteredIndex),\n                file(\"${idSample}.vcf.gz.stats\")\n        \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n                                            \n        \n    output:\n        tuple val(\"Mutect2Single\"), idPatient, idSample,\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz\"),\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz.tbi\"),\n            file(\"mutect2_single_filtered_${idSample}.vcf.gz.filteringStats.tsv\")\n\n                                             \n     when: 'mutect2_single' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --stats ${idSample}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O mutect2_single_filtered_${idSample}.vcf.gz\n    \"\"\"\n}", "\nprocess BwaMapSortedBam {\n    tag { sample_id + \" - BWA align\" }\n\n    memory '4 GB'\n\n    publishDir \"${params.outdir}/BWA\", mode: 'copy'\n    stageInMode 'copy'                                             \n                                    \n\n    input:\n    file bwa_idx\n    tuple sample_id, file(reads)\n\n    output:\n    tuple sample_id,\n        file(\"${sample_id}.bwamem.bam\"),\n        file(\"${sample_id}.bwamem.bam.bai\")\n\n    script:\n    \"\"\"\n    # bwa alignment\n    bwa mem -t ${task.cpus} \\\n        -R \"\\\"@RG\\\\tID:${sample_id}\\\\tSM:${sample_id}\\\\tPL:ILLUMINA\\\\tLB:${sample_id}\\\\tPU:LIB1\\\"\" \\\n        ${bwa_idx} ${reads} | \\\n            samtools sort --threads ${task.cpus} -m 2G - > ${sample_id}.bwamem.bam\n    \n    # index with sambamba\n    sambamba index -t ${task.cpus} ${sample_id}.bwamem.bam\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/DenoiseReadCounts", "ryanlayerlab/layer_lab_chco/FilterMutect2SingleCalls", "sagc-bioinformatics/modules/BwaMapSortedBam"], "list_wf_names": ["javaidm/layer_lab_vc", "sagc-bioinformatics/modules", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FeatureCounts", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["process featureCounts {\n\n    tag { \"FeatureCounts\" }\n    publishDir \"${outdir}/featureCounts\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    file gtf\n    file bams\n    file bais\n    val outdir\n    val opt_args\n\n    output:\n    file \"*\"\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    featureCounts \\\n        ${usr_args} \\\n        -T ${task.cpus} \\\n        -a ${gtf} \\\n        -o counts.txt \\\n        ${bams}\n    \"\"\"\n}", "\nprocess PlotDenoisedCopyRatios {\n    label 'cpus_16'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio)\n        file(dict)\n    \n    output:\n        file(out_dir)  \n\n    when: 'gatkcnv' in tools\n\n    script:\n    out_dir = \"PlotDenoisedReadCounts\" \n\n    \"\"\"\n    init.sh\n    mkdir ${out_dir}\n    gatk PlotDenoisedCopyRatios \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio} \\\n        --sequence-dictionary ${dict} \\\n        --output-prefix ${idSample} \\\n        -O ${out_dir}\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_16'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, out_suffix, file(bams)\n                                                                    \n\n    output:\n        tuple idPatient, idSample,  file(\"${idSample}${out_suffix}.bam\")\n\n    script:\n                                                          \n                                        \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} \"${idSample}${out_suffix}.bam\" ${bams}\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/featureCounts", "javaidm/layer_lab_vc/PlotDenoisedCopyRatios", "ryanlayerlab/layer_lab_chco/MergeBamMapped"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco", "sagc-bioinformatics/modules"]}, {"nb_reuse": 3, "tools": ["BEDTools", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess RunGatkHC {\n    tag { sample_id + \" - GATK\" }\n\n    publishDir \"${params.outdir}/GATK\", mode: 'copy'\n    stageInMode 'copy'                                             \n\n\n    input:\n    file ref\n    file idx\n    file dict\n    tuple sample_id,\n        file(bam),\n        file(bai)\n\n    output:\n    tuple sample_id,\n        file(\"${sample_id}.raw.vcf.gz\"),\n        file(\"${sample_id}.raw.vcf.gz.tbi\")\n\n    script:\n    \"\"\"\n    gatk HaplotypeCaller \\\n        -R ${ref} \\\n        -I ${bam} \\\n        -O ${sample_id}.raw.vcf.gz \\\n        --native-pair-hmm-threads ${task.cpus} \\\n        --dont-use-soft-clipped-bases true \\\n        -stand-call-conf 20 \\\n        --tmp-dir \\${PWD}\n    \"\"\"\n}", "\nprocess ModelSegments {\n    label 'cpus_32'\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}\", mode: params.publish_dir_mode\n    \n    input:\n         tuple idPatient, idSample, file(denoised_copy_ratio)\n\n    output:\n        tuple idPatient, idSample, file(\"${out_dir}/${idSample}.cr.seg\"), file(\"${out_dir}/${idSample}.modelFinal.seg\"), emit: 'modeled_seg'\n\n    when: 'gatkcnv' in tools\n\n    script:\n    out_dir = \"ModeledSegments\"\n\n    \"\"\"\n    init.sh\n    mkdir $out_dir\n    gatk ModelSegments \\\n        --denoised-copy-ratios ${denoised_copy_ratio} \\\n        --output-prefix ${idSample} \\\n        -O ${out_dir}\n    \"\"\"\n}", "\nprocess savvy_to_bed{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/VariantCalling/SavvyIntermediates/\", mode: params.publish_dir_mode\n\n    input:\n    file(SavvycnvResults)\n    file(exon_file)\n\n    output:\n    file(\"*savvy.bed\")\n\n    script:\n    \"\"\"\n    # the file is hard coded for now, change before going live to $SavvycnvResults/cnv_list.csv /scratch/Shares/CHCO/workspace/cna_positive_wes/results/savvycnv/VariantCalling/SavvycnvResults/cnv_list.csv\n    # remove chr from chromosome\n    cat $SavvycnvResults/cnv_list.csv | sed 's/chr//g' | bedtools intersect -wb -a $exon_file -b stdin > savvy-temp.tsv\n    savvy_to_bed.py savvy-temp.tsv \n    ls\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/RunGatkHC", "javaidm/layer_lab_vc/ModelSegments", "ryanlayerlab/layer_lab_chco/savvy_to_bed"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_chco", "sagc-bioinformatics/modules"]}, {"nb_reuse": 3, "tools": ["GATK", "sabre", "BEDTools"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess combine_callers{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/AllCNVCallers/\", mode: params.publish_dir_mode\n\n    input:\n    file(savvy_beds)\n    tuple caller, idPatient, idSample, file(cnvkit_bed)\n                                      \n\n    output:\n    tuple idSample, file(\"${idSample}.multi_caller.bed\"), file(\"${idSample}.log\")\n\n    script:\n    \"\"\"\n    touch ${idSample}.log\n    if [[ \"$savvy_beds\" != \"not_used\"  ]];\n    then    \n        FILE=\"${idSample}.coverageBinner_savvy.bed\"\n        if test -f \"\\$FILE\"; then\n            cat ${idSample}.coverageBinner_savvy.bed >> temp.bed\n        else\n            echo \"# No calls from Savvy for $idSample\" >> ${idSample}.log\n        fi\n    fi \n\n    if [ \"$caller\" != \"cnvkit_not_used\" ];\n    then\n        if [[ \\$(wc -l <$cnvkit_bed) -ge 2 ]]\n        then\n            echo \"\"\n        else\n            echo \"# No calls from CNVKit for $idSample\" >> ${idSample}.log    \n        fi\n        echo \"Adding CNVKit bed\"\n        cat $cnvkit_bed >> temp.bed\n    else\n        echo \"Skipping CNVKit bed addition\"\n    fi\n\n    if test -f \"temp.bed\"; then\n        #cnvkit_file=\\$(find )\n\n        grep -v BND temp.bed | awk '(\\$2 <= \\$3)' >  filtered_temp.bed\n        cat filtered_temp.bed | sort -k1,1V -k2,2n -k3,3n > tripple_sorted.bed\n        bedtools cluster -i tripple_sorted.bed > clustered_test.bed\n        agg_cluster.py clustered_test.bed > ${idSample}.multi_caller.bed\n    else\n        touch ${idSample}.multi_caller.bed\n    fi\n    \"\"\"\n}", "\nprocess PlotModeledSegments {\n    label 'cpus_8'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(\"${idSample}.modelFinal.seg\"), file(\"${idSample}.denoisedCR.tsv\")\n        file(dict)\n    output:\n    file(out_dir)\n    \n    when: 'gatkcnv' in tools\n    script:\n    out_dir = \"PlotsModeledSegments\"\n    \n    \"\"\"\n    init.sh\n    mkdir $out_dir\n    gatk PlotModeledSegments \\\n        --denoised-copy-ratios ${idSample}.denoisedCR.tsv \\\n        --segments ${idSample}.modelFinal.seg \\\n        --sequence-dictionary ${dict} \\\n        --output-prefix ${idSample} \\\n        -O $out_dir\n    \"\"\"\n}", "process sabre {\n\n    tag { \"Sabre\" }\n    publishDir \"${outdir}/sabre\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    tuple val(sample_id), file(reads)\n    val barcodes\n    val outdir\n    val opt_args\n\n    output:\n    file \"${sample_id}_R1.fastq.gz\", emit: R1\n    file \"${sample_id}_R2.fastq.gz\", emit: R2\n    file \"*unmatched-barcodes*\"\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    sabre pe \\\n        -f ${reads[0]} \\\n        -r ${reads[1]} \\\n        -b ${barcodes} \\\n        -u unmatched-barcodes_1.fastq \\\n        -w unmatched-barcodes_2.fastq\n\n    gzip *.fastq\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/combine_callers", "javaidm/layer_lab_vc/PlotModeledSegments", "sagc-bioinformatics/modules/sabre"], "list_wf_names": ["javaidm/layer_lab_vc", "sagc-bioinformatics/modules", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "HISAT2", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_chco", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess CallCopyRatioSegments {\n   label 'cpus_8'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}/CalledCopyRatioSegments\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(\"${idSample}.cr.seg\")\n    \n    output:\n        file(\"${idSample}.called.seg\")\n\n    when: 'gatkcnv' in tools\n    script:\n    \n    \"\"\"\n    init.sh\n    gatk CallCopyRatioSegments \\\n        -I ${idSample}.cr.seg \\\n        -O ${idSample}.called.seg\n    \"\"\"\n}", "\nprocess HISAT2 {\n    tag { sample_id + \" - Hisat2 align\" }\n\n    memory '20 GB'\n\n    publishDir \"${params.outdir}/Hisat2\", mode: 'copy'\n    stageInMode 'copy'                                               \n                                    \n\n    input:\n    path hisat2_idx_dir\n    tuple sample_id, file(read1), file(read2)\n\n    output:\n    tuple sample_id,\n        file(\"${sample_id}.hisat2.bam\"),\n        file(\"${sample_id}.hisat2.bam.bai\")\n\n    script:\n    \"\"\"\n    INDEX=`find -L ${hisat2_idx_dir} -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n\n    # hisat2 alignment\n    hisat2 -p ${task.cpus} \\\\\n         --rg-id \"\\\"@RG\\\\tID:${sample_id}\\\\tSM:${sample_id}\\\\tPL:ILLUMINA\\\\tLB:${sample_id}\\\\tPU:LIB1\\\"\" \\\\\n         -x \\${INDEX} -1 ${read1} -2 ${read2} | \\\\\n            samtools sort --threads ${task.cpus} -m 2G - > ${sample_id}.hisat2.bam\n    \n    # index with sambamba\n    samtools index -@ ${task.cpus} ${sample_id}.hisat2.bam\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/HaplotypeCaller", "javaidm/layer_lab_vc/CallCopyRatioSegments", "sagc-bioinformatics/modules/HISAT2"], "list_wf_names": ["javaidm/layer_lab_vc", "sagc-bioinformatics/modules", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 3, "tools": ["SAMtools", "MultiQC", "GATK"], "nb_own": 3, "list_own": ["sagc-bioinformatics", "ryanlayerlab", "javaidm"], "nb_wf": 3, "list_wf": ["layer_lab_vc", "layer_lab_caw", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "MSBradshaw", "a-lud"], "nb_contrib": 6, "codes": ["process umiDedup {\n    \n    tag { \"UmiToolsDedup - ${sample_id}\" }\n    publishDir \"${params.outdir}/umi_dedup\", mode: 'copy'\n    label 'process_medium'\n\n    input:\n    tuple val(sample_id),\n        file(bam),\n        file(bai)\n    val outdir\n    val opt_args\n\n    output:\n    path \"${sample_id}.umidup.bam\", emit: bam\n    path \"${sample_id}.umidup.bam.bai\", emit: bai\n    file \"*.{log,err,tsv}\"\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    umi_tools dedup \\\n    -I ${bam} \\\n    -L ${sample_id}.log \\\n    -E ${sample_id}.err \\\n    -S ${sample_id}.umidup.bam \\\n    --umi-separator=\":\" \\\n    --temp-dir=\\${PWD} \\\n    --paired \\\n    --output-stats=${sample_id}\n\n    samtools index ${sample_id}.umidup.bam\n    \"\"\"\n}", "\nprocess CallCopyRatioSegments {\n    label 'container_llab'\n   label 'cpus_8'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}/CalledCopyRatioSegments\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(\"${idSample}.cr.seg\")\n    \n    output:\n        file(\"${idSample}.called.seg\")\n\n    when: ('gatk_cnv_somatic' in tools)\n    script:\n    \n    \"\"\"\n    init.sh\n    gatk CallCopyRatioSegments \\\n        -I ${idSample}.cr.seg \\\n        -O ${idSample}.called.seg\n    \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n    input:\n        file (multiqcConfig) \n        file (versions) \n        file ('bamQC/*') \n        file ('FastQC/*') \n        file ('BCFToolsStats/*') \n        file ('VCFTools/*')\n                 'MarkDuplicates/*'  \n        file ('SamToolsStats/*') \n        file ('CollectAlignmentSummary/*')\n        file ('CollectInsertSizeMetrics/*')\n        file ('CollectHsMetrics/*')\n                 'snpEff/*'  \n\n    output:\n        set file(\"*multiqc_report.html\"), file(\"*multiqc_data\") \n\n    when: !('multiqc' in skipQC)\n\n    script:\n    \"\"\"\n    multiqc -f -v .\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/umiDedup", "ryanlayerlab/layer_lab_caw/CallCopyRatioSegments", "javaidm/layer_lab_vc/MultiQC"], "list_wf_names": ["javaidm/layer_lab_vc", "ryanlayerlab/layer_lab_caw", "sagc-bioinformatics/modules"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess gatk_PreprocessIntervals{\n    publishDir \"${params.outdir}/GATK_CNV\",\n    mode: params.publish_dir_mode\n\n    when 'gatk_cnv' in tools\n\n    input:\n    file (fasta)\n    file (fastai) \n    file (fastadict)\n    file (gatk_cnv_contig_list)\n\n    output:\n    file(\"PreprocessIntervals.interval_list\")\n\t\n    \"\"\"\n\tgatk PreprocessIntervals \\\n\t\t\t-R $fasta \\\n\t\t\t--padding 0 \\\n\t\t\t-L $gatk_cnv_contig_list \\\n\t\t\t-imr OVERLAPPING_ONLY \\\n\t\t\t-O PreprocessIntervals.interval_list\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/gatk_PreprocessIntervals"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess gatk_CollectReadCounts {\n    tag {idPatient + \"-\" + idSample}\n\n    when 'gatk_cnv' in tools\n\n    publishDir \"${params.outdir}/GATK_CNV/${idSample}/CollectReadCounts/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(interval_list)\n\t\tfile (fasta)\n\t    file (fastai)\n\t    file (fastadict)\n\n    output:\n        tuple idPatient, idSample, file(\"gatk_${idSample}_CollectReadCounts.tsv\")\n\n    \"\"\"\n\tgatk CollectReadCounts \\\n\t    -L $interval_list \\\n\t    -R $fasta \\\n\t    -imr OVERLAPPING_ONLY \\\n\t    -I $bam \\\n\t    --format TSV \\\n\t    -O gatk_${idSample}_CollectReadCounts.tsv\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/gatk_CollectReadCounts"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess gatk_AnnotateIntervals {\n    tag {idPatient + \"-\" + idSample}\n\n    when 'gatk_cnv' in tools\n\n    publishDir \"${params.outdir}/GATK_CNV/\", mode: params.publish_dir_mode\n\n    input:\n        file(interval_list)\n\tfile (fasta)\n\tfile (fastai)\n\tfile (fastadict)\n\n    output:\n        file(\"AnnotateIntervals.annotated.tsv\")\n\n    \"\"\"\n    gatk AnnotateIntervals \\\n            -L $interval_list \\\n            -R $fasta \\\n            -imr OVERLAPPING_ONLY \\\n            -O AnnotateIntervals.annotated.tsv\n\t\"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/gatk_AnnotateIntervals"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess gatk_FilterIntervals {\n    tag {idPatient + \"-\" + idSample}\n\n    when 'gatk_cnv' in tools\n\n    publishDir \"${params.outdir}/GATK_CNV/\", mode: params.publish_dir_mode\n\n    input:\n        file(vcFiles)\n        file(interval_list)\n        file(annotated_interval_list)\n\n    output:\n        file(\"FilterIntervals.cohort.gc.filtered.interval_list\")\n\n    \"\"\"\n    input_files=\"\"\n    for file in ${vcFiles}; do\n      input_files+=\"-I \\${file} \"\n    done\n\n    echo \\$input_files\n    gatk FilterIntervals \\\n        -L $interval_list \\\n        --annotated-intervals $annotated_interval_list \\\n        \\$input_files \\\n        -imr OVERLAPPING_ONLY \\\n        -O FilterIntervals.cohort.gc.filtered.interval_list\n    \"\"\"\n\n}"], "list_proc": ["javaidm/layer_lab_vc/gatk_FilterIntervals"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess gatk_DetermineGermlineContigPloidy {\n    tag {idPatient + \"-\" + idSample}\n\n    when 'gatk_cnv' in tools\n\n    publishDir \"${params.outdir}/GATK_CNV/\", mode: params.publish_dir_mode\n\n    input:\n        file(vcFiles)\n        file(cohort_filtered_interval_list)\n        file(annotated_interval_list)\n        file(gatk_cnv_contig_ploidy_priors)\n\n    output:\n    file(\"ploidy_calls_models_germlineCNV.tar.gz\")\n\n    \"\"\"\n    input_files=\"\"\n    for file in ${vcFiles}; do\n      input_files+=\"-I \\${file} \"\n    done\n\n    echo \\$input_files\n    gatk DetermineGermlineContigPloidy \\\n        -L $cohort_filtered_interval_list \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        \\$input_files \\\n        --contig-ploidy-priors $gatk_cnv_contig_ploidy_priors \\\n        --output . \\\n        --output-prefix ploidy \\\n        --verbosity DEBUG\n\n    echo \\$input_files\n\n    gatk GermlineCNVCaller \\\n        --run-mode COHORT \\\n        -L $cohort_filtered_interval_list \\\n        \\$input_files \\\n        --contig-ploidy-calls ploidy-calls \\\n        --annotated-intervals $annotated_interval_list \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        --output cohort \\\n        --output-prefix cohort\\\n        --verbosity DEBUG\n\n    tar -czf ploidy_calls_models_germlineCNV.tar.gz ploidy-calls/ ploidy-model/ cohort/\n    \"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/gatk_DetermineGermlineContigPloidy"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["layer_lab_vc"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess gatk_PostprocessGermlineCNVCalls {\n    tag {idPatient + \"-\" + idSample}\n\n    when 'gatk_cnv' in tools\n\n    publishDir \"${params.outdir}/GATK_CNV/\", mode: params.publish_dir_mode\n\n    input:\n        file(vcFiles)\n        file(ploidy_calls_models_germlineCNV_tar_gz)\n        file(fastadict)\n\n    output:\n    file(\"PostprocessGermlineCNVResults\")\n\n    \"\"\"\n    tar -xf $ploidy_calls_models_germlineCNV_tar_gz\n    mkdir PostprocessGermlineCNVResults\n    \n    input_files=\"\"\n    count=0\n    for file in ${vcFiles}; do\n        result=\\$(echo \"\\$file\" | sed \"s/_CollectReadCounts.tsv//\")\n        outname=\"PostprocessGermlineCNVCalls_\\${result}.txt\"\n        ogi=\"genotyped-intervals_\\${result}.vcf.gz\"\n        ogs=\"genotyped-segments_\\${result}.vcf.gz\"\n        echo \\$outname\n        echo \\$ogi\n        echo \\$ogs\n        gatk PostprocessGermlineCNVCalls \\\n            --model-shard-path cohort/cohort-model \\\n            --calls-shard-path cohort/cohort-calls \\\n            --allosomal-contig X --allosomal-contig Y \\\n            --contig-ploidy-calls ploidy-calls \\\n            --sample-index \\$count \\\n            --output-genotyped-intervals \\$ogi \\\n            --output-genotyped-segments \\$ogs \\\n            --sequence-dictionary $fastadict \\\n            --output-denoised-copy-ratios \\$outname\n\n        ((count=count+1))\n    done\n    mv PostprocessGermlineCNVCalls_*.txt PostprocessGermlineCNVResults/\n    mv genotyped-segments_*.vcf.gz PostprocessGermlineCNVResults/     \n\n\t\"\"\"\n}"], "list_proc": ["javaidm/layer_lab_vc/gatk_PostprocessGermlineCNVCalls"], "list_wf_names": ["javaidm/layer_lab_vc"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["javaidm"], "nb_wf": 1, "list_wf": ["obsolete_layer_lab_caw"], "list_contrib": ["ryanlayer", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess RunMultiQC {\n    publishDir \"${OUT_DIR}/multiqc\", mode: 'copy'\n\n    input:\n    file (fastqc:'fastqc/*')\n    file ('gatk_base_recalibration/*')\n    file ('gatk_variant_eval/*')\n    \n    output:\n    file '*multiqc_report.html'\n    file '*_data'\n    file '.command.err'\n    val prefix\n\n    script:\n    prefix = fastqc[0].toString() - '_fastqc.html' - 'fastqc/'\n    rtitle = CUSTOM_RUN_NAME ? \"--title \\\"$CUSTOM_RUN_NAME\\\"\" : ''\n    rfilename = CUSTOM_RUN_NAME ? \"--filename \" + CUSTOM_RUN_NAME.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename  . 2>&1\n    \n    \"\"\"\n}"], "list_proc": ["javaidm/obsolete_layer_lab_caw/RunMultiQC"], "list_wf_names": ["javaidm/obsolete_layer_lab_caw"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["jbelyeu"], "nb_wf": 1, "list_wf": ["sv_calling_nf"], "list_contrib": ["jbelyeu"], "nb_contrib": 1, "codes": ["\nprocess smoove_call {\n    publishDir path: \"$outdir/smoove/called\", mode: \"copy\", pattern: \"*.vcf.gz*\"\n    publishDir path: \"$outdir/logs\", mode: \"copy\", pattern: \"*-stats.txt\"\n    publishDir path: \"$outdir/logs\", mode: \"copy\", pattern: \"*-smoove-call.log\"\n\n    input:\n    set sample, file(bam), file(bai) from call_bams\n    file fasta\n    file faidx\n\n    output:\n    file(\"${sample}-smoove.genotyped.vcf.gz\") into vcfs\n    file(\"${sample}-smoove.genotyped.vcf.gz.csi\") into idxs\n    file(\"${sample}-stats.txt\") into variant_counts\n    file(\"${sample}-smoove-call.log\") into sequence_counts\n\n    script:\n    \"\"\"\n    smoove call --genotype --name $sample --processes ${task.cpus} \\\n        --fasta $fasta \\\n        $bam 2> ${sample}-smoove-call.log\n    bcftools stats ${sample}-smoove.genotyped.vcf.gz > ${sample}-stats.txt\n    \"\"\"\n}"], "list_proc": ["jbelyeu/sv_calling_nf/smoove_call"], "list_wf_names": ["jbelyeu/sv_calling_nf"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["jbelyeu", "rastiks"], "nb_wf": 2, "list_wf": ["sv_calling_nf", "viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "jbelyeu", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess SORT_BAM {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/bam\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".flagstat\")) \"samtools_stats/$filename\"\n                      else if (filename.endsWith(\".idxstats\")) \"samtools_stats/$filename\"\n                      else if (filename.endsWith(\".stats\")) \"samtools_stats/$filename\"\n                      else (params.protocol != 'amplicon' && params.skip_markduplicates) || params.save_align_intermeds ? filename : null\n                }\n\n    when:\n    !params.skip_variants\n\n    input:\n    tuple val(sample), val(single_end), path(bam) from ch_bowtie2_bam\n\n    output:\n    tuple val(sample), val(single_end), path(\"*.sorted.{bam,bam.bai}\"), path(\"*.flagstat\") into ch_sort_bam\n    path \"*.{flagstat,idxstats,stats}\" into ch_sort_bam_flagstat_mqc\n\n    script:\n    \"\"\"\n    samtools sort -@ $task.cpus -o ${sample}.sorted.bam -T $sample $bam\n    samtools index ${sample}.sorted.bam\n    samtools flagstat ${sample}.sorted.bam > ${sample}.sorted.bam.flagstat\n    samtools idxstats ${sample}.sorted.bam > ${sample}.sorted.bam.idxstats\n    samtools stats ${sample}.sorted.bam > ${sample}.sorted.bam.stats\n    \"\"\"\n}", "\nprocess smoove_genotype {\n    publishDir path: \"$outdir/smoove/genotyped\", mode: \"copy\"\n\n    input:\n    set sample, file(bam), file(bai) from genotype_bams\n    file sites\n    file fasta\n    file faidx\n\n    output:\n    file(\"${sample}-smoove.genotyped.vcf.gz.csi\") into genotyped_idxs\n    file(\"${sample}-smoove.genotyped.vcf.gz\") into genotyped_vcfs\n\n    script:\n    \"\"\"\n    wget -q https://raw.githubusercontent.com/samtools/samtools/develop/misc/seq_cache_populate.pl\n    perl seq_cache_populate.pl -root \\$(pwd)/cache $fasta 1> /dev/null 2> err || (cat err; exit 2)\n    export REF_PATH=\\$(pwd)/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s\n    export REF_CACHE=xx\n\n    samtools quickcheck -v $bam\n    smoove genotype --duphold --processes ${task.cpus} --removepr --outdir ./ --name ${sample} --fasta $fasta --vcf $sites $bam\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/SORT_BAM", "jbelyeu/sv_calling_nf/smoove_genotype"], "list_wf_names": ["jbelyeu/sv_calling_nf", "rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["HINT"], "nb_own": 1, "list_own": ["jdurbin"], "nb_wf": 1, "list_wf": ["nf"], "list_contrib": ["jdurbin", "james-dtg"], "nb_contrib": 2, "codes": ["\nprocess hint {\n    cpus 1\n    memory '48 GB'\n    container \"kjdurbin/hint\"\n    \n    publishDir \"${params.outDir}\",\n\tmode: 'copy'\n    \n    input:\n    tuple sampleId,path(pairsfile),path(mcool),path(supportfiles) from hint_ch\n    \n    output: \n    path \"hintout_*\" into hintout_ch\n    \n    script:\n    \n    \"\"\" \n    hint tl -m \\\n    ${mcool}::/resolutions/1000000,${mcool}::/resolutions/100000 \\\n    -f cooler \\\n    --chimeric $pairsfile \\\n    --refdir ${supportfiles}/ref/hg38 \\\n    --backdir ${supportfiles}/matrix/hg38 \\\n    -g hg38 \\\n    -n ${sampleId} \\\n    -c 0.05 \\\n    --ppath /pairix/bin/pairix -p 12 \\\n    -e DpnII \\\n    -o hintout_${id}\n    \"\"\"\n}"], "list_proc": ["jdurbin/nf/hint"], "list_wf_names": ["jdurbin/nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["jdurbin"], "nb_wf": 1, "list_wf": ["nf"], "list_contrib": ["jdurbin", "james-dtg"], "nb_contrib": 2, "codes": ["\nprocess bwa_mem {\n    tag \"_${id}\"\n    cpus 12\n    memory '24 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    tuple id, file(R1s), file(R2s) from fastqs_ch\n\t.map{bs,file ->\n\t    pref = file.name.toString().take(file.name.toString().indexOf('_R'))\n\t    return(tuple(pref,file))\n\t}\n\t.groupTuple()\n\t.flatten()\n\t.collate(3)\n        .mix(fastqDir_ch)\n\t.mix(genewiz_ch)\n\n    tuple index, path(index_files) from bwa_index.first()                                                            \n    \n    output:\n    tuple id, path(\"*.bam\"), path(\"*.tsv\") into  pairtools_parse_ch\n    \n    script:\n    \"\"\"\n    bwa mem -5SP -t ${task.cpus} \\\n    \t${index} \\\n    \t<(zcat ${R1s}) \\\n    \t<(zcat ${R2s}) \\\n\t|samtools view -@ ${task.cpus} -Shb -o ${id}.bam - \\\n\t&& samtools view -H ${id}.bam | \\\n\tawk -v OFS='\\t' '/^@SQ/ && !(\\$2 ~ /:(chr|\"\")M/) {split(\\$2,chr,\":\");split(\\$3,ln,\":\");print chr[2],ln[2]}' | \\\n\tsort -V -k1,1 \\\n\t> chr_size.tsv\n    \"\"\"\n}"], "list_proc": ["jdurbin/nf/bwa_mem"], "list_wf_names": ["jdurbin/nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["jdurbin", "mbosio85"], "nb_wf": 2, "list_wf": ["ngs_variant_calling", "nf"], "list_contrib": ["jdurbin", "mbosio85", "james-dtg"], "nb_contrib": 3, "codes": ["\nprocess merge_bam {\n    tag \"_${id}\"\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    tuple id, path(bam_part) from bam_parts_ch\n\t.map {id, file ->\n\t    if ( id.contains(\"-rep\") ){\n\t\tdef key = id.replaceFirst(/(.*)-rep.*/,'$1')\n\t\treturn tuple(key, file)\n\t    } else {\n\t\treturn( tuple(id,file) )\n\t    }\n\t}\n\t.groupTuple()\n\n    output:\n    tuple id, path(\"*.bam\") into merged_bam_sort_ch\n\n    script:\n    bam_files = bam_part.sort()\n    if (bam_files.size() >1) {\n\t\"\"\"\n\tsamtools merge -@ ${task.cpus} ${id}_MB.bam ${bam_part}\n\t\"\"\"\n    } else {\n\t\"\"\"\n\tln -s ${bam_part} ${id}_MB.bam\n\t\"\"\"\n    }\n}", "\nprocess CNNFilterVariantTrances{\n    label 'cpus_8'\n    label 'CNN'\n    container 'broadinstitute/gatk:4.1.4.1'\n\n    tag {variantCaller + \"CNN_Filter-\" + idSample}\n\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/CNNFiltering/\", mode: params.publishDirMode\n                \n    input:\n        set idSample, file(vcf), file(vcfIndex) from concatCNNvcf\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(hapmap) from ch_hapmap\n        file(hapmapIndex) from ch_hapmapIndex\n        file(onekg) from ch_onekg\n        file(onekgIndex) from ch_onekgIndex\n        file(mills) from ch_mills\n        file(millsIndex) from ch_millsIndex\n\n    output:\n        file(\"HaplotypeCaller_${idSample}.CNN_filtered.vcf.gz\") into finalCNNvcf\n        file(\"HaplotypeCaller_${idSample}.CNN_filtered.vcf.gz.tbi\") into finalCNNvcfIndex\n\n   when: ('haplotypecaller' in tools )\n\n\n        script:\n        \"\"\"\n            gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -I ${vcf}\n\n        # from https://gatk.broadinstitute.org/hc/en-us/articles/360037227632-FilterVariantTranches\n        gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterVariantTranches \\\n        -V ${vcf} \\\n        --output HaplotypeCaller_${idSample}.CNN_filtered.vcf \\\n        -resource ${hapmap}   \\\n        -resource ${onekg}    \\\n        -resource ${mills}    \\\n        -info-key CNN_2D \\\n        --snp-tranche 99.9    \\\n        --indel-tranche 99.4  \\\n  \n        \n\n        bgzip HaplotypeCaller_${idSample}.CNN_filtered.vcf\n        tabix HaplotypeCaller_${idSample}.CNN_filtered.vcf.gz\n\n        \"\"\"\n\n\n}"], "list_proc": ["jdurbin/nf/merge_bam", "mbosio85/ngs_variant_calling/CNNFilterVariantTrances"], "list_wf_names": ["mbosio85/ngs_variant_calling", "jdurbin/nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Cutadapt"], "nb_own": 2, "list_own": ["jdurbin", "mbradyneeley"], "nb_wf": 2, "list_wf": ["RNASeq_Pipeline", "nf"], "list_contrib": ["jdurbin", "mbradyneeley", "james-dtg"], "nb_contrib": 3, "codes": ["\nprocess trimAdapt {\n\n    tag \"$sample_id\"\n\n    input:\n        tuple val(sample_id), path(fqs) from reads_ch\n\n    output:\n        tuple val(sample_id), path(\"*.fq.gz\") into fastqs_ch\n\n    script:\n        fq1 = fqs[0]\n        fq2 = fqs[1]\n\n        \"\"\"\n        cutadapt -j 24 -O 6 -m 20 \\\n            -a CTGTCTCTTATACACATCT \\\n            -A CTGTCTCTTATACACATCT \\\n            -o ${sample_id}.1.fq.gz -p ${sample_id}.2.fq.gz \\\n            $fq1 $fq2\n        \"\"\"\n}", "\nprocess bam_sort {\n    tag \"bam_sort_${id}\"\n    cpus 48\n    memory '150 GB'\n    container 'mblanche/bwa-samtools'\n    \n    publishDir \"${params.outDir}/bam\",\n\tmode: 'copy',\n\tpattern: \"${id}.bam\"\n        \n    input:\n    tuple id, path(bam) from merged_bam_sort_ch\n    \n    output:\n    tuple id, path(\"${id}.bam\"),path(\"${id}.bam.bai\") into bam_bigwig_ch\n\n    script:\n    \"\"\"\n    samtools sort -m 2G \\\n\t-@ ${task.cpus} \\\n\t-o ${id}.bam \\\n\t${bam} \n\n    samtools index -@${task.cpus} ${id}.bam\n    \"\"\"\n}"], "list_proc": ["mbradyneeley/RNASeq_Pipeline/trimAdapt", "jdurbin/nf/bam_sort"], "list_wf_names": ["mbradyneeley/RNASeq_Pipeline", "jdurbin/nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jdurbin"], "nb_wf": 1, "list_wf": ["nf"], "list_contrib": ["jdurbin", "james-dtg"], "nb_contrib": 2, "codes": ["\nprocess sortAndIndexBAM{\n    cpus = 8\n    memory = '20G'    // I'm requesting total memory for all 8 processes... or should this be per vcpu?\n    executor = 'awsbatch'\n    queue = 'nfq-VIP'\n    container '916851041342.dkr.ecr.us-west-2.amazonaws.com/pyselva'                                 \n    \n    input:\n    tuple val(bampath),val(bamID) from bam_ch\n    \n    output:\n    stdout out_ch\n    \"\"\"\n    samtools sort -m 2G -@ 8 ${bampath}/${bamID}.bam -T /mnt/ebs/tmpjd/${bamID} -o ${bampath}/${bamID}_sorted.bam\n    samtools index ${bampath}/${bamID}_sorted.bam\n    \"\"\"\n}"], "list_proc": ["jdurbin/nf/sortAndIndexBAM"], "list_wf_names": ["jdurbin/nf"]}, {"nb_reuse": 1, "tools": ["IBS"], "nb_own": 1, "list_own": ["jdurbin"], "nb_wf": 1, "list_wf": ["nf"], "list_contrib": ["jdurbin", "james-dtg"], "nb_contrib": 2, "codes": [" process get_bs_files {\n\tcpus 1\n\tmemory '1G'\n\tcontainer 'mblanche/basespace-cli'\n\t\n\tinput:\n\tval bs from biosample_ch\n\t\n\toutput:\n\tstdout into bs_id_ch\n\t\n\tscript:\n\t\"\"\"\n\tbs biosample content -n ${bs} -F Id -F FilePath -f csv | \\\n\t    awk 'BEGIN{OFS =\",\"} \\\n\t    NR == 1 {print \"biosample\", \\$0} \\\n\t    NR > 1  {print \"${bs}\", \\$0}'\n\t\"\"\"\n    }"], "list_proc": ["jdurbin/nf/get_bs_files"], "list_wf_names": ["jdurbin/nf"]}, {"nb_reuse": 1, "tools": ["IBS"], "nb_own": 1, "list_own": ["jdurbin"], "nb_wf": 1, "list_wf": ["nf"], "list_contrib": ["jdurbin", "james-dtg"], "nb_contrib": 2, "codes": [" process download_bs {\n\tlabel \"movers\"\n\tcpus 4\n\tmemory '4G'\n\tcontainer 'mblanche/basespace-cli'\n\tqueue 'moversQ'\n\t\n\tpublishDir \"${params.outDir}/fastqs\",\n\t    mode: 'copy'\n\t\n\tinput:\n\ttuple bs, val(oriFname), val(newFname), val(id) from bs_id_ch\n\t    .splitCsv(header: true)\n\t    .map { row -> tuple(row.FilePath,row.biosample, row.Id )}\n\t    .groupTuple()\n\t    .map{if (it[2].size() >1){\n\t\t    x = []\n\t\t    fname = it[0]\n\t\t    id = fname.take(fname.indexOf('_'))\n\t\t    suff = fname.substring(fname.indexOf('_')+1)\n\t\t    bs = it[1][0]\n\t\t    \n\t\t    for (i in (1..it[2].size())) {\n\t\t\tx.add([bs,fname,\"${id}_FC${i}_${suff}\",it[2][i-1]])\n\t\t    }\n\t\t    return(x)\n\t\t} else {\n\t\t    fname = it[0]\n\t\t    bs = it[1][0]\n\t\t    id = it[2]\n\t\t    return([bs,fname,fname,id])\n\t\t}\n\t    }\n\t    .flatten()\n\t    .collate(4)\n\t\n\toutput:\n\ttuple bs, file(\"*.fastq.gz\") into fastqs_ch\n\t\n\tscript:\n\t\"\"\"\n\tbs file download -i ${id} -o . \n\n\tif [ \"${oriFname}\" != \"${newFname}\" ];then \n\t    mv ${oriFname} ${newFname}\n\tfi\n\t\"\"\"\n    }"], "list_proc": ["jdurbin/nf/download_bs"], "list_wf_names": ["jdurbin/nf"]}, {"nb_reuse": 1, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["jeantristanb"], "nb_wf": 1, "list_wf": ["h3abionetimp_spezone"], "list_contrib": ["jeantristanb"], "nb_contrib": 1, "codes": [" process ConvertPosition{\n      input :\n         file(vcfI) from file_vcf_filter_1\n         file(convert) from file_conv_ch\n         file(fasta) from fastafile_ch\n       publishDir \"${params.output_dir}/crossmap_out/\", overwrite:true, mode:'copy'\n       output :\n          file(\"${headfinal}.tmp.vcf.unmap\")\n          file(vcffinal) into file_vcf_filter_2 \n       script :\n        headfinal=\"${params.output}_filt1_newpos\"\n        vcffinal=\"${params.output}_filt1_newpos.recode.vcf\"\n        \"\"\"\n        ${params.bin_crossmap}  vcf $convert $vcfI $fasta $headfinal\".tmp.vcf\"\n        vcftools --vcf $headfinal\".tmp.vcf\"  --chr ${params.chr} --recode --recode-INFO-all --out $headfinal\n        \"\"\"\n    }"], "list_proc": ["jeantristanb/h3abionetimp_spezone/ConvertPosition"], "list_wf_names": ["jeantristanb/h3abionetimp_spezone"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["jennomics"], "nb_wf": 1, "list_wf": ["parallel-blast"], "list_contrib": ["jennomics"], "nb_contrib": 1, "codes": ["\nprocess blast {\n    memory = params.max_memory\n    cpus = params.max_cpus\n                                                                     \n    db_file = params.db_name\n\n    input:\n    file 'query.fa' from chunks\n    path db_dir from file(params.db)\n    \n    output:\n    file \"blast.out\" into blast_results\n    \n    script:\n    \"\"\"\n    blastn -max_target_seqs 5 -num_threads 8 -db $db_dir/$db_file -query query.fa -outfmt 6 > blast.out\n    \"\"\"\n}"], "list_proc": ["jennomics/parallel-blast/blast"], "list_wf_names": ["jennomics/parallel-blast"]}, {"nb_reuse": 1, "tools": ["ColiCoords"], "nb_own": 1, "list_own": ["jerdra"], "nb_wf": 1, "list_wf": ["BOONStim"], "list_contrib": ["jerdra"], "nb_contrib": 1, "codes": ["\nprocess compute_weighted_centroid{\n\n    label 'rtms'\n\n    input:\n    tuple val(sub), path(vol)\n\n    output:\n    tuple val(sub), path(\"${sub}_ras_coord.txt\"), emit: coord\n\n    shell:\n    '''\n    #!/usr/bin/env python\n\n    import nibabel as nib\n    import numpy as np\n\n    #Load image\n    img = nib.load(\"!{vol}\")\n    affine = img.affine\n    data = img.get_data()\n\n    #Mask\n    x,y,z = np.where(data > 0)\n    coords = np.array([x,y,z])\n    vals = data[(x,y,z)]\n\n    #Compute\n    weighted_vox = np.dot(coords,vals)[:,np.newaxis]\n    r_weighted_vox = np.dot(affine[:3,:3],weighted_vox)\n    weighted_coord = r_weighted_vox + affine[:3,3:4]\n\n    #Save\n    np.savetxt(\"!{sub}_ras_coord.txt\",weighted_coord)\n    '''\n\n\n}"], "list_proc": ["jerdra/BOONStim/compute_weighted_centroid"], "list_wf_names": ["jerdra/BOONStim"]}, {"nb_reuse": 1, "tools": ["msms"], "nb_own": 1, "list_own": ["jerdra"], "nb_wf": 1, "list_wf": ["BOONStim"], "list_contrib": ["jerdra"], "nb_contrib": 1, "codes": ["\nprocess msm_sulc{\n\n    label 'connectome'\n\n    input:\n    tuple val(sub), val(hemi), path(sphere), path(sulc), val(structure)\n\n    output:\n    tuple val(sub), val(hemi), path(sphere), path(\"${sub}.${hemi}.sphere.reg_msm.surf.gii\")\n\n    shell:\n    '''\n    /msm/msm --inmesh=!{sphere} \\\n             --indata=!{sulc} \\\n             --refmesh=/atlas/fsaverage.!{hemi}_LR.spherical_std.164k_fs_LR.surf.gii \\\n             --refdata=/atlas/!{hemi}.refsulc.164k_fs_LR.shape.gii \\\n             --conf=/msm_conf/MSMSulcStrainFinalconf \\\n             --out=!{hemi}. \\\n             --verbose\n\n    mv \"!{hemi}.sphere.reg.surf.gii\" \\\n       \"!{sub}.!{hemi}.sphere.reg_msm.surf.gii\"\n\n    wb_command -set-structure !{sub}.!{hemi}.sphere.reg_msm.surf.gii \\\n                                !{structure}\n    '''\n\n}"], "list_proc": ["jerdra/BOONStim/msm_sulc"], "list_wf_names": ["jerdra/BOONStim"]}, {"nb_reuse": 1, "tools": ["SECA", "DBETH"], "nb_own": 1, "list_own": ["jerdra"], "nb_wf": 1, "list_wf": ["TIGR_PURR"], "list_contrib": ["slimnsour", "gabiherman", "jerdra"], "nb_contrib": 3, "codes": ["\nprocess fieldmaps {\n\n    module \"FSL/5.0.11\"\n\n    publishDir \"$params.out/${params.application}/$sub\", \\\n                mode: 'copy', \\\n                pattern:  \"magnitude.nii.gz\" , \\\n                saveAs: { echo1.getName().replace(\"$params.echo1\",\"MAG\") }\n\n    publishDir \"$params.out/${params.application}/$sub\", \\\n                mode: 'copy', \\\n                pattern:  \"fieldmap.nii.gz\" , \\\n                saveAs: { echo1.getName().replace(\"$params.echo1\",\"FIELDMAP\") }\n\n    publishDir \"$params.out/${params.application}/$sub\", \\\n                mode: 'copy', \\\n                pattern: \"json\", \\\n                saveAs: { echo1.getName().replace(\"$params.echo1\",\"FIELDMAP\").replace('.nii.gz','.json') }\n\n    input:\n    set val(sub), file(echo1), file(echo2) from resampled_fieldmaps\n\n    output:\n    set val(sub), file(\"fieldmap.nii.gz\"), file(\"magnitude.nii.gz\"), file(\"json\") into fieldmap_output\n\n\n    shell:\n    '''\n    #!/bin/bash\n\n\n    #Set up logging\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n\n    #Get processID\n    pid=$$\n    log_out=${logging_dir}/!{sub}_${pid}.out\n    log_err=${logging_dir}/!{sub}_${pid}.err\n\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    FM65=!{echo1}\n    FM85=!{echo2}\n\n    echo \"Using ECHO1 $FM65\" >> ${log_out}\n    echo \"Using ECHO2 $FM85\" >> ${log_out}\n\n    ####split (pre) fieldmap files and log\n    (\n    fslsplit ${FM65} split65 -t\n    bet split650000 65mag -R -f 0.5 -m\n    fslmaths split650002 -mas 65mag_mask 65realm\n    fslmaths split650003 -mas 65mag_mask 65imagm\n\n    fslsplit ${FM85} split85 -t\n    bet split850000 85mag -R -f 0.5 -m\n    fslmaths split850002 -mas 85mag_mask 85realm\n    fslmaths split850003 -mas 85mag_mask 85imagm\n\n    ####calc phase difference\n    fslmaths 65realm -mul 85realm realeq1\n    fslmaths 65imagm -mul 85imagm realeq2\n    fslmaths 65realm -mul 85imagm imageq1\n    fslmaths 85realm -mul 65imagm imageq2\n    fslmaths realeq1 -add realeq2 realvol\n    fslmaths imageq1 -sub imageq2 imagvol\n\n    ####create complex image and extract phase and magnitude\n    fslcomplex -complex realvol imagvol calcomplex\n    fslcomplex -realphase calcomplex phasevolume 0 1\n    fslcomplex -realabs calcomplex magnitude 0 1\n\n    ####unwrap phase\n    prelude -a 65mag -p phasevolume -m 65mag_mask -o phasevolume_maskUW\n\n    ####divide by TE diff in seconds -> radians/sec\n    fslmaths phasevolume_maskUW -div 0.002 fieldmap\n\n    ####copy in geometry information\n    fslcpgeom ${FM65} fieldmap.nii.gz -d\n    fslcpgeom ${FM65} magnitude.nii.gz -d\n    ) 2>> ${log_err} 1>> ${log_out}\n\n    ####make a JSON file containing the units\n    echo ' { \"Units\": \"rad/s\" } ' > json\n\n    '''\n}"], "list_proc": ["jerdra/TIGR_PURR/fieldmaps"], "list_wf_names": ["jerdra/TIGR_PURR"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jermth"], "nb_wf": 1, "list_wf": ["hls-workshop"], "list_contrib": ["jermth"], "nb_contrib": 1, "codes": ["\nprocess samToBam {\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    set sample_id, file(read1), file(read2) from read_pairs_bcftools_ch\n    file (\"${sample_id}.${params.prefix}.sam\") from bowtie2_sam_output\n    \n    output:\n    file (\"${sample_id}.${params.prefix}.bam\") into samtools_bam_output\n\n    \"\"\"\n    samtools view -bS ${sample_id}.${params.prefix}.sam | samtools sort > ${sample_id}.${params.prefix}.bam\n    \"\"\"\n\n}"], "list_proc": ["jermth/hls-workshop/samToBam"], "list_wf_names": ["jermth/hls-workshop"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["jermth"], "nb_wf": 1, "list_wf": ["hls-workshop"], "list_contrib": ["jermth"], "nb_contrib": 1, "codes": ["\nprocess bamToVCF {\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    set sample_id, file(read1), file(read2) from read_pairs_samtools_ch\n    file (\"${sample_id}.${params.prefix}.bam\") from samtools_bam_output\n    \n    output:\n    file (\"${sample_id}.${params.prefix}.vcf\") into bcftools_vcf_output\n\n    \"\"\"\n    bcftools mpileup --no-reference ${sample_id}.${params.prefix}.bam > ${sample_id}.${params.prefix}.vcf\n    \"\"\"\n\n}"], "list_proc": ["jermth/hls-workshop/bamToVCF"], "list_wf_names": ["jermth/hls-workshop"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["jfy133"], "nb_wf": 1, "list_wf": ["archaeodiet"], "list_contrib": ["jfy133"], "nb_contrib": 1, "codes": ["\nprocess BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0'\n    } else {\n        container 'quay.io/biocontainers/bowtie2:2.4.4--py36hd4290be_0'\n    }\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path '*.version.txt', emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $options.args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["jfy133/archaeodiet/BOWTIE2_BUILD"], "list_wf_names": ["jfy133/archaeodiet"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["jfy133"], "nb_wf": 1, "list_wf": ["archaeodiet"], "list_contrib": ["jfy133"], "nb_contrib": 1, "codes": ["\nprocess BOWTIE2_MAP {\n    tag \"${meta_reads.id}-${meta_ref.id}\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:$prefix, publish_by_meta:$prefix) }\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    }\n\n    input:\n    tuple val(meta_reads), path(reads), val(meta_ref), path(reference)\n\n    output:\n    tuple val(meta_reads), path(\"*.bam\"), emit: bam\n    tuple val(meta_reads), path('*.log'), emit: log\n    path \"*.version.txt\"          , emit: version\n\n    script:\n    def split_cpus = Math.floor(task.cpus/2)\n    def software = getSoftwareName(task.process)\n    def prefix   = \"${meta_reads.id}-${meta_ref.id}\"\n\n    meta_reads.reference = \"${meta_ref.id}\"\n    meta_reads.longname = \"${meta_reads.id}-${meta_ref.id}\"\n\n\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2//'`\n\n    ## by default unaligned reads only to keep BAMs small!\n    bowtie2 \\\\\n        -x \\$INDEX \\\\\n        -U $reads \\\\\n        --threads ${split_cpus} \\\\\n        $options.args \\\\\n        2> ${prefix}.bowtie2.log \\\\\n        | samtools view -@ ${split_cpus} -F 4 $options.args2 -bhS -o ${prefix}.bam -\n\n    echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["jfy133/archaeodiet/BOWTIE2_MAP"], "list_wf_names": ["jfy133/archaeodiet"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jfy133"], "nb_wf": 1, "list_wf": ["archaeodiet"], "list_contrib": ["jfy133"], "nb_contrib": 1, "codes": ["\nprocess PROFILEDAMAGE {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n                                                    \n                                                                                                 \n                                                                                                                                      \n                                                                                                                                              \n    conda (params.enable_conda ? \"bioconda::damageprofiler=1.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/https://depot.galaxyproject.org/singularity/damageprofiler:1.1--hdfd78af_2\"\n    } else {\n        container \"quay.io/biocontainers/quay.io/biocontainers/damageprofiler:1.1--hdfd78af_2\"\n    }\n\n    input:\n                                                                                                           \n                                                                                 \n                                                                                                                 \n                                                                                              \n                                                                                             \n                                                                                      \n    tuple val(meta), path(bam)\n\n    output:\n                                                                                  \n    tuple val(meta), path(\"*.bam\"), emit: bam\n                                                                         \n    path \"*.version.txt\"          , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n                                                                                                                      \n                                                                                                                               \n                                                                                                             \n                                                                                                                                            \n                                                                                                         \n                                                                                   \n                                                                                                 \n                                                                                                     \n    \"\"\"\n    samtools \\\\\n        sort \\\\\n        $options.args \\\\\n        -@ $task.cpus \\\\\n        -o ${prefix}.bam \\\\\n        -T $prefix \\\\\n        $bam\n\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["jfy133/archaeodiet/PROFILEDAMAGE"], "list_wf_names": ["jfy133/archaeodiet"]}, {"nb_reuse": 1, "tools": ["GUPPY"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess PplacerADCL {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n    output:\n        file 'adcl.csv.gz'\n    \n    \"\"\"\n    (echo name,adcl,weight && \n    guppy adcl --no-collapse ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > adcl.csv.gz\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/PplacerADCL"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GUPPY"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess PplacerPCA {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    errorStrategy = 'ignore'\n\n    input:\n        file refpkg_tgz_f\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'pca/epca.proj'\n        file 'pca/epca.xml'\n        file 'pca/epca.trans'\n        file 'pca/lpca.proj'\n        file 'pca/lpca.xml'\n        file 'pca/lpca.trans'\n    \n    \"\"\"\n    mkdir -p refpkg/ && mkdir -p pca/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/ &&\n    guppy epca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix epca &&\n    guppy lpca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix lpca\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/PplacerPCA"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GUPPY"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess PplacerKR {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file refpkg_tgz_f\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'kr_distance.csv.gz'\n\n    \n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy kr --list-out -c refpkg/ ${dedup_jplace_f}:${sv_map_f} |\n    gzip > kr_distance.csv.gz\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/PplacerKR"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GUPPY"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess ClassifySV {\n    container = \"${container__pplacer}\"\n    label = 'mem_veryhigh'\n    afterScript \"rm -r refpkg/\"\n    cache = false\n\n    input:\n        file refpkg_tgz_f\n        file classify_db_prepped\n        file dedup_jplace_f\n        file sv_refpkg_aln_sto_f\n    \n    output:\n        file 'classify.classified.db'\n\n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy classify --pp \\\n    --classifier ${params.pp_classifer} \\\n    -j ${task.cpus} \\\n    -c refpkg/ \\\n    --nbc-sequences ${sv_refpkg_aln_sto_f} \\\n    --sqlite ${classify_db_prepped} \\\n    --seed ${params.pp_seed} \\\n    --cutoff ${params.pp_likelihood_cutoff} \\\n    --bayes-cutoff ${params.pp_bayes_cutoff} \\\n    --multiclass-min ${params.pp_multiclass_min} \\\n    --bootstrap-cutoff ${params.pp_bootstrap_cutoff} \\\n    --bootstrap-extension-cutoff ${params.pp_bootstrap_extension_cutoff} \\\n    --word-length ${params.pp_nbc_word_length} \\\n    --nbc-rank ${params.pp_nbc_target_rank} \\\n    --n-boot ${params.pp_nbc_boot} \\\n    ${dedup_jplace_f}\n    cp ${classify_db_prepped} classify.classified.db\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/ClassifySV"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["Count"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess SharetableToMapWeight {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path (sharetable)\n\n    output:\n        path (\"sv_sp_map.csv\"), emit: sv_map\n        path (\"sv_weights.csv\"), emit: sv_weights\n        path (\"sp_sv_long.csv\"), emit: sp_sv_long\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nsp_count = {}\nwith open('${sharetable}', 'rt') as st_h:\n    st_r = csv.reader(st_h, delimiter='\\\\t')\n    header = next(st_r)\n    sv_name = header[3:]\n    for r in st_r:\n        sp_count[r[0]] = [int(c) for c in r[3:]]\nweightsL = []\nmapL = []\nsv_long = []\nfor sv_i, sv in enumerate(sv_name):\n    sv_counts = [\n        (sp, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]\n    if len(sv_counts) == 0:\n        continue\n    # Implicit else\n    shared_sv = \"{}:{}\".format(sv, sorted(sv_counts, key=lambda v: -1*v[1])[0][0])\n    sv_long += [\n        (sp, shared_sv, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]    \n    weightsL += [\n        (shared_sv, \"{}:{}\".format(sv, sp), c)\n        for sp, c in \n        sv_counts\n    ]\n    mapL += [\n        (\"{}:{}\".format(sv, sp), sp)\n        for sp, c in \n        sv_counts\n    ]\nwith open(\"sv_sp_map.csv\", \"w\") as map_h:\n    map_w = csv.writer(map_h)\n    map_w.writerows(mapL)\nwith open(\"sv_weights.csv\", \"w\") as weights_h:\n    weights_w = csv.writer(weights_h)\n    weights_w.writerows(weightsL)\nwith open(\"sp_sv_long.csv\", 'wt') as svl_h:\n    svl_w = csv.writer(svl_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))\n    svl_w.writerows(sv_long)\n\"\"\"\n}"], "list_proc": ["jgolob/maliampi/SharetableToMapWeight"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["EPA-ng"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess EPAngPlacement {\n    container = \"${container__epang}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    input:\n        file refpkg_aln_fasta\n        file combined_aln_fasta\n        file model\n        file ref_tree\n\n    output:\n        file 'dedup.jplace'\n    \"\"\"\n    set -e\n\n    epa-ng --split ${refpkg_aln_fasta} ${combined_aln_fasta}\n    model=`cat ${model}`\n    \n    epa-ng -t ${ref_tree} \\\n    -s reference.fasta -q query.fasta \\\n    -m \\$model -T ${task.cpus} \\\n    --baseball-heur\n\n    mv epa_result.jplace dedup.jplace\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/EPAngPlacement"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["RANKS"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess MakeEPAngTaxonomy {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/refpkg\", mode: 'copy'\n\n    input:\n        path leaf_info_f\n        path taxonomy_f\n\n    output:\n        path 'epang_taxon_file.tsv'\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\ntax_dict = {\n    r['tax_id']: r for r in\n    csv.DictReader(open('${taxonomy_f}', 'rt'))\n}\ntax_names = {\n    tax_id: r['tax_name']\n    for tax_id, r in tax_dict.items()\n}\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nwith open('epang_taxon_file.tsv', 'wt') as tf_h:\n    tf_w = csv.writer(tf_h, delimiter='\\\\t')\n    for row in csv.DictReader(open('${leaf_info_f}', 'rt')):\n        tax_id = row.get('tax_id', None)\n        if tax_id is None:\n            continue\n        # Implicit else\n        tax_lineage = tax_dict.get(tax_id, None)\n        if tax_lineage is None:\n            continue\n        lineage_str = \";\".join([\n            tax_names.get(tax_lineage.get(rank, \"\"), \"\")\n            for rank in RANKS\n        ])\n        tf_w.writerow([row['seqname'], lineage_str])\n\n\"\"\"\n\n}"], "list_proc": ["jgolob/maliampi/MakeEPAngTaxonomy"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GAPPA"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess Gappa_Classify {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path dedup_jplace\n        path taxon_file\n    \n    output:\n        path 'per_query.tsv'\n\n    \"\"\"\n    set -e\n\n\n    gappa examine assign \\\n    --per-query-results \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --jplace-path ${dedup_jplace} \\\n    --taxon-file ${taxon_file} \\\n    \n    \"\"\"\n\n}"], "list_proc": ["jgolob/maliampi/Gappa_Classify"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["FractBias"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess Make_Wide_Tax_Table {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n                            \n\n    input:\n        path sv_long\n        path sv_taxonomy\n        val want_rank\n\n    output:\n        path \"tables/taxon_wide_ra.${want_rank}.csv\", emit: ra\n        path \"tables/taxon_wide_nreads.${want_rank}.csv\", emit: nreads\n\n\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\nimport os\n\ntry:\n    os.makedirs('tables')\nexcept:\n    pass\n\nsv_long = pd.read_csv(\"${sv_long}\").rename({\n    'count': 'nreads'\n}, axis=1)\n# Add in rel abund\nfor sp, sp_sv in sv_long.groupby('specimen'):\n    sv_long.loc[sp_sv.index, 'fract'] = sp_sv.nreads / sp_sv.nreads.sum()\n\nsv_taxonomy = pd.read_csv('${sv_taxonomy}')\nsv_long_tax = pd.merge(\n    sv_long,\n    sv_taxonomy[sv_taxonomy.want_rank == '${want_rank}'],\n    on='sv',\n    how='left'\n)\n\nsp_tax = sv_long_tax.groupby(['specimen', 'tax_name']).sum().reset_index()[[\n    'specimen',\n    'tax_name',\n    'nreads',\n    'fract'\n]]\n\nsp_tax_wide_ra = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='fract'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_ra = sp_tax_wide_ra[sp_tax_wide_ra.mean().sort_values(ascending=False).index]\n\nsp_tax_wide_ra.to_csv(\"tables/taxon_wide_ra.${want_rank}.csv\")\n\nsp_tax_wide_nreads = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='nreads'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_nreads = sp_tax_wide_nreads[sp_tax_wide_ra.mean().sort_values(ascending=False).index].astype(int)\nsp_tax_wide_nreads.to_csv(\"tables/taxon_wide_nreads.${want_rank}.csv\")\n\n\"\"\"\n}"], "list_proc": ["jgolob/maliampi/Make_Wide_Tax_Table"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["RANKS", "LineagePulse"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess Gappa_Extract_Taxonomy {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n                            \n\n    input:\n        path gappa_taxonomy\n        path refpkg_taxtable\n\n    output:\n        path \"sv_taxonomy.csv\"\n        path refpkg_taxtable\n\n\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\n\nMIN_AFRACT = 0\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nRANK_DEPTH = {\n    i+1: r for (i, r) in enumerate(RANKS)\n}\nrefpkg_taxtable = pd.read_csv(\"${refpkg_taxtable}\")\ntax_name_to_id = {\n    row.tax_name: row.tax_id for\n    idx, row in refpkg_taxtable.iterrows()\n}\nepa_tax = pd.read_csv('${gappa_taxonomy}', sep='\\t')\nepa_tax['lineage']=epa_tax.taxopath.apply(lambda tp: tp.split(';'))\nepa_tax['rank_depth']=epa_tax.lineage.apply(len)\nsv_tax_list = []\nfor sv, sv_c in epa_tax[epa_tax.taxopath != 'DISTANT'].groupby('name'):\n    sv_tax = pd.DataFrame()\n    rank = None\n    tax_name = None\n    lineage = None\n    afract = None    \n    for rank_depth, want_rank in RANK_DEPTH.items():\n        sv_depth = sv_c[sv_c.rank_depth == rank_depth]\n        if len(sv_depth) > 0 and sv_depth.afract.sum() >= MIN_AFRACT:\n            # Something at this depth, and the cumulative fract likelihood is above our threshold\n            rank = want_rank\n            tax_name = \" / \".join(sv_depth.lineage.apply(lambda L: L[-1]))\n            ncbi_tax_id = \",\".join([str(tax_name_to_id.get(tn, -1)) for tn in sv_depth.lineage.apply(lambda L: L[-1])])\n            lineage = \";\".join(sv_depth.lineage.iloc[0][:-1] + [tax_name])\n            afract = sv_depth.afract.sum()\n            \n            \n        sv_tax.loc[rank, 'sv'] = sv\n        sv_tax.loc[rank, 'want_rank'] = want_rank\n        sv_tax.loc[rank, 'rank'] = rank\n        sv_tax.loc[rank, 'rank_depth'] = rank_depth\n        sv_tax.loc[rank, 'tax_name'] = tax_name\n        sv_tax.loc[rank, 'ncbi_tax_id'] = ncbi_tax_id\n        sv_tax.loc[rank, 'lineage'] = lineage\n        sv_tax.loc[rank, 'afract'] = afract\n        sv_tax.loc[rank, 'ambiguous'] = len(sv_depth) != 1\n    sv_tax_list.append(sv_tax)\n\n\nsv_taxonomy = pd.concat(sv_tax_list, ignore_index=True)\nsv_taxonomy['rank_depth'] = sv_taxonomy.rank_depth.astype(int)\nsv_taxonomy.to_csv('sv_taxonomy.csv', index=None)\n\n\"\"\"\n}"], "list_proc": ["jgolob/maliampi/Gappa_Extract_Taxonomy"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GAPPA"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess GappaSplit {\n    container = \"${container__gappa}\"\n    label = 'multithread'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n\n    input:\n        path dedup_jplace\n        path split_csv\n    \n    output:\n        path 'specimen_jplace/*.jplace.gz'\n\n    \"\"\"\n    set -e\n\n    mkdir specimen_jplace\n\n    gappa edit split \\\n    --jplace-path ${dedup_jplace} \\\n    --split-file ${split_csv} \\\n    --compress \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --out-dir specimen_jplace\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/GappaSplit"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GAPPA"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess Gappa_KRD {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path specimen_jplace\n    \n    output:\n        path 'krd/krd_matrix.csv.gz'\n\n    \"\"\"\n    set -e\n\n    gappa analyze krd \\\n    --jplace-path ${specimen_jplace} \\\n    --krd-out-dir krd/ \\\n    --krd-compress \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/Gappa_KRD"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["GAPPA"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess Gappa_ePCA {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path specimen_jplace\n    \n    output:\n        path 'ePCA/projection.csv'\n        path 'ePCA/transformation.csv'\n\n    \"\"\"\n    set -e\n\n    gappa analyze edgepca \\\n    --jplace-path ${specimen_jplace} \\\n    --out-dir ePCA/ \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    ls -l ePCA\n\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/Gappa_ePCA"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["Count"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess WeightMaptoLong {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path (weight)\n        path (map)\n\n    output:\n        path (\"sp_sv_long.csv\")\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nspecimen_comSV = {\n    r[0]: r[1]\n    for r in \n    csv.reader(\n        open('${map}', 'rt')\n    )\n}\nwith open('${weight}', 'rt') as w_h, open(\"sp_sv_long.csv\", 'wt') as sv_long_h:\n    w_r = csv.reader(w_h)\n    svl_w = csv.writer(sv_long_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))    \n    for row in w_r:\n        svl_w.writerow((\n            specimen_comSV[row[1]],\n            row[0],\n            int(row[2])\n        ))\n    \n\"\"\"\n}"], "list_proc": ["jgolob/maliampi/WeightMaptoLong"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["merger"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess dada2_merge {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(\"R1.dada2.rds\"), file(\"R2.dada2.rds\"), file(\"R1.derep.rds\"), file(\"R2.derep.rds\")\n\n    output:\n        tuple val(batch), val(specimen), file(\"${specimen}.dada2.merged.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dada_1 <- readRDS('R1.dada2.rds');\n    derep_1 <- readRDS('R1.derep.rds');\n    dada_2 <- readRDS('R2.dada2.rds');\n    derep_2 <- readRDS('R2.derep.rds');        \n    merger <- mergePairs(\n        dada_1, derep_1,\n        dada_2, derep_2,\n        verbose=TRUE,\n        trimOverhang=TRUE,\n        maxMismatch=${params.maxMismatch},\n        minOverlap=${params.minOverlap}\n    );\n    saveRDS(merger, \"${specimen}.dada2.merged.rds\");\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/dada2_merge"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["pcaMethods"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess dada2_remove_bimera {\n    container \"${container__dada2}\"\n    label 'mem_veryhigh'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n\n    input:\n        file(combined_seqtab)\n\n    output:\n        file(\"dada2.combined.seqtabs.nochimera.csv\")\n        file(\"dada2.combined.seqtabs.nochimera.rds\")\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    seqtab <- readRDS('${combined_seqtab}');\n    seqtab_nochim <- removeBimeraDenovo(\n        seqtab,\n        method = '${params.chimera_method}',\n        multithread = ${task.cpus}\n    );\n    saveRDS(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.rds'); \n    write.csv(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.csv', na='');\n    print((sum(seqtab) - sum(seqtab_nochim)) / sum(seqtab));\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/dada2_remove_bimera"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["VSEARCH"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess RefpkgSearchRepo {\n    container \"${container__vsearch}\"\n    label = 'multithread'\n\n    input:\n        path(sv_fasta_f)\n        path(repo_fasta)\n    \n    output:\n        path \"${repo_fasta}.repo.recruits.fasta\", emit: recruits\n        path \"${repo_fasta}.uc\", emit: uc\n        path \"${repo_fasta}.sv.nohit.fasta\", emit: nohits\n        path \"${repo_fasta}.vsearch.log\", emit: log\n        \n\n    \"\"\"\n    vsearch \\\n    --threads=${task.cpus} \\\n    --usearch_global ${sv_fasta_f} \\\n    --db ${repo_fasta} \\\n    --id=${params.repo_min_id} \\\n    --strand both \\\n    --uc=${repo_fasta}.uc --uc_allhits \\\n    --notmatched=${repo_fasta}.sv.nohit.fasta \\\n    --dbmatched=${repo_fasta}.repo.recruits.fasta \\\n    --maxaccepts=${params.repo_max_accepts} \\\n    | tee -a ${repo_fasta}.vsearch.log\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/RefpkgSearchRepo"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["LineagePulse"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess CombinedRefFilter {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        path(repo_recruit_f)\n        path(repo_recruit_uc)\n        path(taxdb)\n        path(seq_info)\n\n    output:\n        path \"references.fasta\", emit: recruit_seq\n        path \"references_seq_info.csv\", emit: recruit_si\n\n\n\"\"\"\n#!/usr/bin/env python\nimport fastalite\nimport csv\nimport sqlite3\nfrom collections import defaultdict\n\ndef get_lineage(tax_id, cursor):\n    cur_tax_id = tax_id\n    lineage = [cur_tax_id]\n    while cur_tax_id != '1':\n        cur_tax_id = cursor.execute(\"SELECT parent_id FROM nodes where tax_id=?\", (cur_tax_id,)).fetchone()[0]\n        lineage.append(cur_tax_id)\n    return lineage\n\n# Get the seq <-> ID linkage and ID <-> seq linkage\nseq_ids = defaultdict(set)\nid_seq = {}\nwith open('${repo_recruit_f}', 'rt') as recruit_h:\n    for sr in fastalite.fastalite(recruit_h):\n        seq_ids[sr.seq].add(sr.id)\n        id_seq[sr.id] = sr.seq\n\nall_refs = set(id_seq.keys())\n\n### Start with the UC of alignment SV <-> Repo\nwith open('${repo_recruit_uc}', 'rt') as in_uc:\n    uc_data = [\n        (\n            row[8], # SV\n            row[9], # Reference_id\n            float(row[3]) # Pct ID\n        )\n        for row in \n        csv.reader(in_uc, delimiter='\\\\t')\n        if row[3] != '*' and row[9] in all_refs\n    ]\nsv_max_pctid = defaultdict(float)\n# Figure out the best-pct-id for each SV\nfor sv, ref, pctid in uc_data:\n    sv_max_pctid[sv] = max([sv_max_pctid[sv], pctid])\n\n\n\n# Load in seq_info\nwith open('${seq_info}', 'rt') as sif:\n    si_r = csv.DictReader(sif)\n    seq_info = {\n        r['seqname']: r\n        for r in si_r\n    }\n\n# For each reference sequence, pick a *representitive* seq_id\ntax_db = sqlite3.connect('${taxdb}')\ntax_db_cur = tax_db.cursor()\n\nseq_rep_id = {}\nfor seq, ids in seq_ids.items():\n    if len(ids) == 1:\n        # If there is only one ID for a sequence it automatically passes!\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    tax_ids = {seq_info[i]['tax_id']: i for i in ids if i in seq_info}\n    if len(tax_ids) == 1:\n        # Only one tax id, pick a random one as our champion\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    # Implicit else multiple taxa...\n    # Get the lineages for these taxa to root\n    tax_lineages = {\n        tid: get_lineage(tid, tax_db_cur)\n        for tid in tax_ids\n    }\n    # And the depth of each lineage\n    lin_depth_tax = {\n        len(lineage): tid\n        for tid, lineage in\n        tax_lineages.items()\n    }\n    # Pick the seq from the deepest lineage to be the representitive\n    seq_rep_id[seq] = tax_ids[\n            lin_depth_tax[max(lin_depth_tax.keys())]\n        ]\n\n# Only keep ref seqs as good as the best hit for an SV\nbesthit_ref_seqs = [\n    (sv, id_seq[ref])\n    for sv, ref, pctid\n    in uc_data\n    if sv_max_pctid[sv] == pctid\n]\n\n# Refseq -> besthit SV\nrefseq_sv = defaultdict(set)\nfor sv, refseq in besthit_ref_seqs:\n    refseq_sv[refseq].add(sv)\n\n# How many SV does each ref cover?\nref_sv_cnt = {\n    k: len(v)\n    for k, v in refseq_sv.items()\n}\n\n# Get rid of ref sequences that only represent less than MIN_REF_SV\nMIN_REF_SV = 2\nfiltered_sv_ref = [\n    (sv, seq_rep_id.get(ref))\n    for ref, svs in refseq_sv.items()\n    for sv in svs\n    if len(svs) >= MIN_REF_SV\n]\n\n# For the sv post the shared ref filter who no longer have a best hit, see if there is any hit for them \n# e.g. some reference in the current set that there is some identity, even if not as good as the best hits\ncovered_sv = {sv for sv, ref in filtered_sv_ref}\nextant_refs = {ref for sv, ref in filtered_sv_ref}\naddbacksv_refs = defaultdict(list)\nfor sv, ref_id, pct_id in uc_data:\n    if sv not in covered_sv:\n        if ref_id in extant_refs:\n            addbacksv_refs[sv].append((\n                ref_id, # Ref_ID\n                pct_id, # Pct_ID\n                True # Already part of our reference?\n            ))\n        elif pct_id == sv_max_pctid[sv]:\n            addbacksv_refs[sv].append((\n                ref_id,\n                pct_id,\n                False\n            ))\n# Great, now use this dict to make a decision of which refs to add for each sv\nfor sv, svr in addbacksv_refs.items():\n    sv_seq_pctid = {\n        id_seq.get(r): pct_id\n        for r, pct_id, already_in in svr\n        if not already_in\n    }\n    # Was there totally no representation?\n    if len([r for r, pct_id, already_in in svr if already_in]) == 0:\n        # if not, add in everything\n        filtered_sv_ref += [\n            (sv, seq_rep_id.get(s))\n            for s in sv_seq_pctid.keys()\n        ]\n    else:\n        # It's not perfect, but given we have some representation, just pick the longest sequence\n        filtered_sv_ref.append(\n            (\n                sv,\n                seq_rep_id.get(sorted(sv_seq_pctid.keys(), key=lambda v: len(v))[-1])\n            )\n        )\n\nfiltered_ref = {\n    ref\n    for sv, ref\n    in filtered_sv_ref\n}\n\n# Use this to create out outputted final output\nwith open('references.fasta', 'wt') as ref_out:\n    for ref_id in filtered_ref:\n        ref_out.write(\">{}\\\\n{}\\\\n\".format(\n            ref_id,\n            id_seq.get(ref_id)\n        ))\n\nsi_columns = list(seq_info.values())[0].keys()\n\nwith open('references_seq_info.csv', 'wt') as si_out:\n    si_writer = csv.DictWriter(\n        si_out,\n        fieldnames=si_columns\n    )\n    si_writer.writeheader()\n    si_writer.writerows([\n        r for i, r in seq_info.items()\n        if i in filtered_ref\n    ])\n\"\"\"\n}"], "list_proc": ["jgolob/maliampi/CombinedRefFilter"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["TaxIt"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess DlBuildTaxtasticDB {\n    container = \"${container__taxtastic}\"\n    label = 'io_net'\n    errorStrategy = 'finish'\n\n    output:\n        file \"taxonomy.db\"\n\n    afterScript \"rm -rf dl/\"\n\n\n    \"\"\"\n    set -e\n\n    mkdir -p dl/ && \\\n    taxit new_database taxonomy.db -u ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip -p dl/\n    \"\"\"\n\n}"], "list_proc": ["jgolob/maliampi/DlBuildTaxtasticDB"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["TaxIt"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess BuildTaxtasticDB {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n    errorStrategy = 'finish'\n\n    input:\n        file taxdump_zip_f\n\n    output:\n        file \"taxonomy.db\"\n\n    \"\"\"\n    taxit new_database taxonomy.db -z ${taxdump_zip_f}\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/BuildTaxtasticDB"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["TaxIt"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess ConfirmSI {\n    container = \"${container__taxtastic}\"\n    label = 'io_mem'\n\n    input:\n        file taxonomy_db_f\n        file refpkg_si_f\n    \n    output:\n        file \"${refpkg_si_f.baseName}.corr.csv\"\n    \n    \"\"\"\n    taxit update_taxids \\\n    ${refpkg_si_f} \\\n    ${taxonomy_db_f} \\\n    -o ${refpkg_si_f.baseName}.corr.csv \\\n    -a drop\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/ConfirmSI"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["RAxML-NG"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess RaxmlTreeNG {\n    container = \"${container__raxmlng}\"\n    label = 'mem_veryhigh'\n    errorStrategy = 'finish'\n\n    input:\n        path recruits_aln_fasta_f\n    \n    output:\n        path \"refpkg.raxml.bestTree\", emit: tree\n        path \"refpkg.raxml.log\", emit: log\n        path \"refpkg.raxml.bestModel\", emit: model\n    \n    \"\"\"\n    raxml-ng \\\n    --parse \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f} \\\n    --seed ${params.raxmlng_seed}\n\n    raxml-ng \\\n    --prefix refpkg \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f}.raxml.rba \\\n    --tree pars{${params.raxmlng_parsimony_trees}},rand{${params.raxmlng_random_trees}} \\\n    --bs-cutoff ${params.raxmlng_bootstrap_cutoff} \\\n    --seed ${params.raxmlng_seed} \\\n    --threads ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/RaxmlTreeNG"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["TaxIt"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess TaxtableForSI {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n    errorStrategy = 'finish'\n\n    input:\n        file taxonomy_db_f \n        file refpkg_si_corr_f\n    output:\n        file \"refpkg.taxtable.csv\"\n\n    \"\"\"\n    taxit taxtable ${taxonomy_db_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --outfile refpkg.taxtable.csv\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/TaxtableForSI"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["TaxIt", "MoDEL"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess CombineRefpkg_ng {\n    container = \"${container__taxtastic}\"\n    label = 'io_mem'\n\n    afterScript(\"rm -rf refpkg/*\")\n    publishDir \"${params.output}/refpkg/\", mode: 'copy'\n\n    input:\n        path recruits_aln_fasta_f\n        path recruits_aln_sto_f\n        path refpkg_tree_f \n        path refpkg_tree_stats_clean_f \n        path refpkg_tt_f\n        path refpkg_si_corr_f\n        path refpkg_cm\n        path raxmlng_model\n    \n    output:\n        path \"refpkg.tar.gz\"\n    \n\"\"\"\ntaxit create --locus 16S \\\n--package-name refpkg \\\n--clobber \\\n--aln-fasta ${recruits_aln_fasta_f} \\\n--aln-sto ${recruits_aln_sto_f} \\\n--tree-file ${refpkg_tree_f} \\\n--tree-stats ${refpkg_tree_stats_clean_f} \\\n--taxonomy ${refpkg_tt_f} \\\n--seq-info ${refpkg_si_corr_f} \\\n--profile ${refpkg_cm}\n\ncp ${raxmlng_model} refpkg/raxmlng.model.raw\npython << ENDPYTHON\nimport json\nimport hashlib\n\nmodel_str = open('refpkg/raxmlng.model.raw', 'rt').read()\nmodel = model_str.split(',')[0]\nwith open('refpkg/raxmlng.model', 'wt') as out_h:\n    out_h.write(model)\n\nmodelhash = hashlib.md5(model.encode('utf-8')).hexdigest()\ncontents = json.load(\n    open('refpkg/CONTENTS.json', 'rt')\n)\ncontents['files']['raxml_ng_model'] = 'raxmlng.model'\ncontents['md5']['raxml_ng_model'] = modelhash\n\njson.dump(\n    contents,\n    open('refpkg/CONTENTS.json', 'wt')\n)\nENDPYTHON\ntar cvf refpkg.tar  -C refpkg/ .\ngzip refpkg.tar\n\"\"\"\n}"], "list_proc": ["jgolob/maliampi/CombineRefpkg_ng"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["TaxIt"], "nb_own": 1, "list_own": ["jgolob"], "nb_wf": 1, "list_wf": ["maliampi"], "list_contrib": ["jgolob", "sminot"], "nb_contrib": 2, "codes": ["\nprocess CombineRefpkg_og {\n    container = \"${container__pplacer}\"\n    label = 'io_mem'\n\n    afterScript(\"rm -rf refpkg/*\")\n    publishDir \"${params.output}/refpkg/\", mode: 'copy'\n\n    input:\n        file recruits_aln_fasta_f\n        file recruits_aln_sto_f\n        file refpkg_tree_f \n        file refpkg_tree_stats_clean_f \n        file refpkg_tt_f\n        file refpkg_si_corr_f\n        file refpkg_cm\n    \n    output:\n        file \"refpkg.tar.gz\"\n    \n    \"\"\"\n    taxit create --locus 16S \\\n    --package-name refpkg \\\n    --clobber \\\n    --aln-fasta ${recruits_aln_fasta_f} \\\n    --aln-sto ${recruits_aln_sto_f} \\\n    --tree-file ${refpkg_tree_f} \\\n    --tree-stats ${refpkg_tree_stats_clean_f} \\\n    --taxonomy ${refpkg_tt_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --profile ${refpkg_cm} && \\\n    ls -l refpkg/ && \\\n    tar czvf refpkg.tar.gz  -C refpkg/ .\n    \"\"\"\n}"], "list_proc": ["jgolob/maliampi/CombineRefpkg_og"], "list_wf_names": ["jgolob/maliampi"]}, {"nb_reuse": 1, "tools": ["SAMtools", "mosdepth"], "nb_own": 1, "list_own": ["jguhlin"], "nb_wf": 1, "list_wf": ["useful-nextflow-patterns"], "list_contrib": ["jguhlin"], "nb_contrib": 1, "codes": ["\nprocess markdups {\n  input:\n    file(file) from mapped_ch\n\n  output:\n    file \"*.stats\"\n    file \"*.txt\"\n\n  publishDir \"stats_processed\", mode: 'move', overwrite: true\n\n  cpus 2\n\n  \"\"\"\n    samtools index ${file}\n    samtools stats ${file} > ${file.baseName}.stats\n    mosdepth -n --fasta /mnt/data/stonefly/assembly.fna ${file.baseName} ${file} \n  \"\"\"\n}"], "list_proc": ["jguhlin/useful-nextflow-patterns/markdups"], "list_wf_names": ["jguhlin/useful-nextflow-patterns"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["jguhlin"], "nb_wf": 1, "list_wf": ["useful-nextflow-patterns"], "list_contrib": ["jguhlin"], "nb_contrib": 1, "codes": ["\nprocess map_reads {\n  input:\n    tuple val(id), val(files) from reads_ch\n\n  output:\n    file \"*.cram\"\n\n  publishDir \"mapped_reads\"\n\n  cpus 16\n\n  \"\"\"\n    bwa mem \\\n      -o ${id}.sam \\\n      -t 16 \\\n      -R '@RG\\\\tID:${id}_pair\\\\tSM:${id}' \\\n      /mnt/data/stonefly/assembly.fna \\\n      ${files[0]} ${files[1]}\n\n    samtools sort ${id}.sam -l 1 -o ${id}.sorted.bam -O bam\n\n    bwa mem \\\n      -o ${id}_singles.sam \\\n      -t 16 \\\n      -R '@RG\\\\tID:${id}_single\\\\tSM:${id}' \\\n      /mnt/data/stonefly/assembly.fna \\\n      ${files[2]}\n   \n    samtools sort ${id}_singles.sam -l 1 -o ${id}_singles.sorted.bam -O bam\n    samtools merge -O CRAM --reference /mnt/data/stonefly/assembly.fna \\\n      -o ${id}.cram *.bam\n    rm *.sam\n    rm *.bam\n  \"\"\"\n}"], "list_proc": ["jguhlin/useful-nextflow-patterns/map_reads"], "list_wf_names": ["jguhlin/useful-nextflow-patterns"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jguhlin"], "nb_wf": 1, "list_wf": ["useful-nextflow-patterns"], "list_contrib": ["jguhlin"], "nb_contrib": 1, "codes": ["\nprocess haplotypecaller {\n  input:\n    file(file) from markdup_ch\n\n  output:\n    file \"*.gvcf.gz\"\n\n  publishDir \"gvcfs\", mode: 'move', overwrite: true\n\n  cpus 4\n  memory '16 GB'\n\n  \"\"\"\n    samtools index ${file}\n    ${gatk} --java-options \"-Xmx8g\" HaplotypeCaller -I ${file} \\\n      -R ${ref} \\\n      -O ${file.baseName}.gvcf.gz \\\n      -ERC GVCF\n  \"\"\"\n}"], "list_proc": ["jguhlin/useful-nextflow-patterns/haplotypecaller"], "list_wf_names": ["jguhlin/useful-nextflow-patterns"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jguhlin"], "nb_wf": 1, "list_wf": ["useful-nextflow-patterns"], "list_contrib": ["jguhlin"], "nb_contrib": 1, "codes": ["\nprocess FastQC {\n  input:\n    tuple val(id), val(files) from reads_ch\n  output:\n    file \"*.zip\"\n\n  publishDir 'fastqc'\n  cpus 16\n  conda 'bioconda::fastqc'\n\n  \"\"\"\n    fastqc -o . ${files[0]} ${files[1]}\n  \"\"\"\n}"], "list_proc": ["jguhlin/useful-nextflow-patterns/FastQC"], "list_wf_names": ["jguhlin/useful-nextflow-patterns"]}, {"nb_reuse": 6, "tools": ["BWA", "HISAT2", "Trnascan-SE", "SAMtools", "MultiQC", "BamTools", "QIIME", "FastQC"], "nb_own": 6, "list_own": ["rikenbit", "khigashi1987", "loipf", "mvanins", "jianhong", "jiangfuqing"], "nb_wf": 6, "list_wf": ["ramdaq", "16S_pipeline", "CUTRUN_Nextflow", "DNAseq-pipeline", "CRISPR-Cas-off-target-identification", "stress_granule_RNA_manuscript"], "list_contrib": ["loipf", "khigashi1987", "yuifu", "Zethson", "mvanins", "jianhong", "myoshimura080822"], "nb_contrib": 7, "codes": ["\nprocess FASTQC {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.endsWith('.zip') ? \"zips/$filename\" : filename\n                }\n\n    when:\n    !params.skip_fastqc\n\n    input:\n    tuple val(name), path(reads) from ch_raw_reads_fastqc\n\n    output:\n    path '*.{zip,html}' into ch_fastqc_reports_mqc\n\n    script:\n                                                                           \n    \"\"\"\n    [ ! -f  ${name}_1.fastq.gz ] && ln -s ${reads[0]} ${name}_1.fastq.gz\n    [ ! -f  ${name}_2.fastq.gz ] && ln -s ${reads[1]} ${name}_2.fastq.gz\n    fastqc -q -t $task.cpus ${name}_1.fastq.gz\n    fastqc -q -t $task.cpus ${name}_2.fastq.gz\n    \"\"\"\n}", "\nprocess HISAT2  {\n    tag \"$name\"\n    label 'process_high'\n    \n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy', overwrite: true,\n        saveAs: { filename ->\n                    filename.indexOf(\".summary.txt\") > 0 ? \"summaries/$filename\" : \"$filename\"\n                }\n\n    input:\n    tuple val(name), file(reads)\n    path hs2_indices\n    path tools_dir\n\n    output:\n    tuple val(name), file(\"*.bam\"), file(\"*.bai\"), file(\"*.flagstat\"), emit: hisat2_bam_qc\n    tuple val(name), file(\"${name}.bam\"), file(\"${name}.bam.bai\"), file(\"${name}.bam.flagstat\"), optional:true, emit: hisat2_bam_count\n    path \"*.summary.txt\", emit: hisat2_summary\n\n    script:\n    def prefix = options.suffix ? \"${name}${options.suffix}\" : \"${name}\"\n\n    def strandness = ''\n    if (params.stranded == 'fr-firststrand') {\n        strandness = params.single_end ? \"--rna-strandness R\" : \"--rna-strandness RF\"\n    } else if (params.stranded == 'fr-secondstrand'){\n        strandness = params.single_end ? \"--rna-strandness F\" : \"--rna-strandness FR\"\n    }\n    softclipping = params.softclipping ? '' : \"--no-softclip\"\n    threads_num = params.hs_threads_num > 0 ? \"-p ${params.hs_threads_num}\" : ''\n    index_base = hs2_indices[0].toString() - ~/.\\d.ht2l?/\n\n    if (params.single_end) {\n        if (params.stranded && params.stranded != 'unstranded' && options.suffix != '.rrna') {\n            \"\"\"\n            hisat2 $softclipping $threads_num -x $index_base -U $reads $strandness $options.args --summary-file ${prefix}.summary.txt \\\\\n            | samtools view -bS - | samtools sort - -o ${prefix}.bam\n            samtools index ${prefix}.bam\n            samtools flagstat ${prefix}.bam > ${prefix}.bam.flagstat\n\n            bamtools filter -in ${prefix}.bam -out ${prefix}.forward.bam -script ${tools_dir}/bamtools_f_SE.json\n            samtools index ${prefix}.forward.bam\n            samtools flagstat ${prefix}.forward.bam > ${prefix}.forward.bam.flagstat\n\n            bamtools filter -in ${prefix}.bam -out ${prefix}.reverse.bam -script ${tools_dir}/bamtools_r_SE.json\n            samtools index ${prefix}.reverse.bam\n            samtools flagstat ${prefix}.reverse.bam > ${prefix}.reverse.bam.flagstat\n            \"\"\"\n        } else {\n            \"\"\"\n            hisat2 $softclipping $threads_num -x $index_base -U $reads $strandness $options.args --summary-file ${prefix}.summary.txt \\\\\n            | samtools view -bS - | samtools sort - -o ${prefix}.bam\n            samtools index ${prefix}.bam\n            samtools flagstat ${prefix}.bam > ${prefix}.bam.flagstat\n            \"\"\"\n        }\n\n    } else {\n        if (params.stranded && params.stranded != 'unstranded' && options.suffix != '.rrna') {\n            \"\"\"\n            hisat2 $softclipping $threads_num -x $index_base -1 ${reads[0]} -2 ${reads[1]} $strandness $options.args --summary-file ${prefix}.summary.txt \\\\\n            | samtools view -bS - | samtools sort - -o ${prefix}.bam\n            samtools index ${prefix}.bam\n            samtools flagstat ${prefix}.bam > ${prefix}.bam.flagstat\n\n            bamtools filter -in ${prefix}.bam -out ${prefix}.forward.bam -script ${tools_dir}/bamtools_f_PE.json\n            samtools index ${prefix}.forward.bam\n            samtools flagstat ${prefix}.forward.bam > ${prefix}.forward.bam.flagstat\n\n            bamtools filter -in ${prefix}.bam -out ${prefix}.reverse.bam -script ${tools_dir}/bamtools_r_PE.json\n            samtools index ${prefix}.reverse.bam\n            samtools flagstat ${prefix}.reverse.bam > ${prefix}.reverse.bam.flagstat\n\n            samtools view -bS -f 0x40 ${prefix}.bam -o ${prefix}.R1.bam\n            samtools index ${prefix}.R1.bam\n            samtools flagstat ${prefix}.R1.bam > ${prefix}.R1.bam.flagstat\n\n            samtools view -bS -f 0x80 ${prefix}.bam -o ${prefix}.R2.bam\n            samtools index ${prefix}.R2.bam\n            samtools flagstat ${prefix}.R2.bam > ${prefix}.R2.bam.flagstat\n            \"\"\"\n        } else if (params.stranded == 'unstranded' && options.suffix != '.rrna'){\n            \"\"\"\n            hisat2 $softclipping $threads_num -x $index_base -1 ${reads[0]} -2 ${reads[1]} $options.args --summary-file ${prefix}.summary.txt \\\\\n            | samtools view -bS - | samtools sort - -o ${prefix}.bam\n            samtools index ${prefix}.bam\n            samtools flagstat ${prefix}.bam > ${prefix}.bam.flagstat\n\n            samtools view -bS -f 0x40 ${prefix}.bam -o ${prefix}.R1.bam\n            samtools index ${prefix}.R1.bam\n            samtools flagstat ${prefix}.R1.bam > ${prefix}.R1.bam.flagstat\n\n            samtools view -bS -f 0x80 ${prefix}.bam -o ${prefix}.R2.bam\n            samtools index ${prefix}.R2.bam\n            samtools flagstat ${prefix}.R2.bam > ${prefix}.R2.bam.flagstat\n            \"\"\"\n        } else {\n            \"\"\"\n            hisat2 $softclipping $threads_num -x $index_base -1 ${reads[0]} -2 ${reads[1]} $strandness $options.args --summary-file ${prefix}.summary.txt \\\\\n            | samtools view -bS - | samtools sort - -o ${prefix}.bam\n            samtools index ${prefix}.bam\n            samtools flagstat ${prefix}.bam > ${prefix}.bam.flagstat\n            \"\"\"\n        }\n    }\n}", "\nprocess MAPPING_BWA { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/reads_mapped\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\tcache false\n\n\tinput:\n\t\ttuple val(sample_id), path(reads_prepro) \n\t\tval num_threads\n\t\tpath reference_genome\n\t\tpath bwa_index                               \n\n\n\toutput:\n\t\tpath \"${sample_id}.bam\", emit: reads_mapped\n\t\tpath \"${sample_id}.bam.bai\", emit: reads_mapped_index\n\t\tpath \"*\", emit: all\n\n\n\tshell:\n\t'''\n\tbwa mem -Y -R \"@RG\\\\tID:!{sample_id}\\\\tSM:!{sample_id}\" -t !{num_threads} -K 100000000 !{reference_genome} !{reads_prepro} \\\n\t| samtools view -@ !{num_threads} -h -b -u - \\\n\t| samtools sort -n -@ !{num_threads} - \\\n\t| samtools fixmate -m -@ !{num_threads} - - \\\n\t| samtools sort -@ !{num_threads} - \\\n\t| samtools markdup -@ !{num_threads} -f !{sample_id}_markdup_stats.txt - !{sample_id}.bam\n\n\tsamtools index -b -@ !{num_threads} !{sample_id}.bam\n\tsamtools stats -@ !{num_threads} !{sample_id}.bam > !{sample_id}_stats.txt\n\n\t'''\n}", "process QIIME_EXPORT {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"$prefix\")       , emit: reads\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    qiime tools export \\\\\n        --input-path $reads \\\\\n        --output-path ${prefix} \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}", " process eukaryotic_tRNAscan{\n                                                                     \n                                                            \n                                                            \n\n    cpus 36\n    time '24h'\n    memory '50G'\n\n    publishDir \"${referenceOut}/tRNAScan/eukaryotic/\", mode:'copy', overwrite:true\n\n    tag \"Eukaryotic tRNAScan-SE on $params.genomeFasta\"\n\n    input:\n    set file(genome), file(fai) from genome_euk_tRNA\n\n    output:\n    file(\"${genome.baseName}.Eukaryotic_tRNAs_bp.bed\") into eukar_tRNAs_bed\n    file(\"*\")\n\n    script:\n    \"\"\"\n    tRNAscan-SE \\\n      -HQ \\\n      -o# \\\n      -f# \\\n      -s# \\\n      -m# \\\n      -b# \\\n      -a# \\\n      -l# \\\n      --brief \\\n      --thread ${task.cpus} \\\n      -p ${genome.baseName}.Eukaryotic_tRNAs \\\n      ${genome}\n\n    cat ${genome.baseName}.Eukaryotic_tRNAs.out | tRNAScanToBed.pl > ${genome.baseName}.Eukaryotic_tRNAs_bp.bed\n    sed '/^MT\\\\|^chrM/d' ${genome.baseName}.Eukaryotic_tRNAs_bp.bed > temp.bed\n    mv temp.bed ${genome.baseName}.Eukaryotic_tRNAs_bp.bed\n    \"\"\"\n  }", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/FASTQC", "rikenbit/ramdaq/HISAT2", "loipf/DNAseq-pipeline/MAPPING_BWA", "jianhong/16S_pipeline/QIIME_EXPORT", "mvanins/stress_granule_RNA_manuscript/eukaryotic_tRNAscan", "jiangfuqing/CRISPR-Cas-off-target-identification/get_software_versions"], "list_wf_names": ["jiangfuqing/CRISPR-Cas-off-target-identification", "loipf/DNAseq-pipeline", "rikenbit/ramdaq", "jianhong/16S_pipeline", "khigashi1987/CUTRUN_Nextflow", "mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 16, "tools": ["BCFtools", "MetaPhlAn", "BWA", "SAMtools", "GmT", "MultiQC", "FeatureCounts", "Bowtie", "randompat", "fastPHASE", "G-BLASTN", "ThermoRawFileParser", "BEDTools", "FastQC"], "nb_own": 16, "list_own": ["montilab", "yonghah", "vntasis", "josephhalstead", "simozhou", "samlhao", "replikation", "proteomicsunitcrg", "marcocrotti", "kevbrick", "marchoeppner", "maxibor", "jiangfuqing", "steepale", "likelet", "stevekm"], "nb_wf": 16, "list_wf": ["pipeliner-2", "wgsfastqtobam", "qcloud2-pipeline", "blastn_so_hot", "geodata-pipeline", "radseq-processing-nf", "stan-nf", "humann-nf", "pipeIt", "MuTect2_target_chunking", "CRISPR-Cas-off-target-identification", "epi-awesome", "nextflow_small_germline_panel", "circPipe", "outbreak-monitoring", "nextflow-spid"], "list_contrib": ["simozhou", "lucacozzuto", "kevbrick", "marchoeppner", "rolivella", "yonghah", "replikation", "vntasis", "toniher", "stevekm", "samlhao", "josephhalstead", "marcocrotti", "likelet", "Zethson", "dengshuang0116", "bioinformatist", "maxibor", "steepale", "weiqijin", "anfederico"], "nb_contrib": 21, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess trim_reads_with_fastp{\n\n    input:\n    set val(id), file(read1), file(read2) from reads \n\n    output:\n    set val(id), file(\"${id}_R1_trimmed.fastq.gz\") into trimmed_reads_channel_r1\n    set val(id), file(\"${id}_R2_trimmed.fastq.gz\") into trimmed_reads_channel_r2\n    file \"${id}_trimmed.html\" into fastp_html\n    file \"${id}_trimmed.json\" into fastp_json\n\n\t\"\"\"\n\tfastp \\\n    -i $read1 \\\n    -I $read2 \\\n    -o ${id}_R1_trimmed.fastq.gz \\\n    -O ${id}_R2_trimmed.fastq.gz \\\n    -h ${id}_trimmed.html \\\n    -j ${id}_trimmed.json \\\n    -w $params.fastp_threads\n\t\"\"\"\n}", "\nprocess sampling {\n  tag \"$modelName-$sampleID-$chain\"\n  publishDir \"$params.outdir/$sampleID/samples\", mode: 'copy', pattern: \"*.csv\"\n\n  input:\n  tuple val(sampleID), path(data), val(modelName), path(model), val(chain) from model2sample_ch\n  val(sampleParams) from params.sampleParams\n  val(seed) from params.seed\n  val(numSamples) from params.numSamples\n  val(numWarmup) from params.numWarmup\n  val(threads) from threads\n\n  output:\n  tuple val(modelName), val(sampleID), path(\"${sampleID}_${modelName}_${chain}.csv\") into samples2summary_ch\n  tuple val(modelName), val(sampleID), path(model), path(data), path(\"${sampleID}_${modelName}_${chain}.csv\") into samples2gen_quan_ch\n\n  when:\n  runSample\n\n  script:\n  \"\"\"\n  ./$model sample \\\n    num_samples=$numSamples \\\n    num_warmup=$numWarmup \\\n    $sampleParams \\\n    random seed=$seed id=$chain \\\n    data file=$data \\\n    output file=\"${sampleID}_${modelName}_${chain}.csv\" \\\n    $threads\n  \"\"\"\n}", "\nprocess bowtie2 {\n    tag \"$name\"\n    publishDir \"${params.outdir}/alignment\", mode: 'copy'\n\n    input:\n    file fasta from fasta\n    set val(name), file(reads) from read_files_trimming\n    file index from genome_index.collect()\n\n    output:\n    file '*.bam' into bowtie_output\n\n    script:\n    prefix=reads[0].toString() - ~/(.R1)?(_1)?(_R1)?(_trimmed)?(_val_1)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    \"\"\"\n    bowtie2 -x genome.index -U $reads -p ${params.max_cpus} -S ${prefix}.sam\n    samtools view -bT ${fasta} -@ ${params.max_cpus} -o ${prefix}.bam ${prefix}.sam\n    rm ${prefix}.sam\n    \"\"\"\n}", "\nprocess fastqc_1 {\n    cache true\n    container \"steepale/fastqc:1.0\"\n    publishDir \"${params.workdir}/results/fastqc_1\", mode: 'copy'\n    if (params.echo) {\n        echo true\n    }\n\n    input:\n    set pair_id, file(read1), file(read2) from read_pairs_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n                                            \n    when:\n    !params.skip_fastqc_1\n\n    script:\n    \"\"\"\n    ### Perform fastqc on all fastq files\n    fastqc \\\n    -t 2 \\\n    -q \\\n    ${read1} \\\n    ${read2}\n    \"\"\"\n}", " process build_metaphlan_db {\n    tag \"${params.mpa_db_name}\"\n\n    label 'intenso'\n\n    conda (params.enable_conda ? \"bioconda::metaphlan=3.0.13\" : null)\n    if (workflow.containerEngine == 'singularity') {\n        container \"https://depot.galaxyproject.org/singularity/metaphlan:3.0.13--pyhb7b1952_0\"\n    } else {\n        container \"quay.io/biocontainers/metaphlan:3.0.13--pyhb7b1952_0\"\n    }\n\n    output:\n        val(\"${params.mpa_db_name}\") into mpa_db_path_wait\n    \n    script:\n        \"\"\"\n        metaphlan --install -x ${params.mpa_db_name} --bowtie2db ${params.bt2db} --nproc ${task.cpus}\n        \"\"\"\n    }", "process FEATURE_COUNTS {\n    publishDir \"${params.outdir}/samples/${pid}/featurecounts\", mode: \"copy\"\n\n    input:\n    tuple val(pid), path(bams)\n    path gtf\n\n    output:\n    path '*.txt'\n    file '*.summary'\n\n    script:\n    xargs = \"\"\n    if (params.paired) {\n      xargs = xargs.concat(\"-p \")\n    }\n    \"\"\"\n    featureCounts $xargs \\\n    -T $task.cpus \\\n    -t 'exon' \\\n    -g 'gene_id' \\\n    -a ${gtf} \\\n    -o '${pid}_counts.txt' \\\n    ${bams}\n    \"\"\"\n}", "\nprocess mutect2_noChunk {\n    tag \"${prefix}\"\n    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n\n    input:\n    set val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(targets_bed), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_noChunk_ch\n\n    output:\n    set val(\"${label}\"), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_noChunk\n    file(\"${output_vcf}\")\n    file(\"${multiallelics_stats}\")\n    file(\"${realign_stats}\")\n\n    when: params.disable != \"true\"\n\n    script:\n    label = \"noChunk\"\n    prefix = \"${comparisonID}.${label}\"\n    output_vcf = \"${prefix}.vcf\"\n    output_norm_vcf = \"${prefix}.norm.vcf\"\n    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n    \"\"\"\n    gatk.sh -T MuTect2 \\\n    -dt NONE \\\n    --logging_level WARN \\\n    --standard_min_confidence_threshold_for_calling 30 \\\n    --max_alt_alleles_in_normal_count 10 \\\n    --max_alt_allele_in_normal_fraction 0.05 \\\n    --max_alt_alleles_in_normal_qscore_sum 40 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --cosmic \"${cosmic_ref_vcf}\" \\\n    --intervals \"${targets_bed}\" \\\n    --interval_padding 10 \\\n    --input_file:tumor \"${tumorBam}\" \\\n    --input_file:normal \"${normalBam}\" \\\n    --out \"${output_vcf}\"\n\n    # normalize and split vcf entries\n    cat ${output_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n    \"${output_norm_vcf}\"\n    \"\"\"\n}", "\nprocess runFastp {\n\n\tpublishDir \"${OUTDIR}/${organism}\", mode: 'copy'\n\n\tinput:\n\tset val(id),val(organism_file),fastqR1,fastqR2 from inputFastp\n\n\toutput:\n\tset val(id),val(organism),file(left),file(right) into inputPathoscopeMap\n   \tset file(json),file(html) into fastp_logs\n\n\tscript:\n\torganism = file(organism_file).getText().trim()\n\n\tleft = file(fastqR1).getSimpleName() + \"_trimmed.fastq.gz\"\n\tright = file(fastqR2).getSimpleName() + \"_trimmed.fastq.gz\"\n\tjson = file(fastqR1).getSimpleName() + \".fastp.json\"\n\thtml = file(fastqR1).getSimpleName() + \".fastp.html\"\n\n\t\"\"\"\n\t\tfastp --in1 $fastqR1 --in2 $fastqR2 --out1 $left --out2 $right --detect_adapter_for_pe -w ${task.cpus} -j $json -h $html --length_required 35\n\t\"\"\"\n\n}", "\nprocess rangeplot {\n    publishDir 'output', mode: 'copy'\n    input:\n        path \"lnglat.csv\"\n    output:\n        path \"range.pdf\"\n    script:\n        diameter = [\n            0,          \n            *range.collect{it*2}]\n        \"\"\"\n        gmt begin\n            gmt figure range pdf\n            gmt pscoast -Rd -JE125.75/39.02/${maprange}/20c -Gburlywood -Slightblue -A1000 \n            gmt plot lnglat.csv -Sa.2c -Wthicker,blue\n            for r in ${diameter}\n            do\n                gmt plot lnglat.csv -SE-\\$r -Wthin,firebrick\n            done\n        gmt end\n        \"\"\"\n}", "\nprocess thermofilerawparser {\n    label 'thermoconvert'  \n    tag { \"${labsys}_${qcode}_${checksum}\" }\n\n    input:\n    set orifile, labsys, qcode, checksum, file(zipfile) from zipfiles\n\n    output:\n    set val(\"${labsys}_${qcode}_${checksum}\"), qcode, checksum, file(\"${labsys}_${qcode}_${checksum}.mzML\") into mzmlfiles_for_correction\n    \n    script:\n    def filename = zipfile.getBaseName()\n\tdef extens = filename.split('.')\n    if (extens.length == 0) {\n\t\tfilename = filename + \".raw\"\n\t} else if (extens[-1] != \"raw\" ) {\n\t\tfilename = filename + \".raw\"\n\t}\n    \"\"\"\n    unzip ${zipfile}\n    ThermoRawFileParser -i=${filename} -f=1 -m=0 -o ./\n    mv *.mzML ${labsys}_${qcode}_${checksum}.mzML\n    rm *.raw\n    \"\"\"\n}", "\nprocess samtools_faidx {\n    label 'process_low'\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n    input:\n    path(fasta_gz) from bgzip_fasta_ch\n\n    output:\n    tuple(path(fasta_gz), path(\"${fasta_gz}.fai\")) into (fasta_gz_ch, asm_fasta_ch)\n\n    when:\n    params.fasta\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta_gz}\n    \"\"\"\n}", "\nprocess genome_index {\n\n\ttag \"$genome\"\n\tpublishDir params.resultGenome, mode: params.saveMode\n\t\n\tinput:\n\tfile genome \n\t\n\toutput:\n\tpath '*' into genome_index_ch\n\t\n\tscript:\n\t\n\t\"\"\"\n\tbwa index ${genome}\t\n\t\"\"\"\n\n}", "\nprocess BAMtoFQ {\n\n  label 'getFQ'\n\n  time { 1.hour * bam.size()/4000000000 * task.attempt}\n\n  tag { bam.size() }\n\n  input:\n  file(bam)\n\n  output:\n  tuple(val(\"gz\"), path('*.R1.fastq.gz'), emit: R1)\n  tuple(val(\"gz\"), path('*.R2.fastq.gz'), emit: R2, optional: true)\n\n  script:\n  \"\"\"\n  n=`samtools view -h ${bam} |head -n 100000 |samtools view -f 1 -S /dev/stdin |wc -l`\n\n  if [ \\$n -eq 0 ]; then\n    isSRPE=\"SR\"\n\n    java -jar \\$PICARDJAR SortSam \\\n                   I=${bam}  \\\n                   O=querynameSort.bam \\\n                   SO=queryname \\\n                   TMP_DIR=\\$TMPDIR \\\n                   VALIDATION_STRINGENCY=LENIENT >/dev/null 2>/dev/null\n\n    java -jar \\$PICARDJAR SamToFastq I=querynameSort.bam \\\n                   F=nxf.R1.fastq.gz \\\n                   TMP_DIR=\\$TMPDIR \\\n                   VALIDATION_STRINGENCY=LENIENT >/dev/null 2>/dev/null\n  else\n    isSRPE=\"PE\"\n    java -jar \\$PICARDJAR FixMateInformation \\\n                   I=${bam} \\\n                   O=fixMate.bam \\\n                   SORT_ORDER=queryname \\\n                   TMP_DIR=\\$TMPDIR \\\n                   VALIDATION_STRINGENCY=LENIENT >/dev/null 2>/dev/null\n\n    java -jar \\$PICARDJAR SortSam \\\n                   I=fixMate.bam  \\\n                   O=querynameSort.bam \\\n                   SO=queryname \\\n                   TMP_DIR=\\$TMPDIR \\\n                   VALIDATION_STRINGENCY=LENIENT >/dev/null 2>/dev/null\n\n    java -jar \\$PICARDJAR SamToFastq I=querynameSort.bam \\\n                   F=nxf.R1.fastq.gz F2=nxf.R2.fastq.gz \\\n                   TMP_DIR=\\$TMPDIR \\\n                   VALIDATION_STRINGENCY=LENIENT >/dev/null 2>/dev/null\n    fi\n  \"\"\"\n  }", "\nprocess blastn_local {\n        label 'blast'\n        publishDir \"${params.output}/${name}/\", mode: 'copy'\n    input:\n        tuple val(name), path(fasta)\n        path(database)\n    output:\n\t    tuple val(name), path(\"${name}.xml\") \n    script:\n    \"\"\"\n    blastn -query ${fasta} -db ${database}/${database} -out ${name}.xml -outfmt 5 -num_threads ${task.cpus} -evalue 10E-120 -qcov_hsp_perc 10 -max_hsps 10\n    \"\"\"\n}", " process getPsudoCircSequenceAndBuildHisatIndex {\n      input:\n           file (bed_file) from Bed_for_recount\n           file genomefile\n           file faifile \n      output:\n           file \"*.ht2\" into Candidate_circRNA_index\n      script:\n      \"\"\"\n      # extract bed file for obtaining seqeuence\n      sh ${baseDir}/bin/ProcessBedforGettingSequence.sh ${bed_file} temp.sort.bed temp.start.bed temp.end.bed\n\n      bedtools getfasta -name -fi ${genomefile} -s -bed temp.start.bed > temp.start.fa\n      bedtools getfasta -name -fi ${genomefile} -s -bed temp.end.bed > temp.end.fa\n      # circRNA <= 400 bp\n      bedtools getfasta -name -fi ${genomefile} -s -bed temp.sort.bed > temp.sort.fa \n\n      # merge and get combined fasta formatted psudoCirc sequences\n      sh ${baseDir}/bin/MergeBSJsequence.sh temp.sort.fa temp.start.fa temp.end.fa tmp_candidate.circular_BSJ_flank.fa\n\n      hisat2-build -p ${task.cpus}  tmp_candidate.circular_BSJ_flank.fa candidate_circRNA_BSJ_flank \n      rm temp* \n      rm tmp*\n      \n      \"\"\"\n    }"], "list_proc": ["jiangfuqing/CRISPR-Cas-off-target-identification/multiqc", "josephhalstead/nextflow_small_germline_panel/trim_reads_with_fastp", "vntasis/stan-nf/sampling", "simozhou/epi-awesome/bowtie2", "steepale/wgsfastqtobam/fastqc_1", "maxibor/humann-nf/build_metaphlan_db", "montilab/pipeliner-2/FEATURE_COUNTS", "stevekm/MuTect2_target_chunking/mutect2_noChunk", "marchoeppner/outbreak-monitoring/runFastp", "yonghah/geodata-pipeline/rangeplot", "proteomicsunitcrg/qcloud2-pipeline/thermofilerawparser", "samlhao/nextflow-spid/samtools_faidx", "marcocrotti/radseq-processing-nf/genome_index", "kevbrick/pipeIt/BAMtoFQ", "replikation/blastn_so_hot/blastn_local", "likelet/circPipe/getPsudoCircSequenceAndBuildHisatIndex"], "list_wf_names": ["josephhalstead/nextflow_small_germline_panel", "maxibor/humann-nf", "jiangfuqing/CRISPR-Cas-off-target-identification", "yonghah/geodata-pipeline", "marchoeppner/outbreak-monitoring", "kevbrick/pipeIt", "steepale/wgsfastqtobam", "vntasis/stan-nf", "replikation/blastn_so_hot", "simozhou/epi-awesome", "stevekm/MuTect2_target_chunking", "marcocrotti/radseq-processing-nf", "likelet/circPipe", "montilab/pipeliner-2", "samlhao/nextflow-spid", "proteomicsunitcrg/qcloud2-pipeline"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["jiangfuqing", "markgene"], "nb_wf": 2, "list_wf": ["cutnrun", "Chip-seq"], "list_contrib": ["sofiahaglund", "Rotholandus", "markgene", "winni2k", "ewels", "apeltzer", "tiagochst", "chuan-wang", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess makeGenomeFilter {\n    tag \"$fasta\"\n    publishDir \"${params.outdir}/reference_genome\", mode: 'copy'\n\n    input:\n    file fasta from ch_fasta\n\n    output:\n    file \"$fasta\" into ch_genome_fasta                                      \n    file \"*.fai\" into ch_genome_fai                                                     \n    file \"*.bed\" into ch_genome_filter_regions                                              \n    file \"*.sizes\" into ch_genome_sizes_bigwig                                              \n\n    script:\n    blacklist_filter = params.blacklist ? \"sortBed -i ${params.blacklist} -g ${fasta}.sizes | complementBed -i stdin -g ${fasta}.sizes\" : \"awk '{print \\$1, '0' , \\$2}' OFS='\\t' ${fasta}.sizes\"\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n    $blacklist_filter > ${fasta}.include_regions.bed\n    \"\"\"\n}", "\nprocess SortBAM {\n    tag \"$name\"\n    label 'process_medium'\n    if (params.save_align_intermeds) {\n        publishDir path: \"${params.outdir}/bwa/library\", mode: 'copy',\n            saveAs: { filename ->\n                          if (filename.endsWith(\".flagstat\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".idxstats\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".stats\")) \"samtools_stats/$filename\"\n                          else filename\n                    }\n    }\n\n    input:\n    set val(name), file(bam) from ch_bwa_bam\n\n    output:\n    set val(name), file(\"*.sorted.{bam,bam.bai}\") into ch_sort_bam_merge\n    file \"*.{flagstat,idxstats,stats}\" into ch_sort_bam_flagstat_mqc\n\n    script:\n    prefix = \"${name}.Lb\"\n    \"\"\"\n    samtools sort -@ $task.cpus -o ${prefix}.sorted.bam -T $name $bam\n    samtools index ${prefix}.sorted.bam\n    samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n    samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n    samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/Chip-seq/makeGenomeFilter", "markgene/cutnrun/SortBAM"], "list_wf_names": ["jiangfuqing/Chip-seq", "markgene/cutnrun"]}, {"nb_reuse": 3, "tools": ["seqtk", "Picard", "PLINK"], "nb_own": 3, "list_own": ["markxiao", "jiangfuqing", "jiangweiyao"], "nb_wf": 3, "list_wf": ["Chip-seq", "PRS-dev", "RefNAAP_nf"], "list_contrib": ["sofiahaglund", "Rotholandus", "winni2k", "ewels", "apeltzer", "tiagochst", "jiangweiyao", "markxiao", "chuan-wang", "drpatelh"], "nb_contrib": 10, "codes": ["\nprocess seqtk_trim_filter {\n\n                            \n                                                          \n    memory '3 GB'\n\n    input:\n    tuple name, file(fastq) from fastq_files2\n\n    output:\n    tuple name, file(\"*_filtered.fq\") into fastq_filtered\n\n    \"\"\"\n    seqtk trimfq -b ${params.left} -e ${params.right} $fastq > ${name}_trimmed.fq\n    seqtk seq -L ${params.size} ${name}_trimmed.fq > ${name}_filtered.fq\n    \"\"\"\n\n}", "\nprocess collectMultipleMetrics {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir path: \"${params.outdir}/bwa/mergedLibrary\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.endsWith(\"_metrics\")) \"picard_metrics/$filename\"\n            else if (filename.endsWith(\".pdf\")) \"picard_metrics/pdf/$filename\"\n            else null\n        }\n\n    when:\n    !params.skipPicardMetrics\n\n    input:\n    set val(name), file(bam) from ch_rm_orphan_bam_metrics\n    file fasta from ch_fasta\n\n    output:\n    file \"*_metrics\" into ch_collectmetrics_mqc\n    file \"*.pdf\" into ch_collectmetrics_pdf\n\n    script:\n    prefix=\"${name}.mLb.clN\"\n    if (!task.memory){\n        log.info \"[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.\"\n        avail_mem = 3\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    picard -Xmx${avail_mem}g CollectMultipleMetrics \\\\\n        INPUT=${bam[0]} \\\\\n        OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n        REFERENCE_SEQUENCE=$fasta \\\\\n        VALIDATION_STRINGENCY=LENIENT \\\\\n        TMP_DIR=tmp\n    \"\"\"\n}", "\nprocess target_qc_final {\n\n    publishDir \"${params.outdir}/intermediate_files\", mode: 'copy'\n\n    input:\n    tuple prefix, file(bed), file(bim), file(cov), file(fam), file(height) from target_data.copy6\n    tuple _tmp, file(irem), file(hh), file(snplist), file(fam1), file(prune_in), file(prune_out), file(het) from target_qc_standard_gwas_qc_1_output_copy5\n    tuple file(a1), file(mismatch) from target_qc_mismatching_snps_output\n    file rel_id from target_qc_relatedness_output\n\n    output:\n    tuple val(\"${prefix}\"), file(\"${prefix_qc}.fam\"), file(\"${prefix_qc}.bed\"), file(\"${prefix_qc}.bim\") into target_qc_final_output\n\n    script:\n    prefix_qc = \"${prefix}.QC\"\n    \"\"\"\n    plink --bfile ${prefix} --make-bed --keep ${rel_id} --out ${prefix_qc} --extract ${snplist} --exclude ${mismatch} --a1-allele ${a1}\n    if test -f \"${prefix_qc}.log\";\n        then mv ${prefix_qc}.log ${params.outdir}/logs/${prefix_qc}.log.6\n    fi\n    \"\"\"  \n}"], "list_proc": ["jiangweiyao/RefNAAP_nf/seqtk_trim_filter", "jiangfuqing/Chip-seq/collectMultipleMetrics", "markxiao/PRS-dev/target_qc_final"], "list_wf_names": ["jiangweiyao/RefNAAP_nf", "jiangfuqing/Chip-seq", "markxiao/PRS-dev"]}, {"nb_reuse": 1, "tools": ["FastQC", "FeatureCounts"], "nb_own": 2, "list_own": ["jiangfuqing", "mashehu"], "nb_wf": 1, "list_wf": ["nf-core_test", "Chip-seq"], "list_contrib": ["sofiahaglund", "Rotholandus", "winni2k", "ewels", "apeltzer", "mashehu", "tiagochst", "chuan-wang", "drpatelh"], "nb_contrib": 9, "codes": ["process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess deseqConsensusPeakSet {\n    tag \"${antibody}\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/macs/${peaktype}/consensus/${antibody}/deseq2\", mode: 'copy',\n        saveAs: {filename ->\n                    if (filename.endsWith(\".igv.txt\")) null\n                    else filename\n                }\n\n    when:\n    params.macs_gsize && !params.skipDiffAnalysis && replicatesExist && multipleGroups\n\n    input:\n    set val(antibody), val(replicatesExist), val(multipleGroups), file(bams) ,file(saf) from ch_group_bam_deseq\n    file deseq2_pca_header from ch_deseq2_pca_header\n    file deseq2_clustering_header from ch_deseq2_clustering_header\n\n    output:\n    file \"*featureCounts.txt\" into ch_macs_consensus_counts\n    file \"*featureCounts.txt.summary\" into ch_macs_consensus_counts_mqc\n    file \"*.{RData,results.txt,pdf,log}\" into ch_macs_consensus_deseq_results\n    file \"sizeFactors\" into ch_macs_consensus_deseq_factors\n    file \"*vs*/*.{pdf,txt}\" into ch_macs_consensus_deseq_comp_results\n    file \"*vs*/*.bed\" into ch_macs_consensus_deseq_comp_bed\n    file \"*igv.txt\" into ch_macs_consensus_deseq_comp_igv\n    file \"*.tsv\" into ch_macs_consensus_deseq_mqc\n\n    script:\n    prefix=\"${antibody}.consensus_peaks\"\n    peaktype = params.narrowPeak ? \"narrowPeak\" : \"broadPeak\"\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    bam_ext = params.singleEnd ? \".mLb.clN.sorted.bam\" : \".mLb.clN.bam\"\n    pe_params = params.singleEnd ? '' : \"-p --donotsort\"\n    \"\"\"\n    featureCounts -F SAF \\\\\n        -O \\\\\n        --fracOverlap 0.2 \\\\\n        -T $task.cpus \\\\\n        $pe_params \\\\\n        -a $saf \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bam_files.join(' ')}\n\n    featurecounts_deseq2.r -i ${prefix}.featureCounts.txt -b '$bam_ext' -o ./ -p $prefix -s .mLb\n\n    sed 's/deseq2_pca/deseq2_pca_${task.index}/g' <$deseq2_pca_header >tmp.txt\n    sed -i -e 's/DESeq2:/${antibody} DESeq2:/g' tmp.txt\n    cat tmp.txt ${prefix}.pca.vals.txt > ${prefix}.pca.vals_mqc.tsv\n\n    sed 's/deseq2_clustering/deseq2_clustering_${task.index}/g' <$deseq2_clustering_header >tmp.txt\n    sed -i -e 's/DESeq2:/${antibody} DESeq2:/g' tmp.txt\n    cat tmp.txt ${prefix}.sample.dists.txt > ${prefix}.sample.dists_mqc.tsv\n\n    find * -type f -name \"*.FDR0.05.results.bed\" -exec echo -e \"bwa/mergedLibrary/macs/${peaktype}/consensus/${antibody}/deseq2/\"{}\"\\\\t255,0,0\" \\\\; > ${prefix}.igv.txt\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/Chip-seq/deseqConsensusPeakSet"], "list_wf_names": ["jiangfuqing/Chip-seq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["Chip-seq"], "list_contrib": ["sofiahaglund", "Rotholandus", "winni2k", "ewels", "apeltzer", "tiagochst", "chuan-wang", "drpatelh"], "nb_contrib": 8, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n    when:\n    !params.skipMultiQC\n\n    input:\n    file multiqc_config from ch_multiqc_config\n\n    file ('fastqc/*') from ch_fastqc_reports_mqc.collect()\n    file ('trimgalore/*') from ch_trimgalore_results_mqc.collect()\n    file ('trimgalore/fastqc/*') from ch_trimgalore_fastqc_reports_mqc.collect()\n\n    file ('alignment/library/*') from ch_sort_bam_flagstat_mqc.collect()\n    file ('alignment/mergedLibrary/*') from ch_merge_bam_stats_mqc.collect()\n    file ('alignment/mergedLibrary/*') from ch_rm_orphan_flagstat_mqc.collect{it[1]}\n    file ('alignment/mergedLibrary/*') from ch_rm_orphan_stats_mqc.collect()\n    file ('alignment/mergedLibrary/picard_metrics/*') from ch_merge_bam_metrics_mqc.collect()\n    file ('alignment/mergedLibrary/picard_metrics/*') from ch_collectmetrics_mqc.collect()\n\n    file ('macs/*') from ch_macs_mqc.collect().ifEmpty([])\n    file ('macs/*') from ch_macs_qc_mqc.collect().ifEmpty([])\n    file ('macs/consensus/*') from ch_macs_consensus_counts_mqc.collect().ifEmpty([])\n    file ('macs/consensus/*') from ch_macs_consensus_deseq_mqc.collect().ifEmpty([])\n\n    file ('preseq/*') from ch_preseq_results.collect().ifEmpty([])\n    file ('deeptools/*') from ch_plotfingerprint_mqc.collect().ifEmpty([])\n    file ('deeptools/*') from ch_plotprofile_mqc.collect().ifEmpty([])\n    file ('phantompeakqualtools/*') from ch_spp_out_mqc.collect().ifEmpty([])\n    file ('phantompeakqualtools/*') from ch_spp_csv_mqc.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_mqc.collect()\n    file ('workflow_summary/*') from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    mqcstats = params.skipMultiQCStats ? '--cl_config \"skip_generalstats: true\"' : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename --config $multiqc_config \\\\\n        -m custom_content -m fastqc -m cutadapt -m samtools -m picard -m preseq -m featureCounts -m deeptools -m phantompeakqualtools \\\\\n        $mqcstats\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/Chip-seq/multiqc"], "list_wf_names": ["jiangfuqing/Chip-seq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["atac-seq"], "list_contrib": ["ewels", "apeltzer", "drpatelh"], "nb_contrib": 3, "codes": ["\nprocess makeGenomeFilter {\n    tag \"$fasta\"\n    publishDir \"${params.outdir}/reference_genome\", mode: 'copy'\n\n    input:\n    file fasta from fasta_genome_filter\n\n    output:\n    file \"$fasta\" into genome_fasta                                      \n    file \"*.fai\" into genome_fai                                                     \n    file \"*.txt\" into genome_autosomes                                                                                \n    file \"*.bed\" into genome_filter_regions                                                                                   \n    file \"*.sizes\" into genome_sizes_mlib_bigwig,                                        \n                        genome_sizes_mrep_bigwig\n\n    script:\n    blacklist_filter = params.blacklist ? \"sortBed -i ${params.blacklist} -g ${fasta}.sizes | complementBed -i stdin -g ${fasta}.sizes\" : \"awk '{print \\$1, '0' , \\$2}' OFS='\\t' ${fasta}.sizes\"\n    name_filter = params.mito_name ? \"| awk '\\$1 !~ /${params.mito_name}/ {print \\$0}'\": \"\"\n    mito_filter = params.keepMito ? \"\" : name_filter\n    \"\"\"\n    samtools faidx $fasta\n    get_autosomes.py ${fasta}.fai ${fasta}.autosomes.txt\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n    $blacklist_filter $mito_filter > ${fasta}.include_regions.bed\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/atac-seq/makeGenomeFilter"], "list_wf_names": ["jiangfuqing/atac-seq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["atac-seq"], "list_contrib": ["ewels", "apeltzer", "drpatelh"], "nb_contrib": 3, "codes": ["\nprocess bwa_mem {\n    tag \"$name\"\n    label 'process_big'\n\n    input:\n    set val(name), file(reads) from trimmed_reads\n    file index from bwa_index.first()\n\n    output:\n    set val(name), file(\"*.bam\") into bwa_bam\n\n    script:\n    prefix=\"${name}.Lb\"\n    rg=\"\\'@RG\\\\tID:${name}\\\\tSM:${name.split('_')[0..-2].join('_')}\\\\tPL:ILLUMINA\\\\tLB:${name}\\\\tPU:1\\'\"\n    \"\"\"\n    bwa mem -t $task.cpus -M -R $rg ${index}/${bwa_base} $reads | samtools view -@ $task.cpus -b -h -F 0x0100 -O BAM -o ${prefix}.bam -\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/atac-seq/bwa_mem"], "list_wf_names": ["jiangfuqing/atac-seq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["atac-seq"], "list_contrib": ["ewels", "apeltzer", "drpatelh"], "nb_contrib": 3, "codes": ["\nprocess merge_library_collectmetrics {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir path: \"${params.outdir}/bwa/mergedLibrary\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.endsWith(\"_metrics\")) \"picard_metrics/$filename\"\n            else if (filename.endsWith(\".pdf\")) \"picard_metrics/pdf/$filename\"\n            else null\n        }\n\n    input:\n    set val(name), file(bam) from mlib_rm_orphan_bam_metrics\n    file fasta from fasta_mlib_metrics.collect()\n\n    output:\n    file \"*_metrics\" into mlib_collectmetrics_mqc\n    file \"*.pdf\" into mlib_collectmetrics_pdf\n\n    script:\n    prefix=\"${name}.mLb.clN\"\n    if (!task.memory){\n        log.info \"[Picard CollectMultipleMetrics] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.\"\n        avail_mem = 3\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    \"\"\"\n    picard -Xmx${avail_mem}g CollectMultipleMetrics \\\\\n           INPUT=${bam[0]} \\\\\n           OUTPUT=${prefix}.CollectMultipleMetrics \\\\\n           REFERENCE_SEQUENCE=$fasta \\\\\n           VALIDATION_STRINGENCY=LENIENT \\\\\n           TMP_DIR=tmp\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/atac-seq/merge_library_collectmetrics"], "list_wf_names": ["jiangfuqing/atac-seq"]}, {"nb_reuse": 2, "tools": ["seqtk", "bedGraphToBigWig"], "nb_own": 2, "list_own": ["jiangfuqing", "jiangweiyao"], "nb_wf": 2, "list_wf": ["atac-seq", "metaphlan_nf"], "list_contrib": ["ewels", "apeltzer", "jiangweiyao", "drpatelh"], "nb_contrib": 4, "codes": ["\nprocess merge_library_bigwig {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/bigwig\", mode: 'copy',\n        saveAs: {filename ->\n                    if (filename.endsWith(\".txt\")) \"scale/$filename\"\n                    else if (filename.endsWith(\".bigWig\")) \"$filename\"\n                    else null\n                }\n\n    input:\n    set val(name), file(bam), file(flagstat) from mlib_rm_orphan_bam_bigwig.join(mlib_rm_orphan_flagstat_bigwig, by: [0])\n    file sizes from genome_sizes_mlib_bigwig.collect()\n\n    output:\n    file \"*.bigWig\" into mlib_bigwig_igv\n    file \"*.txt\" into mlib_bigwig_scale\n\n    script:\n    prefix=\"${name}.mLb.clN\"\n    pe_fragment = params.singleEnd ? \"\" : \"-pc\"\n    extend = (params.singleEnd && params.fragment_size > 0) ? \"-fs ${params.fragment_size}\" : ''\n    \"\"\"\n    SCALE_FACTOR=\\$(grep 'mapped (' $flagstat | awk '{print 1000000/\\$1}')\n    echo \\$SCALE_FACTOR > ${prefix}.scale_factor.txt\n    genomeCoverageBed -ibam ${bam[0]} -bg -scale \\$SCALE_FACTOR $pe_fragment $extend | sort -k1,1 -k2,2n >  ${prefix}.bedGraph\n\n    bedGraphToBigWig ${prefix}.bedGraph $sizes ${prefix}.bigWig\n    \"\"\"\n}", "\nprocess seqtk_trim {\n    \n                            \n    publishDir params.out, pattern: \"*.fastq\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*.fastq\" into qc_files, qc_files1\n\n    \"\"\"\n    seqtk trimfq -e ${params.bp_right} ${fastq[0]} > ${name}_R1.fastq\n    seqtk trimfq -e ${params.bp_right} ${fastq[1]} > ${name}_R2.fastq\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/atac-seq/merge_library_bigwig", "jiangweiyao/metaphlan_nf/seqtk_trim"], "list_wf_names": ["jiangfuqing/atac-seq", "jiangweiyao/metaphlan_nf"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["atac-seq"], "list_contrib": ["ewels", "apeltzer", "drpatelh"], "nb_contrib": 3, "codes": ["\nprocess merge_replicate_macs_consensus_deseq {\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedReplicate/macs/consensus/deseq2\", mode: 'copy'\n\n    input:\n    file bams from mlib_name_bam_mrep_counts.collect{ it[1] }\n    file saf from mrep_macs_consensus_saf.collect()\n    file mrep_deseq2_pca_header from mrep_deseq2_pca_header_ch.collect()\n    file mrep_deseq2_clustering_header from mrep_deseq2_clustering_header_ch.collect()\n\n    output:\n    file \"*featureCounts.txt\" into mrep_macs_consensus_counts\n    file \"*featureCounts.txt.summary\" into mrep_macs_consensus_counts_mqc\n    file \"*.{RData,results.txt,pdf,log}\" into mrep_macs_consensus_deseq_results\n    file \"sizeFactors\" into mrep_macs_consensus_deseq_factors\n    file \"*vs*/*.{pdf,txt}\" into mrep_macs_consensus_deseq_comp_results\n    file \"*vs*/*.bed\" into mrep_macs_consensus_deseq_comp_bed_igv\n    file \"*.tsv\" into mrep_macs_consensus_deseq_mqc\n\n    when: !params.skipMergeReplicates && replicates_exist && params.macs_gsize && multiple_samples\n\n    script:\n    prefix=\"consensus_peaks.mRp.clN\"\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    bam_ext = params.singleEnd ? \".mLb.clN.sorted.bam\" : \".mLb.clN.bam\"\n    pe_params = params.singleEnd ? '' : \"-p --donotsort\"\n    \"\"\"\n    featureCounts -F SAF \\\\\n                  -O \\\\\n                  --fracOverlap 0.2 \\\\\n                  -T $task.cpus \\\\\n                  $pe_params \\\\\n                  -a $saf \\\\\n                  -o ${prefix}.featureCounts.txt \\\\\n                  ${bam_files.join(' ')}\n\n    featurecounts_deseq2.r -i ${prefix}.featureCounts.txt -b '$bam_ext' -o ./ -p $prefix -s .mRp\n\n    cat $mrep_deseq2_pca_header ${prefix}.pca.vals.txt > ${prefix}.pca.vals_mqc.tsv\n    cat $mrep_deseq2_clustering_header ${prefix}.sample.dists.txt > ${prefix}.sample.dists_mqc.tsv\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/atac-seq/merge_replicate_macs_consensus_deseq"], "list_wf_names": ["jiangfuqing/atac-seq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["atac-seq"], "list_contrib": ["ewels", "apeltzer", "drpatelh"], "nb_contrib": 3, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n    input:\n    file multiqc_config from multiqc_config_ch.collect()\n\n    file ('fastqc/*') from fastqc_reports_mqc.collect()\n    file ('trimgalore/*') from trimgalore_results_mqc.collect()\n    file ('trimgalore/fastqc/*') from trimgalore_fastqc_reports_mqc.collect()\n\n    file ('alignment/library/*') from sort_bam_flagstat_mqc.collect()\n\n    file ('alignment/mergedLibrary/*') from mlib_bam_stats_mqc.collect()\n    file ('alignment/mergedLibrary/*') from mlib_rm_orphan_flagstat_mqc.collect{it[1]}\n    file ('alignment/mergedLibrary/*') from mlib_rm_orphan_stats_mqc.collect()\n    file ('alignment/mergedLibrary/picard_metrics/*') from mlib_metrics_mqc.collect()\n    file ('alignment/mergedLibrary/picard_metrics/*') from mlib_collectmetrics_mqc.collect()\n    file ('macs/mergedLibrary/*') from mlib_macs_peaks_mqc.collect().ifEmpty([])\n    file ('macs/mergedLibrary/*') from mlib_macs_qc_mqc.collect().ifEmpty([])\n    file ('macs/mergedLibrary/consensus/*') from mlib_macs_consensus_counts_mqc.collect().ifEmpty([])\n    file ('macs/mergedLibrary/consensus/*') from mlib_macs_consensus_deseq_mqc.collect().ifEmpty([])\n\n    file ('alignment/mergedReplicate/*') from mrep_flagstat_mqc.collect{it[1]}.ifEmpty([])\n    file ('alignment/mergedReplicate/*') from mrep_stats_mqc.collect().ifEmpty([])\n    file ('alignment/mergedReplicate/*') from mrep_metrics_mqc.collect().ifEmpty([])\n    file ('macs/mergedReplicate/*') from mrep_macs_peak_mqc.collect().ifEmpty([])\n    file ('macs/mergedReplicate/*') from mrep_macs_qc_mqc.collect().ifEmpty([])\n    file ('macs/mergedReplicate/consensus/*') from mrep_macs_consensus_counts_mqc.collect().ifEmpty([])\n    file ('macs/mergedReplicate/consensus/*') from mrep_macs_consensus_deseq_mqc.collect().ifEmpty([])\n\n    file ('software_versions/*') from software_versions_mqc.collect()\n    file ('workflow_summary/*') from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename --config $multiqc_config \\\\\n        -m custom_content -m fastqc -m cutadapt -m samtools -m picard -m featureCounts\n    \"\"\"\n}"], "list_proc": ["jiangfuqing/atac-seq/multiqc"], "list_wf_names": ["jiangfuqing/atac-seq"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["scrna-seq"], "list_contrib": ["PeterBailey"], "nb_contrib": 1, "codes": [" process build_salmon_index {\n         tag \"$fasta\"\n         publishDir \"${params.outdir}/salmon_index\", mode: 'copy'\n\n         input:\n         file fasta from fasta_alevin\n\n\n         output:\n         file \"salmon_index\" into salmon_index_alevin\n\n         script:\n\n         \"\"\"\n         salmon index -i salmon_index --gencode -k 31 -p 4 -t $fasta\n         \"\"\"\n     }"], "list_proc": ["jiangfuqing/scrna-seq/build_salmon_index"], "list_wf_names": ["jiangfuqing/scrna-seq"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["jiangfuqing"], "nb_wf": 1, "list_wf": ["scrna-seq"], "list_contrib": ["PeterBailey"], "nb_contrib": 1, "codes": [" process run_alevin {\n    tag \"$name\"\n    publishDir \"${params.outdir}/alevin\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from read_files_alevin\n    file index from salmon_index_alevin.collect()\n    file txp2gene from txp2gene_alevin.collect()\n\n\n    output:\n    file \"${name}_alevin_results\" into alevin_results\n\n    script:\n    \"\"\"\n    salmon alevin -l ISR -1 ${reads[0]} -2 ${reads[1]} --chromium -i $index -o ${name}_alevin_results -p 5 --tgMap $txp2gene --dumpFeatures\n    \"\"\"\n  }"], "list_proc": ["jiangfuqing/scrna-seq/run_alevin"], "list_wf_names": ["jiangfuqing/scrna-seq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["RefNAAP_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    \n                            \n                                                          \n    memory '3 GB' \n\n\n    input:\n    set val(name), file(fastq) from fastq_files1 \n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files\n    file \"*_fastqc.{zip,html}\" into qc_files1\n\n    \"\"\"\n    fastqc ${fastq}\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/RefNAAP_nf/fastqc"], "list_wf_names": ["jiangweiyao/RefNAAP_nf"]}, {"nb_reuse": 2, "tools": ["BWA", "SAMtools", "4peaks", "MultiQC", "FDR", "totalVI"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 2, "list_wf": ["nf-core-hicar", "RefNAAP_nf", "nextflowTutorial"], "list_contrib": ["nf-core-bot", "ewels", "yuxuth", "jianhong", "jiangweiyao"], "nb_contrib": 5, "codes": ["\nprocess CHECK_PEAKS {\n    publishDir \"${params.outdir}\",\n        mode: \"copy\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    } else {\n        container \"quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    }\n    input:\n    tuple val(meta), path(peak)\n    path peak2\n    path peak1\n    path pairs\n    path interactions\n    path hdf5\n\n    output:\n    path \"results.txt\", emit: qc\n    path \"results.csv\", emit: res\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(rtracklayer)\n    library(InteractionSet)\n    library(rhdf5)\n    hdf5 <- \"$hdf5\"\n    peaks1 <- import(\"$peak1\")\n    peaks2 <- import(\"$peak2\")\n    reads <- read.table(\"$pairs\")\n    h5content <- h5ls(hdf5)\n    h5content <- h5content[, \"group\"]\n    h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n    h5content <- unique(h5content)\n    h5reads_pos <- lapply(paste0(h5content, \"/position\"), h5read, file=hdf5)\n    h5reads_pos <- do.call(rbind, h5reads_pos)\n    stopifnot(\"There are bugs in creating pairs2hdf5.nf\"=identical(nrow(h5reads_pos), nrow(reads)))\n    peaks <- read.table(\"$peak\", header=TRUE)\n    gi <- with(reads, GInteractions(GRanges(V2, IRanges(V3, width=150)), GRanges(V4, IRanges(V5, width=150))))\n    peaks_gi <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)), GRanges(chr2, IRanges(start2, end2))))\n    peaks1\\$count <- countOverlaps(peaks1, gi)\n    peaks2\\$count <- countOverlaps(peaks2, gi)\n    peaks_gi\\$count <- countOverlaps(peaks_gi, gi, use.region=\"both\")\n    pct <- sum(peaks_gi\\$count==peaks\\$count)/length(peaks_gi)\n    stopifnot(\"There are bugs in counting\"=pct>.99)\n    real_gi <- import(\"$interactions\", format=\"bedpe\")\n    detected_r <- subsetByOverlaps(real_gi, peaks_gi)\n    detected_p <- subsetByOverlaps(peaks_gi, real_gi)\n    P <- length(real_gi)                             ## condition positive\n    N <- length(peaks1)*length(peaks2) - P           ## condition negative\n    TP <- length(detected_r)                         ## True positive\n    FP <- length(peaks_gi) - length(detected_p)      ## False positive\n    sensitivity <- TP/P                              ## sensitivity\n    FDR <- FP/length(peaks_gi)                       ## false discovery rate\n    write.csv(c(\"Total reads\"=nrow(reads),\n                \"Total true connections\"=P,\n                \"True positive\"=TP,\n                \"False positive\"=FP,\n                \"sensitivity\"=sensitivity),\n                \"results.csv\")\n    writeLines(ifelse(FDR<0.1, \"YES\", \"NO\"), \"results.txt\")\n    \"\"\"\n}", "\nprocess multiqc {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '3 GB'\n\n    input:\n    file reports  from qc_files.collect().ifEmpty([])\n\n    output:\n    path \"multiqc_report.html\" into final1\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n}", "\nprocess BWA_MEM {\n  tag \"$meta.id\"\n  publishDir \"${params.outdir}/${params.options.publish_dir}\", mode: 'copy'\n  conda (params.conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.09\" : null)\n\n  input:\n    tuple val(meta), path(reads), path(genome), path(index)\n\n  output:\n    tuple val(meta), path(\"${meta.id}.srt.bam\"), path(\"${meta.id}.srt.bam.bai\"), emit: bam\n\n  script:\n  \"\"\"\n  bwa mem -t task.cpus $genome $reads \\\\\n    | samtools view -@ $task.cpus -bS -o ${meta.id}.bam -\n  samtools sort -o ${meta.id}.srt.bam ${meta.id}.bam\n  samtools index ${meta.id}.srt.bam\n  \"\"\"\n}"], "list_proc": ["jiangweiyao/RefNAAP_nf/multiqc", "jianhong/nextflowTutorial/BWA_MEM"], "list_wf_names": ["jiangweiyao/RefNAAP_nf", "jianhong/nextflowTutorial"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["RefNAAP_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess minimap2 {\n\n    errorStrategy 'ignore'\n                                                          \n    memory '7 GB'\n\n    input:\n    tuple val(name), file(fastq) from fastq_filtered\n    file ref from reference1.first()\n\n    output:\n    tuple file(fastq), file(\"${fastq.simpleName}.sam\") into sam_files\n\n    \"\"\"\n    minimap2 -ax map-ont $ref ${fastq} > ${fastq.simpleName}.sam\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/RefNAAP_nf/minimap2"], "list_wf_names": ["jiangweiyao/RefNAAP_nf"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["RefNAAP_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess consensus_call {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '7 GB'\n\n    input:\n    tuple file(fastq), file(scaffold_file) from final_files\n\n    output:\n    path(\"${fastq.simpleName}_final.fasta\") into consensus_output\n                                                               \n    \"\"\"\n    medaka_consensus -i $fastq -d $scaffold_file -o ${fastq.simpleName}_medaka2 -m $model\n    bcftools mpileup -f $scaffold_file ${fastq.simpleName}_medaka2/calls_to_draft.bam | bcftools call -mv -Oz -o ${fastq.simpleName}.vcf.gz --ploidy 1\n    bcftools index ${fastq.simpleName}.vcf.gz\n    cat $scaffold_file | bcftools consensus ${fastq.simpleName}.vcf.gz -H 1 > ${fastq.simpleName}_int.fasta\n    medaka_consensus -i $fastq -d ${fastq.simpleName}_int.fasta -o ${fastq.simpleName}_medaka3 -m $model\n    cp ${fastq.simpleName}_medaka3/consensus.fasta ${fastq.simpleName}_final.fasta\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/RefNAAP_nf/consensus_call"], "list_wf_names": ["jiangweiyao/RefNAAP_nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "collapsereads", "4peaks", "MultiQC", "CNTools", "restrict", "FIT", "gencore"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 1, "list_wf": ["nf-core-hicar", "coregenome_align_nf"], "list_contrib": ["nf-core-bot", "ewels", "yuxuth", "jianhong", "jiangweiyao"], "nb_contrib": 5, "codes": ["\nprocess multiqc {\n\n    errorStrategy 'ignore'\n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    file reports from qc_files.collect().ifEmpty([])\n\n    output:\n    path \"multiqc_report.html\" into multiqc_output\n\n    \"\"\"\n    multiqc $reports\n    \"\"\"\n}", "process PAIRTOOLS_RESTRICT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(pairs)\n    path frag\n\n    output:\n    tuple val(meta), path(\"*.pairs.gz\"), emit: restrict\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        restrict \\\\\n        -f $frag \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $pairs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}", "process DIFF_HIPEAK {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-diffhic=1.24.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bioconductor-diffhic:1.24.0--r41h399db7b_0 \"\n    } else {\n        container \"quay.io/biocontainers/bioconductor-diffhic:1.24.0--r41h399db7b_0\"\n    }\n\n    input:\n    path peaks, stageAs: \"peaks/*\"\n    path distalpair, stageAs: \"pairs/*\"\n\n    output:\n    path \"${prefix}/*\"                        , emit: diff\n    path \"${prefix}/*.qc.json\", optional: true, emit: stats\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix   = task.ext.prefix ? \"${task.ext.prefix}\" : \"diffhicar\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call edgeR\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"edgeR\", \"InteractionSet\", \"rhdf5\", \"BiocParallel\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"snow_type\"=c(\"--snow_type\", \"-t\", \"character\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    PREFIX <- \"$prefix\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SNOW_TYPE <- \"SOCK\"\n    if(!is.null(opt\\$snow_type)){\n        SNOW_TYPE <- opt\\$snow_type\n    }\n\n    ## get peaks\n    pf <- dir(\"peaks\", \"peaks\", full.names = TRUE)\n    peaks <- lapply(pf, read.table, header=TRUE)\n    ### reduce the peaks\n    peaks <- unique(do.call(rbind, peaks)[, c(\"chr1\", \"start1\", \"end1\",\n                                            \"chr2\", \"start2\", \"end2\")])\n    peaks <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)),\n                                        GRanges(chr2, IRanges(start2, end2))))\n    reducePeaks <- function(x){\n        y <- reduce(x)\n        ol <- findOverlaps(x, y)\n        stopifnot(all(seq_along(x) %in% queryHits(ol)))\n        ol <- as.data.frame(ol)\n        y[ol[match(seq_along(x), ol\\$queryHits), \"subjectHits\"]]\n    }\n    first <- reducePeaks(first(peaks))\n    second <- reducePeaks(second(peaks))\n    peaks <- unique(GInteractions(first, second))\n\n    ## get counts\n    if(SNOW_TYPE==\"FORK\"){\n        param <- MulticoreParam(workers = NCORE, progressbar = TRUE)\n    }else{\n        param <- SnowParam(workers = NCORE, progressbar = TRUE, type = SNOW_TYPE)\n    }\n    pc <- dir(\"pairs\", \"h5\\$\", full.names = FALSE)\n    countByOverlaps <- function(pairs, peaks, sep=\"___\"){\n        getPath <- function(root, ...){\n            paste(root, ..., sep=\"/\")\n        }\n        readPairs <- function(pair, chrom1, chrom2){\n            h5content <- rhdf5::h5ls(pair)\n            h5content <- h5content[, \"group\"]\n            h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n            h5content <- unique(h5content)\n            n <- h5content[grepl(paste0(\"data.\", chrom1, \".\", chrom2), h5content)]\n            n <- getPath(n, \"position\")\n            inf <- rhdf5::H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n            on.exit({rhdf5::H5Fclose(inf)})\n            pc <- lapply(n, function(.ele){\n                if(rhdf5::H5Lexists(inf, .ele)){\n                    rhdf5::h5read(inf, .ele)\n                }\n            })\n            rhdf5::H5Fclose(inf)\n            rhdf5::h5closeAll()\n            on.exit()\n            pc <- do.call(rbind, pc)\n        }\n        cnt <- lapply(names(peaks), function(chr){\n            .peak <- peaks[[chr]]\n            chr_ <- strsplit(chr, sep)[[1]]\n            chrom1 <- chr_[1]\n            chrom2 <- chr_[2]\n            ps <- readPairs(pairs, chrom1, chrom2)\n            counts_total <- rhdf5::h5read(pairs, \"header/total\")\n            if(length(ps)<1){\n                return(NULL)\n            }\n            ps <- InteractionSet::GInteractions(\n                    GenomicRanges::GRanges(chrom1, IRanges::IRanges(ps[, 1], width=150)),\n                    GenomicRanges::GRanges(chrom2, IRanges::IRanges(ps[, 2], width=150)))\n            counts_tab <- IRanges::countOverlaps(.peak, ps, use.region=\"both\")\n            counts_tab <- cbind(ID=.peak\\$ID, counts_tab)\n            list(count=counts_tab, total=counts_total)\n        })\n        cnt <- cnt[lengths(cnt)>0]\n        counts_total <- vapply(cnt, FUN=function(.ele) .ele\\$total,\n                            FUN.VALUE = numeric(1))\n        counts_total <- sum(counts_total)\n        counts_tab <- do.call(rbind, lapply(cnt, function(.ele) .ele\\$count))\n        list(count=counts_tab, total=counts_total)\n    }\n\n    peaks\\$ID <- seq_along(peaks)\n    peaks.s <- split(peaks, paste(seqnames(first(peaks)), seqnames(second(peaks)), sep=\"___\"))\n    try_res <- try({cnts <- bplapply(file.path(\"pairs\", pc), countByOverlaps, peaks=peaks.s, sep=\"___\", BPPARAM = param)})\n    if(inherits(try_res, \"try-error\")){\n        cnts <- lapply(file.path(\"pairs\", pc), countByOverlaps, peaks=peaks.s, sep=\"___\")\n    }\n    h5closeAll()\n    rm(peaks.s)\n    samples <- sub(\"(_REP\\\\\\\\d+)\\\\\\\\.(.*?)h5\\$\", \"\\\\\\\\1\", pc)\n    sizeFactor <- vapply(cnts, FUN=function(.ele) .ele\\$total,\n                        FUN.VALUE = numeric(1))\n    names(sizeFactor) <- samples\n    cnts <- lapply(cnts, function(.ele) .ele\\$count)\n    cnts <- mapply(cnts, samples, FUN=function(.d, .n){\n        colnames(.d)[colnames(.d)!=\"ID\"] <- .n\n        .d\n    }, SIMPLIFY=FALSE)\n    cnts <- Reduce(function(x, y) merge(x, y, by=\"ID\"), cnts)\n    cnts <- cnts[match(peaks\\$ID, cnts[, \"ID\"]), , drop=FALSE]\n    cnts <- cnts[, colnames(cnts)!=\"ID\", drop=FALSE]\n    colnames(cnts) <- samples\n    rownames(cnts) <- seq_along(peaks)\n    mcols(peaks) <- cnts\n\n    pf <- as.character(PREFIX)\n    dir.create(pf)\n\n    fname <- function(subf, ext, ...){\n        pff <- ifelse(is.na(subf), pf, file.path(pf, subf))\n        dir.create(pff, showWarnings = FALSE, recursive = TRUE)\n        file.path(pff, paste(..., ext, sep=\".\"))\n    }\n\n    ## write counts\n    write.csv(peaks, fname(NA, \"csv\", \"raw.counts\"), row.names = FALSE)\n    ## write sizeFactors\n    write.csv(sizeFactor, fname(NA, \"csv\", \"library.size\"), row.names = TRUE)\n\n    ## coldata\n    sampleNames <- colnames(cnts)\n    condition <- make.names(sub(\"_REP.*\\$\", \"\", sampleNames), allow_=TRUE)\n    coldata <- data.frame(condition=factor(condition),\n                        row.names = sampleNames)\n    ## write designtable\n    write.csv(coldata, fname(NA, \"csv\", \"designTab\"), row.names = TRUE)\n\n    contrasts.lev <- levels(coldata\\$condition)\n\n    if(length(unique(contrasts.lev))>1 && any(table(condition)>1)){\n        contrasts <- combn(contrasts.lev, 2, simplify = FALSE)\n        ## create DGEList\n        group <- coldata\\$condition\n        y <- DGEList(counts = cnts,\n                    lib.size = sizeFactor,\n                    group = group)\n\n        ## do differential analysis\n        names(contrasts) <- vapply(contrasts,\n                                    FUN=paste,\n                                    FUN.VALUE = character(1),\n                                    collapse = \"-\")\n        y <- calcNormFactors(y)\n        design <- model.matrix(~0+group)\n        colnames(design) <- levels(y\\$samples\\$group)\n        y <- estimateDisp(y,design)\n        fit <- glmQLFit(y, design)\n\n        ## PCA\n        pdf(fname(NA, \"pdf\", \"Multidimensional.scaling.plot-plot\"))\n        mds <- plotMDS(y)\n        dev.off()\n        ## PCA for multiQC\n        try_res <- try({\n        json <- data.frame(x=mds\\$x, y=mds\\$y)\n        rownames(json) <- rownames(mds\\$distance.matrix.squared)\n        json <- split(json, coldata[rownames(json), \"condition\"])\n        json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n            .ele <- cbind(.ele, \"name\"=rownames(.ele))\n            .ele <- apply(.ele, 1, function(.e){\n                x <- names(.e)\n                y <- .e\n                .e <- paste0('{\"x\":', .e[1],\n                            ', \"y\":', .e[2],\n                            ', \"color\":\"', .color,\n                            '\", \"name\":\"', .e[3],\n                            '\"}')\n            })\n            .ele <- paste(.ele, collapse=\", \")\n            .ele <- paste(\"[\", .ele, \"]\")\n        })\n        json <- paste0('\"', names(json), '\" :', json)\n        json <- c(\n                \"{\",\n                '\"id\":\"sample_pca\",',\n                '\"data\":{',\n                paste(unlist(json), collapse=\", \"),\n                \"}\",\n                \"}\")\n        writeLines(json, fname(NA, \"json\", \"HiPeak.Multidimensional.scaling.qc\"))\n        })\n        if(inherits(try_res, \"try-error\")){\n            message(try_res)\n        }\n\n        ## plot dispersion\n        pdf(fname(NA, \"pdf\", \"DispersionEstimate-plot\"))\n        plotBCV(y)\n        dev.off()\n        ## plot QL dispersions\n        pdf(fname(NA, \"pdf\", \"Quasi-Likelihood-DispersionEstimate-plot\"))\n        plotQLDisp(fit)\n        dev.off()\n\n        res <- mapply(contrasts, names(contrasts), FUN = function(cont, name){\n            BvsA <- makeContrasts(contrasts = name, levels = design)\n            qlf <- glmQLFTest(fit, contrast = BvsA)\n            rs <- topTags(qlf, n = nrow(qlf), sort.by = \"none\")\n            ## MD-plot\n            pdf(fname(name, \"pdf\", \"Mean-Difference-plot\", name))\n            plotMD(qlf)\n            abline(h=0, col=\"red\", lty=2, lwd=2)\n            dev.off()\n            ## PValue distribution\n            pdf(fname(name, \"pdf\", \"PValue-distribution-plot\", name))\n            hist(rs\\$table\\$PValue, breaks = 20)\n            dev.off()\n            ## save res\n            res <- as.data.frame(rs)\n            res <- cbind(peaks[as.numeric(rownames(res))], res)\n            colnames(res) <- sub(\"seqnames\", \"chr\", colnames(res))\n            write.csv(res, fname(name, \"csv\", \"edgeR.DEtable\", name), row.names = FALSE)\n            ## save metadata\n            elementMetadata <- do.call(rbind, lapply(c(\"adjust.method\",\"comparison\",\"test\"), function(.ele) rs[[.ele]]))\n            rownames(elementMetadata) <- c(\"adjust.method\",\"comparison\",\"test\")\n            colnames(elementMetadata)[1] <- \"value\"\n            write.csv(elementMetadata, fname(name, \"csv\", \"edgeR.metadata\", name), row.names = TRUE)\n            ## save subset results\n            res.s <- res[res\\$FDR<0.05 & abs(res\\$logFC)>1, ]\n            write.csv(res.s, fname(name, \"csv\", \"edgeR.DEtable\", name, \"padj0.05.lfc1\"), row.names = FALSE)\n            ## Volcano plot\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            pdf(fname(name, \"pdf\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            png(fname(name, \"png\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n        })\n    }\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/coregenome_align_nf/multiqc"], "list_wf_names": ["jiangweiyao/coregenome_align_nf"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["coregenome_align_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess mash_screen_genome {\n\n                            \n                                                          \n    memory '8 GB'\n\n    input:\n    tuple val(name), file(fastq) from fastq_files3\n\n    output:\n    tuple val(name), path(\"*_pathogen_id_raw.out\") into mash_screen_genome_out\n\n    \"\"\"\n    cat ${fastq} | mash screen ${mash_genome_db} - | sort -gr > ${name}_pathogen_id_raw.out\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/coregenome_align_nf/mash_screen_genome"], "list_wf_names": ["jiangweiyao/coregenome_align_nf"]}, {"nb_reuse": 1, "tools": ["Cutadapt", "BUSCO"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 1, "list_wf": ["nf-core-hicar", "coregenome_align_nf"], "list_contrib": ["nf-core-bot", "ewels", "yuxuth", "jianhong", "jiangweiyao"], "nb_contrib": 5, "codes": ["process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess busco {\n\n    errorStrategy 'ignore'\n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '8 GB'\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output2\n\n    output:\n    path(\"*/short_summary*.txt\") into busco_output\n\n    \"\"\"\n    busco --auto-lineage-prok -f -m geno -o ${name}_busco -i ${assembly} -c 1\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/coregenome_align_nf/busco"], "list_wf_names": ["jiangweiyao/coregenome_align_nf"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["coregenome_align_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess prokka_assembly {\n\n                            \n                                                          \n\n    cpus params.thread\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output\n\n    output:\n    tuple val(name), path(\"*.gff\") into prokka_assembly_output\n\n    \"\"\"\n    prokka --cpus ${params.thread} --outdir ${name}_prokka --prefix ${name} ${assembly}\n    cp ${name}_prokka/${name}.gff ./\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/coregenome_align_nf/prokka_assembly"], "list_wf_names": ["jiangweiyao/coregenome_align_nf"]}, {"nb_reuse": 1, "tools": ["Roary"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["coregenome_align_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess roary {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    memory { 2.GB * params.roarythread }\n    cpus params.roarythread\n\n    input:\n    file gff from prokka_fasta_output.mix(prokka_assembly_output).collect()\n\n    output:\n    path(\"*\") into roary_output\n\n    \"\"\"\n    roary -f . -e -n -v -r *.gff -p ${params.roarythread}\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/coregenome_align_nf/roary"], "list_wf_names": ["jiangweiyao/coregenome_align_nf"]}, {"nb_reuse": 1, "tools": ["RAxML-NG"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["coregenome_align_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess raxmlng {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    memory '16 GB'\n    cpus params.thread\n\n    input:\n    file roary_output from roary_output\n\n    output:\n    path(\"core_gene.raxml*\") into raxml_output\n\n    \"\"\"\n    raxml-ng --msa core_gene_alignment.aln --model GTR+G --prefix core_gene --threads ${params.thread} --seed 1234 \n    \"\"\"\n}"], "list_proc": ["jiangweiyao/coregenome_align_nf/raxmlng"], "list_wf_names": ["jiangweiyao/coregenome_align_nf"]}, {"nb_reuse": 1, "tools": ["Bio-Cigar", "4peaks", "FastQC", "BaMM", "totalVI"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 1, "list_wf": ["nf-core-hicar", "kraken2_biom_nf"], "list_contrib": ["nf-core-bot", "ewels", "yuxuth", "jianhong", "jiangweiyao"], "nb_contrib": 5, "codes": ["process PAIR2BAM {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(peak), path(pairs)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), path(\"*.bam.bai\")    , emit: bam\n    path \"versions.yml\"                                  , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Oct. 2021 convert pairs.gz to bam file for visualization\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"Rsamtools\", \"InteractionSet\", \"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    peaks <- \"$peak\"\n    pairs <- dir(\".\", \"h5\\$\")\n\n    ## load header\n    getHeader <- function(file){\n        header <- h5read(file, \"header/header\")\n        header <- header[grepl(\"#samheader: @SQ\", header)]\n        header <- sub(\"#samheader: \", \"\", header)\n    }\n    ## loading data\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    createReadsName <- function(ids, width=6, prefix=\"r\"){\n        paste0(prefix, formatC(ids, width=width, flag=\"0\"))\n    }\n    filterByPeak <- function(pos, strand, chr, peaks){\n        gi <- GInteractions(anchor1=GRanges(chr, IRanges(pos[, 1], width=150)),\n                            anchor2=GRanges(chr, IRanges(pos[, 2], width=150)))\n        ol <- findOverlaps(gi, peaks)\n        keep <- sort(unique(queryHits(ol)))\n        list(pos=pos[keep, , drop=FALSE], strand=strand[keep, , drop=FALSE])\n    }\n    createAlginment <- function(file, p, idx, width, peaks){\n        chr_ <- strsplit(p, \"/\")[[1]]\n        chr_id <- which(chr_==\"data\")[1]\n        chr1 <- chr_[chr_id+1]\n        chr2 <- chr_[chr_id+2]\n        if(chr1!=chr2){\n            return(NULL)\n        }\n        pos <- h5read(file, getPath(p, \"position\"))\n        strand <- h5read(file, getPath(p, \"strand\"))\n        fil <- filterByPeak(pos, strand, chr1, peaks)\n        pos <- fil[[\"pos\"]]\n        strand <- fil[[\"strand\"]]\n        if(nrow(pos)){\n            name <- createReadsName(idx+seq.int(nrow(pos)), width=width)\n            flag <- ifelse(strand[, 1]==\"-\", 16, 0)\n            posL <- rowMins(pos)\n            isize <- abs(pos[, 1] - pos[, 2])\n            cigar <- paste0(\"100M\", isize, \"N100M\")\n            aln <- paste(name, flag, chr1, posL, \"50\", cigar, \"*\", 0, isize+201, \"*\", \"*\", sep = \"\\\\t\")\n        }else{\n            return(NULL)\n        }\n    }\n    exportBamFile <- function(file, peaks){\n        con <- sub(\".h5\\$\", \"\", file)\n        sam_path <- sub(\"h5\\$\", \"sam\", file, ignore.case = TRUE)\n        if(sam_path==con){\n            sam_path <- paste0(con, \".sam\")\n        }\n        sam_con <- file(sam_path, \"w\")\n        on.exit(close(sam_con))\n        ## write header\n        header <- getHeader(file)\n        writeLines(header, sam_con)\n        ## write data\n        total <- h5read(file, \"header/total\")\n        total_n <- nchar(total)\n        h5content <- h5ls(file)\n        h5content <- h5content[, \"group\"]\n        h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n        idx <- 0\n        for(p in h5content){\n            aln <- createAlginment(file, p, idx, total_n, peaks)\n            if(length(aln)){\n                writeLines(aln, sam_con)\n                idx <- idx + length(aln)\n            }\n        }\n        close(sam_con)\n        h5closeAll()\n        on.exit()\n        si <- do.call(rbind, strsplit(header, \"\\\\\\\\t\"))\n        si <- as.numeric(sub(\"LN:\", \"\", si[, 3]))\n        si <- si[!is.na(si)]\n        if(length(si)){\n            si <- any(si>536870912)\n        }else{\n            si <- TRUE\n        }\n        if(si){\n            bam <- asBam(sam_path, con,\n                        overwrite = TRUE, indexDestination = FALSE)\n        }else{\n            bam <- asBam(sam_path, con,\n                        overwrite = TRUE, indexDestination = TRUE)\n        }\n        unlink(sam_path)\n        invisible(bam)\n    }\n\n    peaks <- read.csv(peaks)\n    peaks <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)),\n                                        GRanges(chr2, IRanges(start2, end2))))\n    # output\n    null <- lapply(pairs, exportBamFile, peaks=peaks)\n    \"\"\"\n}", "\nprocess fastqc {\n    \n                            \n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files, qc_files1\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "process MAPS_REFORMAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"conda-forge::r-data.table=1.12.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-data.table:1.12.2' :\n        'quay.io/biocontainers/r-data.table:1.12.2' }\"\n\n    input:\n    tuple val(meta), val(bin_size), path(peak)\n\n    output:\n    tuple val(meta), val(bin_size), path(\"*.sig3Dinteractions.bedpe\"), emit: bedpe\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #########################################\n    # Author: [Ivan Juric](https://github.com/ijuric)\n    # File: MAPS_peak_formatting.r\n    # Source: https://github.com/ijuric/MAPS/blob/master/bin/MAPS/MAPS_peak_formatting.r\n    # Source+commit: https://raw.githubusercontent.com/ijuric/MAPS/46f92a6c7965a3b855ba7558c50d6c021b1d677c/bin/MAPS/MAPS_peak_formatting.r\n    # Data: 11/08/2021, commit:46f92a6\n    # modified by Jianhong:\n    # ## 1. set the Rscript environment.\n    # ## 2. simplify the input and output parameters by input filename and output filename\n    # ## 3. handle multiple input files\n    # ## 4. handle the empty input file\n    #########################################\n    versions <- c(\"${task.process}:\", \"    MAPS: 1.1.0\")\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    options(\"scipen\"=999)\n    library(data.table)\n    RESOLUTION = as.numeric($bin_size)\n    infs = strsplit(\"${peak}\", \"\\\\\\\\s+\")[[1]]\n\n    for(inf in infs){\n        peaks_raw = read.table(inf,header=TRUE, stringsAsFactors=FALSE)\n\n        peaks = as.data.table(subset(peaks_raw, ClusterType != 'Singleton' | (ClusterType == 'Singleton' & fdr < 1e-4))) # remove singletons\n        if(nrow(peaks)==0){\n            peaks\\$bin1_end <- peaks\\$bin2_end <- peaks\\$summit <- numeric(0)\n        }else{\n            peaks[, summit := 1*(fdr == min(fdr)), by = lab]\n            peaks\\$summit[ peaks\\$ClusterType == 'Singleton'] = 1\n\n            singleton_labs = peaks\\$lab[ peaks\\$ClusterType == 'Singleton']\n            peaks\\$lab[ peaks\\$ClusterType == 'Singleton'] = paste(singleton_labs, 1:length(singleton_labs),sep='')\n\n            peaks\\$bin1_end = peaks\\$bin1_mid + RESOLUTION\n            peaks\\$bin2_end = peaks\\$bin2_mid + RESOLUTION\n        }\n        peaks_final = subset(peaks, select = c(\"chr\", \"bin1_mid\", \"bin1_end\", \"chr\", \"bin2_mid\", \"bin2_end\", \"count\", \"expected2\", \"fdr\", \"lab\", \"ClusterSize\", \"ClusterType\", \"NegLog10P\", \"summit\"))\n        colnames(peaks_final) = c('chr1', 'start1', 'end1', 'chr2', 'start2', 'end2', 'count', 'expected', 'fdr', 'ClusterLabel', 'ClusterSize', 'ClusterType', 'ClusterNegLog10P', 'ClusterSummit')\n        fout = sub(\".peaks\",'.sig3Dinteractions.bedpe',inf)\n        write.table(peaks_final, fout, row.names = FALSE, col.names = TRUE, quote=FALSE, sep='\\t')\n    }\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/kraken2_biom_nf/fastqc"], "list_wf_names": ["jiangweiyao/kraken2_biom_nf"]}, {"nb_reuse": 3, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 3, "list_wf": ["quaisar_nf", "kraken2_biom_nf", "metaphlan_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n\n    errorStrategy 'ignore'\n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    file reports from qc_files.collect().ifEmpty([])\n\n    output:\n    path \"multiqc_report.html\" into multiqc_output\n\n    \"\"\"\n    multiqc $reports\n    \"\"\"\n}", "\nprocess multiqc {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    file reports from qc_files.collect().ifEmpty([])\n\n    output:\n    path \"multiqc_report.html\" into multiqc_output\n\n    \"\"\"\n    multiqc $reports\n    \"\"\"\n}", "\nprocess multiqc {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    file reports from qc_files.collect().ifEmpty([])\n\n    output:\n    path \"multiqc_report.html\" into multiqc_output\n\n    \"\"\"\n    multiqc $reports\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/metaphlan_nf/multiqc", "jiangweiyao/kraken2_biom_nf/multiqc", "jiangweiyao/quaisar_nf/multiqc"], "list_wf_names": ["jiangweiyao/metaphlan_nf", "jiangweiyao/kraken2_biom_nf", "jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["kraken2_biom_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess kraken_fastq {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '8 GB'\n\n    input:\n    tuple val(name), file(fastq) from fastq_files2\n\n    output:\n    file(\"*.summary\") into kraken_fastq_out\n\n    \"\"\"\n    kraken2 --db ${kraken_db} --paired ${fastq} --memory-mapping --report ${name}_reads.summary --output ${name}_reads.output\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/kraken2_biom_nf/kraken_fastq"], "list_wf_names": ["jiangweiyao/kraken2_biom_nf"]}, {"nb_reuse": 1, "tools": ["BioMe"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["kraken2_biom_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess biom_convert {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    file(tax_table) from kraken_biom_output\n\n    output:\n    path \"taxonomy_table.tsv\" into biom_convert_output\n\n    \"\"\"\n    biom convert -i ${tax_table} -o taxonomy_table.tsv --to-tsv --header-key taxonomy\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/kraken2_biom_nf/biom_convert"], "list_wf_names": ["jiangweiyao/kraken2_biom_nf"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["metaphlan_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess seqtk_subset {\n\n                            \n                                                          \n    memory '2 GB'\n    cpus 1\n    input:\n    tuple val(spacing), val(name), file(fastq) from spacinglist\n    output:\n    tuple val(name), val(spacing), file(\"*.fastq\") into seqtk_subset_out\n\n    \"\"\"\n    seqtk sample -s100 ${fastq[0]} ${spacing} > ${name}_R1_${spacing}.fastq\n    seqtk sample -s100 ${fastq[1]} ${spacing} > ${name}_R2_${spacing}.fastq\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/metaphlan_nf/seqtk_subset"], "list_wf_names": ["jiangweiyao/metaphlan_nf"]}, {"nb_reuse": 1, "tools": ["MetaPhlAn"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["metaphlan_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess mpa_fastq {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '8 GB'\n    cpus 4\n    input:\n    tuple val(name), file(fastq) from fastq_files2\n\n    output:\n    file(\"*.txt\") into mpa_fastq_out\n\n    \"\"\"\n    metaphlan ${fastq[0]},${fastq[1]} --input_type fastq -o ${name}.txt --bowtie2out ${name}.bowtie2.bz2 --index mpa_v30_CHOCOPhlAn_201901 --bowtie2db ${mpa_db} -t rel_ab_w_read_stats\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/metaphlan_nf/mpa_fastq"], "list_wf_names": ["jiangweiyao/metaphlan_nf"]}, {"nb_reuse": 2, "tools": ["FastQC", "kmacs"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 2, "list_wf": ["quaisar_nf", "metaphlan_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    \n    errorStrategy 'ignore'\n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files, qc_files1\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}", "\nprocess kma_index_plasmid {\n\n                            \n                                            \n\n    output:\n    path \"plasmid*\" into kma_index_plasmid_out\n\n    \"\"\"\n    kma index -i ${plasmid_db} -o plasmid\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/metaphlan_nf/fastqc", "jiangweiyao/quaisar_nf/kma_index_plasmid"], "list_wf_names": ["jiangweiyao/metaphlan_nf", "jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["paramsReadernf2"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n\n    cpus 1\n    memory 1.GB\n    errorStrategy 'ignore'\n    publishDir \"${params.publish_dir}\", mode: \"copy\", overwrite: true, enabled: params.publish_dir\n                                                          \n\n                                                                                                                                                      \n    input:\n    tuple val(state), file(fastq) \n\n    output:\n    file \"*_fastqc.{zip,html}\" \n    \"\"\"\n    fastqc ${fastq}\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/paramsReadernf2/fastqc"], "list_wf_names": ["jiangweiyao/paramsReadernf2"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["paramsReadernf2"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    cpus 2\n    memory 2.GB\n    publishDir \"${params.publish_dir}\", mode: \"copy\", overwrite: true, enabled: params.publish_dir\n                                                          \n\n    input:\n    file(reports)\n\n    output:\n    file \"multiqc_report.html\"\n\n    \"\"\"\n    multiqc $reports\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/paramsReadernf2/multiqc"], "list_wf_names": ["jiangweiyao/paramsReadernf2"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    \n                            \n    publishDir params.out, pattern: \"*.html\", mode: 'copy', overwrite: true\n\n    input:\n    set val(name), file(fastq) from fastq_files\n \n    output:\n    file \"*_fastqc.{zip,html}\" into qc_files, qc_files1\n\n    \"\"\"\n    fastqc -q ${fastq}\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/fastqc"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess mash_screen_genome {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '8 GB'\n\n    input:\n    tuple val(name), file(fastq) from fastq_files4\n\n    output:\n    tuple val(name), path(\"*_pathogen_id.out\") into mash_screen_genome_out\n\n    \"\"\"\n    cat ${fastq} | mash screen -w ${mash_genome_db} - | sort -gr > ${name}_pathogen_id.out\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/mash_screen_genome"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 2, "tools": ["QIIME", "kraken2"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 2, "list_wf": ["quaisar_nf", "16S_pipeline"], "list_contrib": ["jianhong", "jiangweiyao"], "nb_contrib": 2, "codes": ["process QIIME_EXPORT {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"$prefix\")       , emit: reads\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    qiime tools export \\\\\n        --input-path $reads \\\\\n        --output-path ${prefix} \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess kraken_fastq {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    errorStrategy  { task.attempt <= maxRetries  ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    memory { 8.GB * task.attempt * task.attempt }\n\n\n    input:\n    tuple val(name), file(fastq) from fastq_files7\n\n    output:\n    tuple val(name), file(\"*.{summary,output}\") into kraken_fastq_out\n\n    \"\"\"\n    kraken2 --db ${kraken_db} --paired ${fastq} --report ${name}_reads.summary --output ${name}_reads.output\n    \"\"\"\n}"], "list_proc": ["jianhong/16S_pipeline/QIIME_EXPORT", "jiangweiyao/quaisar_nf/kraken_fastq"], "list_wf_names": ["jiangweiyao/quaisar_nf", "jianhong/16S_pipeline"]}, {"nb_reuse": 2, "tools": ["Trimmomatic", "kmacs"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 2, "list_wf": ["quaisar_nf", "16S_pipeline"], "list_contrib": ["jianhong", "jiangweiyao"], "nb_contrib": 2, "codes": ["\nprocess kma_index_abr {\n\n                            \n                                            \n\n    output:\n    path \"abr*\" into kma_index_abr_out\n\n    \"\"\"\n    kma index -i ${abr_ref} -o abr\n    \"\"\"\n}", "process REMOVE_PRIMERS {\n    tag \"$meta.id\"\n    tag 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::trimmomatic=0.39\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/trimmomatic:0.39--hdfd78af_2' :\n        'quay.io/biocontainers/trimmomatic:0.39--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val single_end\n\n    output:\n    tuple val(meta), path(\"${prefix}_R1.paired.fastq.gz\"), path(\"${prefix}_R2.paired.fastq.gz\"), emit: paired\n    tuple val(meta), path(\"${prefix}_R1.unpaired.fastq.gz\"), path(\"${prefix}_R2.unpaired.fastq.gz\"), optional:true, emit: unpaired\n    path '*.trim_out.log', emit: log\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    if [ \"${single_end}\" == \"true\" ]; then\n        touch ${prefix}_R2.paired.fastq.gz\n        trimmomatic SE \\\\\n                    -threads $task.cpus \\\\\n                    ${reads[0]} \\\\\n                    ${prefix}_R1.paired.fastq.gz \\\\\n                    $args \\\\\n                    > ${prefix}.trim_out.log 2>&1\n    else\n        trimmomatic PE \\\\\n                    -threads $task.cpus \\\\\n                    ${reads[0]} ${reads[1]} \\\\\n                    ${prefix}_R1.paired.fastq.gz \\\\\n                    ${prefix}_R1.unpaired.fastq.gz \\\\\n                    ${prefix}_R2.paired.fastq.gz \\\\\n                    ${prefix}_R2.unpaired.fastq.gz \\\\\n                    $args \\\\\n                    > ${prefix}.trim_out.log 2>&1\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        trimmomatic: \\$(trimmomatic -version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/kma_index_abr", "jianhong/16S_pipeline/REMOVE_PRIMERS"], "list_wf_names": ["jiangweiyao/quaisar_nf", "jianhong/16S_pipeline"]}, {"nb_reuse": 1, "tools": ["kmacs"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess kma_map_abr {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    path index from kma_index_abr_out\n    tuple val(name), file(fastq) from fastq_files6\n\n    output:\n    tuple val(name), file(\"*_abr*\") into kma_abr_out\n\n                                                                           \n\n\n    \"\"\"\n    kma -ipe ${fastq} -o ${name}_abr -t_db abr -1t1\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/kma_map_abr"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["kmacs"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess kma_map_plasmid {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    path index from kma_index_plasmid_out\n    tuple val(name), file(fastq) from fastq_files2\n\n    output:\n    tuple val(name), file(\"*_plasmid*\") into kma_plasmid_out\n\n\n    \"\"\"\n    kma -ipe ${fastq} -o ${name}_plasmid -t_db plasmid -1t1\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/kma_map_plasmid"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess kraken_assembly {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    errorStrategy  { task.attempt <= maxRetries  ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    memory { 8.GB * task.attempt * task.attempt }\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output5\n\n    output:\n    tuple val(name), file(\"*.{summary,output}\") into kraken_assembly_out\n\n    \"\"\"\n    kraken2 --db ${kraken_db} ${assembly} --memory-mapping --report ${name}_assembly.summary --output ${name}_assembly.output\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/kraken_assembly"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["MLST", "4peaks", "Annot", "GTfold"], "nb_own": 2, "list_own": ["jianhong", "jiangweiyao"], "nb_wf": 1, "list_wf": ["nf-core-hicar", "quaisar_nf"], "list_contrib": ["nf-core-bot", "ewels", "yuxuth", "jianhong", "jiangweiyao"], "nb_contrib": 5, "codes": ["process BIOC_CHIPPEAKANNO {\n    tag \"$bin_size\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-chippeakanno=3.26.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(bin_size), path(diff)\n    path gtf\n\n    output:\n    tuple val(bin_size), path(\"${prefix}/anno/*\"), emit: anno\n    tuple val(bin_size), path(\"${prefix}/anno/**.anno.csv\"), emit: csv\n    path \"${prefix}/anno/*.png\", optional:true, emit: png\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call ChIPpeakAnno\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"ChIPpeakAnno\", \"rtracklayer\", \"GenomicFeatures\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    gtf <- \"${gtf}\"\n    pf <- file.path(\"${prefix}\", \"anno\")\n    bin_size <- \"${prefix}\"\n\n    detbl <- dir(\".\", \"DEtable.*.csv|sig3Dinteractions.bedpe|peaks\",\n                recursive = TRUE, full.names = TRUE)\n    detbl <- detbl[!grepl(\"anno.csv\", detbl)] ## in case of re-run\n\n    txdb <- makeTxDbFromGFF(gtf) ## create annotation data from gtf file\n    gtf <- import(gtf)\n    id2symbol <- function(gtf){ ## convert entriz id to gene symbol\n        if(is.null(gtf\\$gene_name)) return(NULL)\n        x <- data.frame(id=gtf\\$gene_id, symbol=gtf\\$gene_name)\n        x <- unique(x)\n        x <- x[!duplicated(x\\$id), ]\n        x <- x[!is.na(x\\$id), , drop=FALSE]\n        if(nrow(x)==0) return(NULL)\n        y <- x\\$symbol\n        names(y) <- x\\$id\n        y\n    }\n    id2symbol <- id2symbol(gtf)\n    anno <- toGRanges(txdb)\n    promoters <- promoters(anno, upstream=2000, downstream=500)\n    resList <- list() # save annotation results to a list\n    peaks <- list()\n    promoterList <- list() # list to save the distal sites of promoters interactions\n\n    dir.create(pf, showWarnings = FALSE, recursive = TRUE)\n    for(det in detbl){\n        if(grepl(\"csv\\$\", det)) {\n            DB <- read.csv(det)\n        }else{\n            if(grepl(\"peaks\\$\", det)){\n                DB <- read.table(det, header=TRUE)\n            }else{\n                DB <- read.delim(det)\n            }\n        }\n        if(nrow(DB)<1) next\n        rownames(DB) <- paste0(\"p\", seq.int(nrow(DB)))\n        DB.gr1 <- with(DB, GRanges(chr1, IRanges(start1, end1, name=rownames(DB))))\n        DB.gr2 <- with(DB, GRanges(chr2, IRanges(start2, end2, name=rownames(DB))))\n        # Annotation\n        DB.anno1 <- annotatePeakInBatch(DB.gr1, AnnotationData = anno,\n                                        output = \"both\",\n                                        PeakLocForDistance = \"middle\",\n                                        FeatureLocForDistance = \"TSS\",\n                                        ignore.strand = TRUE)\n        if(length(id2symbol)>0) DB.anno1\\$symbol[!is.na(DB.anno1\\$feature)] <- id2symbol[DB.anno1\\$feature[!is.na(DB.anno1\\$feature)]]\n        DB.anno2 <- annotatePeakInBatch(DB.gr2, AnnotationData = anno,\n                                        output = \"both\",\n                                        PeakLocForDistance = \"middle\",\n                                        FeatureLocForDistance = \"TSS\",\n                                        ignore.strand = TRUE)\n        if(length(id2symbol)>0) DB.anno2\\$symbol[!is.na(DB.anno2\\$feature)] <- id2symbol[DB.anno2\\$feature[!is.na(DB.anno2\\$feature)]]\n        groupName <- sub(\".sig3Dinteractions.bedpe|csv\", \"\", basename(det))\n        if(grepl(\"padj\", det)){\n            resList[[groupName]] <- c(DB.anno1, DB.anno2)\n        }else{\n            peaks[[groupName]] <- unique(c(DB.gr1, DB.gr2))\n            ol1 <- findOverlaps(DB.gr1, promoters)\n            ol2 <- findOverlaps(DB.gr2, promoters)\n            promoterList[[groupName]] <- unique(c(DB.gr2[unique(queryHits(ol1))], DB.gr1[unique(queryHits(ol2))]))\n        }\n        # Summary the annotations\n        DB.anno1 <- mcols(DB.anno1)\n        DB.anno2 <- mcols(DB.anno2)\n        DB.anno <- merge(DB.anno1, DB.anno2, by=\"peak\",\n                        suffixes = c(\".anchor1\",\".anchor2\"))\n        DB <- cbind(DB[DB.anno\\$peak, ], DB.anno)\n        pff <- file.path(pf, sub(\".(csv|bedpe|peaks)\", \".anno.csv\", det))\n        dir.create(dirname(pff), recursive = TRUE, showWarnings = FALSE)\n        write.csv(DB, pff, row.names = FALSE)\n    }\n\n\n    if(packageVersion(\"ChIPpeakAnno\")>=\"3.23.12\"){\n        if(length(resList)>0){\n            if(is.list(resList)){\n                resList <- GRangesList(resList[lengths(resList)>0])\n            }\n            out <- genomicElementDistribution(resList,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                            # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuiton.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuiton.\", bin_size, \".png\")), plot=out\\$plot)\n            out <- metagenePlot(resList, txdb)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSS.\", bin_size, \".pdf\")), plot=out, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSS.\", bin_size, \".png\")), plot=out)\n        }\n        if(length(peaks)>0){\n            peaks <- GRangesList(peaks[lengths(peaks)>0])\n            out <- genomicElementDistribution(peaks,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                                # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfEachPeakList.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfEachPeakList.\", bin_size, \".png\")), plot=out\\$plot)\n\n            out <- metagenePlot(peaks, txdb)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSSOfEachPeakList.\", bin_size, \".pdf\")), plot=out, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSSOfEachPeakList.\", bin_size, \".png\")), plot=out)\n\n            if(length(peaks)<=5 && length(peaks)>1){\n                ol <- findOverlapsOfPeaks(peaks)\n                png(file.path(pf, paste0(\"vennDiagram.all.\", bin_size, \".png\")))\n                vd <- makeVennDiagram(ol, connectedPeaks=\"keepAll\")\n                dev.off()\n                write.csv(vd\\$vennCounts, file.path(pf, paste0(\"vennDiagram.all.\", bin_size, \".csv\")), row.names=FALSE)\n            }\n        }\n        if(length(promoterList)>0){\n            promoterList <- GRangesList(promoterList[lengths(promoterList)>0])\n            out <- genomicElementDistribution(promoterList,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                                # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfremoteInteractionPeaks.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfremoteInteractionPeaks.\", bin_size, \".png\")), plot=out\\$plot)\n\n            if(length(promoterList)<=5 && length(promoterList)>1){\n                ol <- findOverlapsOfPeaks(promoterList)\n                png(file.path(pf, paste0(\"vennDiagram.remote.interaction.peak.with.promoters.all.\", bin_size, \".png\")))\n                vd <- makeVennDiagram(ol, connectedPeaks=\"keepAll\")\n                dev.off()\n                write.csv(vd\\$vennCounts, file.path(pf, paste0(\"vennDiagram.remote.interaction.peak.with.promoters.all.\", bin_size, \".csv\")), row.names=FALSE)\n            }\n        }\n    }\n    \"\"\"\n}", "\nprocess mlst {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output4\n\n    output:\n    tuple val(name), path(\"*.mlst\") into mlst_output\n\n    \"\"\"\n    mlst ${assembly} > ${name}.mlst\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/mlst"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess prokka {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n    \n    cpus params.thread\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output\n\n    output:\n    tuple val(name), path(\"*\") into prokka_output\n\n    \"\"\"\n    prokka --cpus ${params.thread} --outdir ${name}_prokka --prefix ${name} ${assembly}\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/prokka"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["QUAST"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess quast {\n\n                            \n    publishDir params.out, mode: 'copy', overwrite: true\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output2\n\n    output:\n    path(\"*\") into quast_output\n\n    \"\"\"\n    quast ${assembly} -o ${name}_quast\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/quast"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["quaisar_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess busco {\n\n    errorStrategy 'ignore'\n    publishDir params.out, mode: 'copy', overwrite: true\n    memory '8 GB'\n\n    input:\n    tuple val(name), file(assembly) from assembly_filter_output3\n\n    output:\n    path(\"*/short_summary*.txt\") into busco_output\n\n    \"\"\"\n    busco --auto-lineage-prok -f -m geno -o ${name}_busco -i ${assembly} -c 1\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/quaisar_nf/busco"], "list_wf_names": ["jiangweiyao/quaisar_nf"]}, {"nb_reuse": 1, "tools": ["Mash"], "nb_own": 1, "list_own": ["jiangweiyao"], "nb_wf": 1, "list_wf": ["sra_mash_nf"], "list_contrib": ["jiangweiyao"], "nb_contrib": 1, "codes": ["\nprocess mash {\n\n                            \n    publishDir params.out, mode: 'copy', pattern: \"*.out\", overwrite: true\n\n    input:\n    tuple val(name), file(fastq) from fastqs\n\n    output:\n    file(\"${name}.out\") into results\n\n    \"\"\"\n    mash screen -w ${reference} $fastq | sort -gr > ${name}.out\n    \"\"\"\n}"], "list_proc": ["jiangweiyao/sra_mash_nf/mash"], "list_wf_names": ["jiangweiyao/sra_mash_nf"]}, {"nb_reuse": 1, "tools": ["gencore"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["16S_pipeline"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process FILTERING {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val single_end\n\n    output:\n    tuple val(meta), path(\"$prefix\"), path(\"*.rds\") , emit: reads\n    path '*.png'                                    , emit: qc\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/bigdata.html\n\n    pkgs <- c(\"dada2\", \"ShortRead\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    DEMUXD_READS = \"$reads\"\n    TRIM = FALSE\n    OUTFOLDER = \"$prefix\"\n    FILTER_STATS = \"filter_stats.rds\"\n    NCORE <- ifelse(.Platform[[\"OS.type\"]]!=\"windows\", as.numeric(\"$task.cpus\"), FALSE)\n    PAIRED_END <- \"$single_end\" != \"true\"\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    grepl(\"trimming_reads\", \"$args\")\n    option_list <- list(\"trim\"=c(\"--trimming_reads\", \"-t\", \"logical\"),\n                        \"trimLeft\"=c(\"--trim_left\", \"-a\", \"integer\"),\n                        \"trimRight\"=c(\"--trim_right\", \"-b\", \"integer\"),\n                        \"truncLenLeft\"=c(\"--trunc_length_left\", \"-m\", \"integer\"),\n                        \"truncLenRight\"=c(\"--trunc_length_right\", \"-n\", \"integer\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"trim\"]])){\n        TRIM <- TRUE\n    }\n    if(!is.null(opt[[\"trimLeft\"]])){\n        trimLeft <- trimLeft1 <- opt[[\"trimLeft\"]]\n    }else{\n        trimLeft1 <- 0\n        trimLeft <- NULL\n    }\n    if(!is.null(opt[[\"trimRight\"]])){\n        trimLeft <- c(trimLeft1, opt[[\"trimRight\"]])\n    }\n    if(!is.null(opt[[\"truncLenLeft\"]])){\n        truncLen <- truncLen1 <- opt[[\"truncLenLeft\"]]\n    }else{\n        truncLen1 <- 0\n        truncLen <- NULL\n    }\n    if(!is.null(opt[[\"truncLenRight\"]])){\n        truncLen <- c(truncLen1, opt[[\"truncLenRight\"]])\n    }\n\n    # Filtering and Trimming --------------------------------------------------\n    #note s1 - is run number\n    #note r1 - is forward, r2 - is reverse\n    # Forward and Reverse Filenames\n    if(dir.exists(DEMUXD_READS)){\n        files <- list.files(DEMUXD_READS)\n    }else{\n        files <- strsplit(DEMUXD_READS, \"\\\\\\\\s+\")[[1]]\n    }\n\n    fnFs.s1 <- files[grepl(\"_R1[_.]\", files)]\n    fnRs.s1 <- files[grepl(\"_R2[_.]\", files)]\n\n    # Sort to ensure filenames are in the same order\n    fnFs.s1 <- sort(fnFs.s1)\n    fnRs.s1 <- sort(fnRs.s1)\n\n    sample.names.1 <- sapply(strsplit(fnFs.s1, \"_R1[_.].*?(fastq|fq)\", fixed=FALSE), `[`, 1)\n    if(PAIRED_END){\n        ## match pairs\n        sample.names.2 <- sapply(strsplit(fnRs.s1,\"_R2[_.].*?(fastq|fq)\", fixed=FALSE), `[`, 1)\n        sample.names.shared <- intersect(sample.names.1, sample.names.2)\n        fnFs.s1 <- fnFs.s1[match(sample.names.shared, sample.names.1)]\n        fnRs.s1 <- fnRs.s1[match(sample.names.shared, sample.names.2)]\n    }\n\n    # Fully Specify the path for the fnFs and fnRs\n    if(dir.exists(DEMUXD_READS)){\n        fnFs.s1 <- file.path(DEMUXD_READS, fnFs.s1)\n        fnRs.s1 <- file.path(DEMUXD_READS, fnRs.s1)\n    }\n\n    # Examine qulaity profiles of the forward and reverse reads\n    p <- plotQualityProfile(fnFs.s1[[1]])\n    ggsave('Forward_quality_profile_s1.png', plot=p)\n    p_F <- plotQualityProfile(fnFs.s1, aggregate=TRUE)\n    ggsave('Forward_quality_profile_aggregate.png', plot=p_F)\n    if(PAIRED_END){\n        p <- plotQualityProfile(fnRs.s1[[1]])\n        ggsave('Reverse_quality_profile_s1.png', plot=p)\n        p_R <- plotQualityProfile(fnRs.s1, aggregate=TRUE)\n        ggsave('Reverse_quality_profile_aggregate.png', plot=p_R)\n    }\n    if(TRIM){ # Reads look really good quality don't filter here\n        getTrimRange <- function(x){\n            l <- lapply(x[[\"layers\"]], function(.ele) .ele[[\"data\"]])\n            m <- lapply(x[[\"layers\"]], function(.ele) .ele[[\"mapping\"]])\n            id <- lapply(m, function(.ele) any(grepl(\"Mean\", as.character(.ele[[\"y\"]]))))\n            id <- which(unlist(id))\n            if(length(id)>0){\n                d <- l[[id[1]]]\n                pos <- which(d[, \"Mean\"] < 30)\n                if(length(pos)>0){\n                    pos <- which(d[, \"Mean\"] >= 30)\n                    ## start pos\n                    pos_l <- pos[1]\n                    ## end pos\n                    pos_r <- pos[length(pos)]\n                    c(pos_l, pos_r-pos_l+1)\n                }else{\n                    c(0, 0)\n                }\n            }else{\n                c(0, 0)\n            }\n        }\n        if(is.null(trimLeft[1]) || is.null(truncLen[1])){\n            trim_range_L <- getTrimRange(p_F)\n            if(PAIRED_END){\n                trim_range_R <- getTrimRange(p_R)\n                trimLeft = c(trim_range_L[1], trim_range_R[1])\n                truncLen = c(trim_range_L[2], trim_range_R[2])\n            }else{\n                trimLeft = c(trim_range_L[1])\n                truncLen = c(trim_range_L[2])\n            }\n        }\n    }else{\n        if(PAIRED_END){\n            trimLeft = c(0, 0)\n            truncLen = c(0, 0)\n        }else{\n            trimLeft = c(0)\n            truncLen = c(0)\n        }\n    }\n\n    # Perform filtering and trimming\n\n    #update trimLeft based on plot output\n    # For the first sequencing run\n    dir.create(OUTFOLDER)\n    filtFs.s1 <- file.path(OUTFOLDER, paste0(sample.names.1,\"_F_filt.fastq.gz\"))\n    filtRs.s1 <- file.path(OUTFOLDER, paste0(sample.names.1,\"_R_filt.fastq.gz\"))\n    if(PAIRED_END){\n        out <- filterAndTrim(fwd=fnFs.s1, filt=filtFs.s1,\n                            rev=fnRs.s1, filt.rev=filtRs.s1,\n                            trimLeft=trimLeft, truncLen=truncLen,\n                            maxN=0, maxEE=2, truncQ=2,\n                            compress=TRUE, verbose=TRUE,\n                            rm.phix=TRUE, multithread=NCORE)\n    }else{\n        out <- filterAndTrim(fwd=fnFs.s1, filt=filtFs.s1,\n                            trimLeft=trimLeft, truncLen=truncLen,\n                            maxN=0, maxEE=2, truncQ=2,\n                            compress=TRUE, verbose=TRUE,\n                            rm.phix=TRUE, multithread=NCORE)\n    }\n    saveRDS(out, FILTER_STATS)\n    \"\"\"\n}"], "list_proc": ["jianhong/16S_pipeline/FILTERING"], "list_wf_names": ["jianhong/16S_pipeline"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["16S_pipeline"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process QIIME_IMPORT {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(reads1, stageAs: \"sync/forward.fastq.gz\"), path(reads2, stageAs: \"sync/reverse.fastq.gz\"), path(index, stageAs: \"sync/barcodes.fastq.gz\")\n    val single_end\n\n    output:\n    tuple val(meta), path(\"${prefix}_emp-sequences.qza\")       , emit: reads\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix     = task.ext.prefix ?: \"${meta.id}\"\n    def type   = single_end ? 'EMPSingleEndSequences' : 'EMPPairedEndSequences'\n    \"\"\"\n    if [ \"${single_end}\" == \"true\" ]; then\n        mv sync/forward.fastq.gz sync/sequences.fastq.gz\n    fi\n    qiime tools import \\\\\n        --type $type \\\\\n        --input-path sync \\\\\n        --output-path ${prefix}_emp-sequences.qza \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/16S_pipeline/QIIME_IMPORT"], "list_wf_names": ["jianhong/16S_pipeline"]}, {"nb_reuse": 1, "tools": ["M-TRACK", "gencore", "getnumber"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["16S_pipeline"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process DADA2 {\n    tag \"$meta.id\"\n    tag \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads), path(stats)\n    path train_set\n    path species_assignment\n\n    output:\n    tuple val(meta), path(\"*.rds\")       , emit: robj\n    path '*.{png,csv}'                   , emit: qc\n    path 'dada2.out.txt'                 , emit: log\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n\n    pkgs <- c(\"dada2\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    set.seed(4)\n\n    FILTPATH <- \"$reads\"\n    NCORE <- ifelse(.Platform[[\"OS.type\"]]!=\"windows\", as.numeric(\"$task.cpus\"), FALSE)\n    TRAIN_SET <- \"$train_set\"\n    SPECIES_ASSIGNMENT <- \"$species_assignment\"\n    STATS <- \"$stats\"\n    SEQL1 <- 0\n    SEQL2 <- 0\n    TRYRC <- FALSE\n    SAMPLENAMES <- \"samplenames.1.rds\"\n    SEQTAB_S1 <- \"seqtab.s1.rds\"\n    SEQTAB <- \"seqtab.nochim.rds\"\n    TAXTAB <- \"taxtab.rds\"\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"seqlen1\"=c(\"--seq1\", \"-a\", \"integer\"),\n                        \"seqlen2\"=c(\"--seq2\", \"-b\", \"integer\"),\n                        \"tryRC\"=c(\"--tryRC\", \"-r\", \"logical\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"seqlen1\"]])){\n        SEQL1 <- opt[[\"seqlen1\"]]\n    }\n    if(!is.null(opt[[\"seqlen2\"]])){\n        SEQL2 <- opt[[\"seqlen2\"]]\n    }\n    if(!is.null(opt[[\"tryRC\"]])){\n        TRYRC <- opt[[\"tryRC\"]]\n    }\n\n    # stats\n    getN <- function(x) sum(getUniques(x))\n    track <- readRDS(STATS)\n\n    # Find filenames ----------------------------------------------------------\n\n    # Forward and reverse filenames\n    filts.s1 <- list.files(FILTPATH, full.names=TRUE)\n\n    # Sort to ensure fileneames are in the same order\n    filts.s1 <- sort(filts.s1)\n    sample.names.1 <- sapply(strsplit(basename(filts.s1),\"_\"), `[`, 1)\n    names(filts.s1) <- sample.names.1\n\n\n    # Separate forward and reverse samples\n    filtFs.s1 <- filts.s1[grepl(\"_F_filt\",filts.s1)]\n    filtRs.s1 <- filts.s1[grepl(\"_R_filt\",filts.s1)]\n\n    PAIRED_END <- length(filtRs.s1) == length(filtFs.s1)\n\n    sample.names.1 <- sapply(strsplit(basename(filtFs.s1), \"_\"), `[`, 1)\n    saveRDS(sample.names.1, SAMPLENAMES)\n\n    # Dereplication -----------------------------------------------------------\n\n    # Learn Error Rates\n    ## aim to learn from about 1M total reads - so just need subset of samples\n    ## source: http://benjjneb.github.io/dada2_pipeline_MV/bigdata.html\n    filts.learn.s1 <- sample(sample.names.1, 36)\n\n    derepFs.s1.learn <- derepFastq(filtFs.s1[filts.learn.s1], verbose=TRUE)\n    if(PAIRED_END) derepRs.s1.learn <- derepFastq(filtRs.s1[filts.learn.s1], verbose=TRUE)\n\n    # Sample Inference --------------------------------------------------------\n\n    dadaFs.s1.learn <- dada(derepFs.s1.learn, err=NULL, selfConsist=TRUE, multithread=NCORE)\n    if(PAIRED_END) dadaRs.s1.learn <- dada(derepRs.s1.learn, err=NULL, selfConsist=TRUE, multithread=NCORE)\n    rm(derepFs.s1.learn)\n    if(PAIRED_END) rm(derepRs.s1.learn)\n\n    # # Visualize estimated error rates\n    p<- plotErrors(dadaFs.s1.learn[[1]], nominalQ=TRUE)\n    ggsave(\"dada_errors_F_s1.png\", plot=p)\n    if(PAIRED_END) {\n        p<- plotErrors(dadaRs.s1.learn[[1]], nominalQ=TRUE)\n        ggsave(\"dada_errors_R_s1.png\", plot=p)\n    }\n\n    # Just keep the error profiles\n    errFs.s1 <- dadaFs.s1.learn[[1]][[\"err_out\"]]\n    if(PAIRED_END) errRs.s1 <- dadaRs.s1.learn[[1]][[\"err_out\"]]\n    rm(dadaFs.s1.learn)\n    if(PAIRED_END) rm(dadaRs.s1.learn)\n\n    # Now sample inference for entire dataset\n    # Run 1\n    derepFs.s1 <- vector(\"list\", length(sample.names.1))\n    dadaFs.s1 <- vector(\"list\", length(sample.names.1))\n    names(dadaFs.s1) <- sample.names.1\n    names(derepFs.s1) <- sample.names.1\n    if(PAIRED_END){\n        derepRs.s1 <- vector(\"list\", length(sample.names.1))\n        dadaRs.s1 <- vector(\"list\", length(sample.names.1))\n        names(dadaRs.s1) <- sample.names.1\n        names(derepRs.s1) <- sample.names.1\n    }\n\n    for (sam in sample.names.1){\n        message(\"Processing:\", sam, \"\\n\")\n        derepFs.s1[[sam]] <- derepFastq(filtFs.s1[[sam]])\n        dadaFs.s1[[sam]] <- dada(derepFs.s1[[sam]], err=errFs.s1, multithread=NCORE)\n        if(PAIRED_END){\n            derepRs.s1[[sam]] <- derepFastq(filtRs.s1[[sam]])\n            dadaRs.s1[[sam]] <- dada(derepRs.s1[[sam]], err=errRs.s1, multithread=NCORE)\n        }\n    }\n\n    # Run 1: Merge Paired Reads\n    if(PAIRED_END){\n        mergers.s1 <- mergePairs(dadaFs.s1, derepFs.s1, dadaRs.s1, derepRs.s1, verbose=TRUE)\n        head(mergers.s1)\n        track <- cbind(track, sapply(dadaFs.s1, getN), sapply(dadaRs.s1, getN),\n                        sapply(mergers.s1, getN))\n        colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\")\n        # Run 1: Clear up space\n        rm(derepFs.s1, derepRs.s1, dadaFs.s1, dadaRs.s1)\n    }else{\n        mergers.s1 <- dadaFs.s1\n        track <- cbind(track, sapply(dadaFs.s1, getN))\n        colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\")\n        rm(derepFs.s1, dadaFs.s1)\n    }\n\n    # Construct Sequence Table ------------------------------------------------\n\n    #To use only the forward reads\n    #follow https://github.com/benjjneb/dada2/issues/134\n\n    seqtab.s1 <- makeSequenceTable(mergers.s1)\n    saveRDS(seqtab.s1, SEQTAB_S1)\n    dim(seqtab.s1)\n    # Inspect the distributioh of sequence lengths\n    seqlenTab <- table(nchar(colnames(seqtab.s1)))\n    write.csv(seqlenTab, \"seqlenTab.csv\", row.names=FALSE)\n    if(SEQL1==0 && SEQL2==0){## auto detect cutoff range\n        maxV <- which.max(seqlenTab)\n        maxV <- as.numeric(names(seqlenTab)[maxV])\n        if(length(maxV)>1){\n            if(any(abs(diff(maxV))>2)){\n                stop(\"can not determine cutoff SEQ1 and SEQ2\")\n            }\n            maxV <- maxV[median(seq_along(maxV))]\n        }\n        SEQL1 <- maxV - 2\n        SEQL2 <- maxV + 2\n    }\n    #Trim sequences of interest\n    seqtab.s1 <- seqtab.s1[,nchar(colnames(seqtab.s1)) %in% seq(SEQL1,SEQL2)]\n    # Inspect the distributioh of sequence lengths\n    seqlenTab <- table(nchar(colnames(seqtab.s1)))\n    write.csv(seqlenTab, \"seqlenTab.filt.csv\", row.names=FALSE)\n\n    # Remove Chimeras ---------------------------------------------------------\n\n    seqtab.s1.nochim <- removeBimeraDenovo(seqtab.s1, method='consensus', multithread=NCORE, verbose=TRUE)\n    freq_chimeric <- c(number_row=nrow(seqtab.s1.nochim), number_col=ncol(seqtab.s1.nochim), frequency=sum(seqtab.s1.nochim)/sum(seqtab.s1))\n    write.csv(t(freq_chimeric), \"freq_chimeric.csv\", row.names=FALSE)\n    saveRDS(seqtab.s1.nochim, \"seqtab.s1.nochim.rds\")\n\n    track <- cbind(track, \"nonchim\"=rowSums(seqtab.s1.nochim))\n    rownames(track) <- sample.names.1\n    write.csv(track, \"processing_tracking.csv\")\n\n    # Merge Sequence Tables Together ------------------------------------------\n\n    seqtab.nochim <- seqtab.s1.nochim\n    saveRDS(seqtab.nochim, SEQTAB)\n\n\n    # Simplify naming ---------------------------------------------------------\n\n    seqtab <- seqtab.nochim\n\n    # Assign Taxonomy ---------------------------------------------------------\n    # Following: http://benjjneb.github.io/dada2_pipeline_MV/species.html\n\n    # Assign using Naive Bayes RDP\n    taxtab <- assignTaxonomy(colnames(seqtab), TRAIN_SET, tryRC=TRYRC, multithread=NCORE)\n\n    # improve with exact genus-species matches\n    # this step is pretty slow, should improve in later releases\n    # - note: Not allowing multiple species matches in default setting\n    taxtab <- addSpecies(taxtab, SPECIES_ASSIGNMENT, tryRC=TRYRC, verbose=TRUE)\n    saveRDS(taxtab, TAXTAB)\n\n    # How many sequences are classified at different levels? (percent)\n    classify_levels <- colSums(!is.na(taxtab))/nrow(taxtab)\n    write.csv(t(classify_levels), \"classify_levels.csv\", row.names=FALSE)\n\n    # copy the log file\n    file.copy(\".command.log\", \"dada2.out.txt\")\n    \"\"\"\n}"], "list_proc": ["jianhong/16S_pipeline/DADA2"], "list_wf_names": ["jianhong/16S_pipeline"]}, {"nb_reuse": 1, "tools": ["gencore", "refseqget", "MAP"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["16S_pipeline"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process PHYLOSEQ {\n    tag \"$meta.id\"\n    tag \"process_high\"\n    tag \"error_ignore\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-phyloseq=1.38.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-phyloseq:1.38.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-phyloseq:1.38.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(rds)\n    path metadata\n\n    output:\n    tuple val(meta), path(\"$prefix\")     , emit: phyloseq\n    path 'phyloseq.out.txt'              , emit: log\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n\n    pkgs <- c(\"phyloseq\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    set.seed(4)\n\n    MAPPING <- \"$metadata\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SEQTAB_S1 <- \"seqtab.s1.rds\"\n    SEQTAB <- \"seqtab.nochim.rds\"\n    TAXTAB <- \"taxtab.rds\"\n    SAMPLENAMES <- \"samplenames.1.rds\"\n    OUTFOLDER <- \"$prefix\"\n\n    # Make phyloseq object ----------------------------------------------------\n    # import data from dada2 output\n    sample.names.1 <- readRDS(SAMPLENAMES)\n    seqtab.s1 <- readRDS(SEQTAB_S1)\n    seqtab <- readRDS(SEQTAB)\n    taxtab <- readRDS(TAXTAB)\n\n    # Import mapping\n    map1 <- read.csv(MAPPING, stringsAsFactors = FALSE)\n    map1 <- map1[map1[, \"SampleID\"] %in% sample.names.1,]\n    map <- as.data.frame(map1) # without this line get sam_data slot empty error from phyloseq\n    rownames(map) <- map[, \"SampleID\"]\n\n    # Make refseq object and extract sequences from tables\n    refseq <- colnames(seqtab)\n    names(refseq) <- paste0('seq_', seq_along(refseq))\n    colnames(seqtab) <- names(refseq[match(colnames(seqtab), refseq)])\n    rownames(taxtab) <- names(refseq[match(rownames(taxtab), refseq)])\n\n    # Write the taxtable, seqtable, and refseq to ascii ------------------------\n    dir.create(OUTFOLDER, recursive=TRUE)\n    write.table(seqtab, file=file.path(OUTFOLDER, 'seqtab.nochim.tsv'), quote=FALSE, sep='\\\\t')\n    write.table(taxtab, file=file.path(OUTFOLDER, 'taxtab.nochim.tsv'), quote=FALSE, sep='\\\\t')\n    write.table(refseq, file=file.path(OUTFOLDER, 'refseqs.nochim.tsv'), quote=FALSE, sep='\\\\t', col.names = FALSE)\n\n    # Combine into phyloseq object\n    ps <- phyloseq(otu_table(seqtab, taxa_are_rows = FALSE), sample_data(map), tax_table(taxtab))\n    saveRDS(ps, file.path(OUTFOLDER, 'phyloseq.rds'))\n\n    # copy the log file\n    file.copy(\".command.log\", \"phyloseq.out.txt\")\n    \"\"\"\n}"], "list_proc": ["jianhong/16S_pipeline/PHYLOSEQ"], "list_wf_names": ["jianhong/16S_pipeline"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["16S_pipeline"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process QIIME_DEMUX {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(qza)\n    path barcodes\n    val single_end\n\n    output:\n    tuple val(meta), path(\"${prefix}_demux-full.qza\")   , emit: reads\n    tuple val(meta), path(\"${prefix}_demux-details.qza\"), emit: details\n    tuple val(meta), path(\"${prefix}_demux-summary.qzv\"), emit: summary\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mkdir -p tmp\n    export TMPDIR=./tmp\n    export TMP=./tmp\n    export TEMP=./tmp\n\n    if [ \"${single_end}\" == \"true\" ]; then\n        qiime demux emp-single \\\\\n            --m-barcodes-file $barcodes \\\\\n            --i-seqs $qza \\\\\n            --o-per-sample-sequences ${prefix}_demux-full.qza \\\\\n            --o-error-correction-details ${prefix}_demux-details.qza \\\\\n            $args\n    else\n        qiime demux emp-paired \\\\\n            --m-barcodes-file $barcodes \\\\\n            --i-seqs $qza \\\\\n            --o-per-sample-sequences ${prefix}_demux-full.qza \\\\\n            --o-error-correction-details ${prefix}_demux-details.qza \\\\\n            $args\n    fi\n\n    qiime demux summarize \\\\\n        --i-data ${prefix}_demux-full.qza \\\\\n        --o-visualization ${prefix}_demux-summary.qzv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/16S_pipeline/QIIME_DEMUX"], "list_wf_names": ["jianhong/16S_pipeline"]}, {"nb_reuse": 1, "tools": ["JSpecies", "G-BLASTN", "SC3"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["ATACseqQCnextflow"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["\nprocess runBlast {\n  cpus params.blastn.cpus\n  \n  input:\n    set runID, file(readL), file(readR) from readPairsTrimmed4blast\n  \n  storeDir \"${params.outdir}/taxReport/${runID}\"\n  \n  output:\n    set runID, file(\"${runID}.blastn.txt\"), file(\"${runID}.csv\") into blastnFiles\n  \n  script:\n    \"\"\"\n    zcat ${readL} | head -n ${params.blastn.numOfLine} > ${runID}.fq\n    fq2sc -i ${runID}.fq -o ${runID}.txt\n    s2c_2_fasta.pl --in ${runID}.txt --out ${runID}.fa --notfilterN\n    blastn -num_threads ${params.blastn.cpus} -db nt -max_target_seqs 1 \\\\\n    -outfmt '6 qaccver saccver staxid sblastname ssciname scomname pident length mismatch gapopen qstart qend sstart send evalue bitscore' \\\\\n    -query ${runID}.fa -out ${runID}.blastn.txt\n    cat <<EOT | R --vanilla\nx <- read.delim(\"${runID}.blastn.txt\", header=FALSE, stringsAsFactor=FALSE)\ncolnames(x) <- c(\"qaccver\", \"saccver\", \"staxid\", \"sblastname\", \"ssciname\", \"scomname\", \"pident\", \"length\", \"mismatch\", \"gapopen\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\")\nx <- x[x\\$mismatch<=${params.blastn.mismatch}, ]\nx <- x[order(x\\$qaccver, -x\\$pident, x\\$evalue), ]\nx <- x[!duplicated(x\\$qaccver), ]\nsc <- as.numeric(sub(\"SEQ_.*?_x\", \"\", x\\$qaccver))\nspecies <- paste0(x\\$ssciname, \" (\", x\\$scomname, \")\", \" [\", x\\$sblastname, \"]\")\nw <- rowsum(sc, species)\nw <- w[order(-w[, 1]), ]\np <- w/${params.blastn.numOfLine}\nwrite.table(p, '${runID}.percentage.txt', quote=FALSE, col.names=FALSE, sep='\\t')\nf <- dir(\".\", \"percentage.txt\", full.names = TRUE)\nd <- lapply(f, read.delim, nrows=3, header=FALSE)\ne <- sapply(d, function(.ele){\n  if(nrow(.ele)>1){\n    .ele[1, 2]/sum(.ele[-1, 2])\n  }else{\n    100\n  }\n})\ne <- cut(e, breaks=c(0, 3, 5, 10, Inf), labels=c(\"contaminated\", \"warning\", \"minor\", \"clean\"))\ne <- rep(e, each=3)\nfq <- sub(\".percentage.txt\", \"\", basename(f))\nfq <- rep(fq, each=3)\nd <- do.call(rbind, d)\nd <- cbind(d, fastq=fq, cross.species.contamination=e)\ncolnames(d) <- c(\"species\", \"percentage\", \"fastq\", \"cross.species.contamination\")\nwrite.csv(d, \"${runID}.csv\", row.names=FALSE)\nEOT\n    \"\"\"\n}"], "list_proc": ["jianhong/ATACseqQCnextflow/runBlast"], "list_wf_names": ["jianhong/ATACseqQCnextflow"]}, {"nb_reuse": 1, "tools": ["TSSer", "PhySigs", "TSSV", "Mgenome"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["ATACseqQCnextflow"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["\nprocess runATACseqQC{\n  cpus params.R.cpus\n  \n  input:\n    set runID, file(bam), file(bai) from mappedFiles4ATACseqQC\n  \n  storeDir \"${params.outdir}/ATACseqQC/${runID}\"\n  \n  output:\n    set runID, file(\"*.pdf\") into ATACseqQCout\n    set runID, file(\"*.bam\"), file(\"*.bai\") into ATACseqQCbam\n  \n  script:\n    \"\"\"\n    cat <<EOF | ${params.R.path} --vanilla\n    library(BiocManager)\n    install(\"${params.R.BSgenome}\")\n    install(\"${params.R.TxDb}\")\n    library(ATACseqQC)\n    bamfile <- \"${bam}\"\n    bamfile.labels <- gsub(\".bam\", \"\", basename(bamfile))\n    pdf(\"fragmentSizeDistribution.pdf\")\n    fragSize <- fragSizeDist(bamfile, bamfile.labels)\n    dev.off()\n    library(${params.R.TxDb})\n    txs <- transcripts(${params.R.TxDb})\n    library(${params.R.BSgenome})\n    seqlev <- paste0(\"chr\", c(1:21, \"X\", \"Y\"))\n    which <- as(seqinfo(${params.R.BSgenome})[seqlev], \"GRanges\")\n    gal <- readBamFile(bamfile, which=which, asMates=TRUE, bigFile=TRUE)\n    gal1 <- shiftGAlignmentsList(gal, outbam=\"shifted.bam\")\n    pt <- PTscore(gal1, txs)\n    pdf(\"PTscore.pdf\")\n    plot(mcols(pt)[, \"log2meanCoverage\"], mcols(pt)[, \"PT_score\"], \n     xlab=\"log2 mean coverage\",\n     ylab=\"Promoter vs Transcript\")\n    dev.off()\n    nfr <- NFRscore(gal1, txs)\n    pdf(\"NFRscore.pdf\")\n    plot(mcols(nfr)[, \"log2meanCoverage\"], mcols(nfr)[, \"NFR_score\"], \n     xlab=\"log2 mean coverage\",\n     ylab=\"Nucleosome Free Regions score\",\n     main=\"NFRscore for 200bp flanking TSSs\",\n     xlim=c(-10, 0), ylim=c(-5, 5))\n    dev.off()\n    tsse <- TSSEscore(gal1, txs)\n    pdf(\"TSSEscore.pdf\")\n    hist(mcols(tsse)[, \"TSS.enrichment.score\"], breaks=100, \n    main=\"Transcription Start Site (TSS) Enrichment Score\", \n    xlab=\"TSS enrichment score\")\n    dev.off()\n    gc(reset=TRUE)\n    genome <- ${params.R.BSgenome}\n    objs <- splitGAlignmentsByCut(gal1, txs=txs, genome=genome, outPath = \".\")\n    rm(gal1)\n    gc(reset=TRUE)\n    library(ChIPpeakAnno)\n    bamfiles <- file.path(\".\",\n                     c(\"NucleosomeFree.bam\",\n                     \"mononucleosome.bam\",\n                     \"dinucleosome.bam\",\n                     \"trinucleosome.bam\"))\n    pdf(\"cumulativePercentage.pdf\")\n    cumulativePercentage(bamfiles[1:2], as(seqinfo(${params.R.BSgenome})[seqlev], \"GRanges\"))\n    dev.off()\n    TSS <- promoters(txs, upstream=0, downstream=1)\n    TSS <- unique(TSS)\n    librarySize <- estLibSize(bamfiles)\n    NTILE <- 101\n    dws <- ups <- 1010\n    sigs <- enrichedFragments(gal=objs[c(\"NucleosomeFree\", \n                                     \"mononucleosome\",\n                                     \"dinucleosome\",\n                                     \"trinucleosome\")], \n                          TSS=TSS,\n                          librarySize=librarySize,\n                          seqlev=seqlev,\n                          TSS.filter=0.5,\n                          n.tile = NTILE,\n                          upstream = ups,\n                          downstream = dws)\n      sigs.log2 <- lapply(sigs, function(.ele) log2(.ele+1))\n      pdf(\"featureAligndHeatmap.pdf\")\n      featureAlignedHeatmap(sigs.log2, reCenterPeaks(TSS, width=ups+dws),\n                      zeroAt=.5, n.tile=NTILE)\n      dev.off()\n      out <- featureAlignedDistribution(sigs, \n                                  reCenterPeaks(TSS, width=ups+dws),\n                                  zeroAt=.5, n.tile=NTILE, type=\"l\", \n                                  ylab=\"Averaged coverage\")\n    range01 <- function(x){(x-min(x))/(max(x)-min(x))}\n    out <- apply(out, 2, range01)\n    pdf(\"TSS.profile.pdf\")\n    matplot(out, type=\"l\", xaxt=\"n\", \n        xlab=\"Position (bp)\", \n        ylab=\"Fraction of signal\")\n    axis(1, at=seq(0, 100, by=10)+1, \n     labels=c(\"-1K\", seq(-800, 800, by=200), \"1K\"), las=2)\n    abline(v=seq(0, 100, by=10)+1, lty=2, col=\"gray\")\n    dev.off()\n    library(MotifDb)\n    CTCF <- query(MotifDb, c(\"CTCF\"))\n    CTCF <- as.list(CTCF)\n    pdf(\"CTCF.footprint.pdf\")\n    sigs <- factorFootprints(\"shifted.bam\", pfm=CTCF[[1]], \n                         genome=genome,\n                         min.score=\"90%\", seqlev=seqlev,\n                         upstream=100, downstream=100)\n    dev.off()\n    pdf(\"CTCF.Vplot.pdf\")\n    vp <- vPlot(\"shifted.bam\", pfm=CTCF[[1]], \n            genome=genome, min.score=\"90%\", seqlev=seqlev,\n            upstream=200, downstream=200, \n            ylim=c(30, 250), bandwidth=c(2, 1))\n    dev.off()\n    unlink(\"Rplots.pdf\")\nEOF\n    \"\"\"\n}"], "list_proc": ["jianhong/ATACseqQCnextflow/runATACseqQC"], "list_wf_names": ["jianhong/ATACseqQCnextflow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["nextflowTutorial"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["\nprocess FASTQC {\n  tag \"$meta.id\"\n  publishDir \"${params.outdir}/${params.options.publish_dir}\", mode: 'copy'\n  conda (params.conda ? \"bioconda::fastqc=0.11.9\" : null)\n\n  input:\n    tuple val(meta), path(reads)\n\n  output:\n    tuple val(meta), path(\"*.html\"), emit: html\n\n  script:\n    def prefix = meta.id\n    if(meta.single_end){\n      \"\"\"\n      [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads[0] ${prefix}.fastq.gz\n      fastqc --threads $task.cpus ${prefix}.fastq.gz\n      \"\"\"\n    }else{\n      \"\"\"\n      [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n      [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n      fastqc --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n      \"\"\"\n    }\n}"], "list_proc": ["jianhong/nextflowTutorial/FASTQC"], "list_wf_names": ["jianhong/nextflowTutorial"]}, {"nb_reuse": 0, "tools": ["GIV", "IDSM"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process ENSEMBL_UCSC_CONVERT {\n    tag \"$fname\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-rtracklayer=1.50.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' :\n        'quay.io/biocontainers/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' }\"\n\n    input:\n    tuple val(bin_size), path(fname)\n\n    output:\n    tuple val(bin_size), path(\"{UCSC,ensembl}.${fname}\"), emit: tab\n    path \"versions.yml\"                                 , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    pkgs <- c(\"GenomeInfoDb\", \"rtracklayer\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    toUCSC = \"$args\"==\"toUCSC\"\n    inf = \"$fname\"\n    ## check file format\n    ## if it is bigwig file\n    isBWF <- grepl(\"\\\\\\\\.(bw|bigwig)\", inf, ignore.case=TRUE)\n    if(isBWF){## decrease the memory cost\n        bwfile <- BigWigFile(inf)\n        seqinfo <- seqinfo(bwfile)\n        seqstyle <- seqlevelsStyle(seqinfo)\n    }else{\n        data <- import(inf)\n        seqstyle <- seqlevelsStyle(data)\n    }\n    readBWFile <- function(f, seqinfo){\n        gr <- as(seqinfo, \"GRanges\")\n        data <- GRanges()\n        for(s in seq_along(gr)){\n            dat <- import.bw(f, which = gr[s])\n            dat <- coverage(dat, weight = dat\\$score)\n            dat <- as(dat, \"GRanges\")\n            dat <- dat[dat\\$score > 0] ## negative scores are not allowed\n            data <- c(data, dat)\n        }\n        data <- coverage(data, weight = data\\$score)\n        data <- as(data, \"GRanges\")\n        data <- data[data\\$score > 0]\n        return(data)\n    }\n    if(toUCSC){\n        if(!\"UCSC\" %in% seqstyle){ ## convert to UCSC style\n            if(isBWF){\n                data <- readBWFile(inf, seqinfo)\n            }\n            seqlevelsStyle(data) <- \"UCSC\"\n            ## double check\n            if(sum(grepl(\"^chr\", seqlevels(data)))==0){\n                ids <- grepl(\"^((\\\\\\\\d{1,2})|(IX|IV|V?I{0,3})|([XYMT]{1,2}))\\$\", seqlevels(data))\n                seqlevels(data)[ids] <- paste0(\"chr\", seqlevels(data)[ids])\n            }\n            export(data, file.path(dirname(inf), paste0(\"UCSC.\", basename(inf))))\n        }else{\n            file.copy(inf, file.path(dirname(inf), paste0(\"UCSC.\", basename(inf))))\n        }\n    }else{\n        if(!\"Ensembl\" %in% seqstyle){## convert to Ensembl style\n            if(isBWF){\n                data <- readBWFile(inf, seqinfo)\n            }\n            seqlevelsStyle(data) <- \"Ensembl\"\n            ## double check\n            if(sum(grepl(\"^chr\", seqlevels(data)))>0){\n                ids <- grepl(\"^(chr)((\\\\\\\\d{1,2})|(IX|IV|V?I{0,3})|([XYMT]{1,2}))\\$\", seqlevels(data))\n                seqlevels(data)[ids] <- sub(\"chr\", \"\", seqlevels(data)[ids])\n            }\n            export(data, file.path(dirname(inf), paste0(\"ENSEMBL.\", basename(inf))))\n        }else{\n            file.copy(inf, file.path(dirname(inf), paste0(\"ENSEMBL.\", basename(inf))))\n        }\n    }\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["4peaks"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process MAPS_RAW2BG2 {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"conda-forge::r-data.table=1.12.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-data.table:1.12.2' :\n        'quay.io/biocontainers/r-data.table:1.12.2' }\"\n\n    input:\n    tuple val(meta), val(bin_size), path(peak)\n\n    output:\n    tuple val(meta), val(bin_size), path(\"${prefix}.bg2\")          , emit: bg2\n    tuple val(meta), val(bin_size), path(\"${prefix}.ginteractions\"), emit: gi\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix   = task.ext.prefix ? \"${task.ext.prefix}\" : \"${meta.id}_${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #########################################\n    # Author: jianhong ou\n    # create single files from reg_raw to bedgraph for cool load and juicerbox\n    #########################################\n    versions <- c(\"${task.process}:\", \"    MAPS: 1.1.0\")\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    options(\"scipen\"=999)\n    library(data.table)\n    RESOLUTION = as.numeric($bin_size)\n    BG2_OUT = \"${prefix}.bg2\"\n    GI_OUT  = \"${prefix}.ginteractions\"\n    infs = strsplit(\"${peak}\", \"\\\\\\\\s+\")[[1]]\n\n    peaks_final_out <- lapply(infs, function(inf){\n        peaks = as.data.table(read.table(inf, header=TRUE, stringsAsFactors=FALSE))\n        if(nrow(peaks)==0){\n            peaks\\$bin1_end <- peaks\\$bin2_end <- numeric(0)\n        }else{\n            peaks\\$bin1_end = peaks\\$bin1_mid + RESOLUTION\n            peaks\\$bin2_end = peaks\\$bin2_mid + RESOLUTION\n        }\n        peaks_final = subset(peaks, select = c( \"chr\", \"bin1_mid\", \"bin1_end\",\n                                                \"chr\", \"bin2_mid\", \"bin2_end\",\n                                                \"ratio2\"))\n        colnames(peaks_final) = c('chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2', 'count')\n        peaks_final\n    })\n    peaks_final_out <- do.call(rbind, peaks_final_out)\n    peaks_final_out <- peaks_final_out[peaks_final_out\\$count>0, , drop=FALSE]\n    peaks_final_out <- peaks_final_out[order(peaks_final_out\\$chrom1,\n                                            peaks_final_out\\$start1,\n                                            peaks_final_out\\$chrom2,\n                                            peaks_final_out\\$start2), , drop=FALSE]\n    write.table(peaks_final_out, BG2_OUT, row.names = FALSE, col.names = FALSE, quote=FALSE, sep='\\t')\n    #print <strand1> <chr1> <pos1> <frag1> <strand2> <chr2> <pos2> <frag2> <score>\n    peaks_final_out = cbind(0, peaks_final_out[, c('chrom1', 'start1')], 0,\n                            0, peaks_final_out[, c('chrom2', 'start2')], 1,\n                            peaks_final_out[, 'count'])\n    write.table(peaks_final_out, GI_OUT, row.names = FALSE, col.names = FALSE, quote=FALSE, sep='\\t')\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["LORD", "Q2LM", "Gene"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["\nprocess CREATE_PAIRS {\n    publishDir \"${params.outdir}/pairs\",\n        mode: \"copy\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    } else {\n        container \"quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    }\n\n    input:\n    path chromsizes\n    path gtf\n    path digest_genome_bed\n    val reads\n    val seed\n\n    output:\n    path \"$peak1\"       , emit: peak1\n    path \"$peak2\"       , emit: peak2\n    path \"$pairs\"       , emit: pairs\n    path \"$distalpairs\" , emit: distalpairs\n    path \"$interactions\", emit: interactions\n\n    script:\n    peak1=\"peak1.bed\"\n    peak2=\"peak2.bed\"\n    pairs=\"test.unselected.pairs.gz\"\n    distalpairs=\"distal.pairs.gz\"\n    cut=\"CviQI.bed\"\n    interactions=\"interaction.bedpe\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(GenomicRanges)\n    library(rtracklayer)\n    library(GenomicFeatures)\n    library(InteractionSet)\n    options(scipen=10)\n    set.seed($seed)\n    peak_num_1 <- 10000      # fake R1 events\n    peak_num_2 <- 1000       # fake R2 events\n    real_r1 <- 1000          # real R1 events\n    real_r2 <- 100           # real R2 events\n    real_conn_num  <- 1e4    # real connection events\n    peak_reads_num <- $reads # total signal reads\n    background_lambda <- .5  # background lambda\n    real_connection <- expand.grid(sample.int(peak_num_1, real_r1), sample.int(peak_num_2, real_r1))\n    real_connection <- real_connection[sample.int(nrow(real_connection), real_conn_num), ]\n    rtnorm <- function(n, mean = 0, sd = 1, min = 0, max = 1) { ## help function\n        bounds <- pnorm(c(min, max), mean, sd)\n        u <- runif(n, bounds[1], bounds[2])\n        round(qnorm(u, mean, sd))\n    }\n    ## R1 peak candidates\n    r1s <- import(\"$digest_genome_bed\")\n    r1s <- r1s[-1]\n    width(r1s) <- 1\n    export(r1s, \"$cut\")\n    chromsize <- read.delim(\"$chromsizes\", header=FALSE)\n    chromosomes <- chromsize[, 1]\n    seqlengths <- chromsize[, 2]\n    names(seqlengths) <- chromosomes\n    tileGenome <- tileGenome(seqlengths, tilewidth=500)\n    tileGenome <- unlist(tileGenome)\n    tileGenome\\$count <- countOverlaps(tileGenome, r1s)\n    tileGenome <- tileGenome[tileGenome\\$count>0]\n    stopifnot(length(tileGenome)>peak_num_1*2)\n    peaks1 <- sample(seq_along(tileGenome), peak_num_1*2, replace=FALSE)\n    peaks1 <- tileGenome[peaks1]\n    peaks1 <- reduce(peaks1)\n    peaks1 <- peaks1[sample.int(length(peaks1), peak_num_1)]\n\n    ## R2 peak candidates from promoter\n    txdb <- makeTxDbFromGFF(\"$gtf\")\n    gene <- genes(txdb)\n    pro <- promoters(gene, upstream=4000, downstream=1000)\n    pro <- reduce(pro)\n    stopifnot(length(pro)>peak_num_2)\n    peaks2 <- sample.int(length(pro), peak_num_2, replace=FALSE)\n    peaks2 <- pro[peaks2]\n    peaks2 <- shift(peaks2, shift=2000)\n    w <- rtnorm(peak_num_2, mean=800, sd=300, min=350, max=5000)\n    width(peaks2) <- w\n\n    ## real connections\n    real_gi <-GInteractions(anchor1=peaks1[real_connection[, 1]],\n                            anchor2=peaks2[real_connection[, 2]])\n    ## generate R1 background reads\n    ol <- findOverlaps(peaks1, r1s)\n    q1 <- unique(subjectHits(ol))\n    bg_ids <- rpois(length(q1), lambda=background_lambda) #background for r1\n    bg_ids <- rep(q1, bg_ids)\n    r1_background <- r1s[bg_ids]\n    ## generate R2 background reads\n    q2 <- tile(peaks2, width=1)\n    q2_bkg <- unlist(q2)\n    q2_bkg <- q2_bkg[sample.int(length(q2_bkg), length(r1s))]\n    r2_background <- q2_bkg[bg_ids]\n    ## generate ATAC reads, r1/r2 distance < 1e4\n    ol <- findOverlaps(peaks2, r1s, maxgap=1e3)\n    q1 <- split(subjectHits(ol), queryHits(ol))\n    keep <- unique(queryHits(ol))\n    mn <- peak_reads_num*.6/length(peaks2)\n    atac_ids <- rtnorm(length(keep), mean = mn, sd = mn/10, min=mn/2, max=mn*10) # 60%\n    atac_r2_ids <- mapply(lengths(q2[keep]), atac_ids, FUN=sample.int, replace=TRUE, SIMPLIFY=FALSE)\n    atac_r2_ids <- c(atac_r2_ids[[1]], mapply(cumsum(lengths(q2[keep][-length(q2[keep])])), atac_r2_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r2_atac <- unlist(q2[keep])[unlist(atac_r2_ids)]\n    atac_r1_ids <- mapply(lengths(q1), atac_ids, FUN=sample.int, replace=TRUE, SIMPLIFY=FALSE)\n    atac_r1_ids <- c(atac_r1_ids[[1]], mapply(cumsum(lengths(q1[-length(q1)])), atac_r1_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r1_atac <- r1s[unlist(q1)[unlist(atac_r1_ids)]]\n\n    ## generate real reads\n    mn <- peak_reads_num*.4/real_conn_num\n    mcols(real_gi)[, \"counts\"] <- rtnorm(real_conn_num, mean=mn, sd=mn/2, min=mn/10, max=mn*10)\n    ol <- findOverlaps(first(real_gi), r1s)\n    ols <- split(subjectHits(ol), queryHits(ol))\n    r1_ids <- mapply(lengths(ols), mcols(real_gi)[, \"counts\"], FUN=sample, replace=TRUE, SIMPLIFY=FALSE)\n    r1_ids <- mapply(ols, r1_ids, FUN=function(a, i) a[i], SIMPLIFY=FALSE)\n    r1_ids <- unlist(r1_ids)\n    r1_signal <- r1s[r1_ids]\n    q2 <- tile(second(real_gi), width=50)\n    r2_ids <- mapply(lengths(q2), mcols(real_gi)[, \"counts\"], FUN=sample, replace=TRUE, SIMPLIFY=FALSE)\n    r2_ids <- c(r2_ids[[1]], mapply(cumsum(lengths(q2[-length(q2)])), r2_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r2_signal <- unlist(q2)[unlist(r2_ids)]\n    r2_signal <- shift(r2_signal, shift=sample.int(50, length(r2_signal), replace=TRUE))\n    width(r2_signal) <- 1\n    reads1 <- c(r1_background, r1_atac, r1_signal)\n    reads2 <- c(r2_background, r2_atac, r2_signal)\n    ## sort the reads by r1\n    ord <- order(reads1, reads2)\n    reads1 <- reads1[ord]\n    reads2 <- reads2[ord]\n\n    N <- length(reads1)\n    readID <- paste0(\"r\", formatC(seq.int(N), width=nchar(as.character(N)), flag=\"0\"))\n    strand1 <- sample(c(\"+\", \"-\"), N, replace=TRUE)\n    strand2 <- ifelse(strand1==\"-\", \"+\", \"-\")\n\n    # write peaks1.bed\n    export(peaks1, \"$peak1\")\n    # write peaks2.bed\n    export(peaks2, \"$peak2\")\n    # write pairs\n    header <- c(\"## pairs format v1.0.0\",\n                \"#shape: whole matrix\",\n                \"#genome_assembly: unknown\",\n                paste(\"#chromosomes:\", paste(chromsize[, 1], collapse=\" \")),\n                paste(\"#chromsize:\", chromsize[, 1], chromsize[, 2]),\n                paste0(\"#samheader: @SQ\tSN:\", chromsize[, 1],\"\tLN:\", chromsize[, 2]),\n                \"#columns: readID chrom1 pos1 chrom2 pos2 strand1 strand2 pair_type\")\n    content <- paste(readID,\n                    as.character(seqnames(reads1)),\n                    start(reads1),\n                    as.character(seqnames(reads2)),\n                    start(reads2),\n                    strand1, strand2, \"UU\",\n                    sep=\"\\\\t\")\n    gz <- gzfile(\"$pairs\", \"w\")\n    writeLines(c(header, content), gz)\n    close(gz)\n    # write distalpair\n    dist <- distance(reads1, reads2)\n    content_dist <- content[dist>2000]\n    gz2 <- gzfile(\"$distalpairs\", \"w\")\n    writeLines(c(header, content_dist), gz2)\n    close(gz2)\n    # write real connections\n    export(real_gi, \"$interactions\")\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process MERGE_PEAK {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    path peak\n\n    output:\n    path \"merged_peak.bed\"    , emit: peak\n    path \"versions.yml\"       , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cat $peak | \\\\\n        cut -f1-3 | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        bedtools merge $args \\\\\n            -i stdin > merged_peak.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["4peaks", "FDR"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process ASSIGN_TYPE {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-chippeakanno=3.26.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(counts)\n\n    output:\n    tuple val(meta), path(\"summary.*.txt\"), optional: true, emit: summary\n    tuple val(meta), path(\"*.peaks\")      , optional: true, emit: peak\n    tuple val(meta), path(\"*.bedpe\")      , optional: true, emit: bedpe\n    path \"versions.yml\"                                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 assign interacion type for the peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"graph\", \"RBGL\", \"InteractionSet\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## Options\n    ## make_option(c(\"-c\", \"--count_cutoff\"), type=\"integer\", default=12, help=\"count cutoff, default 12\", metavar=\"integer\")\n    ## make_option(c(\"-r\", \"--ratio_cutoff\"), type=\"numeric\", default=2.0, help=\"ratio cutoff, default 2.0\", metavar=\"float\")\n    ## make_option(c(\"-f\", \"--fdr\"), type=\"integer\", default=2, help=\"-log10(fdr) cutoff, default 2\", metavar=\"integer\")\n    ## make_option(c(\"-i\", \"--interactions\"), type=\"character\", default=NULL, help=\"interactions output by call hipeak\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"peaks\", help=\"sample name of the output prefix\", metavar=\"string\")\n\n    OUTPUT = \".\"\n    GROUP_ID = \"$meta.id\"\n    COUNT_CUTOFF = 12\n    RATIO_CUTOFF = 2.0\n    FDR = 2\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"count_cutoff\"=c(\"--count_cutoff\", \"-c\", \"integer\"),\n                        \"ratio_cutoff\"=c(\"--ratio_cutoff\", \"-r\", \"numeric\"),\n                        \"fdr\"=c(\"--fdr\", \"-f\", \"integer\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    if(!is.null(opt\\$count_cutoff)){\n        COUNT_CUTOFF <- opt\\$count_cutoff\n    }\n    if(!is.null(opt\\$ratio_cutoff)){\n        RATIO_CUTOFF <- opt\\$ratio_cutoff\n    }\n    if(!is.null(opt\\$fdr)){\n        FDR <- opt\\$fdr\n    }\n\n    peaks <- read.csv(\"$counts\")\n\n    if(!all(c(\"chr1\", \"start1\", \"end1\", \"width1\",\n            \"chr2\", \"start2\", \"end2\", 'width2',\n            \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount',\n            \"ratio2\", 'fdr') %in% colnames(peaks))){\n        stop(\"count table is not in correct format.\")\n    }\n\n    classify_peaks <- function(final) {\n        # group the interactions\n        gi <- with(final, GInteractions(GRanges(chr1, IRanges(start1, end1)), GRanges(chr2, IRanges(start2, end2))))\n        ol1 <- findOverlaps(first(gi), drop.self = TRUE, drop.redundant = TRUE)\n        ol2 <- findOverlaps(second(gi), drop.self = TRUE, drop.redundant = TRUE)\n        ol <- unique(c(queryHits(ol1), subjectHits(ol1), queryHits(ol2), subjectHits(ol2)))\n        ol_ <- seq_along(gi)[-ol]\n\n        group <- unique(rbind(as.data.frame(ol1), as.data.frame(ol2)))\n        colnames(group) <- c(\"from\", \"to\")\n        group\\$weight <- rep(1L, nrow(group))\n        group <- split(group, seqnames(first(gi)[group\\$from]))\n        group <- mapply(group, names(group), FUN=function(.ele, .name){\n            if(length(unique(c(.ele[, 1], .ele[, 2])))>sqrt(.Machine\\$integer.max)){\n                .nodes <- unique(c(.ele[, 1], .ele[, 2]))\n                .max_nodes <- floor(sqrt(.Machine\\$integer.max)/2)\n                .nodes_group <- rep(seq.int(ceiling(length(.nodes)/.max_nodes)),\n                                    each=.max_nodes)[seq_along(.nodes)]\n                names(.nodes_group) <- .nodes\n                .ele <- split(.ele, paste(.nodes_group[as.character(.ele[, 1])],\n                                        .nodes_group[as.character(.ele[, 2])],\n                                        sep=\"_\"))\n                .ele <- lapply(.ele, function(.e){\n                    .e <- graphBAM(.e)\n                    .e <- connectedComp(ugraph(.e))\n                    .e <- lapply(.e, as.numeric)\n                })\n                .g <- mapply(.ele, seq_along(.ele), FUN=function(.e, .id){\n                    data.frame( nodes=unlist(.e),\n                                group=rep(paste(.id, names(.e), sep=\"_\"), lengths(.e)))\n                }, SIMPLIFY=FALSE)\n                .g <- do.call(rbind, .g)\n                .gs <- split(.g[, 2], .g[, 1])\n                .gs <- lapply(.gs, unique)\n                .gs <- .gs[!duplicated(.gs)]\n                while(any(lengths(.gs)>1)){\n                    ## merge parents\n                    .gsn <- vapply(.gs, FUN=function(.e) sort(.e)[1], FUN.VALUE=character(1))\n                    .gsn <- rep(.gsn, lengths(.gs))\n                    names(.gsn) <- unlist(.gs)\n                    .gsn <- .gsn[names(.gsn)!=.gsn]\n                    .k <- .g[, \"group\"] %in% names(.gsn)\n                    .g[.k, \"group\"] <- .gsn[.g[.k, \"group\"]]\n                    .gs <- split(.g[, 2], .g[, 1])\n                    .gs <- lapply(.gs, unique)\n                    .gs <- .gs[!duplicated(.gs)]\n                }\n                .g <- unique(.g)\n                .ele <- split(.g[, \"nodes\"], .g[, \"group\"])\n                names(.ele) <- seq_along(.ele)\n                rm(.g, .gs, .gsn, .nodes_group, .nodes)\n            }else{\n                .ele <- graphBAM(.ele)\n                .ele <- connectedComp(ugraph(.ele))\n                .ele <- lapply(.ele, as.numeric)\n            }\n            data.frame( id=unlist(.ele),\n                        g=rep(paste(.name, seq_along(.ele), sep=\"_\"), lengths(.ele)))\n        }, SIMPLIFY=FALSE)\n        group <- do.call(rbind, group)\n\n        final\\$Cluster <- NA\n        final\\$Cluster[group\\$id] <- group\\$g\n        final\\$ClusterSize <- 0\n        final\\$ClusterSize[group\\$id] <- table(group\\$g)[group\\$g]\n        if(any(is.na(final\\$Cluster))) final\\$Cluster[is.na(final\\$Cluster)] <- paste0(\"Singleton_\", seq.int(sum(is.na(final\\$Cluster))))\n        final\\$NegLog10P <- -log10( final\\$p_val_reg2 )\n        final\\$NegLog10P[is.na(final\\$NegLog10P)] <- 0\n        final\\$NegLog10P[is.infinite(final\\$NegLog10P)] <- max(final\\$NegLog10P[!is.infinite(final\\$NegLog10P)]+1)\n        NegLog10P <- rowsum(final\\$NegLog10P, final\\$Cluster)\n        final\\$NegLog10P <- NegLog10P[final\\$Cluster, 1]\n\n        x <- unique( final[ final\\$ClusterSize != 0, c('chr1', 'Cluster', 'NegLog10P', 'ClusterSize')] )\n        if(nrow(x)==0){\n            final\\$ClusterType <- 'Singleton'\n            return(final)\n        }\n\n        # sort rows by cumulative -log10 P-value\n        x <- x[ order(x\\$NegLog10P) ,]\n        y<-sort(x\\$NegLog10P)\n        z<-cbind( seq(1,length(y),1), y )\n\n        # keep a record of z before normalization\n        z0 <- z\n\n        z[,1]<-z[,1]/max(z[,1], na.rm=TRUE)\n        z[,2]<-z[,2]/max(z[,2], na.rm=TRUE)\n\n        u<-z\n        u[,1] <-  1/sqrt(2)*z[,1] + 1/sqrt(2)*z[,2]\n        u[,2] <- -1/sqrt(2)*z[,1] + 1/sqrt(2)*z[,2]\n\n        v<-cbind(u, seq(1,nrow(u),1) )\n        RefPoint <- v[ v[,2]==min(v[,2], na.rm=TRUE) , 3]\n        RefValue <- z0[RefPoint,2]\n\n        # define peak cluster type\n        final\\$ClusterType <- rep(NA, nrow(final))\n        if(length(ol_)) final\\$ClusterType[ ol_ ] <- 'Singleton'\n        if(length(ol)){\n            final\\$ClusterType[ seq_along(gi) %in% ol & final\\$NegLog10P < RefValue ] <-  'SharpPeak'\n            final\\$ClusterType[ seq_along(gi) %in% ol & final\\$NegLog10P >= RefValue ] <- 'BroadPeak'\n        }\n        return(final)\n    }\n\n    peaks <- if(nrow(peaks)>0) subset(peaks, count >= COUNT_CUTOFF & ratio2 >= RATIO_CUTOFF & -log10(fdr) > FDR) else data.frame()\n    if (dim(peaks)[1] == 0) {\n        print(paste('ERROR caller_hipeak.r: 0 bin pairs with count >= ',COUNT_CUTOFF,' observed/expected ratio >= ',RATIO_CUTOFF,' and -log10(fdr) > ',FDR,sep=''))\n        quit()\n    }\n\n    peaks = classify_peaks(peaks)\n\n    outf_name = paste(GROUP_ID, '.',FDR,'.peaks',sep='')\n    dir.create(OUTPUT, recursive=TRUE)\n    peaks <- unique(peaks)\n    write.table(peaks, file.path(OUTPUT, outf_name),\n                row.names = FALSE, col.names = TRUE, quote=FALSE)\n    peaks1 <- cbind(peaks[, c(\"chr1\", \"start1\", \"end1\", \"chr2\", \"start2\", \"end2\")], \"*\", peaks[, \"NegLog10P\", drop=FALSE])\n    peaks1 <- unique(peaks1)\n    write.table(peaks1,\n                file.path(OUTPUT, paste0(GROUP_ID, '.', FDR, '.bedpe')),\n                row.names = FALSE, col.names = FALSE, quote=FALSE, sep=\"\\t\")\n\n    summary_all_runs <- split(peaks, peaks\\$ClusterType)\n    summary_all_runs <- lapply(summary_all_runs, function(.ele){\n        c(count = nrow(.ele),\n        minWidth1 = min(.ele\\$width1),\n        medianWidth1 = median(.ele\\$width1),\n        maxWidth1 = max(.ele\\$width1),\n        minWidth2 = min(.ele\\$width2),\n        medianWidth2 = median(.ele\\$width2),\n        maxWidth2 = max(.ele\\$width2),\n        minFoldChange = min(.ele\\$ratio2),\n        medianFoldChange = median(.ele\\$ratio2),\n        maxFoldChange = max(.ele\\$ratio2))\n    })\n    summary_all_runs <- do.call(rbind, summary_all_runs)\n    summary_outf_name = paste('summary.',GROUP_ID,'.txt',sep='')\n    write.table(summary_all_runs, file.path(OUTPUT, summary_outf_name), row.names = TRUE, col.names = TRUE, quote=FALSE)\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["SEQL", "Chromas", "BAIT", "IDSM", "GREG", "IRanges", "GTfold", "totalVI", "SLINK", "GIS", "GAP", "MAP"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process BIOC_TRACKVIEWER {\n    tag \"$bin_size\"\n    label 'process_high'\n    label 'process_long'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(bin_size), path(events), path(mcools)\n    path raw_pairs                                   \n    path gtf\n    path chrom_sizes\n    path restrict\n\n    output:\n    path \"${prefix}/*\"            , emit: v4c\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 for trackViewer parser\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n\n    pkgs <- c(\"trackViewer\", \"GenomicFeatures\", \"InteractionSet\", \"rtracklayer\", \"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    # Options\n    ## make_option(c(\"-g\", \"--gtf\"), type=\"character\", default=NULL, help=\"filename of gtf\", metavar=\"string\")\n    ## make_option(c(\"-s\", \"--chromsize\"), type=\"character\", default=NULL, help=\"filename of chrome size\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-r\", \"--resolution\"), type=\"integer\", default=NULL, help=\"resolution\", metavar=\"integer\")\n    ## make_option(c(\"-d\", \"--gap\"), type=\"integer\", default=NULL, help=\"gap, default 2*resolution\", metavar=\"integer\")\n    ## make_option(c(\"-l\", \"--readlength\"), type=\"integer\", default=NULL, help=\"reads length used to strengthen the signal, default resolution/10\", metavar=\"integer\")\n    ## make_option(c(\"-m\", \"--maxevents\"), type=\"integer\", default=25, help=\"max events to plot\", metavar=\"integer\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\".\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-c\", \"--cores\"), type=\"integer\", default=1, help=\"Number of cores\", metavar=\"integer\")\n    ## make_option(c(\"-e\", \"--events\"), type=\"character\", default=NULL, help=\"given events csv file, must be ginteractions file\", metavar=\"string\")\n    maxRegionWidth <- 1e6\n    maxEvent <- 25\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"gap\"=c(\"--gap\", \"-d\", \"integer\"),\n                        \"readlength\"=c(\"--readlength\", \"-l\", \"integer\"),\n                        \"maxevents\"=c(\"--maxevents\", \"-m\", \"integer\"))\n    opt <- parse_args(option_list, args)\n\n    gtf <- \"${gtf}\"\n    resolution <- ${bin_size}\n    gap <- 2 * resolution\n    readwidth <- ceiling(resolution/10)\n    output <- \"${prefix}\"\n    chrom_size <- \"${chrom_sizes}\"\n    restrict_cut <- \"${restrict}\"\n    if(!is.null(opt\\$gap)){\n        gap <- as.numeric(opt\\$gap)\n    }\n    if(!is.null(opt\\$readlength)){\n        readwidth <- as.numeric(opt\\$readlength)\n    }\n    if(!is.null(opt\\$maxevents)){\n        maxEvent <- as.numeric(opt\\$maxevents)\n    }\n\n    # read the signals\n    cools <- dir(\".\", \"mcool\\$\", full.names = TRUE, recursive = TRUE)\n    pairfiles <- dir(\".\", \".h5\\$\", full.names = TRUE, recursive = TRUE)\n    if(!is.null(opt\\$events)){\n        evts <- read.csv(opt\\$events)\n        if(!\"fdr\" %in% colnames(evts)){\n            evts\\$fdr <- rep(1, nrow(evts))\n        }\n    }else{\n        evts <- dir(\".\", \".anno.csv\\$\", full.names = TRUE, recursive = TRUE)\n        if(length(evts)<1) stop(\"No events file.\")\n        evts <- lapply(evts, read.csv)\n        evts <- do.call(rbind, evts)\n    }\n    stopifnot(nrow(evts)>0)\n    colnames(evts) <- tolower(colnames(evts))\n    stopifnot(all(c(\"chr1\", \"chr2\", \"start1\", \"start2\", \"end1\", \"end2\", \"fdr\") %in%\n        colnames(evts)))\n    evts <- evts[order(evts\\$fdr), , drop=FALSE]\n    if(nrow(evts)<1) stop(\"No events in files.\")\n    dir.create(output, recursive = TRUE, showWarnings = FALSE)\n\n    ## get events ranges\n    restrict_pos <- import(restrict_cut, format = \"BED\")\n    seqlen <- read.delim(chrom_size, header=FALSE, row.names=1)\n    seql <- as.numeric(seqlen[, 1])\n    names(seql) <- rownames(seqlen)\n    txdb <- makeTxDbFromGFF(gtf)\n    gtf <- import(gtf, format = \"GTF\")\n    map <- gtf\\$gene_name\n    names(map) <- gtf\\$gene_id\n    map <- map[!duplicated(names(map))]\n    grs <- with(evts, GInteractions(anchor1 = GRanges(chr1, IRanges(start1, end1)),\n                                    anchor2 = GRanges(chr2, IRanges(start2, end2)),\n                                    mode = \"strict\",\n                                    score = fdr))\n    grs\\$score <- 1-grs\\$score\n    grs <- unique(grs)\n    grs_narrow <- grs\n    regions(grs_narrow) <-\n        resize(regions(grs), width = ceiling(width(regions(grs))/2), fix = \"center\")\n    link <- gi2track(grs_narrow)\n    setTrackStyleParam(link, \"tracktype\", \"link\")\n    setTrackStyleParam(link, \"color\", c(\"gray80\", \"yellow\", \"brown\"))\n    reg0 <- GRanges(seqnames(first(grs)),\n                            IRanges(start = start(first(grs)),\n                                    end = end(second(grs))))\n    reg0 <- reg0[width(reg0)<maxRegionWidth]\n    reg <- reduce(reg0, min.gapwidth = gap)\n    gr1 <- unique(subsetByOverlaps(reg, grs))\n    gr1 <- gr1[seq.int(min(maxEvent, length(gr1)))]\n    prettyMax <- function(x){\n        if(x<1) return(round(x, digits = 2))\n        if(x<=5) return(ceiling(x))\n        if(x<10) return(2*ceiling(x/2))\n        if(x<100) return(10*ceiling(x/10))\n        if(x<1000) return(50*ceiling(x/50))\n        n <- 10^ceiling(log10(x))\n        return(n*ceiling(x/n))\n    }\n\n    ## read file and summary counts\n    getIndex <- function(pos, tileWidth, ext=150){\n        A <- ceiling((start(pos)-ext)/rep(tileWidth, length(pos)))\n        B <- ceiling((end(pos)+ext)/rep(tileWidth, length(pos)))\n        out <- mapply(A, B, FUN=seq, SIMPLIFY=FALSE)\n        sort(unique(unlist(out)))\n    }\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    readPairs <- function(pair, chrom, range){\n        tileWidth <- h5read(pair, \"header/tile_width\")\n        idx <- getIndex(range, tileWidth)\n        idx <- expand.grid(idx, idx)\n        idx <- paste(idx[, 1], idx[, 2], sep=\"_\")\n        inf <- H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n        on.exit(H5Fclose(inf))\n        pc <- lapply(idx, function(.ele){\n            n <- getPath(\"data\", chrom, chrom, .ele, \"position\")\n            if(H5Lexists(inf, n)){\n                h5read(inf, n)\n            }\n        })\n        pc <- do.call(rbind, pc)\n        strand <- lapply(idx, function(.ele){\n            n <- getPath(\"data\", chrom, chrom, .ele, \"strand\")\n            if(H5Lexists(inf, n)){\n                h5read(inf, n)\n            }\n        })\n        strand <- do.call(rbind, strand)\n        H5Fclose(inf)\n        h5closeAll()\n        on.exit()\n        GInteractions(anchor1 = GRanges(chrom, IRanges(pc[, 1], width=readwidth), strand = strand[, 1]),\n                    anchor2 = GRanges(chrom, IRanges(pc[, 2], width=readwidth), strand = strand[, 2]))\n    }\n    loadPairFile <- function(filenames, ranges, resolution){\n        stopifnot(is.character(filenames))\n        stopifnot(all(file.exists(filenames)))\n        start(ranges) <- start(ranges) - 2*resolution\n        end(ranges) <- end(ranges) + 2*resolution\n        names(filenames) <- sub(\".h5\\$\", \"\", basename(filenames))\n        total <- lapply(filenames, h5read, name=\"header/total\")\n        chrom <- as.character(seqnames(ranges)[1])\n        ranges <- ranges[seqnames(ranges)==chrom]\n        gi <- lapply(filenames, readPairs, chrom=chrom, range=ranges)\n        list(gi=gi, total=total)\n    }\n\n    readPairFile <- function(filenames, ranges, resolution){\n        stopifnot(is(ranges, \"GRanges\"))\n        chunks <- loadPairFile(filenames, ranges, resolution)\n        out <- list()\n        total <- list()\n        for(fn in names(chunks[[\"gi\"]])){\n            chunk <- chunks[[\"gi\"]][[fn]]\n            if(length(chunk)){\n                total[[fn]] <- chunks[[\"total\"]][[fn]]\n                out[[fn]] <-\n                    subsetByOverlaps(chunk, ranges = ranges, use.region=\"both\")\n            }\n            rm(chunk)\n        }\n        ## split by group\n        f <- sub(\"_REP\\\\\\\\d+\", \"\", names(out))\n        out <- split(out, f)\n        giRbind <- function(a, b){\n            GInteractions(anchor1 = c(first(a), first(b)),\n                        anchor2 = c(second(a), second(b)),\n                        regions=sort(unique(c(first(a), first(b), second(a), second(b)))))\n        }\n        out <- lapply(out, function(.ele){\n            Reduce(giRbind, .ele)\n        })\n        total <- split(unlist(total), f)\n        total <- sapply(total, sum)\n        total <- median(total)/total\n        return(list(gi=out, norm_factor=total))\n    }\n\n    for(i in seq_along(gr1)){\n        if(i < maxEvent){\n            try_res <- try({\n                gr <- gr1[i]\n                start(gr) <- start(gr) - 2* resolution\n                end(gr) <- end(gr) + 2* resolution\n                seqlevelsStyle(gr) <- seqlevelsStyle(txdb)[1]\n                seqlengths(gr) <- seql[seqlevels(gr)]\n                gr <- GenomicRanges::trim(gr)\n                chr_gr <- as(seqinfo(gr), \"GRanges\")[seqnames(gr)[1]]\n                bait <- subsetByOverlaps(grs, gr, use.region=\"both\")\n                ## merge the bait by ends\n                bait <- split(second(bait), as.character(first(bait)))\n                bait <- lapply(bait, range)\n                bait <- unlist(GRangesList(bait))\n                bait <- GInteractions(anchor1 = parse2GRanges(names(bait)),\n                                    anchor2 = unname(bait))\n                ## get the v4c\n                info <- readPairFile(pairfiles,\n                                    ranges = c(first(bait), second(bait)),\n                                    resolution=resolution)\n                h5closeAll()\n                total <- info\\$norm_factor\n                names(cools) <- sub(\"\\\\\\\\d+\\\\\\\\.mcool\\$\", \"\", basename(cools))\n                gis <- lapply(cools, importGInteractions,\n                            resolution = resolution,\n                            format=\"cool\",\n                            ranges=gr, out = \"GInteractions\")\n                gis <- mapply(gis, total[names(gis)], FUN=function(.ele, .total){\n                    .ele\\$score <- log2(.ele\\$score+1) * .total\n                    .ele\n                })\n                maxV <- max(sapply(gis, function(.ele) max(.ele\\$score, na.rm=TRUE)))\n                maxV <- prettyMax(maxV)\n                heat <- lapply(gis, function(.ele){\n                    .ele <- gi2track(.ele)\n                    setTrackStyleParam(.ele, \"breaks\", seq(from=0, to=maxV, by=maxV/10))\n                    setTrackStyleParam(.ele, \"color\", c(\"lightblue\", \"yellow\", \"red\"))\n                    #setTrackStyleParam(.ele, \"ylim\", c(0, .5))\n                    .ele\n                })\n                names(heat) <- paste(\"valid pairs\", names(heat))\n                get_v4c <- function(bait1, bait2){\n                    v4c <- lapply(seq_along(bait1), function(.e){\n                        vp1 <- mapply(info\\$gi, total[names(info\\$gi)], FUN=function(.ele, .total){\n                            .ele <- subsetByOverlaps(.ele, bait1[.e], use.region=\"both\")\n                            .ele <- GRanges(coverage(c(first(.ele), second(.ele))))\n                            .ele\\$score <- .ele\\$score*.total\n                            new(\"track\", dat=.ele,\n                                type=\"data\", format = \"BED\",\n                                name = paste0(seqnames(bait1[.e]), \":\",\n                                            start(bait1[.e]), \"-\",\n                                            end(bait1[.e])))\n                        })\n                        maxY <- sapply(vp1, FUN = function(.ele){\n                            max(subsetByOverlaps(.ele\\$dat, bait2[.e])\\$score)\n                        })\n                        maxY <- prettyMax(max(maxY))\n                        vp1 <- lapply(vp1, function(.ele){\n                            setTrackStyleParam(.ele, \"ylim\", c(0, maxY))\n                            setTrackStyleParam(.ele, \"color\", \"gray80\")\n                            .ele\n                        })\n                    })\n                    names(v4c) <- paste0(seqnames(bait1), \":\",\n                                        start(bait1), \"-\",\n                                        end(bait1))\n                    v4c <- unlist(v4c, recursive = FALSE)\n                }\n                v4c_first <- get_v4c(first(bait), second(bait))\n                v4c_second <- get_v4c(second(bait), first(bait))\n                ids <- getGeneIDsFromTxDb(gr, txdb)\n                if(length(ids)){\n                    genes <- geneTrack(ids, txdb, map[ids], asList=FALSE)\n                    tL <- trackList(genes, link, heat, v4c_first, v4c_second,\n                                    heightDist = c(1, 1, 2*length(heat),\n                                    2*length(v4c_first), 2*length(v4c_second)))\n                    names(tL)[2] <- \"called links\"\n                }else{\n                    tL <- trackList(link, heat, v4c_first, v4c_second,\n                                    heightDist = c(1, 2*length(heat),\n                                    2*length(v4c_first), 2*length(v4c_second)))\n                    names(tL)[1] <- \"called links\"\n                }\n                pdf(file.path(output, paste0(\"event_\", i, \"_\", seqnames(gr), \":\", start(gr), \"-\", end(gr), \".pdf\")),\n                    width = 9, height = length(tL))\n                viewTracks(tL, gr=gr, autoOptimizeStyle = TRUE)\n                dev.off()\n            })\n            if(inherits(try_res, \"try-error\")){\n                message(try_res)\n            }\n        }\n    }\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process MAPS_FEND {\n    tag \"$bin_size\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(bin_size), path(cut)\n    path chrom_sizes\n\n    output:\n    tuple val(bin_size), path(\"*.bed\")      , emit: bed\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    awk -vOFS=\"\\t\" '{print \\$3,\\$4,\\$4,\\$3\"_\"\\$1,\"0\",\\$2}' $cut | \\\\\n        bedtools slop $args \\\\\n            -r $bin_size -g $chrom_sizes > \\\\\n            ${cut}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["STRAW"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process READS_STAT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"r::r-magrittr=1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-magrittr:1.5--r3.2.2_0' :\n        'quay.io/biocontainers/r-magrittr:1.5--r3.2.2_0' }\"\n\n    input:\n    tuple val(meta), path(raw), path(dedup)\n\n    output:\n    tuple val(meta), path(\"*.csv\"), emit: stat\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    ## generate the statistis for each samples\n    versions <- c(\"${task.process}:\",\n        paste0(\"    R:\", paste(R.version\\$major, R.version\\$minor, sep=\".\")))\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    raw = \"$raw\"\n    dedup = \"$dedup\"\n    out = \"${prefix}.reads_stats.csv\"\n\n    sample_name <- sub(\".raw.pairsam.stat\", \"\", basename(raw))\n    getDat <- function(f){\n        dat <- read.delim(f, header=FALSE)\n        res <- dat[, 2]\n        names(res) <- dat[, 1]\n        return(res)\n    }\n    all_pairs <- getDat(raw)\n    dep_pairs <- getDat(dedup)\n\n    df <- data.frame(sample=sample_name,\n            total=all_pairs[\"total\"],\n            duplicate=dep_pairs['total_dups'],\n            non_duplicated=dep_pairs['total_nodups'],\n            duplication_rate=round(100*dep_pairs['total_dups']/all_pairs[\"total\"],2),\n            trans=dep_pairs['trans'],\n            cis=dep_pairs['cis'],\n            longRange=dep_pairs['cis_20kb+'])\n    write.csv(df, out, row.names=FALSE)\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["Circos"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process CIRCOS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::circos=0.69.8\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/circos:0.69.8--hdfd78af_1' :\n        'quay.io/biocontainers/circos:0.69.8--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(data)\n    path configfile\n\n    output:\n    path \"*.png\"                  , emit: circos\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    circos\n    mv circos.png ${meta.id}.png\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        circos: \\$(echo \\$(circos -v 2>&1) | sed 's/circos.*v //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["PCFamily", "FIT"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process CALL_HIPEAK {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-monocle=2.20.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-monocle:2.20.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-monocle:2.20.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(counts)\n\n    output:\n    tuple val(meta), path(\"${meta.id}.peaks.csv\"), emit: peak\n    path \"versions.yml\"                          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 call peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"VGAM\", \"MASS\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    options(warn=-1)\n    ### Options\n    ## make_option(c(\"-m\", \"--regression_type\"), type=\"character\", default='pospoisson', help=\"pospoisson for positive poisson regression, negbinom for negative binomial. default is pospoisson\", metavar=\"string\"),\n    ## make_option(c(\"-t\", \"--count\"), type=\"character\", default=NULL, help=\"count table output by prepare count\", metavar=\"string\"),\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"peaks.csv\", help=\"output folder\", metavar=\"string\")\n\n    OUTPUT = \"${meta.id}.peaks.csv\"\n    REG_TYPE = 'pospoisson'\n    mm <- read.csv(\"$counts\")\n    if(grepl(\"-m\", \"$args\") || grepl(\"--regression_type\", \"$args\")){\n        args <- strsplit(\"$args\", \"\\\\\\\\s+\")[[1]]\n        id <- args==\"-m\" | args==\"--regression_type\"\n        id <- which(id)[1]\n        if(args[id+1] %in% c(\"negbinom\", \"pospoisson\")){\n            REG_TYPE <- args[id+1]\n        }\n    }\n\n    if(!all(c(\"chr1\", \"start1\", \"end1\", \"width1\",\n            \"chr2\", \"start2\", \"end2\", 'width2',\n            \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount') %in% colnames(mm))){\n        stop(\"count table is not in correct format.\")\n    }\n\n    ## doing statistics and resampling\n    getFormula <- function(mm){\n        coln <- c(\"logl\", \"loggc\", \"logm\", \"logdist\", \"logShortCount\", \"logn\")\n        ln <- vapply(coln, FUN=function(.ele){\n            length(unique(mm[, .ele]))>1\n        }, FUN.VALUE=logical(1))\n        list(\"formula\"=paste(\"count ~\", paste(coln[ln], collapse=\"+\")),\n            \"factor\"=coln[ln])\n    }\n    loglikelihood <- function (mu, y, w, residuals = FALSE, eta, extra = NULL, summation = TRUE) {\n        lambda <- eta2theta(eta, \"loglink\", earg = list(bvalue = NULL, inverse = FALSE, deriv = 0, short = TRUE,\n            tag = FALSE))\n        if (residuals) {\n            stop(\"loglikelihood residuals not implemented yet\")\n        }\n        else {\n            if(exists('dgaitdpois', where = 'package:VGAM', mode='function')){\n                dgaitpois <- dgaitdpois\n            }\n            ll.elts <- c(w) * dgaitpois(y, lambda, truncate = 0,\n                log = TRUE)\n            if (summation) {\n                sum(ll.elts[!is.infinite(ll.elts)])\n            }\n            else {\n                ll.elts\n            }\n        }\n    }\n    pospoisson_regression <- function(mm) {\n        dataset_length<- nrow(mm)\n        formula <- getFormula(mm)\n        fac <- formula[[\"factor\"]]\n        formula <- formula[[\"formula\"]]\n        family <- pospoisson()\n        family@loglikelihood <- loglikelihood\n        fit <- vglm(formula=as.formula(formula), family = family, data = mm)\n        mm\\$expected = fitted(fit)\n        mm\\$p_val = ppois(mm\\$count, mm\\$expected, lower.tail = FALSE, log.p = FALSE) / ppois(0, mm\\$expected, lower.tail = FALSE, log.p = FALSE)\n        m1 = mm[ mm\\$p_val > 1/length(mm\\$p_val),]\n        fit <- vglm(formula=as.formula(formula), family = family, data = m1)\n        coeff<-round(coef(fit),10)\n        mm\\$expected2 <- round(exp(coeff[1] + rowSums(t(coeff[-1]*t(mm[, fac])))), 10)\n        mm\\$expected2 <- mm\\$expected2 /(1-exp(-mm\\$expected2))\n        mm\\$ratio2 <- mm\\$count / mm\\$expected2\n        mm\\$p_val_reg2 = ppois(mm\\$count, mm\\$expected2, lower.tail = FALSE, log.p = FALSE) / ppois(0, mm\\$expected2, lower.tail = FALSE, log.p = FALSE)\n        mm\\$p_bonferroni = mm\\$p_val_reg2 * dataset_length\n        mm\\$fdr <- p.adjust(mm\\$p_val_reg2, method='fdr')\n        return(mm)\n    }\n\n    negbinom_regression <- function(mm) {\n        formula <- getFormula(mm)\n        fac <- formula[[\"factor\"]]\n        formula <- formula[[\"formula\"]]\n        fit <- glm.nb(formula=as.formula(formula), data = mm)\n        mm\\$expected = fitted(fit)\n        sze = fit\\$theta ##size parameter\n        mm\\$p_val = pnbinom(mm\\$count, mu = mm\\$expected, size = sze, lower.tail = FALSE)\n        m1 = mm[ mm\\$p_val > ( 1 / length(mm\\$p_val)),]\n        ## second regression\n        fit <- glm.nb(formula=as.formula(formula), data = m1)\n        coeff<-round(fit\\$coefficients,10)\n        sze = fit\\$theta\n        mm\\$expected2 <- round(exp(coeff[1] + rowSums(t(coeff[-1]*t(mm[, fac])))), 10) ## mu parameter\n        mm\\$ratio2 <- mm\\$count / mm\\$expected2\n        mm\\$p_val_reg2 = pnbinom(mm\\$count, mu = mm\\$expected2, size = sze, lower.tail = FALSE)\n        mm\\$p_bonferroni = mm\\$p_val_reg2 * nrow(mm)\n        mm\\$fdr <- p.adjust(mm\\$p_val_reg2, method='fdr')\n        return(mm)\n    }\n\n    mm <- switch(REG_TYPE,\n                \"pospoisson\"=pospoisson_regression(mm),\n                \"negbinom\"=negbinom_regression(mm))\n\n    write.csv(mm, OUTPUT, row.names=FALSE)\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GIS", "gencore"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process PREPARE_COUNTS {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(r2peak, stageAs:\"R2peak/*\"), path(r1peak, stageAs: \"R1peak/*\"), path(distalpair, stageAs: \"pairs/*\"), val(chrom1)\n\n    output:\n    tuple val(meta), path(\"*.rds\"), optional: true, emit: counts\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 count reads for peak filtering\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"InteractionSet\", \"rhdf5\", \"BiocParallel\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    ## make_option(c(\"-a\", \"--r1peak\"), type=\"character\", default=NULL, help=\"filename of r1 peak\", metavar=\"string\")\n    ## make_option(c(\"-b\", \"--r2peak\"), type=\"character\", default=NULL, help=\"filename of r2 peak\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-p\", \"--pairs\"), type=\"character\", default=NULL, help=\"folder of valid distal pairs\", metavar=\"string\")\n    ## make_option(c(\"-m\", \"--mappability\"), type=\"character\", default=NULL, help=\"mappability file\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"counts.csv\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-f\", \"--fasta\"), type=\"character\", default=NULL, help=\"genome fasta file\", metavar=\"string\")\n    ## make_option(c(\"-1\", \"--chrom1\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"peak_pair_block\"=c(\"--peak_pair_block\", \"-b\", \"integer\"),\n                        \"snow_type\"=c(\"--snow_type\", \"-t\", \"character\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    CHROM1 <- \"$chrom1\"\n    OUTPUT <- \"counts.${meta.id}.${chrom1}.rds\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SNOW_TYPE <- \"SOCK\"\n    peak_pair_block <- 1e9\n    if(!is.null(opt\\$peak_pair_block)){\n        peak_pair_block <- opt\\$peak_pair_block\n    }\n    if(!is.null(opt\\$snow_type)){\n        SNOW_TYPE <- opt\\$snow_type\n    }\n    pattern <- \"h5\" ## h5 is postfix of output of pairtools pairs2hdf5\n    pairs <- dir(\"pairs\", paste0(pattern, \"\\$\"), full.names=TRUE)\n    names(pairs) <- sub(paste0(\"\\\\\\\\.\", pattern), \"\", basename(pairs))\n    R1PEAK <- import(\"$r1peak\")\n    R2PEAK <- import(\"$r2peak\")\n    mcols(R1PEAK) <- NULL\n    mcols(R2PEAK) <- NULL\n    ## split by chromsome\n    R1PEAK <- split(R1PEAK, seqnames(R1PEAK))\n    R2PEAK <- split(R2PEAK, seqnames(R2PEAK))\n    R1PEAK <- R1PEAK[lengths(R1PEAK)>0]\n    R2PEAK <- R2PEAK[lengths(R2PEAK)>0]\n    chromosomes <- intersect(names(R1PEAK), names(R2PEAK))\n    chromosomes <- chromosomes[!grepl(\"_\", chromosomes)]\n    chromosomes <- chromosomes[!grepl(\"M\", chromosomes)] ## remove chrM/chrMT\n    if(length(chromosomes)==0){\n        stop(\"no valid data in same chromosome.\")\n    }\n\n    ## loading data\n    readPairs <- function(pair, chrom1, chrom2){\n        h5content <- rhdf5::h5ls(pair)\n        h5content <- h5content[, \"group\"]\n        h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n        h5content <- unique(h5content)\n        n <- h5content[grepl(paste0(\"data.\", chrom1, \".\", chrom2), h5content)]\n        n <- paste(n, \"position\", sep=\"/\")\n        inf <- rhdf5::H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n        on.exit({rhdf5::H5Fclose(inf)})\n        pc <- lapply(n, function(.ele){\n            if(rhdf5::H5Lexists(inf, .ele)){\n                rhdf5::h5read(inf, .ele)\n            }\n        })\n        rhdf5::H5Fclose(inf)\n        rhdf5::h5closeAll()\n        on.exit()\n        pc <- do.call(rbind, pc)\n    }\n\n    ### load counts\n    gis <- NULL\n\n    if(CHROM1 %in% chromosomes){\n        gc(reset=TRUE)\n        if(SNOW_TYPE==\"FORK\"){\n            param <- MulticoreParam(workers = NCORE, progressbar = TRUE)\n        }else{\n            param <- SnowParam(workers = NCORE, progressbar = TRUE, type = SNOW_TYPE)\n        }\n        chrom1 <- CHROM1\n        parallel <- TRUE\n        for(chrom2 in chromosomes){\n            message(\"working on \", chrom1, \" and \", chrom2, \" from \", Sys.time())\n            r1peak <- R1PEAK[[chrom1]]\n            r2peak <- R2PEAK[[chrom2]]\n            message(\"read reads\")\n            if(parallel){\n                try_res <- try({reads <- bplapply(pairs, readPairs, chrom1=chrom1, chrom2=chrom2, BPPARAM = param)})\n                if(inherits(try_res, \"try-error\")){\n                    parallel <- FALSE\n                }\n            }\n            if(!parallel){\n                reads <- lapply(pairs, readPairs, chrom1=chrom1, chrom2=chrom2)\n            }\n            h5closeAll()\n            reads <- do.call(rbind, c(reads, make.row.names = FALSE))\n            if(length(reads) && length(r1peak) && length(r2peak)){\n                reads <- GInteractions(GRanges(chrom1, IRanges(reads[, 1], width=150)),\n                                        GRanges(chrom2, IRanges(reads[, 2], width=150)))\n                ## count twice,\n                ## first, merge the r1peak with gap 5k and filter the peak with 0 counts\n                ## second, count for filtered r1peak\n                r1peak_s <- reduce(r1peak, min.gapwidth=5000, with.revmap=TRUE)\n                countFUN <- function(peak_pair, reads, r1peak, r2peak){\n                    r1peak\\$revmap <- NULL\n                    .gi <- InteractionSet::GInteractions(r1peak[peak_pair[, 1]], r2peak[peak_pair[, 2]],\n                                                        p1=peak_pair[, 1], p2=peak_pair[, 2])\n                    reads <- IRanges::subsetByOverlaps(reads, InteractionSet::regions(.gi))\n                    ## remove the interactions with distance smaller than 1K\n                    .dist <- IRanges::distance(first(.gi), second(.gi))\n                    .dist[is.na(.dist)] <- 3e9\n                    S4Vectors::mcols(.gi)[, \"count\"] <- InteractionSet::countOverlaps(.gi, reads, use.region=\"both\")\n                    S4Vectors::mcols(.gi)[, \"shortCount\"] <- GenomicRanges::countOverlaps(S4Vectors::second(.gi), S4Vectors::second(reads))\n                    .gi[S4Vectors::mcols(.gi)[, \"count\"]>0 & S4Vectors::mcols(.gi)[, \"shortCount\"]>0 & .dist>1000]\n                }\n                countFUNbyPairs <- function(r1peak, r2peak, peak_pairs, reads, parallel){\n                    peak_pairs_group <- ceiling(nrow(peak_pairs)/peak_pair_block)\n                    if(peak_pairs_group>1){\n                        peak_pairs_group <- rep(seq.int(peak_pairs_group), each= peak_pair_block)\n                        peak_pairs <- split(peak_pairs,\n                                            peak_pairs_group[seq.int(nrow(peak_pairs))])\n                        if(parallel){\n                            try_res <- try({gi <- bplapply(peak_pairs, FUN=countFUN, reads=reads, r1peak=r1peak, r2peak=r2peak, BPPARAM = param)})\n                            if(inherits(try_res, \"try-error\")){\n                                parallel <- FALSE\n                            }\n                        }\n                        if(!parallel){\n                            gi <- lapply(peak_pairs, FUN=countFUN, reads=reads, r1peak=r1peak, r2peak=r2peak)\n                        }\n                        gi <- Reduce(c, gi)\n                    }else{\n                        gi <- countFUN(peak_pairs, reads, r1peak, r2peak)\n                    }\n                    gi\n                }\n                message(\"count reads\")\n                peak_pairs <- expand.grid(seq_along(r1peak_s), seq_along(r2peak))\n                gi <- countFUNbyPairs(r1peak_s, r2peak, peak_pairs, reads, parallel)\n                peak_pairs <- mcols(r1peak_s)[mcols(gi)[, \"p1\"], \"revmap\"]\n                peak_pairs <- data.frame(p1=unlist(peak_pairs),\n                                        p2=rep(mcols(gi)[, \"p2\"], lengths(peak_pairs)))\n                rm(gi)\n                gi <- countFUNbyPairs(r1peak, r2peak, peak_pairs, reads, parallel)\n                if(length(gi)>0){\n                    gis <- c(gis, gi)\n                }\n                rm(peak_pairs, gi)\n                gc(reset=TRUE)\n            }\n        }\n    }\n    if(is.list(gis)) gis <- do.call(c, gis)\n    saveRDS(gis, OUTPUT)\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 3, "list_own": ["rbpisupati", "mahesh-panchal", "jianhong"], "nb_wf": 2, "list_wf": ["nf-core-hicar", "test_nfcore_workflow_chain", "nf-haplocaller"], "list_contrib": ["nf-core-bot", "ewels", "yuxuth", "jianhong", "rbpisupati", "mahesh-panchal"], "nb_contrib": 6, "codes": ["process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process CHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0' :\n        'quay.io/biocontainers/samtools:1.12--hd5e65b6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai | sort -k 1,1 > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess processBam {\n  tag \"$name\"\n  label 'env_picard_medium'\n\n  input:\n  set val(name), file(sam) from aligned_sam\n\n  output:\n  set val(name), file(\"${name}.sorted.bam\") into sorted_bam\n\n\n  script:\n  \"\"\"\n  samtools view -b -o ${name}.bam -S $sam\n  samtools sort -m 10G --threads ${task.cpus} -o ${name}.sorted.bam ${name}.bam\n  \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/SAMTOOLS_STATS", "rbpisupati/nf-haplocaller/processBam"], "list_wf_names": ["mahesh-panchal/test_nfcore_workflow_chain", "rbpisupati/nf-haplocaller"]}, {"nb_reuse": 0, "tools": ["RDF", "4peaks", "CCdigest", "Qcut"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process CALL_R1PEAK {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path cut\n    val pval\n\n    output:\n    tuple val(meta), path(\"*.narrowPeak\")            , emit: peak\n    path  \"versions.yml\"                             , emit: versions\n\n    tuple val(meta), path(\"*.bed\")                   , emit: bed\n    tuple val(meta), path(\"*.bdg\")                   , emit: bdg\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on DEC. 13, 2021 call R1 peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    options(scipen=10)\n    pkgs <- c(\"rtracklayer\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    R1READS <- \"$reads\"\n    CUT <- \"$cut\"\n    r1reads <- read.delim(R1READS, header=FALSE)\n    r1reads <- GRanges(r1reads[, 1], IRanges(r1reads[, 2], width=1), r1reads[, 6])\n    digest <- import(CUT, format=\"BED\")\n    width(digest) <- 1\n    ## peaks gap 5\n    peaks <- reduce(digest, min.gapwidth=5L)\n    mcols(peaks)[, \"signalValue\"] <- countOverlaps(peaks, r1reads, maxgap=5L)\n    lambda <- mean(mcols(peaks)[, \"signalValue\"])\n    p <- ppois(mcols(peaks)[, \"signalValue\"], lambda, lower.tail=FALSE)\n    mcols(peaks)[, \"pValue\"] <- -10*log10(p)\n    mcols(peaks)[, \"qValue\"] <- -10*log10(p.adjust(p, method=\"BH\"))\n    mcols(peaks)[, \"score\"] <- round((1-p)*100)\n    peaks <- peaks[p<$pval]\n    ## calculate the distance between digest sites\n    dist <- distanceToNearest(digest)\n    dist <- median(mcols(dist)[, \"distance\"])\n    peaks <- promoters(peaks, upstream=dist, downstream=dist)\n    peaks <- GenomicRanges::trim(peaks)\n    rd <- reduce(peaks, with.revmap=TRUE)\n    revmap <- mcols(rd)[, \"revmap\"]\n    l <- lengths(revmap)>1\n    if(any(l)){\n        rd_id <- unlist(revmap[l])\n        rd_gp <- rep(seq_along(revmap[l]), lengths(revmap[l]))\n        peaks_rd <- peaks[rd_id]\n        peaks_rd_data <- split(as.data.frame(mcols(peaks_rd)), rd_gp)\n        peaks_rd_data <- lapply(peaks_rd_data, FUN=colMeans)\n        peaks_rd_data <- peaks_rd_data[order(as.numeric(names(peaks_rd_data)))]\n        peaks_rd_data <- do.call(rbind, peaks_rd_data)\n        peaks_rd <- rd[l]\n        mcols(peaks_rd) <- peaks_rd_data\n        peaks <- c(peaks[!l], peaks_rd)\n        peaks <- sort(peaks)\n    }\n    if(any(start(peaks)<1)){\n        start(peaks[start(peaks)<1]) <- 1\n    }\n    export(peaks, \"${prefix}.bed\")\n    np <- paste(as.character(seqnames(peaks)), start(peaks)-1, end(peaks),\n                \".\", mcols(peaks)[, \"score\"],\n                \".\", mcols(peaks)[, \"signalValue\"],\n                mcols(peaks)[, \"pValue\"],\n                mcols(peaks)[, \"qValue\"],\n                dist, sep=\"\\t\")\n    writeLines(np, \"${prefix}.narrowPeak\")\n    r1reads <- promoters(r1reads, upstream=75, downstream=75)\n    cvg <- coverage(r1reads)\n    export(cvg, \"${prefix}_pileup.bdg\", format=\"bedgraph\")\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["totalVI"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process BIOC_PAIRS2HDF5 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(pairs)\n    path chromsizes\n\n    output:\n    tuple val(meta), path(\"*.h5\") , emit: hdf5\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: 'keep-dup'\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Dec. 03, 2021 to convert pairs to hdf5\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    ## hdf5 format:\n    ## - header, including total reads, chromosome name and sizes, tileWidth\n    ##    * header/chrom_sizes COMPOUND\n    ##    * header/header      STRING\n    ##    * header/tile_width  INTEGER\n    ##    * header/total       INTEGER\n    ## - data, pairs in path data/chr1/chr2/tileIndex1_tileIndex2/\n    ##    * position, in path data/chr1/chr2/tileIndex1_tileIndex2/position\n    ##    * strand, in path data/chr1/chr2/tileIndex1_tileIndex2/strand\n    #######################################################################\n    #######################################################################\n\n    pkgs <- c(\"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    comment_char <- \"#\"\n    pattern <- \"unselected.pairs.gz\" ## this is from upstream output file.\n    tileWidth <- 1e7 # this will create about 200 groups for human data\n    block_size <- 1e7 # the size of block for tempfile\n    keepDup <- FALSE # remove duplicates or not\n    if(grepl(\"keep-dup\", \"$args\")){\n        keepDup <- TRUE\n    }\n    chrom_sizes <- read.delim(\"$chromsizes\", header=FALSE)\n    infs <- \"$pairs\"\n    infs <- strsplit(infs, \"\\\\\\\\s+\")[[1]]\n\n    getHeader <- function(inf){\n        f <- gzfile(inf, open = \"r\")\n        on.exit(close(f))\n        header <- c()\n        while(length(chunk <- readLines(f, n=1))){\n            if(substr(chunk, 1, 1) == comment_char){\n                header <- c(header, chunk)\n            }else{\n                break\n            }\n        }\n        close(f)\n        on.exit()\n        header\n    }\n    getData <- function(f, block_size, n=7){\n        if(n<0) return(data.frame())\n        pc <- try({read.table(f, nrow=block_size, comment.char = \"#\",\n                            colClasses=c(\n                                \"NULL\", # reads name, skip\n                                \"character\", # chrom1\n                                \"integer\", # start1\n                                \"character\", # chrom2\n                                \"integer\", # start2\n                                \"character\", #strand1\n                                \"character\", #strand2\n                                rep(\"NULL\", n)))}, silent = TRUE)\n        if(inherits(pc, \"try-error\")){\n            data.frame()\n        }else{\n            pc\n        }\n    }\n    getIndex <- function(pos, tileWidth){\n        ceiling(pos/tileWidth)\n    }\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    createGroup <- function(obj, path){\n        if(!H5Lexists(obj, path)){\n            h5createGroup(obj, path)\n        }\n    }\n    read_pair_write_tmp <- function(inf, out){\n        ## check ncol\n        h <- read.table(inf, nrow=1, comment.char = \"#\")\n        n <- ncol(h) - 7\n        f <- gzfile(inf, open = \"r\")\n        on.exit({\n            close(f)\n        })\n        filenames <- c()\n\n        while(nrow(pc <- getData(f, block_size, n))>0){\n            idx1 <- getIndex(pc[, 2], tileWidth)\n            idx2 <- getIndex(pc[, 4], tileWidth)\n            idx1_2 <- paste(pc[, 1], pc[, 3], paste(idx1, idx2, sep=\"_\"), sep=\"___\")\n            pc <- split(pc[, -c(1, 3)], f=idx1_2)\n            filenames <- unique(c(filenames, file.path(out, names(pc))))\n            mapply(pc, names(pc), FUN=function(.data, .name){\n                write.table(.data, file = file.path(out, .name),\n                            append = TRUE, sep=\"\\t\", quote=FALSE,\n                            col.names=FALSE, row.names=FALSE)\n            })\n        }\n\n        close(f)\n        on.exit()\n        sort(filenames)\n    }\n    rewrite_hd5 <- function(filenames, out_h5, keepDup, root){\n        total <- lapply(filenames, function(n){\n            if(file.exists(n)){\n                pc <- read.delim(n, header=FALSE)\n                if(!keepDup) pc <- unique(pc)\n                npc <- nrow(pc)\n                chunk_size <- ceiling(sqrt(npc)/1000)*1000\n                n <- getPath(root, gsub(\"___\", \"/\", basename(n)))\n                h5createGroup(out_h5, n)\n                pos <- getPath(n, \"position\")\n                if(npc>1000){\n                    h5createDataset(out_h5, pos, dims = dim(pc[, c(1, 2)]),\n                                    storage.mode = \"integer\",\n                                    chunk = c(chunk_size, 2))\n                }\n                h5write(as.matrix(pc[, c(1, 2)]), out_h5, pos)\n                strand <- getPath(n, \"strand\")\n                if(npc>1000){\n                    h5createDataset(out_h5, strand, dims = dim(pc[, c(3, 4)]),\n                                    storage.mode = \"character\", size = 1,\n                                    chunk = c(chunk_size, 2))\n                }\n                h5write(as.matrix(pc[, c(3, 4)]), out_h5, strand)\n                npc\n            }else{\n                0\n            }\n        })\n        sum(unlist(total))\n    }\n    root <- \"data\"\n    for(inf in infs){\n        out <- sub(pattern, \"h5\", basename(inf))\n        if(!h5testFileLocking(dirname(inf))){\n            h5disableFileLocking()\n        }\n        h5createFile(out)\n        #header start as comment char'#'\n        header <- getHeader(inf)\n        h5createGroup(out, \"header\")\n        h5write(header, out, \"header/header\")\n        h5write(chrom_sizes, out, \"header/chrom_sizes\")\n        h5write(as.integer(tileWidth), out, \"header/tile_width\")\n        h5createGroup(out, root)\n        # create groups for tiles\n        n_chrom <- nrow(chrom_sizes)\n        for(i in seq.int(n_chrom)){\n            h5createGroup(out, getPath(root, chrom_sizes[i, 1]))\n            for(j in seq.int(n_chrom)){\n                h5createGroup(out, getPath(root, chrom_sizes[i, 1],\n                                        chrom_sizes[j, 1]))\n            }\n        }\n\n        # read pairs\n        #columns: readID chrom1 pos1 chrom2 pos2 strand1 strand2 pair_type\n        tmp_dir <- \"tmp_files\"\n        dir.create(tmp_dir)\n        filenames <- try(read_pair_write_tmp(inf, tmp_dir))\n        #rewrite\n        total <- rewrite_hd5(filenames, out, keepDup, root)\n        h5write(total, out, \"header/total\")\n        h5closeAll()\n        unlink(tmp_dir, recursive=TRUE)\n    }\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["PMG", "RDFScape"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process READS_SUMMARY {\n    label 'process_low'\n\n    conda (params.enable_conda ? \"r::r-magrittr=1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-magrittr:1.5--r3.2.2_0' :\n        'quay.io/biocontainers/r-magrittr:1.5--r3.2.2_0' }\"\n\n    input:\n    path stat\n\n    output:\n    path \"*.{csv,json}\"           , emit: summary\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    versions <- c(\"${task.process}:\",\n        paste0(\"    R:\", paste(R.version\\$major, R.version\\$minor, sep=\".\")))\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    fs <- dir(\".\", \"*.reads_stats.csv\")\n    if(length(fs)>0){\n        df <- do.call(rbind, lapply(fs, read.csv))\n        con <- file(\"reads_summary.csv\", open=\"wt\")\n        write.csv(df, con, row.names=FALSE)\n        close(con)\n    }\n\n    fs <- dir(\".\", \"*.summary.out\\$\")\n    if(length(fs)>0){\n        df <- t(do.call(cbind, lapply(fs, read.delim, header=FALSE, row.names=1)))\n        df <- gsub(\",\", \"\", df)\n        mode(df) <- \"numeric\"\n        df <- cbind(sample=sub(\".summary.out\", \"\", basename(fs)), df)\n        con <- file(\"pairsqc_summary_out.csv\", open=\"wt\")\n        write.csv(df, con, row.names=FALSE)\n        close(con)\n    }\n\n    fs <- dir(\".\", \"^summary\")\n    if(length(fs)>0){\n        df <- do.call(rbind, lapply(fs, read.delim, header=TRUE, sep=\" \"))\n        df[, 1] <- sub(\"summary.(.*?).txt\", \"\\\\\\\\1\", basename(fs))\n        write.csv(df, \"MAPS_summary_out.csv\", row.names=FALSE)\n    }\n\n    fs <- dir(\".\", \"*.distance.vs.proportion.csv\\$\")\n    if(length(fs)>0){\n        dfs <- lapply(fs, function(.f){\n            x <- read.csv(.f)[, -2, drop=FALSE]\n            colnames(x)[-1] <- paste0(sub(\"distance.vs.proportion.csv\", \"\", .f), colnames(x)[-1])\n            x\n        })\n\n        json <- lapply(dfs, function(.ele){\n            ## convert to list, named as colnames, for the vectors in list, named as distance\n            .df <- as.data.frame(.ele[, -1])\n            .df <- as.list(.df)\n            .df <- lapply(.df, function(.e){\n                names(.e) <- .ele[, 1]\n                as.list(.e)\n            })\n            .df\n        })\n        json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n            .ele <- sapply(.ele, function(.e){\n                x <- names(.e)\n                y <- .e\n                .e <- paste('{ \"x\":', x, ', \"y\":', y, ', \"color\":\"', .color, '\"', \"}\")\n                .e <- paste(.e, collapse=\", \")\n                paste(\"[\", .e, \"]\")\n            })\n            .ele <- paste0('\"', names(.ele), '\" : ', .ele)\n            .ele <- paste(.ele, collapse=\", \")\n        })\n        json <- c(\n                \"{\",\n                '\"id\":\"pairs_reads_proportion\",',\n                '\"data\":{',\n                paste(unlist(json), collapse=\", \"),\n                \"}\",\n                \"}\")\n        writeLines(json, \"dist.prop.qc.json\")\n\n        ## merge by first columns\n        mg <- function(...){\n            merge(..., all = TRUE)\n        }\n        dfs <- Reduce(mg, dfs)\n        write.table(t(dfs), \"dist.prop.csv\", col.names=FALSE, quote=FALSE, sep=\",\", row.names=TRUE)\n    }\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["IdeoViz", "SEQL", "GTfold", "TargetScore"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 0, "list_wf": ["nf-core-hicar"], "list_contrib": ["ewels", "jianhong", "nf-core-bot", "yuxuth"], "nb_contrib": 4, "codes": ["process CIRCOS_PREPARE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-rtracklayer=1.50.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' :\n        'quay.io/biocontainers/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' }\"\n\n    input:\n    tuple val(meta), path(bedpe), val(ucscname), path(gtf), path(chromsize)\n\n    output:\n    tuple val(meta), path(\"${meta.id}/*\")           , emit: circos\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Oct. 16, 2021 prepare data for circos\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    options(scipen=10)\n    library(rtracklayer)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    rtracklayer:\", as.character(packageVersion(\"rtracklayer\"))))\n    writeLines(versions, \"versions.yml\")\n\n    ## Options\n    ## make_option(c(\"-i\", \"--interaction\"), type=\"character\", default=NULL, help=\"interaction bedpe file\", metavar=\"string\")\n    ## make_option(c(\"-g\", \"--gtf\"), type=\"character\", default=NULL, help=\"annotation gtf file\", metavar=\"string\")\n    ## make_option(c(\"-c\", \"--chromsize\"), type=\"character\", default=NULL, help=\"filename of chromosome size\", metavar=\"string\")\n    ## make_option(c(\"-u\", \"--ucscname\"), type=\"character\", default=NULL, help=\"ucsc annotation name\", metavar=\"string\")\n    interaction <- \"$bedpe\"\n    chromsize <- \"$chromsize\"\n    gtf <- \"$gtf\"\n    ucscname <- \"$ucscname\"\n    outfolder <- \"${meta.id}\"\n\n    dir.create(outfolder, showWarnings = FALSE)\n\n    headerline <- readLines(interaction, n=1)\n    if(grepl(\"start1\", headerline[1])){\n        ## output from MAPS\n        pe <- read.delim(interaction)\n        pe <- Pairs(GRanges(pe[, \"chr1\"], IRanges(pe[, \"start1\"]+1, pe[, \"end1\"])),\n                    GRanges(pe[, \"chr2\"], IRanges(pe[, \"start2\"]+1, pe[, \"end2\"])),\n                    score = pe[, \"ClusterNegLog10P\"])\n    }else{\n        pe <- import(interaction, format=\"BEDPE\")\n    }\n    seqlevelsStyle(first(pe)) <- seqlevelsStyle(second(pe)) <- \"UCSC\"\n    pes <- pe[order(mcols(pe)\\$score, decreasing=TRUE)]\n    pes_cis <- pes[seqnames(first(pe))==seqnames(second(pe))]\n    pes_trans <- pes[seqnames(first(pe))!=seqnames(second(pe))]\n    if(length(pes_cis)>0){ # keep top 10K events for plot\n        pes <- pes_cis[seq.int(min(1e4, length(pes_cis)))]\n    }else{\n        stop(\"No data available for plot\")\n    }\n    if(length(pes_trans)>0){\n        pes <- sort(c(pes,\n                    pes_trans[seq.int(min(1e4, length(pes_trans)))])) ## keep top 10K links only. otherwise hard to plot.\n    }\n    out <- as.data.frame(pes)\n    scores <- sqrt(range(mcols(pe)\\$score)/10)\n    scores <- c(floor(scores[1]), ceiling(scores[2]))\n    cid <- cut(sqrt(mcols(pes)\\$score/10), breaks = seq(scores[1], scores[2]))\n    levels(cid) <- seq_along(levels(cid))\n    out <- cbind(out[, c(\"first.seqnames\", \"first.start\", \"first.end\",\n                        \"second.seqnames\", \"second.start\", \"second.end\")],\n                thickness=paste0(\"thickness=\", cid))\n\n    write.table(out, file.path(outfolder, \"link.txt\"),\n        quote=FALSE, col.names=FALSE, row.names=FALSE,\n        sep=\" \") ## output cis- and trans- interactions\n\n    ## create karyotype file\n    chromsize <- read.delim(chromsize, header=FALSE)\n    chromsize[, 1] <- as.character(chromsize[, 1])\n    seqlevelsStyle(chromsize[, 1]) <- \"UCSC\"\n    chromsize <- cbind(\"chr\", \"-\", chromsize[, c(1, 1)], 0, chromsize[, 2],\n                        paste0(\"chr\", sub(\"^chr\", \"\", chromsize[, 1])))\n    colnames(chromsize) <- c(\"chr\", \"-\", \"chrname\", \"chrlabel\",\n                            0, \"chrlen\", \"chrcolor\")\n    chromsize <- chromsize[order(as.numeric(sub(\"chr\", \"\", chromsize[, \"chrname\"])),\n                            chromsize[, \"chrname\"]), , drop=FALSE]\n    write.table(chromsize, file.path(outfolder, \"karyotype.tab\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE, sep=\" \")\n\n    getScore <- function(seql, rg){\n        gtile <- tileGenome(seqlengths = seql, tilewidth = 1e5)\n        gtile <- unlist(gtile)\n        gtile <- split(gtile, seqnames(gtile))\n        sharedChr <- intersect(names(rg), names(gtile))\n        rg <- rg[sharedChr]\n        gtile <- gtile[names(rg)]\n        vw <- Views(rg, gtile)\n        vm <- viewSums(vw, na.rm = TRUE)\n        stopifnot(identical(names(vm), names(gtile)))\n        gtile <- unlist(gtile, use.names = FALSE)\n        gtile\\$score <- unlist(vm, use.names = FALSE)\n        gtile\n    }\n    ## create link desity file\n    rg <- coverage(c(first(pe), second(pe)))\n    seql <- chromsize\\$chrlen\n    names(seql) <- chromsize\\$chrname\n    gtile <- getScore(seql, rg)\n    rg <- as.data.frame(gtile)\n    rg <- rg[rg\\$score>0, c(\"seqnames\", \"start\", \"end\", \"score\"), drop=FALSE]\n    write.table(rg, file.path(outfolder, \"hist.link.txt\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE,\n                sep=\" \")\n    seqn <- sort(as.character(unique(rg\\$seqnames)))[1]\n    labelA <- labelB <- c(seqn, 0, 5000)\n    labelA <- c(labelA, \"interaction-density\")\n    labelB <- c(labelB, \"exon-density\")\n    writeLines(labelA, file.path(outfolder, \"labelA.txt\"), sep=\" \")\n    writeLines(labelB, file.path(outfolder, \"labelB.txt\"), sep=\" \")\n\n    ## create exon desity file\n    gtf <- import(gtf)\n    gtf <- gtf[gtf\\$type %in% \"exon\"]\n    seqlevelsStyle(gtf) <- \"UCSC\"\n    rg <- coverage(gtf)\n    gtile <- getScore(seql, rg)\n    rg <- as.data.frame(gtile)\n    rg <- rg[rg\\$score>0, c(\"seqnames\", \"start\", \"end\", \"score\"), drop=FALSE]\n    write.table(rg, file.path(outfolder, \"hist.exon.txt\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE,\n                sep=\" \")\n    try_res <- try({\n        session <- browserSession()\n        genome(session) <- ucscname\n        ideo <- getTable(ucscTableQuery(session, table=\"cytoBandIdeo\"))\n        ideo <- ideo[ideo\\$chrom %in% chromsize\\$chrname, , drop=FALSE]\n        ideo <- data.frame(\"band\", ideo\\$chrom, ideo\\$name, ideo\\$name,\n                    ideo\\$chromStart, ideo\\$chromEnd, ideo\\$gieStain)\n        colnames(ideo) <- colnames(chromsize)\n        ideo <- rbind(chromsize, ideo)\n        for(i in c(\"chrname\", \"chrlabel\")){\n            ideo[ideo[, i]==\"\", i] <- make.names(ideo[ideo[, i]==\"\", 2], unique=TRUE)\n        }\n        write.table(ideo, file.path(outfolder, \"karyotype.tab\"),\n                    quote=FALSE, col.names=FALSE, row.names=FALSE, sep=\" \")\n    })\n    if(inherits(try_res, \"try-error\")){\n        message(try_res)\n    }\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["ngsReports"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["shotgun"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process SANKEY {\n    label 'process_low'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    path reports\n\n    output:\n    path \"*_sankey.html\"                                 , emit: summary\n    path \"versions.yml\"                                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"pvaian\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on March 10, 2022 use pavian to create sankey plots\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    if(!require(pavian)) {\n        if(!require(remotes)) { install.packages(\"remotes\", repos=\"https://cloud.r-project.org\")}\n        remotes::install_github(\"fbreitwieser/pavian\")\n    }\n    versions <- c(\"${task.process}:\")\n    pkgs <- \"pavian\"\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    getNames <- function(n){\n        x <- strsplit(n, \"\")\n        maxN <- max(lengths(x))\n        x <- lapply(x, function(.ele){\n            rev(c(rep(\"_\", maxN), .ele))[seq.int(maxN)]\n        })\n        x <- do.call(rbind, x)\n        x <- apply(x, 2, unique, simply=FALSE)\n        xl <- lengths(x)\n        stopID <- which(xl>1)\n        if(length(stopID)){\n            stopID <- stopID[1]\n            if(stopID>1){\n                x <- x[seq.int(stopID-1)]\n                x <- paste(rev(x), collapse=\"\")\n                n <- sub(x, \"\", n, fixed=TRUE)\n            }\n        }\n        n\n    }\n\n    REPORTS <- strsplit(\"$reports\", \" \")[[1]]\n    NAMES <- getNames(REPORTS)\n    FILENAME <- \"${prefix}_sankey.html\"\n\n    ## prepare data from out to data frame with colnames:\n    ## \"name\",\"taxLineage\",\"taxonReads\", \"cladeReads\",\"depth\", \"taxRank\"\n    reports <- pavian::read_reports(REPORTS, NAMES)\n    null <- mapply(reports, names(reports), FUN=function(rep, name){\n        network <- pavian:::build_sankey_network(rep)\n        htmlwidgets::saveWidget(network, paste(name, FILENAME, sep=\"_\"))\n    })\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/SANKEY"], "list_wf_names": ["jianhong/shotgun"]}, {"nb_reuse": 1, "tools": ["Centrifuge"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["shotgun"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process CENTRIFUGE_RUN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::centrifuge=1.0.4_beta\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/centrifuge:1.0.4_beta--py36pl526he941832_2' :\n        'quay.io/biocontainers/centrifuge:1.0.4_beta--py36pl526he941832_2' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path reference_db\n\n    output:\n    tuple val(meta), path(\"${prefix}_centrifuge.out\")    , emit: centrifuge_out\n    tuple val(meta), path(\"${prefix}_centrifuge.report\") , emit: centrifuge_report\n    tuple val(meta), path(\"${prefix}_kraken_like.report\"), emit: kraken_like_report\n    path \"versions.yml\"                                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    def inputs = meta.single_end ? \"-U $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    centrifuge $args \\\\\n        -x $reference_db/nt \\\\\n        --threads $task.cpus \\\\\n        $inputs \\\\\n        --report-file ${prefix}_centrifuge.report \\\\\n        -S ${prefix}_centrifuge.out\n\n    centrifuge-kreport \\\\\n        -x $reference_db/nt \\\\\n        ${prefix}_centrifuge.out > \\\\\n        ${prefix}_kraken_like.out\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        centrifuge: \\$(echo \\$(centrifuge --version 2>&1) | sed 's/version //g; s/ .*\\$//g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/CENTRIFUGE_RUN"], "list_wf_names": ["jianhong/shotgun"]}, {"nb_reuse": 1, "tools": ["MetaPhlAn"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["shotgun"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process METAPHLAN_INSTALL {\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::metaphlan=3.0.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.14--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.14--pyhb7b1952_0' }\"\n\n    output:\n    path \"$prefix\"     , emit: metaphlan_db\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"metaphlan_db\"\n    \"\"\"\n    metaphlan --install --bowtie2db $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan: \\$(metaphlan --version | sed 's/MetaPhlAn version //g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/METAPHLAN_INSTALL"], "list_wf_names": ["jianhong/shotgun"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["shotgun"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process SEQTK_SEQ {\n    tag '$sequences'\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqtk:1.3--h5bf99c6_3' :\n        'quay.io/biocontainers/seqtk:1.3--h5bf99c6_3' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.$ext\")   , emit: reads\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    ext    = task.ext.args2  ?: 'fa'\n    \"\"\"\n    seqtk \\\\\n        seq \\\\\n        $args \\\\\n        $reads \\\\\n        > ${reads}.${ext}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqtk: \\$(echo \\$(seqtk 2>&1) | sed 's/^.*Version: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/SEQTK_SEQ"], "list_wf_names": ["jianhong/shotgun"]}, {"nb_reuse": 1, "tools": ["Bowtie", "MetaPhlAn"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["shotgun"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process METAPHLAN_RUN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::metaphlan=3.0.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.14--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.14--pyhb7b1952_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path reference_db\n\n    output:\n    tuple val(meta), path(\"${prefix}_metaphlan.txt\")     , emit: counts\n    path \"versions.yml\"                                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    metaphlan \\\\\n        $reads \\\\\n        --nproc $task.cpus \\\\\n        --bowtie2db $reference_db \\\\\n        -o ${prefix}_metaphlan.txt \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan: \\$(metaphlan --version | sed 's/MetaPhlAn version //g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/METAPHLAN_RUN"], "list_wf_names": ["jianhong/shotgun"]}, {"nb_reuse": 1, "tools": ["Kaiju"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["shotgun"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["process KAIJU_RUN {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_high_memory'\n\n    conda (params.enable_conda ? \"bioconda::kaiju=1.8.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/kaiju:1.8.2--h2e03b76_0' :\n        'quay.io/biocontainers/kaiju:1.8.2--h2e03b76_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path reference_db\n\n    output:\n    tuple val(meta), path(\"${prefix}_kaiju.out\")         , emit: kaiju_out\n    tuple val(meta), path(\"${prefix}_kaiju.out.krona\")   , emit: krona_input\n    path \"versions.yml\"                                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    def inputs = meta.single_end ? \"-i $reads\" : \"-i ${reads[0]} -i ${reads[1]}\"\n    \"\"\"\n    kaiju -t nodes.dmp -f kaiju_db_*.fmi \\\\\n        $inputs \\\\\n        -o ${prefix}_kaiju.out \\\\\n        -z $task.cpus \\\\\n        $args\n\n    kaiju2krona -t nodes.dmp -n names.dmp \\\\\n        -i ${prefix}_kaiju.out -o ${prefix}_kaiju.out.krona\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kaiju: \\$(echo \\$(kaiju -h 2>&1) | sed 's/Kaiju //g; s/Copyright.*\\$//g')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["jianhong/shotgun/KAIJU_RUN"], "list_wf_names": ["jianhong/shotgun"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["universalModule"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": [" process BWA_MEM {\n     tag \"$meta.id\"\n     label 'process_high'\n     publishDir \"${params.outdir}/${options.publish_dir}\",\n         mode: options.publish_mode,\n         enabled: options.publish_enabled\n\n\n     conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n     if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n         container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n     } else {\n         container \"biocontainers/bwa:v0.7.17_cv1\"\n     }\n\n     input:\n     tuple val(meta), path(reads)\n     tuple path(index), path(fasta)\n\n     output:\n     tuple val(meta), path(\"*.bam\"), emit: bam\n     path \"bwa.version.txt\", emit: version\n\n     script:\n     params.options.forEach{key, value -> options[key]=value}\n     def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n     def rg       = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n     \"\"\"\n     bwa mem \\\\\n        $options.args \\\\\n        $rg \\\\\n        -t $task.cpus \\\\\n        $fasta \\\\\n        $reads \\\\\n        | samtools view $options.samtools_args -@ $task.cpus -bS -o ${prefix}.bam -\n     echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > bwa.version.txt\n     \"\"\"\n }"], "list_proc": ["jianhong/universalModule/BWA_MEM"], "list_wf_names": ["jianhong/universalModule"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["universalModule"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": [" process FASTQC {\n     tag \"$meta.id\"\n     label 'process_medium'\n     publishDir \"${params.outdir}/${options.publish_dir}\",\n         mode: options.publish_mode,\n         enabled: options.publish_enabled\n\n     conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n     if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n         container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n     } else {\n         container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n     }\n\n     input:\n     tuple val(meta), path(reads)\n     tuple path(index), path(fasta)\n\n     output:\n     tuple val(meta), path(\"*.html\"), emit: html\n     tuple val(meta), path(\"*.zip\"), emit: zip\n     path \"fastqc.version.txt\", emit: version\n\n     script:\n     params.options.forEach{key, value -> options[key]=value}\n      if (meta.single_end) {\n          \"\"\"\n          [ ! -f  ${meta.id}.fastq.gz ] && ln -s ${reads[0]} ${meta.id}.fastq.gz\n          fastqc $options.args --threads $task.cpus ${meta.id}.fastq.gz\n          fastqc --version | sed -e \"s/FastQC v//g\" > fastqc.version.txt\n          \"\"\"\n      } else {\n          \"\"\"\n          [ ! -f  ${meta.id}_1.fastq.gz ] && ln -s ${reads[0]} ${meta.id}_1.fastq.gz\n          [ ! -f  ${meta.id}_2.fastq.gz ] && ln -s ${reads[1]} ${meta.id}_2.fastq.gz\n          fastqc $options.args --threads $task.cpus ${meta.id}_1.fastq.gz ${meta.id}_2.fastq.gz\n          fastqc --version | sed -e \"s/FastQC v//g\" > fastqc.version.txt\n          \"\"\"\n      }\n }"], "list_proc": ["jianhong/universalModule/FASTQC"], "list_wf_names": ["jianhong/universalModule"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["universalModule"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": [" process SAMTOOLS_STATS {\n     tag \"$meta.id\"\n     label 'process_medium'\n     publishDir \"${params.outdir}/${options.publish_dir}\",\n         mode: options.publish_mode,\n         enabled: options.publish_enabled\n\n     conda (params.enable_conda ? \"bioconda::samtools=1.09\" : null)\n     if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n         container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n     } else {\n         container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n     }\n\n     input:\n     tuple val(meta), path(bam), path(bai)\n\n     output:\n     tuple val(meta), path(\"*.stats\"), emit: stats\n     tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n     tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n     path \"samtools.version.txt\", emit: version\n\n     script:\n     params.options.forEach{key, value -> options[key]=value}\n     \"\"\"\n     samtools stats $bam > ${bam}.stats\n     samtools flagstat $bam > ${bam}.flagstat\n     samtools idxstats $bam > ${bam}.idxstats\n     echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > samtools.version.txt\n     \"\"\"\n }"], "list_proc": ["jianhong/universalModule/SAMTOOLS_STATS"], "list_wf_names": ["jianhong/universalModule"]}, {"nb_reuse": 1, "tools": ["noreturn", "BiocStyle", "ADaCGH2"], "nb_own": 1, "list_own": ["jianhong"], "nb_wf": 1, "list_wf": ["universalModule"], "list_contrib": ["jianhong"], "nb_contrib": 1, "codes": ["\nprocess MD_HTML {\n  publishDir \"${params.outdir}/${options.publish_dir}\",\n             mode: options.publish_mode\n\n  conda (params.enable_conda ? \"markdown=3.2.2 conda-forge::pymdown-extensions=7.1\" : null)\n\n  input:\n    tuple val(output), path(md)\n  output: path(\"*.html\"), emit: html\n  script:\n  params.options.forEach{key, value -> options[key]=value}\n  \"\"\"\n  # write markdown_to_html.py\n  SEP='\"'\n  SEP=\\$SEP\\$SEP\\$SEP\n  cat <<EOT > markdown_to_html.py\n#!/usr/bin/env python\nfrom __future__ import print_function\nimport argparse\nimport markdown\nimport os\nimport sys\nimport io\n\n\ndef convert_markdown(in_fn):\n    input_md = io.open(in_fn, mode=\"r\", encoding=\"utf-8\").read()\n    html = markdown.markdown(\n        \"[TOC]\\\\n\" + input_md,\n        extensions=[\"pymdownx.extra\", \"pymdownx.b64\", \"pymdownx.highlight\", \"pymdownx.emoji\", \"pymdownx.tilde\", \"toc\"],\n        extension_configs={\n            \"pymdownx.b64\": {\"base_path\": os.path.dirname(in_fn)},\n            \"pymdownx.highlight\": {\"noclasses\": True},\n            \"toc\": {\"title\": \"Table of Contents\"},\n        },\n    )\n    return html\n\n\ndef wrap_html(contents):\n    header = \\${SEP}<!DOCTYPE html><html>\n    <head>\n        <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\" integrity=\"sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T\" crossorigin=\"anonymous\">\n        <style>\n            body {\n              font-family: -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helvetica Neue\",Arial,\"Noto Sans\",sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\",\"Segoe UI Symbol\",\"Noto Color Emoji\";\n              padding: 3em;\n              margin-right: 350px;\n              max-width: 100%;\n            }\n            .toc {\n              position: fixed;\n              right: 20px;\n              width: 300px;\n              padding-top: 20px;\n              overflow: scroll;\n              height: calc(100% - 3em - 20px);\n            }\n            .toctitle {\n              font-size: 1.8em;\n              font-weight: bold;\n            }\n            .toc > ul {\n              padding: 0;\n              margin: 1rem 0;\n              list-style-type: none;\n            }\n            .toc > ul ul { padding-left: 20px; }\n            .toc > ul > li > a { display: none; }\n            img { max-width: 800px; }\n            pre {\n              padding: 0.6em 1em;\n            }\n            h2 {\n\n            }\n        </style>\n    </head>\n    <body>\n    <div class=\"container\">\n    \\${SEP}\n    footer = \\${SEP}\n    </div>\n    </body>\n    </html>\n    \\${SEP}\n    return header + contents + footer\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"mdfile\", type=argparse.FileType(\"r\"), nargs=\"?\", help=\"File to convert. Defaults to stdin.\")\n    parser.add_argument(\n        \"-o\", \"--out\", type=argparse.FileType(\"w\"), default=sys.stdout, help=\"Output file name. Defaults to stdout.\"\n    )\n    return parser.parse_args(args)\n\n\ndef main(args=None):\n    args = parse_args(args)\n    converted_md = convert_markdown(args.mdfile.name)\n    html = wrap_html(converted_md)\n    args.out.write(html)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n\nEOT\n  # run the script\n  python markdown_to_html.py $md -o $output\n  \"\"\"\n}"], "list_proc": ["jianhong/universalModule/MD_HTML"], "list_wf_names": ["jianhong/universalModule"]}, {"nb_reuse": 1, "tools": ["SISTR"], "nb_own": 1, "list_own": ["jimmyliu1326"], "nb_wf": 1, "list_wf": ["SamnSero_Nextflow"], "list_contrib": ["jimmyliu1326"], "nb_contrib": 1, "codes": ["\nprocess sistr {\n    tag \"SISTR Serotyping for ${assembly.baseName}\"\n    label \"process_low\"\n    publishDir \"$params.outdir\"+\"/sistr_res\", mode: \"copy\"\n\n    input:\n        path(assembly)\n    output:\n        file(\"${assembly.baseName}.csv\")\n    shell:\n        \"\"\"\n        sistr -i ${assembly} ${assembly.baseName} -o ${assembly.baseName}.csv -t ${task.cpus} -f csv --qc\n        \"\"\"   \n}"], "list_proc": ["jimmyliu1326/SamnSero_Nextflow/sistr"], "list_wf_names": ["jimmyliu1326/SamnSero_Nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["jimmyliu1326"], "nb_wf": 1, "list_wf": ["tCANS"], "list_contrib": ["jimmyliu1326"], "nb_contrib": 1, "codes": ["\nprocess minimap2 {\n    tag \"Minimap2 alignment on ${reads.simpleName}\"\n    label \"process_medium\"\n\n    input:\n        path(reads)\n        path(reference)\n    output:\n        file(\"${reads.simpleName}.bam\")\n    shell:\n        \"\"\"\n        minimap2 -ax map-ont -t ${task.cpus} ${reference} ${reads} | samtools view -bS -@ ${task.cpus} - | samtools sort - > ${reads.simpleName}.bam\n        \"\"\"\n}"], "list_proc": ["jimmyliu1326/tCANS/minimap2"], "list_wf_names": ["jimmyliu1326/tCANS"]}, {"nb_reuse": 1, "tools": ["AIVAR"], "nb_own": 1, "list_own": ["jimmyliu1326"], "nb_wf": 1, "list_wf": ["tCANS"], "list_contrib": ["jimmyliu1326"], "nb_contrib": 1, "codes": ["\nprocess ivar_trim {\n    tag \"Primer trimming on ${bam.simpleName}\"\n    label \"process_medium\"\n\n    input:\n        path(bam)\n        path(bed)\n    output:\n        file(\"${bam.simpleName}.trimmed.bam\")\n    shell:\n        \"\"\"\n        ivar trim -b ${bed} -p ${bam.simpleName}.trimmed -i ${bam} -q 0\n        \"\"\"\n}"], "list_proc": ["jimmyliu1326/tCANS/ivar_trim"], "list_wf_names": ["jimmyliu1326/tCANS"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jimmyliu1326"], "nb_wf": 1, "list_wf": ["tCANS"], "list_contrib": ["jimmyliu1326"], "nb_contrib": 1, "codes": ["\nprocess ampliconclip {\n    tag \"Primer trimming on ${bam.simpleName}\"\n    label \"process_low\"\n\n    input:\n        path(bam)\n        path(bed)\n    output:\n        file(\"${bam.simpleName}.trimmed.bam\")\n    shell:\n        \"\"\"\n        samtools ampliconclip --no-excluded -@ ${task.cpus} --hard-clip --both-ends --filter-len 0 -b ${bed} ${bam} > ${bam.simpleName}.trimmed.bam\n        \"\"\"\n}"], "list_proc": ["jimmyliu1326/tCANS/ampliconclip"], "list_wf_names": ["jimmyliu1326/tCANS"]}, {"nb_reuse": 1, "tools": ["Jellyfish"], "nb_own": 1, "list_own": ["jkirangw"], "nb_wf": 1, "list_wf": ["WORMpipe"], "list_contrib": ["jkirangw", "b16joski"], "nb_contrib": 2, "codes": ["\nprocess run_jellyfish {\n\ttag \"$read_file\"\n\tpublishDir \"${params.outdir}/jellyfish_result\", mode: 'copy'\n\n\tinput:\n\tpath read_file from jellyfish_ch\n\n\n\toutput:\n\tfile \"*\"\n\n\t\n\tscript:\n\tjelly_out = read_file.baseName\n\t\"\"\"\n\tjellyfish count -C -m 21 -s 100M -t $params.threads $read_file -o ${jelly_out}.jf\n\tjellyfish histo -t 10 ${jelly_out}.jf > ${jelly_out}.histo\n\t\"\"\"\n}"], "list_proc": ["jkirangw/WORMpipe/run_jellyfish"], "list_wf_names": ["jkirangw/WORMpipe"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["jkirangw"], "nb_wf": 1, "list_wf": ["WORMpipe"], "list_contrib": ["jkirangw", "b16joski"], "nb_contrib": 2, "codes": ["\nprocess run_busco {\n\ttag\"$fa\"\n\tpublishDir \"${params.outdir}/busco_result\", mode: 'copy'\n\t\n\tinput:\n\tpath fa from busco_ch\n\n\toutput:\n\tfile \"*\"\n\n\t\"\"\"\n\tbusco -i $fa -l nematoda_odb10 -o ${fa}.out -m genome \n\t\"\"\"\n}"], "list_proc": ["jkirangw/WORMpipe/run_busco"], "list_wf_names": ["jkirangw/WORMpipe"]}, {"nb_reuse": 1, "tools": ["BlobTools"], "nb_own": 1, "list_own": ["jkirangw"], "nb_wf": 1, "list_wf": ["WORMpipe"], "list_contrib": ["jkirangw", "b16joski"], "nb_contrib": 2, "codes": ["\nprocess run_blobtools {\n\ttag \"$busco_out\"\n\ttag \"$fasta\"\n\tpublishDir \"${params.outdir}/blobtools2_result\", mode: 'copy'\n\n\tinput:\n\tpath fasta from blobtools2_ch1\n\tpath cov from blobtools2_ch3\n\tpath hits from blobtools2_ch2\n\n\toutput:\n\tfile \"*\"\n\n\t\"\"\"\n\tblobtools create --fasta $fasta \\\n\t\t\t --cov $cov \\\n\t\t         --hits $hits \\\n\t\t\t --taxdump $params.taxdump \\\n\t\t\t ${fasta}_blobdir\n\t\"\"\"\n}"], "list_proc": ["jkirangw/WORMpipe/run_blobtools"], "list_wf_names": ["jkirangw/WORMpipe"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["jlboat"], "nb_wf": 1, "list_wf": ["NextflowScripts"], "list_contrib": ["jlboat"], "nb_contrib": 1, "codes": ["\nprocess cleanReads {\n    publishDir \"$baseDir/results/reads/\"\n\n    input:\n      set val(dataset_id), file(forward) from reads\n \n    output:\n      set dataset_id, file(\"${dataset_id}.TRIMMED.fq.gz\") into trimmed_reads\n      file(\"${dataset_id}.TRIMMED.fq.gz\") into trimmed_reads_2\n \n    clusterOptions = { \"--account=icbrbi --qos=icbrbi --time=4:00:00 --mem-per-cpu=3gb --cpus-per-task=1\" }\n    module 'trimmomatic'\n \n    script:\n    \"\"\"\n    trimmomatic \\\nSE \\\n-trimlog /dev/null \\\n-threads ${task.cpus} \\\n${forward} ${dataset_id}.TRIMMED.fq.gz \\\nILLUMINACLIP:/apps/trimmomatic/0.36/adapters/TruSeq3-SE.fa:2:30:10:2:true \\\nLEADING:3 \\\nTRAILING:3 \\\nSLIDINGWINDOW:4:15 \\\nMAXINFO:40:0.3 \\\nMINLEN:40\n    \"\"\"\n}"], "list_proc": ["jlboat/NextflowScripts/cleanReads"], "list_wf_names": ["jlboat/NextflowScripts"]}, {"nb_reuse": 2, "tools": ["FastQC", "SamPler"], "nb_own": 2, "list_own": ["jlboat", "jtmccr1"], "nb_wf": 2, "list_wf": ["bft-nf", "NextflowScripts"], "list_contrib": ["jlboat", "jtmccr1"], "nb_contrib": 2, "codes": ["\nprocess postFastqc {\n    publishDir \"results/fastqc/post/\"\n\n    input:\n    set val(dataset_id), file(r1) from trimmed_reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into trimmed_fastqc_results\n\n    clusterOptions = { \"--account=icbrbi --qos=icbrbi --time=4:00:00 --mem-per-cpu=2gb --cpus-per-task=1\" }\n    module 'fastqc'\n\n    script:\n    \"\"\"\n    fastqc -q ${r1} \n    \"\"\"\n}", "\nprocess sample_meta{\n    tag \"$key\"\n    publishDir \"${params.outDir}/filter\" , pattern: \"*tsv\", saveAs: {it.replaceAll(\"meta\",key)}\n    input:\n        tuple val(key), path(metadata),val(n),val(wieghts)\n    output:\n        path(\"meta.tsv\")\n        tuple val(key), path(\"names.txt\"), emit : seqNames\n\"\"\"\nsampler -i $metadata -n $n -w $wieghts >meta.tsv;\n\ncut -f1 meta.tsv | sed 1,1d > names.txt\n\"\"\"\n}"], "list_proc": ["jlboat/NextflowScripts/postFastqc", "jtmccr1/bft-nf/sample_meta"], "list_wf_names": ["jlboat/NextflowScripts", "jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["jlboat"], "nb_wf": 1, "list_wf": ["NextflowScripts"], "list_contrib": ["jlboat"], "nb_contrib": 1, "codes": ["\nprocess alignReads {\n    publishDir \"$baseDir/results/alignments/\"\n\n    input:\n        file '*' from indexed\n        file r1 from trimmed_reads_2\n\n    output:\n        file \"*.bam\" into bam_files\n\n    clusterOptions = { \"--account=icbrbi --qos=icbrbi --time=4:00:00 --mem-per-cpu=10gb --cpus-per-task=4\" }\n    module 'star'\n\n    script:\n    \"\"\"\n    STAR --runThreadN 4 \\\n--readFilesCommand zcat \\\n--genomeDir ${baseDir}/data/references/ \\\n--genomeFastaFiles ${genome_fna} \\\n--readFilesIn ${r1} \\\n--outFileNamePrefix ${r1.simpleName}. \\\n--outSAMattributes Standard \\\n--outSAMtype BAM SortedByCoordinate\n    \"\"\"\n\n}"], "list_proc": ["jlboat/NextflowScripts/alignReads"], "list_wf_names": ["jlboat/NextflowScripts"]}, {"nb_reuse": 1, "tools": ["StringTie"], "nb_own": 1, "list_own": ["jlboat"], "nb_wf": 1, "list_wf": ["NextflowScripts"], "list_contrib": ["jlboat"], "nb_contrib": 1, "codes": ["\nprocess makeTranscript {\n\n    input:\n      file(bam_file) from bam_files\n    \n    output:\n      file '*' into transcripts\n  \n    clusterOptions = { \"--account=icbrbi --qos=icbrbi --time=4:00:00 --mem-per-cpu=3gb --cpus-per-task=2 --no-requeue\" }\n    module 'stringtie'\n\n    \"\"\"\n    stringtie -e -B -p ${task.cpus} -j 2 -o ${bam_file.simpleName}_transcripts.gtf -G ${genome_gtf} ${bam_file}\n    \"\"\"\n}"], "list_proc": ["jlboat/NextflowScripts/makeTranscript"], "list_wf_names": ["jlboat/NextflowScripts"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["jlboat"], "nb_wf": 1, "list_wf": ["NextflowScripts"], "list_contrib": ["jlboat"], "nb_contrib": 1, "codes": ["\nprocess mapping {    \n    input:\n      file genome from genome_file\n      set dataset_id, file(forward), file(reverse) from paired_fastq\n  \n    output:\n      set dataset_id, \"${dataset_id}_Aligned.sortedByCoord.out.bam\" into bam_files\n  \n    executor 'slurm'\n    cpus 4\n    clusterOptions = { \"--account=icbrbi --qos=icbrbi-b --time=4:00:00 --mem-per-cpu=10gb --cpus-per-task=4\" }\n    module 'star'\n\n    \"\"\"\n    STAR \\\n  --genomeDir ${genome} \\\n  --readFilesIn ${forward},${reverse} \\\n  --readFilesCommand 'zcat -fc' \\\n  --runThreadN ${task.cpus} \\\n  --outSAMtype BAM SortedByCoordinate \\\n  --outReadsUnmapped Fastx \\\n  --outFilterType BySJout \\\n  --outFilterMultimapNmax 20 \\\n  --outFileNamePrefix ${dataset_id}_\n    \"\"\"\n}"], "list_proc": ["jlboat/NextflowScripts/mapping"], "list_wf_names": ["jlboat/NextflowScripts"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jlboat"], "nb_wf": 1, "list_wf": ["NextflowScripts"], "list_contrib": ["jlboat"], "nb_contrib": 1, "codes": ["\nprocess bwa {\n    publishDir \"results\"\n\n    input:\n        set dataset_id, file(r1), file(r2) from bwa_paired_fastq\n        file genome from genome_file\n\n    output:\n        file \"*.bam\" into bam_files\n\n    clusterOptions = { \"-l select=1:ncpus=2:mem=6gb,walltime=04:00:00\" }\n    module 'singularity'\n    module 'anaconda'\n\n    script:\n    \"\"\"\n# load samtools from aligners env\nsource activate aligners\nsingularity run -B /scratch2 ~/singularity_containers/bwa.simg mem -t ${task.cpus} ${params.genome} ${r1} ${r2} | samtools view -b - > ${dataset_id}.bam\n    \"\"\"\n\n}"], "list_proc": ["jlboat/NextflowScripts/bwa"], "list_wf_names": ["jlboat/NextflowScripts"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["jonbra"], "nb_wf": 1, "list_wf": ["nf-desensitize"], "list_contrib": ["jonbra"], "nb_contrib": 1, "codes": ["process Multiqc {\n\n  publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n  input:\n  path 'data/*'\n\n  output:\n  path 'multiqc_report.html'\n\n  shell:\n  '''\n  multiqc data\n  '''\n}"], "list_proc": ["jonbra/nf-desensitize/Multiqc"], "list_wf_names": ["jonbra/nf-desensitize"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["jonbra"], "nb_wf": 1, "list_wf": ["nf-desensitize"], "list_contrib": ["jonbra"], "nb_contrib": 1, "codes": ["process pe_First_Fastqc {\n\n  echo true\n\n  input:\n  tuple val(sampleName), path(read1), path(read2)\n\n  publishDir \"${params.outdir}/fastqc\", mode: 'link', pattern:'*.{zip,html}'\n\n  output:\n  tuple val(sampleName), path (\"*.{zip,html}\"), emit: FASTQC_out\n\n  shell:\n  '''\n  fastqc -t !{params.cpu} !{read1} !{read2}\n  '''\n}"], "list_proc": ["jonbra/nf-desensitize/pe_First_Fastqc"], "list_wf_names": ["jonbra/nf-desensitize"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["jordwil"], "nb_wf": 1, "list_wf": ["nextflow-bowtie2"], "list_contrib": ["jordwil"], "nb_contrib": 1, "codes": [" process alignment {\n    publishDir \"${params.outdir}/bowtie2\", mode: 'copy'\n\n    input:\n    file indices from bt_indices.collect()\n    set name, file(reads) from ch_read_files_trimming\n\n    output:\n    file '*.bam' into bowtie_bam, bowtie_bam_for_unmapped\n\n    script:\n    index_base = indices[0].toString() - ~/.rev.\\d.bt2?/ - ~/.\\d.bt2?/\n    \"\"\"\n    bowtie2 -x $index_base -U $reads -p ${task.cpus} \\\\\n    | samtools view -bS - > ${name}.bam\n    \"\"\"\n\n }"], "list_proc": ["jordwil/nextflow-bowtie2/alignment"], "list_wf_names": ["jordwil/nextflow-bowtie2"]}, {"nb_reuse": 14, "tools": ["Cutadapt", "RDFScape", "BCFtools", "Salmon", "seqtk", "PLINK", "SAMtools", "kallisto", "MMseqs", "Sailfish", "MultiQC", "FastQC", "Picard", "GATK", "gffread"], "nb_own": 15, "list_own": ["msmallegan", "seandavi", "sripaladugu", "loipf", "rikenbit", "markgene", "supark87", "ralsallaq", "mbosio85", "jordwil", "wslh-bio", "markxiao", "zamanianlab", "kevinpryan", "t-neumann"], "nb_wf": 14, "list_wf": ["curatedMetagenomicsNextflow", "cutnrun", "prac_nextflow", "metaGx_nf", "RNAseq-VC-nf", "salmon-nf", "rmghc-workshop-19", "RamDAQ-prototype", "DNAseq-pipeline", "germline_somatic", "nextflow-bowtie2", "nf-core-gatkcohortcall", "rtp_workshop", "spriggan", "PRS-dev"], "list_contrib": ["chelauk", "shbrief", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "supark87", "jwokaty", "tiagochst", "markxiao", "davidmasp", "drpatelh", "Rotholandus", "k-florek", "markgene", "pcantalupo", "malinlarsson", "olgabot", "jrdemasi", "manabuishii", "skrakau", "kevinpryan", "t-neumann", "msmallegan", "sofiahaglund", "seandavi", "lescai", "yuifu", "ewels", "ralsallaq", "szilvajuhos", "FriederikeHanssen", "chuan-wang", "nf-core-bot", "loipf", "maxulysse", "ggabernet", "winni2k", "apeltzer", "wheelern", "mbosio85", "jordwil", "adrlar", "AbigailShockey", "myoshimura080822", "fbeghini", "hynesgra"], "nb_contrib": 47, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess TX{\n    publishDir \"${params.outdir}/reference\", mode: 'copy',\n        saveAs: { params.save_transcriptome ? \"reference/transcriptome/${it}\" : null }\n\n    when:\n        !params.transcriptome && params.fasta\n\n    input:\n    file(fasta) from ch_fasta\n    file(gtf) from ch_gtf\n\n    output:\n    file(\"${fasta.baseName}.tx.fa\") into transcriptome_created\n\n    script:\n    \"\"\"\n    gffread -F -w \"${fasta.baseName}.tx.fa\" -g $fasta $gtf\n    \"\"\"\n}", "\nprocess run_sailfish  {\n\n    tag {\"${proj_id}\"}\n    publishDir \"output_${proj_id}/${run_id}/06_sailfish\", mode: 'copy', overwrite: true\n\n    container \"docker.io/myoshimura080822/sailfish:1.0\"\n\n    input:\n    val proj_id\n    set run_id, fastq_name, file(fastq), option_name, option, index_name, index, file(index_files) from sailfish_conditions\n\n    output:\n    set run_id, fastq_name, file(\"sailfish_${fastq_name}_trim\") into sailfish_output\n\n    script:\n\n    \"\"\"\n    sailfish quant -i ${index} ${option} -r <(gzip -dc ${fastq}) -o ./sailfish_${fastq_name}_trim\n    \"\"\"\n}", "\nprocess PREPROCESS_READS { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/reads_prepro\", pattern:\"*cutadapt_output.txt\", mode: \"copy\", saveAs: { filename -> \"${sample_id}/$filename\" }\n\tstageInMode = 'copy'                                    \n\tcache false\n\n\tinput:\n\t\ttuple val(sample_id), path(reads) \n\t\tval num_threads\n\t\tpath adapter_3_seq_file\n\t\tpath adapter_5_seq_file\n\n\toutput:\n\t\ttuple val(sample_id), path(\"${sample_id}_prepro_1.fastq.gz\"), path(\"${sample_id}_prepro_2.fastq.gz\"), emit: reads_prepro\n\t\tpath \"${sample_id}_cutadapt_output.txt\", emit: cutadapt\n\n\tshell:\n\t'''\n\n\t### adapter 3' input as string or file\n\tif [[ !{adapter_3_seq_file} == *\".fasta\"* ]]\n\tthen\n  \t\tADAPTER_3=file:!{adapter_3_seq_file}\n\telse\n\t\tADAPTER_3=\"!{adapter_3_seq_file}\"\n\tfi\n\n\t### adapter 5' input as string or file\n\tif [[ !{adapter_5_seq_file} == *\".fasta\"* ]]\n\tthen\n  \t\tADAPTER_5=file:!{adapter_5_seq_file}\n\telse\n\t\tADAPTER_5=\"!{adapter_5_seq_file}\"\n\tfi\n\t\n\t### theoretically sorting not needed but maybe speeds up things\n\treads_sorted=$(echo !{reads} | xargs -n1 | sort | xargs)\n\n\t### combine multiple seq files in the same sample directory with same direction together\n\treads_sorted_1=$(find $reads_sorted -name \"*_1.fq.gz\" -o -name \"*_1.fastq.gz\")\n\treads_sorted_2=$(find $reads_sorted -name \"*_2.fq.gz\" -o -name \"*_2.fastq.gz\")\n\tcat $reads_sorted_1 > !{sample_id}_raw_reads_connected_1.fastq.gz\n\tcat $reads_sorted_2 > !{sample_id}_raw_reads_connected_2.fastq.gz\n\n\tcutadapt --cores=!{num_threads} --max-n 0.1 --discard-trimmed --pair-filter=any --minimum-length 10 -a $ADAPTER_3 -A $ADAPTER_5 -o !{sample_id}_prepro_1.fastq.gz -p !{sample_id}_prepro_2.fastq.gz !{sample_id}_raw_reads_connected_1.fastq.gz !{sample_id}_raw_reads_connected_2.fastq.gz > !{sample_id}_cutadapt_output.txt\n\t'''\n}", "\nprocess quast_summary {\n  publishDir \"${params.outdir}/quast\",mode:'copy'\n\n  input:\n  file(files) from quast_files.collect()\n\n  output:\n  file(\"quast_results.tsv\") into quast_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing quast output\n  def summarize_quast(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      # read in data frame from file\n      df = pd.read_csv(file, sep='\\\\t')\n      # get contigs, total length and assembly length columns\n      df = df.iloc[:,[1,7,17]]\n      # assign sample id as column\n      df = df.assign(Sample=sample_id)\n      # rename columns\n      df = df.rename(columns={'# contigs (>= 0 bp)':'Contigs','Total length (>= 0 bp)':'Assembly Length (bp)'})\n      # re-order data frame\n      df = df[['Sample', 'Contigs','Assembly Length (bp)', 'N50']]\n      return df\n\n  # get quast output files\n  files = glob.glob(\"*.quast.tsv\")\n\n  # summarize quast output files\n  dfs = map(summarize_quast,files)\n\n  # concatenate dfs and write data frame to file\n  dfs_concat = pd.concat(dfs)\n  dfs_concat.to_csv(f'quast_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess foo {\n  script:\n  if( params.aligner == 'kallisto' )\n    \"\"\"\n    kallisto --reads /some/data.fastq\n    \"\"\"\n  else if( params.aligner == 'salmon' )\n    \"\"\"\n    salmon --reads /some/data.fastq\n    \"\"\"\n  else\n    throw new IllegalArgumentException(\"Unknown aligner $params.aligner\")\n}", "\nprocess GatherVcfs {\n\n\tlabel 'memory_singleCPU_2_task'\n    label 'cpus_1'\n\t\n\ttag \"${params.cohort}\"\n\n    publishDir path:\"${params.outdir}/CohortGenotype/\", mode: params.publishDirMode, pattern: '*.{vcf,idx}'\n\n    input:\n    file (vcf) from vcf_hf_ch.collect()\n\tfile (vcf_idx) from vcf_idx_hf_ch.collect()\n    val genome from params.genome\n\n\toutput:\n    tuple file(\"${params.cohort}.vcf\"), file(\"${params.cohort}.vcf.idx\") into (vcf_snv_ch, vcf_sid_ch, vcf_recal_ch)\n\n                                                 \n                                                                                                                                              \n                                                                                      \n                                                                                                        \n                                                                                                       \n                                                                          \n                \n                                        \n\n    script:\n\n    if( genome == 'GRCh38' )\n      \t\"\"\"\n     \tgatk --java-options -Xmx${task.memory.toGiga()}g  \\\n        GatherVcfs \\\n           ${vcf.findAll{ it=~/chr\\d+/ }.collect().sort{ it.name.tokenize('.')[1].substring(3).toInteger() }.plus(vcf.find{ it=~/chrX/ }).plus(vcf.find{ it=~/chrY/ }).minus(null).collect{ \"--INPUT $it \" }.join() } \\\n        --OUTPUT ${params.cohort}.vcf\n        \"\"\"\n    else\n      \"\"\"\n       gatk --java-options -Xmx${task.memory.toGiga()}g  \\\n        GatherVcfs \\\n           ${vcf.findAll{ it=~/\\d+/ }.collect().sort{ it.name.tokenize('.')[1].toInteger() }.plus(vcf.find{ it=~/X/ }).plus(vcf.find{ it=~/Y/ }).minus(null).collect{ \"--INPUT $it \" }.join() }\\\n        --OUTPUT ${params.cohort}.vcf\n       \"\"\" \n\t\n}", "\nprocess salmon {\n\n  publishDir 'results/salmon'\n\n  input:\n  set sample_id, file(bam), file(transcript_fasta) from mapped_transcriptome.combine(transcriptome)\n\n  output:\n  file(\"*\") into salmon\n  file(\"*\") into salmon_for_de\n\n  script:\n  \"\"\"\n  salmon quant -l A \\\n    -p 1 \\\n    -t ${transcript_fasta} \\\n    -o ${sample_id} \\\n    -a ${bam} \\\n    --numBootstraps 30\n  \"\"\"\n}", "\nprocess salmon {\n\n\ttag { lane }\n\n    input:\n    set val(lane), file(reads) from fastqChannel\n    file index from indexChannel.first()\n\n    output:\n    file (\"${lane}_salmon/quant.sf\") into salmonChannel\n    file (\"${lane}_pseudo.bam\") into pseudoBamChannel\n\n    shell:\n\n    def single = reads instanceof Path\n\n    if (!single)\n\n      '''\n      salmon quant -i !{index} -l A -1 !{reads[0]} -2 !{reads[1]} -o !{lane}_salmon -p !{task.cpus} --validateMappings --no-version-check -z | samtools view -Sb -F 256 - > !{lane}_pseudo.bam\n\t    '''\n    else\n      '''\n      salmon quant -i !{index} -l A -r !{reads} -o !{lane}_salmon -p !{task.cpus} --validateMappings --no-version-check -z | samtools view -Sb -F 256 - > !{lane}_pseudo.bam\n\t    '''\n\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/multiqc/${PEAK_TYPE}\", mode: 'copy'\n\n    when:\n    !params.skip_multiqc\n\n    input:\n    file multiqc_config from ch_multiqc_config\n\n    file ('software_versions/*') from ch_software_versions_mqc.collect()\n    file ('workflow_summary/*') from create_workflow_summary(summary)\n\n    file ('fastqc/*') from ch_fastqc_reports_mqc.collect().ifEmpty([])\n    file ('trimgalore/*') from ch_trimgalore_results_mqc.collect().ifEmpty([])\n    file ('trimgalore/fastqc/*') from ch_trimgalore_fastqc_reports_mqc.collect().ifEmpty([])\n\n    file ('alignment/library/*') from ch_sort_bam_flagstat_mqc.collect()\n    file ('alignment/mergedLibrary/*') from ch_merge_bam_stats_mqc.collect()\n    file ('alignment/mergedLibrary/*') from ch_rm_orphan_flagstat_mqc.collect{it[1]}\n    file ('alignment/mergedLibrary/*') from ch_rm_orphan_stats_mqc.collect()\n    file ('alignment/mergedLibrary/picard_metrics/*') from ch_merge_bam_metrics_mqc.collect()\n    file ('alignment/mergedLibrary/picard_metrics/*') from ch_collectmetrics_mqc.collect()\n\n    file ('macs/*') from ch_macs_mqc.collect().ifEmpty([])\n    file ('macs/*') from ch_macs_qc_mqc.collect().ifEmpty([])\n    file ('macs/consensus/*') from ch_macs_consensus_counts_mqc.collect().ifEmpty([])\n    file ('macs/consensus/*') from ch_macs_consensus_deseq_mqc.collect().ifEmpty([])\n\n    file ('preseq/*') from ch_preseq_mqc.collect().ifEmpty([])\n    file ('deeptools/*') from ch_plotfingerprint_mqc.collect().ifEmpty([])\n    file ('deeptools/*') from ch_plotprofile_mqc.collect().ifEmpty([])\n    file ('phantompeakqualtools/*') from ch_spp_out_mqc.collect().ifEmpty([])\n    file ('phantompeakqualtools/*') from ch_spp_csv_mqc.collect().ifEmpty([])\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename --config $multiqc_config \\\\\n        -m custom_content -m fastqc -m cutadapt -m samtools -m picard -m preseq -m featureCounts -m deeptools -m phantompeakqualtools\n    \"\"\"\n}", "\nprocess generate_intervals {\n\n      input:\n          file(gtf) from annotations\n\n      output:\n          file(\"transcript.interval_list\") into (intervals1, intervals2, intervals3)\n          file(\"genome_picard.dict\") into (dict1, dict2, dict3)\n          file(\"transcript_intervals.bed\") into intervals_bed\n\n      when:\n          params.bam\n\n      shell:\n      '''\n          cat !{gtf} | grep -E '\\\\stranscript\\\\s' | awk '{print $1 \"\\t\" $4-1 \"\\t\" $5}' > transcript_intervals.bed\n\n          picard CreateSequenceDictionary \\\n            R=\"!{aedesgenome}/genome.fa\" \\\n            O=genome_picard.dict\n\n          picard BedToIntervalList \\\n            I=transcript_intervals.bed \\\n            O=transcript.interval_list \\\n            SD=genome_picard.dict\n      '''\n\n}", " process alnReadsToDBWmmseq {\n        container \"${container_mmseqs2}\"\n        label 'multithread'\n        publishDir \"${params.outD}/read_based_annotation/\", mode: 'copy'\n        maxForks = 4\n        \n        input:\n        set sname, file(qtrimmedF) from qtrimmed_ch8\n        path targetDB_parent \n        \n        output:\n        set sname, file(\"all_hits_in_${sname}.m8\") into sampleHitsMMseq_ch\n\n                                                             \n        when:\n        ! params.useDiamond\n        \n        \"\"\"\n        #mmseqs2 way\n        mkdir tmp\n        mmseqs createdb ${qtrimmedF} queryDB --dbtype 0 --shuffle 1 --createdb-mode 0 --id-offset 0 --compressed 0 -v 3 \n        wait\n\n        mmseqs search queryDB ${targetDB_parent}/targetDBMMseq alnDB tmp \\\n                -a  --alignment-mode 4 --threads ${task.cpus} \\\n                -s 2 --cov-mode 2 -c 0.95 -e 0.00001 \\\n                --remove-tmp-files true  \n        wait\n\n        echo \"the format of the output is set to be able to run famli fltr next\"\n        mmseqs convertalis queryDB ${targetDB_parent}/targetDBMMseq alnDB all_hits_in_${sname}.m8 \\\n                --format-output \"query,target,alnlen,qlen,tlen,qstart,qend,tstart,tend,pident,mismatch,gapopen,evalue,bits,qcov,tcov\"\n\n        \"\"\"\n    }", "\nprocess fasterq_dump {\n    publishDir \"${params.publish_dir}/${meta.sample}/fasterq_dump\", pattern: \"{fastq_line_count.txt,*_fastqc/fastqc_data.txt,sampleinfo.txt,.command*}\"\n    \n    maxForks 80\n    cpus 4\n    memory \"16g\"\n    errorStrategy  { task.attempt <= maxRetries  ? 'retry' : 'finish' }\n    maxRetries 3\n    disk \"500 GB\"\n\n    tag \"${meta.sample}\"\n\n    input:\n    val meta\n\n    output:\n    val(meta), emit: meta\n    path \"out.fastq.gz\", emit: fastq\n    path \"*_fastqc/fastqc_data.txt\", emit: fastqc_data\n    path \"fastq_line_count.txt\"\n    path \".command*\"\n    path \"sampleinfo.txt\"\n\n    script:\n      \"\"\"\n      echo \"accessions: ${meta.accessions}\" > sampleinfo.txt\n      fasterq-dump \\\n          --skip-technical \\\n          --force \\\n          --threads ${task.cpus} \\\n          --split-files ${meta.accessions.join(\" \")}\n      cat *.fastq | gzip > out.fastq.gz\n      gunzip -c out.fastq.gz | wc -l > fastq_line_count.txt\n      rm *.fastq\n      seqtk sample -s100 out.fastq.gz 50000 > out_sample.fastq\n      fastqc --extract out_sample.fastq\n      rm out_sample.fastq\n      \"\"\"\n}", "\nprocess target_qc_final {\n\n    publishDir \"${params.outdir}/intermediate_files\", mode: 'copy'\n\n    input:\n    tuple prefix, file(bed), file(bim), file(cov), file(fam), file(height) from target_data.copy6\n    tuple _tmp, file(irem), file(hh), file(snplist), file(fam1), file(prune_in), file(prune_out), file(het) from target_qc_standard_gwas_qc_1_output_copy5\n    tuple file(a1), file(mismatch) from target_qc_mismatching_snps_output\n    file rel_id from target_qc_relatedness_output\n\n    output:\n    tuple val(\"${prefix}\"), file(\"${prefix_qc}.fam\"), file(\"${prefix_qc}.bed\"), file(\"${prefix_qc}.bim\") into target_qc_final_output\n\n    script:\n    prefix_qc = \"${prefix}.QC\"\n    \"\"\"\n    plink --bfile ${prefix} --make-bed --keep ${rel_id} --out ${prefix_qc} --extract ${snplist} --exclude ${mismatch} --a1-allele ${a1}\n    if test -f \"${prefix_qc}.log\";\n        then mv ${prefix_qc}.log ${params.outdir}/logs/${prefix_qc}.log.6\n    fi\n    \"\"\"  \n}"], "list_proc": ["jordwil/nextflow-bowtie2/multiqc", "kevinpryan/rtp_workshop/TX", "rikenbit/RamDAQ-prototype/run_sailfish", "loipf/DNAseq-pipeline/PREPROCESS_READS", "wslh-bio/spriggan/quast_summary", "sripaladugu/germline_somatic/BcftoolsStats", "supark87/prac_nextflow/foo", "msmallegan/rmghc-workshop-19/salmon", "t-neumann/salmon-nf/salmon", "markgene/cutnrun/MultiQC", "zamanianlab/RNAseq-VC-nf/generate_intervals", "ralsallaq/metaGx_nf/alnReadsToDBWmmseq", "seandavi/curatedMetagenomicsNextflow/fasterq_dump", "markxiao/PRS-dev/target_qc_final"], "list_wf_names": ["sripaladugu/germline_somatic", "markgene/cutnrun", "loipf/DNAseq-pipeline", "seandavi/curatedMetagenomicsNextflow", "msmallegan/rmghc-workshop-19", "zamanianlab/RNAseq-VC-nf", "ralsallaq/metaGx_nf", "wslh-bio/spriggan", "t-neumann/salmon-nf", "supark87/prac_nextflow", "markxiao/PRS-dev", "kevinpryan/rtp_workshop", "jordwil/nextflow-bowtie2", "rikenbit/RamDAQ-prototype"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["josephhalstead"], "nb_wf": 1, "list_wf": ["nextflow_small_germline_panel"], "list_contrib": ["josephhalstead"], "nb_contrib": 1, "codes": ["\nprocess align_reads_with_bwa{\n\n    input:\n    set val(id1), file(read1) from trimmed_reads_channel_r1\n    set val(id2), file(read2) from trimmed_reads_channel_r2\n\n    output:\n    file \"${id1}.bam\" into inital_bam_channel\n    file \"${id1}.bam.bai\" into inital_bam_indexes_channel\n\n    script:\n    lane = read1.name.toString().tokenize('_').get(2)\n    sample_id = read1.name.toString().tokenize('_').get(0)\n\n    \"\"\"\n\tbwa mem \\\n    -t $params.bwa_threads \\\n    -M \\\n    -R '@RG\\\\tID:${params.sequencing_run}.${lane}\\\\tCN:${params.sequencing_centre}\\\\tSM:${sample_id}\\\\tLB:${params.sequencing_run}\\\\tPL:ILLUMINA' \\\n    $reference_genome \\\n    $read1 \\\n    $read2 | samtools view -Sb - | \\\n    samtools sort -T $params.bwa_temp_dir -O bam > ${id1}.bam\n\n    samtools index ${id1}.bam\n\t\"\"\"\n}"], "list_proc": ["josephhalstead/nextflow_small_germline_panel/align_reads_with_bwa"], "list_wf_names": ["josephhalstead/nextflow_small_germline_panel"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["josephhalstead"], "nb_wf": 1, "list_wf": ["nextflow_small_germline_panel"], "list_contrib": ["josephhalstead"], "nb_contrib": 1, "codes": ["\nprocess fix_platypus_headers{\n\n    input:\n    file(per_chr_vcfs) from per_chromsome_vcf_channel\n\n    output:\n    file (\"${params.sequencing_run}_${chr}_sd.vcf\") into per_chromsome_vcf_sd_channel\n\n    script:\n    chr = per_chr_vcfs.baseName.toString().tokenize('_').get(1)\n\n    \"\"\"\n    picard UpdateVcfSequenceDictionary \\\n    I=${per_chr_vcfs} \\\n    O=${params.sequencing_run}_${chr}_sd.vcf \\\n    SD=${sequence_dict}\n    \"\"\"\n}"], "list_proc": ["josephhalstead/nextflow_small_germline_panel/fix_platypus_headers"], "list_wf_names": ["josephhalstead/nextflow_small_germline_panel"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["joshua-d-campbell"], "nb_wf": 1, "list_wf": ["nf-GATK_Exome_Preprocess"], "list_contrib": ["joshua-d-campbell"], "nb_contrib": 1, "codes": ["\nprocess runBWA {\n    tag \"${indivID}|${sampleID}|${libraryID}|${rgID}\"\n\tpublishDir \"${OUTDIR}/${indivID}/${sampleID}/Processing/Libraries/${libraryID}/${rgID}/BWA/\"\n\t\n    input:\n    set indivID, sampleID, libraryID, rgID, ubam, ubamxt, metrics from runMarkIlluminaAdaptersOutput\n    \n    output:\n    set indivID, sampleID, file(outfile_bam) into runBWAOutput\n    \n    script:\n    outfile_bam = sampleID + \"_\" + libraryID + \"_\" + rgID + \".aligned.bam\"\n\t\n    \"\"\"\n\tset -o pipefail\n\tjava -Dsamjdk.buffer_size=131072 -Dsamjdk.compression_level=1 -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:ParallelGCThreads=1 -Xmx5G -jar ${PICARD} SamToFastq \\\n\t\tI=${ubamxt} \\\n\t\tFASTQ=/dev/stdout \\\n\t\tCLIPPING_ATTRIBUTE=XT CLIPPING_ACTION=2 INTERLEAVE=true NON_PF=true \\\n\t\tTMP_DIR=tmp | \\\n\tbwa mem -M -t 14 -p ${REF} /dev/stdin | \\\n\tjava -XX:ParallelGCThreads=1 -Xmx5G -jar ${PICARD} MergeBamAlignment \\\n\t\tALIGNED_BAM=/dev/stdin \\\n\t\tUNMAPPED_BAM=${ubamxt} \\\n\t\tOUTPUT=${outfile_bam} \\\n\t\tR=${REF} CREATE_INDEX=true ADD_MATE_CIGAR=true \\\n\t\tCLIP_ADAPTERS=false \\\n\t\tCLIP_OVERLAPPING_READS=true \\\n\t\tINCLUDE_SECONDARY_ALIGNMENTS=true \\\n\t\tMAX_INSERTIONS_OR_DELETIONS=-1 \\\n\t\tPRIMARY_ALIGNMENT_STRATEGY=MostDistant \\\n\t\tATTRIBUTES_TO_RETAIN=XS \\\n\t\tTMP_DIR=tmp\n\t\t\n\trm -rf tmp\t\n\t\"\"\"\t\n}"], "list_proc": ["joshua-d-campbell/nf-GATK_Exome_Preprocess/runBWA"], "list_wf_names": ["joshua-d-campbell/nf-GATK_Exome_Preprocess"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["joshua-d-campbell"], "nb_wf": 1, "list_wf": ["nf-GATK_Exome_Preprocess"], "list_contrib": ["joshua-d-campbell"], "nb_contrib": 1, "codes": ["\nprocess runPrintReads {\n    tag \"${indivID}|${sampleID}\"\n\tpublishDir \"${OUTDIR}/${indivID}/${sampleID}/\"\n\t    \n    input:\n    set indivID, sampleID, realign_bam, recal_table from runBaseRecalibratorOutput \n\n    output:\n    set indivID, sampleID, file(outfile_bam), file(outfile_bai), file(outfile_bai2) into runPrintReadsOutput_for_DepthOfCoverage, runPrintReadsOutput_for_HC_Metrics, runPrintReadsOutput_for_Multiple_Metrics, runPrintReadsOutput_for_OxoG_Metrics\n    set indivID, sampleID, realign_bam, recal_table into runPrintReadsOutput_for_PostRecal\n            \n    script:\n    outfile_bam = sampleID + \".clean.bam\"\n    outfile_bai = sampleID + \".clean.bai\"\n    outfile_bai2 = sampleID + \".clean.bam.bai\"           \n    \"\"\"\n\tjava -XX:ParallelGCThreads=2 -Xmx25g -Djava.io.tmpdir=tmp/ -jar ${GATK} \\\n\t\t-T PrintReads \\\n\t\t-R ${REF} \\\n\t\t-I ${realign_bam} \\\n\t\t-BQSR ${recal_table} \\\n\t\t-o ${outfile_bam}\n    samtools index ${outfile_bam}\n    \"\"\"\n}"], "list_proc": ["joshua-d-campbell/nf-GATK_Exome_Preprocess/runPrintReads"], "list_wf_names": ["joshua-d-campbell/nf-GATK_Exome_Preprocess"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["joshua-d-campbell"], "nb_wf": 1, "list_wf": ["nf-GATK_Exome_Preprocess"], "list_contrib": ["joshua-d-campbell"], "nb_contrib": 1, "codes": ["\nprocess runMultiQCFastq {\n    tag \"${indivID}\"\n\tpublishDir \"${OUTDIR}/${indivID}/QC/Fastq\"\n\t    \n    input:\n    set indivID, zip_files from FastQCOutput_grouped_by_indiv\n    \n    output:\n    set file(\"multiqc_fastq_file_list.txt\"), file(\"fastq_multiqc*\") into runMultiQCFastqOutput\n    \t\n    script:\n     \n    \"\"\"\n    echo -e \"${zip_files.flatten().join('\\n')}\" > multiqc_fastq_file_list.txt\n    multiqc -n fastq_multiqc --file-list multiqc_fastq_file_list.txt\n    \"\"\"\n}"], "list_proc": ["joshua-d-campbell/nf-GATK_Exome_Preprocess/runMultiQCFastq"], "list_wf_names": ["joshua-d-campbell/nf-GATK_Exome_Preprocess"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["joshua-d-campbell"], "nb_wf": 1, "list_wf": ["nf-GATK_Exome_Preprocess"], "list_contrib": ["joshua-d-campbell"], "nb_contrib": 1, "codes": ["\nprocess runMultiQCLibrary {\n    tag \"${indivID}\"\n\tpublishDir \"${OUTDIR}/${indivID}/QC/Library\"\n\t    \n    input:\n    set indivID, files from runMarkDuplicatesOutput_QC_grouped_by_indiv\n\n    output:\n    set file(\"multiqc_library_file_list.txt\"), file(\"library_multiqc*\") into runMultiQCLibraryOutput\n    \t\n    script:\n    \"\"\"\n    echo -e \"${files.flatten().join('\\n')}\" > multiqc_library_file_list.txt\n    multiqc -n library_multiqc --file-list multiqc_library_file_list.txt\n    \"\"\"\n}"], "list_proc": ["joshua-d-campbell/nf-GATK_Exome_Preprocess/runMultiQCLibrary"], "list_wf_names": ["joshua-d-campbell/nf-GATK_Exome_Preprocess"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["joshua-d-campbell"], "nb_wf": 1, "list_wf": ["nf-GATK_Exome_Preprocess"], "list_contrib": ["joshua-d-campbell"], "nb_contrib": 1, "codes": ["\nprocess runMultiQCSample {\n    tag \"${indivID}\"\n\tpublishDir \"${OUTDIR}/${indivID}/QC/Sample\"\n\t    \n    input:\n\tset indivID, metrics_files from CollectMultipleMetricsOutput_grouped_by_indiv\n    set indivID, hybrid_files from HybridCaptureMetricsOutput_grouped_by_indiv\n    set indivID, oxog_files from runOxoGMetricsOutput_grouped_by_indiv\n        \n    output:\n    set file(\"sample_multiqc*\"), file(\"multiqc_sample_file_list.txt\") into runMultiQCSampleOutput\n    \t\n    script:\n    \"\"\"\n    echo -e \"${metrics_files.flatten().join('\\n')}\" > multiqc_sample_file_list.txt\n    echo -e \"${hybrid_files.flatten().join('\\n')}\" >> multiqc_sample_file_list.txt\n    echo -e \"${oxog_files.flatten().join('\\n')}\" >> multiqc_sample_file_list.txt\n            \n    multiqc -n sample_multiqc --file-list multiqc_sample_file_list.txt\n    \"\"\"\n}"], "list_proc": ["joshua-d-campbell/nf-GATK_Exome_Preprocess/runMultiQCSample"], "list_wf_names": ["joshua-d-campbell/nf-GATK_Exome_Preprocess"]}, {"nb_reuse": 1, "tools": ["SamPler"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess sample_metadata {\n    tag \"$key\"\n    label 'concensus_processing'\n\n    input:\n        tuple val(key), path(metadata),val(n), val(sample_options)\n    output:\n        tuple val(key), path(\"sampled.tsv\")\n\n\"\"\"\n sampler -i $metadata -n $n ${sample_options} > sampled.tsv\n\"\"\"\n\n}"], "list_proc": ["jtmccr1/bft-nf/sample_metadata"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess minimap2{\n    tag \"$key\"\n    label 'concensus_processing'\n    input:\n        tuple val(key), path(fasta),path(reference)\n    output:\n        tuple val(key), path(\"sam.sam\"), path(reference)\n\"\"\"\n minimap2 -t 3 -a -x asm5 $reference $fasta > sam.sam\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/minimap2"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["goalign"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess mask{\n    tag \"$key\"\n    label 'concensus_processing'\n    publishDir \"${params.outDir}/alignments\", overwrite:\"true\", saveAs:{\"${key}.fa\"}\n    \n    input:\n        tuple val(key), path(fasta), val(masked_sites)\n    output:\n        tuple val(key), path(\"masked.fa\")\n\"\"\"\ngoalign mask -t 3 -s $masked_sites -l 1 -i $fasta -o masked.fa\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/mask"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess refine {\n    tag \"$key\"\n    input:\n    tuple val(key),path(tree),path(alignment)\n    output:\n    tuple val(key), path(\"refined.tree\"), path('node.data.json')\n\n\"\"\"\n#make date file\necho -e 'name\\tdate'>input_dates.tsv;\nfertree extract taxa -i $tree | \\\nawk '{n=split(\\$1,a,\"\\\\|\");printf \"%s\\\\t%s\\\\n\",\\$1,a[n]}'>>input_dates.tsv;\n\naugur refine \\\n            --tree $tree \\\n            --metadata input_dates.tsv \\\n            --alignment $alignment \\\n            --output-tree refined.tree \\\n            --output-node-data node.data.json \\\n            --timetree \\\n            --keep-root \\\n            --keep-polytomies \\\n            --clock-rate $params.clock_rate \\\n            --date-inference marginal \\\n            --divergence-unit mutations \\\n            --no-covariance \\\n            --clock-filter-iqd $params.clock_filter\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/refine"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess ancestral {\n    input:\n     path(tree)\n     path(alignment)\n    output:\n    path('nt_muts.json')\n\n\"\"\"\naugur ancestral \\\n            --tree $tree \\\n            --alignment $alignment \\\n            --output-node-data nt_muts.json \\\n            --inference joint \\\n            --infer-ambiguous\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/ancestral"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess translate {\n    input:\n    path(tree)\n    path(nt_muts)\n    output:\n    path('aa_muts.json')\n\n\"\"\"\naugur translate \\\n            --tree $tree \\\n            --ancestral-sequences $nt_muts \\\n            --reference-sequence ${projectDir}/reference/reference_seq.gb \\\n            --output-node-data aa_muts.json\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/translate"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess v2 {\n    publishDir \"${params.outDir}/auspice\" , pattern: \"final.json\", mode:\"move\", saveAs: {\"${key}.json\"}\n\n    input:\n    path(tree)\n    path(nt_muts)\n    path(aa_muts)\n    path(node_data)\n    path(metadata)\n    path(config)\n    output:\n    path('final.json')\n\n\"\"\"\naugur export v2 \\\n            --tree $tree \\\n             --node-data $nt_muts $aa_muts $node_data \\\n             --auspice-config $config \\\n             --metadata $metadata \\\n             --output final.json\t\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/v2"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["BEAST"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess preliminary_beast_process{\n    stageInMode 'copy'\n    tag \"${key}-${seed}\"\n    label 'beast'\n    publishDir \"${params.outDir}/preliminary/${key}\", overwrite:\"true\"\n    input:\n        tuple val(key), path(xml_file), val(seed)\n    output:\n        tuple val(key), path(\"${seed}_${key}.log\"), emit: logs\n        tuple val(key), path(\"${seed}_${key}.trees\"), emit:trees\n        path(\"${seed}_${key}.ops\")\n        path(\"${key}-${seed}.out\")\n        path(\"${seed}_${key}.chkpt\") \n\"\"\"\nbeast   -save_every ${params.save_every} -save_state ${seed}_${key}.chkpt  -prefix ${seed}_ -seed ${seed}  ${xml_file} > ${key}-${seed}.out\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/preliminary_beast_process"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 1, "tools": ["BEAST"], "nb_own": 1, "list_own": ["jtmccr1"], "nb_wf": 1, "list_wf": ["bft-nf"], "list_contrib": ["jtmccr1"], "nb_contrib": 1, "codes": ["\nprocess DTA_beast_process{\n    tag \"${key}-${seed}\"\n    label 'beast'\n    publishDir \"${params.outDir}/DTA/${key}\", overwrite:\"true\"\n    input:\n        tuple val(key), path(xml_file), path(trees),val(seed)\n    output:\n        tuple val(key), path(\"${seed}_${key}.log\"), emit: logs\n        tuple val(key), path(\"${seed}_${key}.trees\"), emit:trees\n        path(\"${seed}_${key}.ops\")\n        path(\"${key}-${seed}.out\")\n                                          \n                                                    \n                                                        \n                                                      \n\n\"\"\"\nbeast   -beagle_scaling always -prefix ${seed}_ -seed ${seed}  ${xml_file} > ${key}-${seed}.out\n\"\"\"\n}"], "list_proc": ["jtmccr1/bft-nf/DTA_beast_process"], "list_wf_names": ["jtmccr1/bft-nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "JSpecies", "MultiQC"], "nb_own": 3, "list_own": ["steepale", "juneb4869", "kkerns85"], "nb_wf": 2, "list_wf": ["midas_nextflow", "new_meripseqpipe", "nf-core-mutenrich"], "list_contrib": ["steepale", "juneb4869", "kkerns85"], "nb_contrib": 3, "codes": ["\nprocess midas_merge_species {\n    container \"quay.io/fhcrc-microbiome/midas:v1.3.2--6\"\n    label \"mem_veryhigh\"\n    publishDir \"${params.output_folder}\"\n\n    input:\n    file species_tar_list from species_ch.toSortedList()\n    file DB from file(params.db_midas)\n\n    output:\n    file \"SPECIES/*\"\n\n\"\"\"\n#!/bin/bash\nset -e\nls -lahtr\n# Keep track of the folders created while unpacking input files\ninput_string=\"\"\necho \"Unpacking all of the input files\"\nfor tarfile in ${species_tar_list}; do\n    echo \"Making sure that \\$tarfile was downloaded correctly\"\n    [[ -s \\$tarfile ]]\n    echo \"Unpacking \\$tarfile\"\n    tar xzvf \\$tarfile\n    # Add this folder to the input string\n    input_string=\"\\$input_string,\\$( echo \\$tarfile | sed 's/.species.tar.gz//' )\"\n    echo \"Updated input string: \\$input_string\"\ndone\n# Remove the leading comma from the input string\ninput_string=\\$( echo \\$input_string | sed 's/^,//' )\necho \"Merging species results\"\nmerge_midas.py \\\n    species \\\n    SPECIES \\\n    -i \\$input_string \\\n    -t list \\\n    -d ${DB} \\\n    --sample_depth ${params.merge_sample_depth}\necho \"Done merging data\"\nls -lahtr SPECIES\necho \"Compressing output files\"\nfind SPECIES -type f | xargs gzip\necho \"Done\"\n\"\"\"\n}", "\nprocess makechromesize {\n    label 'build_index'\n    tag \"gtf2bed12\"\n    publishDir path: { params.saveReference ? \"${params.outdir}/Genome/reference_genome\" : params.outdir },\n                saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n    when:\n    true\n\n    input:\n    file fasta\n    \n    output:\n    file \"chromsizes.file\" into chromsizesfile\n\n    shell:      \n    \"\"\"\n    samtools faidx ${fasta}\n    cut -f1,2 ${fasta}.fai > chromsizes.file\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}"], "list_proc": ["kkerns85/midas_nextflow/midas_merge_species", "juneb4869/new_meripseqpipe/makechromesize"], "list_wf_names": ["juneb4869/new_meripseqpipe", "kkerns85/midas_nextflow"]}, {"nb_reuse": 4, "tools": ["BWA", "SAMtools", "sickle", "Bowtie", "JSpecies"], "nb_own": 4, "list_own": ["lengfei5", "steepale", "juneb4869", "kkerns85"], "nb_wf": 4, "list_wf": ["wgsfastqtobam", "new_meripseqpipe", "midas_nf_tower", "atacseq_nf"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "kkerns85", "ewels", "apeltzer", "ggabernet", "jinmingda", "mashehu", "drpatelh", "steepale", "juneb4869"], "nb_contrib": 12, "codes": ["\nprocess sickle {\n    cache true\n    container \"steepale/sickle:1.0\"\n    publishDir \"${params.workdir}/test\", mode: 'copy'\n    if (params.echo) {\n        echo true\n    }\n\n    input:\n    file(read1_paired) from trim_ch1.flatten().filter( ~/.*R1.*_paired.fastq.gz/ )\n    file(read2_paired) from trim_ch2.flatten().filter( ~/.*R2.*_paired.fastq.gz/ )\n    file(read1_unpaired) from trim_ch3.flatten().filter( ~/.*R1.*_unpaired.fastq.gz/ )\n    file(read2_unpaired) from trim_ch4.flatten().filter( ~/.*R2.*_unpaired.fastq.gz/ )\n\n    output:\n    file \"*_sickle.fastq\" into sickle_out_ch\n\n                                            \n    when:\n    !params.skip_sickle\n\n    script:\n    \"\"\"\n    ### Perform fastqc on all fastq files\n    echo \"read1_paired: ${read1_paired}\"\n    echo \"read2_paired: ${read2_paired}\"\n    echo \"read1_unpaired: ${read1_unpaired}\"\n    echo \"read2_unpaired: ${read2_unpaired}\"\n    # Trim the paired reads\n    sickle pe -f ${read1_paired} \\\n    -r ${read2_paired} \\\n    -t sanger \\\n    -o ${read1_paired.simpleName}_sickle.fastq \\\n    -p ${read2_paired.simpleName}_sickle.fastq \\\n    -s singles_PE_sickle.fastq \\\n    -q 20 -l 50 -g\n    \"\"\"\n}", "\nprocess midas {\n    container \"quay.io/fhcrc-microbiome/midas:latest\"\n    label \"mem_veryhigh\"\n    publishDir \"${params.output_folder}/${specimen}\"\n\n    input:\n    tuple val(specimen), file(\"${specimen}.R*.fastq.gz\") from fastq_ch\n    file DB from file(params.db_midas)\n\n    output:\n    file \"${specimen}.species.tar.gz\" into species_ch\n    file \"${specimen}.genes.tar.gz\" into gene_ch\n    file \"${specimen}.snps.tar.gz\" into snps_ch\n\n\"\"\"\n#!/bin/bash\nset -e\necho \"Running species summary\"\n# If the input is single-end, change the filename to match the pattern used for paired-end\nif [[ ! -s ${specimen}.R2.fastq.gz ]]; then\n    mv ${specimen}.R.fastq.gz ${specimen}.R1.fastq.gz\nfi\n# Run the same command differently, depending on whether the input is single- or paired-end\nif [[ -s ${specimen}.R2.fastq.gz ]]; then\n    # Run the species abundance summary\n    run_midas.py \\\n        species \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -2 ${specimen}.R2.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB}\nelse\n    # Run the species abundance summary\n    run_midas.py \\\n        species \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB}\nfi\n# Run the gene abundance summary\nif [[ -s ${specimen}.R2.fastq.gz ]]; then\n    echo \"Running gene summary\"\n    run_midas.py \\\n        genes \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -2 ${specimen}.R2.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nelse\n    echo \"Running gene summary\"\n    run_midas.py \\\n        genes \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nfi\n# Run the SNP summary\necho \"Running SNP summary\"\nif [[ -s ${specimen}.R2.fastq.gz ]]; then\n    run_midas.py \\\n        snps \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -2 ${specimen}.R2.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nelse\n    run_midas.py \\\n        snps \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nfi\necho \"Gathering output files\"\n# Species-level results\necho \"Tarring up species results\"\ntar cvf ${specimen}.species.tar ${specimen}/species/*\ngzip ${specimen}.species.tar\n# Gene-level results\necho \"Tarring up gene results\"\ntar cvf ${specimen}.genes.tar ${specimen}/genes/*\ngzip ${specimen}.genes.tar\n# SNP-level results\necho \"Tarring up SNP results\"\ntar cvf ${specimen}.snps.tar ${specimen}/snps/*\ngzip ${specimen}.snps.tar\necho \"Done\"\n\"\"\"\n}", " process MakeTophat2Index {\n        label 'build_index'\n        tag \"tophat2_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\": params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n\n        output:\n        file \"Tophat2Index/*\" into tophat2_index\n\n        when:\n        aligner == \"tophat2\"\n\n        script:\n        tophat2_index = \"Tophat2Index/\" + fasta.baseName.toString()\n        \"\"\"\n        mkdir Tophat2Index\n        ln $fasta Tophat2Index\n        bowtie2-build -f $fasta $tophat2_index\n        \"\"\"\n    }", "\nprocess BWA_MEM {\n    tag \"$name\"\n    label 'process_high'\n\n    input:\n    tuple val(name), path(reads) from ch_trimmed_reads\n    path index from ch_bwa_index.collect()\n\n    output:\n    tuple val(name), path('*.bam') into ch_bwa_bam\n\n    script:\n    prefix = \"${name}.Lb\"\n    rg = \"\\'@RG\\\\tID:${name}\\\\tSM:${name.split('_')[0..-2].join('_')}\\\\tPL:ILLUMINA\\\\tLB:${name}\\\\tPU:1\\'\"\n    if (params.seq_center) {\n        rg = \"\\'@RG\\\\tID:${name}\\\\tSM:${name.split('_')[0..-2].join('_')}\\\\tPL:ILLUMINA\\\\tLB:${name}\\\\tPU:1\\\\tCN:${params.seq_center}\\'\"\n    }\n    score = params.bwa_min_score ? \"-T ${params.bwa_min_score}\" : ''\n    \"\"\"\n    bwa mem \\\\\n        -t $task.cpus \\\\\n        -M \\\\\n        -R $rg \\\\\n        $score \\\\\n        ${index}/${bwa_base} \\\\\n        $reads \\\\\n        | samtools view -@ $task.cpus -b -h -F 0x0100 -O BAM -o ${prefix}.bam -\n    \"\"\"\n}"], "list_proc": ["steepale/wgsfastqtobam/sickle", "kkerns85/midas_nf_tower/midas", "juneb4869/new_meripseqpipe/MakeTophat2Index", "lengfei5/atacseq_nf/BWA_MEM"], "list_wf_names": ["juneb4869/new_meripseqpipe", "steepale/wgsfastqtobam", "lengfei5/atacseq_nf", "kkerns85/midas_nf_tower"]}, {"nb_reuse": 4, "tools": ["MarkDuplicates (IP)", "BWA", "SAMtools", "Picard", "SNPs"], "nb_own": 4, "list_own": ["lengfei5", "steepale", "juneb4869", "kkerns85"], "nb_wf": 4, "list_wf": ["wgsfastqtobam", "new_meripseqpipe", "midas_nf_tower", "atacseq_nf"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "kkerns85", "ewels", "apeltzer", "ggabernet", "jinmingda", "mashehu", "drpatelh", "steepale", "juneb4869"], "nb_contrib": 12, "codes": ["\nprocess smr_by_lane {\n    cache true\n    container \"steepale/gatk:3.5\"\n    publishDir \"${params.workdir}/test\", mode: 'copy'\n    if (params.echo) {\n        echo true\n    }\n\n    input:\n    val smr_by_lane_mem from params.smr_by_lane_mem\n    file rg_sam from rg_out_ch\n    set val(sample_id), val(barcode), val(lane), val(suffix), val(sample_label), val(type) from manifest_ch3\n    file genome from ref_smr_by_lane_ch\n    file genome_ann from ref_smr_by_lane_ch2\n\n    output:\n    file \"*.{bam,bai,list,txt}\" into rsmr_by_lane_out_ch\n\n                                            \n    when:\n    !params.skip_smr_by_lane\n\n    script:\n    \"\"\"\n    # Sort the sam file\n    java -Xmx${smr_by_lane_mem} \\\n    -jar /opt/picard/build/libs/picard.jar \\\n    SortSam \\\n    I=${rg_sam} \\\n    O=${rg_sam.baseName}.bam \\\n    SORT_ORDER=coordinate\n\n    echo \"Bam file sorted\"\n\n    # Mark duplicates in the bam file\n\tjava -Xmx${smr_by_lane_mem} \\\n    -jar /opt/picard/build/libs/picard.jar \\\n    MarkDuplicates \\\n    I=${rg_sam.baseName}.bam \\\n    O=${rg_sam.baseName}_marked.bam \\\n    METRICS_FILE=${rg_sam.baseName}_metrics.txt \\\n    MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000\n\n    echo \"Bam file duplicates marked\"\n\n    # Build an index of bam file\n\tjava -Xmx${smr_by_lane_mem} \\\n    -jar /opt/picard/build/libs/picard.jar \\\n    BuildBamIndex \\\n    I=${rg_sam.baseName}_marked.bam \\\n    O=${rg_sam.baseName}_marked.bai\n\n    echo \"Bam indexed\"\n\n    # Create targets for indel realignment\n\tjava -Xmx${smr_by_lane_mem} \\\n    -jar /opt/gatk3.5/GenomeAnalysisTK.jar \\\n    -T RealignerTargetCreator \\\n    -R ${genome} \\\n    -I ${rg_sam.baseName}_marked.bam \\\n    -o ${rg_sam.baseName}_intervals.list\n\n    echo \"Targets created for indel relaignment\"\n\n    # Realign around indels\n\tjava -Xmx${smr_by_lane_mem} \\\n    -jar /opt/gatk3.5/GenomeAnalysisTK.jar \\\n    -T IndelRealigner \\\n    -R ${genome} \\\n    -I ${rg_sam.baseName}_marked.bam \\\n    -targetIntervals ${rg_sam.baseName}_intervals.list \\\n    -o ${rg_sam.baseName}_realigned.bam\n\n    echo \"Bam realigned around indels\"\n\n    \"\"\"\n}", "\nprocess midas_merge_snps {\n    container \"quay.io/fhcrc-microbiome/midas:v1.3.2--6\"\n    label \"mem_veryhigh\"\n    publishDir \"${params.output_folder}\"\n\n    input:\n    file snps_tar_list from snps_ch.toSortedList()\n    file DB from file(params.db_midas)\n\n    output:\n    file \"SNPS/*\"\n\n\"\"\"\n#!/bin/bash\nset -e\nls -lahtr\n# Keep track of the folders created while unpacking input files\ninput_string=\"\"\necho \"Unpacking all of the input files\"\nfor tarfile in ${snps_tar_list}; do\n    echo \"Making sure that \\$tarfile was downloaded correctly\"\n    [[ -s \\$tarfile ]]\n    echo \"Unpacking \\$tarfile\"\n    tar xzvf \\$tarfile\n    # Add this folder to the input string\n    input_string=\"\\$input_string,\\$( echo \\$tarfile | sed 's/.snps.tar.gz//' )\"\n    echo \"Updated input string: \\$input_string\"\ndone\n# Remove the leading comma from the input string\ninput_string=\\$( echo \\$input_string | sed 's/^,//' )\necho \"Merging snps results\"\nmerge_midas.py \\\n    snps \\\n    SNPS \\\n    -i \\$input_string \\\n    -t list \\\n    -d ${DB} \\\n    --sample_depth ${params.merge_sample_depth}\necho \"Done merging data\"\ntouch SNPS/DONE\nls -lahtr SNPS\necho \"Compressing output files\"\nfind SNPS -type f | xargs gzip\necho \"Done\"\n\"\"\"\n}", " process MakeBWAIndex {\n        label 'build_index'\n        tag \"bwa_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta\n\n        output:\n        file \"BWAIndex/*\" into bwa_index\n\n        when:\n        aligner == \"bwa\"\n     \n        script:\n        \"\"\"\n        mkdir BWAIndex\n        cd BWAIndex/\n        bwa index -p ${fasta.baseName} -a bwtsw ../$fasta\n        cd ../\n        \"\"\"\n    }", "\nprocess MERGED_LIB_BAM {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith('.flagstat')) \"samtools_stats/$filename\"\n                      else if (filename.endsWith('.idxstats')) \"samtools_stats/$filename\"\n                      else if (filename.endsWith('.stats')) \"samtools_stats/$filename\"\n                      else if (filename.endsWith('.metrics.txt')) \"picard_metrics/$filename\"\n                      else params.save_align_intermeds ? filename : null\n                }\n\n    input:\n    tuple val(name), path(bams) from ch_sort_bam_merge\n\n    output:\n    tuple val(name), path(\"*${prefix}.sorted.{bam,bam.bai}\") into ch_mlib_bam_filter,\n                                                                  ch_mlib_bam_preseq,\n                                                                  ch_mlib_bam_ataqv\n    path '*.{flagstat,idxstats,stats}' into ch_mlib_bam_stats_mqc\n    path '*.txt' into ch_mlib_bam_metrics_mqc\n\n    script:\n    prefix = \"${name}.mLb.mkD\"\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.toGiga()\n    }\n    if (bam_files.size() > 1) {\n        \"\"\"\n        picard -Xmx${avail_mem}g MergeSamFiles \\\\\n            ${'INPUT='+bam_files.join(' INPUT=')} \\\\\n            OUTPUT=${name}.sorted.bam \\\\\n            SORT_ORDER=coordinate \\\\\n            VALIDATION_STRINGENCY=LENIENT \\\\\n            TMP_DIR=tmp\n        samtools index ${name}.sorted.bam\n\n        picard -Xmx${avail_mem}g MarkDuplicates \\\\\n            INPUT=${name}.sorted.bam \\\\\n            OUTPUT=${prefix}.sorted.bam \\\\\n            ASSUME_SORTED=true \\\\\n            REMOVE_DUPLICATES=false \\\\\n            METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt \\\\\n            VALIDATION_STRINGENCY=LENIENT \\\\\n            TMP_DIR=tmp\n\n        samtools index ${prefix}.sorted.bam\n        samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n        samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n        samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n        \"\"\"\n    } else {\n      \"\"\"\n      picard -Xmx${avail_mem}g MarkDuplicates \\\\\n          INPUT=${bam_files[0]} \\\\\n          OUTPUT=${prefix}.sorted.bam \\\\\n          ASSUME_SORTED=true \\\\\n          REMOVE_DUPLICATES=false \\\\\n          METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt \\\\\n          VALIDATION_STRINGENCY=LENIENT \\\\\n          TMP_DIR=tmp\n\n      samtools index ${prefix}.sorted.bam\n      samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n      samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n      samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n      \"\"\"\n    }\n}"], "list_proc": ["steepale/wgsfastqtobam/smr_by_lane", "kkerns85/midas_nf_tower/midas_merge_snps", "juneb4869/new_meripseqpipe/MakeBWAIndex", "lengfei5/atacseq_nf/MERGED_LIB_BAM"], "list_wf_names": ["juneb4869/new_meripseqpipe", "steepale/wgsfastqtobam", "lengfei5/atacseq_nf", "kkerns85/midas_nf_tower"]}, {"nb_reuse": 4, "tools": ["SAMtools", "FastQC", "MAFFT", "STAR"], "nb_own": 4, "list_own": ["klamkiew", "lengfei5", "steepale", "juneb4869"], "nb_wf": 4, "list_wf": ["atacseq_nf", "new_meripseqpipe", "viralclust", "wgsfastqtobam"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "ggabernet", "ewels", "apeltzer", "jinmingda", "klamkiew", "mashehu", "drpatelh", "steepale", "juneb4869"], "nb_contrib": 12, "codes": [" process MERGED_LIB_BAM_REMOVE_ORPHAN {\n        tag \"$name\"\n        label 'process_medium'\n        publishDir path: \"${params.outdir}/bwa/mergedLibrary\", mode: params.publish_dir_mode,\n            saveAs: { filename ->\n                          if (filename.endsWith('.flagstat')) \"samtools_stats/$filename\"\n                          else if (filename.endsWith('.idxstats')) \"samtools_stats/$filename\"\n                          else if (filename.endsWith('.stats')) \"samtools_stats/$filename\"\n                          else if (filename.endsWith('.sorted.bam')) filename\n                          else if (filename.endsWith('.sorted.bam.bai')) filename\n                          else null\n                    }\n\n        input:\n        tuple val(name), path(bam) from ch_mlib_filter_bam\n\n        output:\n        tuple val(name), path('*.sorted.{bam,bam.bai}') into ch_mlib_rm_orphan_bam_metrics,\n                                                             ch_mlib_rm_orphan_bam_bigwig,\n                                                             ch_mlib_rm_orphan_bam_macs,\n                                                             ch_mlib_rm_orphan_bam_plotfingerprint,\n                                                             ch_mlib_rm_orphan_bam_mrep\n        tuple val(name), path(\"${prefix}.bam\") into ch_mlib_name_bam_mlib_counts,\n                                                    ch_mlib_name_bam_mrep_counts\n        tuple val(name), path('*.flagstat') into ch_mlib_rm_orphan_flagstat_bigwig,\n                                                 ch_mlib_rm_orphan_flagstat_macs,\n                                                 ch_mlib_rm_orphan_flagstat_mqc\n        path '*.{idxstats,stats}' into ch_mlib_rm_orphan_stats_mqc\n\n        script:                                                                     \n        prefix = \"${name}.mLb.clN\"\n        \"\"\"\n        bampe_rm_orphan.py ${bam[0]} ${prefix}.bam --only_fr_pairs\n\n        samtools sort -@ $task.cpus -o ${prefix}.sorted.bam -T $prefix ${prefix}.bam\n        samtools index ${prefix}.sorted.bam\n        samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n        samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n        samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n        \"\"\"\n    }", "\nprocess mafft {\n  label 'mafft'\n  publishDir \"${params.output}/${params.mafft_output}\", mode: 'copy', pattern: '*aln'\n\n  input:\n    tuple val(name), val(path), path(sequences), path(cluster)\n\n  output:\n    tuple val(name), path(\"${sequences.baseName}_mafft.aln\"), emit: mafft_result\n\n  script:\n  \"\"\"\n  mafft --thread ${task.cpus} --reorder ${sequences} > \"${sequences.baseName}_mafft.aln\"\n  \"\"\"\n\n\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", " process MakeStarIndex {\n        label 'build_index'\n        tag \"star_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n        file gtf\n\n        output:\n        file \"StarIndex\" into star_index\n\n        when:\n        aligner == \"star\"\n\n        script:\n        readLength = 50\n        overhang = readLength - 1\n        \"\"\"\n        mkdir StarIndex\n        STAR --runThreadN ${task.cpus} \\\n        --runMode genomeGenerate \\\n        --genomeDir StarIndex \\\n        --genomeFastaFiles $fasta \\\n        --sjdbGTFfile $gtf \\\n        --sjdbOverhang $overhang \\\n\t--limitGenomeGenerateRAM 36000000000\n        \"\"\"\n    }"], "list_proc": ["lengfei5/atacseq_nf/MERGED_LIB_BAM_REMOVE_ORPHAN", "klamkiew/viralclust/mafft", "steepale/wgsfastqtobam/fastqc", "juneb4869/new_meripseqpipe/MakeStarIndex"], "list_wf_names": ["juneb4869/new_meripseqpipe", "klamkiew/viralclust", "steepale/wgsfastqtobam", "lengfei5/atacseq_nf"]}, {"nb_reuse": 3, "tools": ["MMseqs", "fastPHASE", "BCFtools"], "nb_own": 3, "list_own": ["klamkiew", "stevekm", "juneb4869"], "nb_wf": 3, "list_wf": ["MuTect2_target_chunking", "new_meripseqpipe", "viralclust"], "list_contrib": ["klamkiew", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess remove_redundancy {\n  label 'remove'\n  publishDir \"${params.output}/${params.nr_output}\", mode: 'copy', pattern: \"*_nr.fasta\"\n\n\n  input:\n    path(sequences)\n\n  output:\n    path \"${sequences.baseName}_nr.fasta\", emit: nr_result\n\n\n  script:\n  \"\"\"\n    mmseqs easy-linclust --threads ${task.cpus} --min-seq-id 1.0 \"${sequences}\" \"${sequences.baseName}_nr\" tmp\n    mv \"${sequences.baseName}_nr_rep_seq.fasta\" \"${sequences.baseName}_nr.fasta\"\n  \"\"\"\n}", "\nprocess mutect2_noChunk {\n    tag \"${prefix}\"\n    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n\n    input:\n    set val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(targets_bed), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_noChunk_ch\n\n    output:\n    set val(\"${label}\"), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_noChunk\n    file(\"${output_vcf}\")\n    file(\"${multiallelics_stats}\")\n    file(\"${realign_stats}\")\n\n    when: params.disable != \"true\"\n\n    script:\n    label = \"noChunk\"\n    prefix = \"${comparisonID}.${label}\"\n    output_vcf = \"${prefix}.vcf\"\n    output_norm_vcf = \"${prefix}.norm.vcf\"\n    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n    \"\"\"\n    gatk.sh -T MuTect2 \\\n    -dt NONE \\\n    --logging_level WARN \\\n    --standard_min_confidence_threshold_for_calling 30 \\\n    --max_alt_alleles_in_normal_count 10 \\\n    --max_alt_allele_in_normal_fraction 0.05 \\\n    --max_alt_alleles_in_normal_qscore_sum 40 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --cosmic \"${cosmic_ref_vcf}\" \\\n    --intervals \"${targets_bed}\" \\\n    --interval_padding 10 \\\n    --input_file:tumor \"${tumorBam}\" \\\n    --input_file:normal \"${normalBam}\" \\\n    --out \"${output_vcf}\"\n\n    # normalize and split vcf entries\n    cat ${output_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n    \"${output_norm_vcf}\"\n    \"\"\"\n}", "\nprocess Fastp{\n    label 'fastp'\n    tag \"$sample_name\"\n                            \n    publishDir path: { params.skip_fastp ? params.outdir : \"${params.outdir}/QC/fastp\" },\n             saveAs: { params.skip_fastp ? null : it }, mode: 'link'\n        \n    input:\n    set val(sample_id), file(reads), val(reads_single_end), val(gzip), val(input), val(group) from raw_fastq\n\n    output:\n    set val(sample_name), file(\"*_aligners.fastq*\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into fastqc_reads, fastp_reads\n    file \"*\" into fastp_results\n\n    when:\n    aligner != \"none\"\n\n    shell:\n    skip_fastp = params.skip_fastp\n    if ( reads_single_end ){\n        filename = reads.toString() - ~/(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners = sample_name + \"_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then\n            fastp -i ${reads} -o ${add_aligners} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads} ${add_aligners}\n        fi\n        \"\"\"\n    } else {\n        filename = reads[0].toString() - ~/(_R[0-9])?(_[0-9])?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners_1 = sample_name + \"_1_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        add_aligners_2 = sample_name + \"_2_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then  \n            fastp -i ${reads[0]} -o ${add_aligners_1} -I ${reads[1]} -O ${add_aligners_2} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads[0]} ${add_aligners_1}\n            mv ${reads[1]} ${add_aligners_2}\n        fi\n        \"\"\"\n    } \n}"], "list_proc": ["klamkiew/viralclust/remove_redundancy", "stevekm/MuTect2_target_chunking/mutect2_noChunk", "juneb4869/new_meripseqpipe/Fastp"], "list_wf_names": ["stevekm/MuTect2_target_chunking", "klamkiew/viralclust", "juneb4869/new_meripseqpipe"]}, {"nb_reuse": 4, "tools": ["FastQC", "BCFtools", "FeatureCounts"], "nb_own": 4, "list_own": ["kmayerb", "lengfei5", "stevekm", "juneb4869"], "nb_wf": 4, "list_wf": ["MuTect2_target_chunking", "new_meripseqpipe", "knead-docker", "atacseq_nf"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "ggabernet", "ewels", "apeltzer", "jinmingda", "kmayerb", "mashehu", "drpatelh", "stevekm", "juneb4869"], "nb_contrib": 12, "codes": ["\nprocess Fastqc{\n    tag \"$sample_name\"\n    publishDir path: { params.skip_fastqc ? params.outdir : \"${params.outdir}/QC\" },\n             saveAs: { params.skip_fastqc ? null : it }, mode: 'link'\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from fastqc_reads\n\n    output:\n    file \"fastqc/*\" into fastqc_results\n\n    when:\n    aligner != \"none\" && !params.skip_fastqc\n\n    shell:\n    skip_fastqc = params.skip_fastqc\n    if ( reads_single_end){\n        \"\"\"\n        mkdir fastqc\n        fastqc -o fastqc --noextract ${reads}\n        \"\"\"       \n    } else {\n        \"\"\"\n        mkdir fastqc   \n        fastqc -o fastqc --noextract ${reads[0]}\n        fastqc -o fastqc --noextract ${reads[1]}\n        \"\"\"      \n    }\n}", "\nprocess fastqc_on_trimmed_files {\n\n\ttag \"FASTQC ON POST KNEADDATA .fq FILES\"\n\n\tcontainer 'quay.io/kmayerb/mitochondria@sha256:d48892f367b217116874ca18e5f5fa602413d6a6030bccd02228f2a4153a3067'\n\n\tpublishDir params.output_folder\n\n\tinput:\n\tset sample_name, file(fastq1), file(fastq2) from post_knead_channel\n\n\toutput:\n    file(\"outputs2/${fastq1.getBaseName()}_fastqc.{zip,html}\") into postknead_fastqc_R1\n    file(\"outputs2/${fastq2.getBaseName()}_fastqc.{zip,html}\") into postknead_fastqc_R2\n\n\tscript:\n\t\"\"\"\n\tmkdir outputs2\n\tfastqc -t $task.cpus -o outputs2 -f fastq -q ${fastq1}\n\tfastqc -t $task.cpus -o outputs2 -f fastq -q ${fastq2}\n\t\"\"\"\n}", "\nprocess mutect2_nChunk {\n    tag \"${prefix}\"\n    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n    echo true\n\n    input:\n    set val(chunkLabel), val(targetChunkNum), val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(\"targets.bed\"), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_nChunk_ch\n\n    output:\n    set val(chunkLabel), val(targetChunkNum), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_nChunk\n    file(\"${output_vcf}\")\n    file(\"${multiallelics_stats}\")\n    file(\"${realign_stats}\")\n\n    when: params.disable != \"true\"\n\n    script:\n                                           \n    prefix = \"${comparisonID}.${chunkLabel}.${targetChunkNum}\"\n    output_vcf = \"${prefix}.vcf\"\n    output_norm_vcf = \"${prefix}.norm.vcf\"\n    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n    \"\"\"\n    gatk.sh -T MuTect2 \\\n    -dt NONE \\\n    --logging_level WARN \\\n    --standard_min_confidence_threshold_for_calling 30 \\\n    --max_alt_alleles_in_normal_count 10 \\\n    --max_alt_allele_in_normal_fraction 0.05 \\\n    --max_alt_alleles_in_normal_qscore_sum 40 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --cosmic \"${cosmic_ref_vcf}\" \\\n    --intervals \"targets.bed\" \\\n    --interval_padding 10 \\\n    --input_file:tumor \"${tumorBam}\" \\\n    --input_file:normal \"${normalBam}\" \\\n    --out \"${output_vcf}\"\n\n    # normalize and split vcf entries\n    cat ${output_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n    \"${output_norm_vcf}\"\n    \"\"\"\n}", "\nprocess MERGED_LIB_CONSENSUS_COUNTS {\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/macs/${PEAK_TYPE}/consensus\", mode: params.publish_dir_mode\n\n    when:\n    params.macs_gsize && (replicatesExist || multipleGroups) && !params.skip_consensus_peaks\n\n    input:\n    path bams from ch_mlib_name_bam_mlib_counts.collect{ it[1] }\n    path saf from ch_mlib_macs_consensus_saf.collect()\n\n    output:\n    path '*featureCounts.txt' into ch_mlib_macs_consensus_counts\n    path '*featureCounts.txt.summary' into ch_mlib_macs_consensus_counts_mqc\n\n    script:\n    prefix = 'consensus_peaks.mLb.clN'\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    pe_params = params.single_end ? '' : '-p --donotsort'\n    \"\"\"\n    featureCounts \\\\\n        -F SAF \\\\\n        -O \\\\\n        --fracOverlap 0.2 \\\\\n        -T $task.cpus \\\\\n        $pe_params \\\\\n        -a $saf \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bam_files.join(' ')}\n    \"\"\"\n}"], "list_proc": ["juneb4869/new_meripseqpipe/Fastqc", "kmayerb/knead-docker/fastqc_on_trimmed_files", "stevekm/MuTect2_target_chunking/mutect2_nChunk", "lengfei5/atacseq_nf/MERGED_LIB_CONSENSUS_COUNTS"], "list_wf_names": ["juneb4869/new_meripseqpipe", "kmayerb/knead-docker", "lengfei5/atacseq_nf", "stevekm/MuTect2_target_chunking"]}, {"nb_reuse": 3, "tools": ["SAMtools", "HISAT2", "MCRestimate"], "nb_own": 3, "list_own": ["kmayerb", "stevekm", "juneb4869"], "nb_wf": 3, "list_wf": ["nf-corncob", "fastq-bed-subset", "new_meripseqpipe"], "list_contrib": ["kmayerb", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": [" process FilterrRNA {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/rRNA_dup\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from rRNA_reads\n    file index from rRNA_index.collect()\n\n    output:\n    set val(sample_name), file(\"*.fastq.gz\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into tophat2_reads, hisat2_reads, bwa_reads, star_reads\n    file \"*_summary.txt\" into rRNA_log\n\n    when:\n    params.rRNA_fasta && !params.skip_filterrRNA\n\n    script:\n    gzip = true\n    index_base = index[0].toString() - ~/(\\.exon)?(\\.\\d)?(\\.fa)?(\\.gtf)?(\\.ht2)?$/\n    if (reads_single_end) {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-gz ${sample_id}.fastq.gz \\\n            -x $index_base \\\n            -U $reads | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        \"\"\"\n    } else {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-conc-gz ${sample_name}_fastq.gz \\\n            -x $index_base \\\n            -1 ${reads[0]} -2 ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        mv ${sample_name}_fastq.1.gz ${sample_name}_1.fastq.gz\n        mv ${sample_name}_fastq.2.gz ${sample_name}_2.fastq.gz\n        \"\"\"\n    }\n    }", "\nprocess fastq_merge_bam_index {\n                                                                                         \n    input:\n    set val(sampleID), file(fastq_r1: \"*\"), file(fastq_r2: \"*\"), file(bam) from samples_R1_R2\n\n    output:\n    set val(sampleID), file(\"${merged_fastq_R1}\"), file(\"${merged_fastq_R2}\"), file(bam), file(\"${bai}\") into samples_fastq_merged\n\n    script:\n    prefix = \"${sampleID}\"\n    merged_fastq_R1 = \"${prefix}_R1.fastq.gz\"\n    merged_fastq_R2 = \"${prefix}_R2.fastq.gz\"\n    bai = \"${bam}.bai\"\n    \"\"\"\n    cat ${fastq_r1} > \"${merged_fastq_R1}\"\n    cat ${fastq_r2} > \"${merged_fastq_R2}\"\n    samtools index \"${bam}\"\n    \"\"\"\n}", "\nprocess runCorncob {\n    tag \"Perform corncob analysis\"\n    \n    container \"quay.io/fhcrc-microbiome/corncob\"\n    \n    label \"mem_veryhigh\"\n    \n    errorStrategy \"retry\"\n    \n    publishDir params.outdir\n\n    input:\n    set name, file(readcounts_csv_gz), file(metadata_csv), formula from input_channel\n   \n    output:\n    file \"${name}.corncob.results.csv\" into junkbonds\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    # Get the arguments passed in by the user\n\n    library(tidyverse)\n    library(corncob)\n    library(parallel)\n\n    Sys.setenv(\"VROOM_CONNECTION_SIZE\" = 13107200 * ${task.attempt})\n\n    numCores = ${task.cpus}\n\n    ##  READCOUNTS CSV should have columns `specimen` (first col) and `total` (last column).\n    ##  METADATA CSV should have columns `specimen` (which matches up with `specimen` from\n    ##         the recounts file), and additional columns with covariates matching `formula`\n\n    ##  corncob analysis (coefficients and p-values) are written to OUTPUT CSV on completion\n\n    print(\"Reading in ${metadata_csv}\")\n    metadata <- vroom::vroom(\"${metadata_csv}\", delim=\",\")\n\n    print(\"Reading in ${readcounts_csv_gz}\")\n    counts <- vroom::vroom(\"${readcounts_csv_gz}\", delim=\",\")\n\n    if (dim(counts)[2] > 25){\n        counts = counts[,1:25]\n    }\n    if (\"total\" %in% names(counts)){\n        print(\"total not found\")\n        total_counts <- counts[,c(\"specimen\", \"total\")]\n    }else{\n        counts[\"total\"] = apply(counts[,-which(names(counts) %in% c(\"specimen\"))],1,sum)\n        total_counts = counts[,c(\"specimen\", \"total\")]\n    }\n    \n    print(\"Merging total counts with metadata\")\n    total_and_meta <- metadata %>% \n    right_join(total_counts, by = c(\"specimen\" = \"specimen\"))\n    \n    print(head(total_and_meta))\n\n    #### Run the analysis for every individual CAG\n    print(sprintf(\"Starting to process %s columns (CAGs)\", dim(counts)[2]))\n    corn_tib <- do.call(rbind, mclapply(\n        c(2:(dim(counts)[2] - 1)),\n        function(i){\n            try_bbdml <- try(\n                counts[,c(1, i)] %>%\n                rename(W = 2) %>%\n                right_join(\n                    total_and_meta, \n                    by = c(\"specimen\" = \"specimen\")\n                ) %>%\n                corncob::bbdml(\n                    formula = cbind(W, total - W) ~ ${formula},\n                    phi.formula = ~ 1,\n                    data = .\n                )\n            )\n\n        if (class(try_bbdml) == \"bbdml\") {\n            return(\n                summary(\n                    try_bbdml\n                )\\$coef %>%\n                as_tibble %>%\n                mutate(\"parameter\" = summary(try_bbdml)\\$coef %>% row.names) %>%\n                rename(\n                    \"estimate\" = Estimate,\n                    \"std_error\" = `Std. Error`,\n                    \"p_value\" = `Pr(>|t|)`\n                ) %>%\n                select(-`t value`) %>%\n                gather(key = type, ...=estimate:p_value) %>%\n                mutate(\"CAG\" = names(counts)[i])\n            )\n        } else {\n            return(\n                tibble(\n                    \"parameter\" = \"all\",\n                    \"type\" = \"failed\", \n                    \"value\" = NA, \n                    \"CAG\" = names(counts)[i]\n                )\n            )\n        }   \n        },\n        mc.cores = numCores\n    ))\n\n    print(sprintf(\"Writing out %s rows to corncob.results.csv\", nrow(corn_tib)))\n    print(head(corn_tib))\n    write_csv(corn_tib, \"${name}.corncob.results.csv\")\n    \"\"\"\n}"], "list_proc": ["juneb4869/new_meripseqpipe/FilterrRNA", "stevekm/fastq-bed-subset/fastq_merge_bam_index", "kmayerb/nf-corncob/runCorncob"], "list_wf_names": ["juneb4869/new_meripseqpipe", "kmayerb/nf-corncob", "stevekm/fastq-bed-subset"]}, {"nb_reuse": 4, "tools": ["FeatureCounts", "MiXCR", "TopHat", "SyConn"], "nb_own": 4, "list_own": ["kojix2", "lengfei5", "stevekm", "juneb4869"], "nb_wf": 4, "list_wf": ["nextflow-demos", "new_meripseqpipe", "simple_mixcr_nextflow", "atacseq_nf"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "ggabernet", "ewels", "apeltzer", "jinmingda", "kojix2", "pditommaso", "mashehu", "drpatelh", "stevekm", "juneb4869"], "nb_contrib": 13, "codes": ["\nprocess MERGED_REP_CONSENSUS_COUNTS {\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedReplicate/macs/${PEAK_TYPE}/consensus\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_merge_replicates && replicatesExist && params.macs_gsize && multipleGroups && !params.skip_consensus_peaks\n\n    input:\n    path bams from ch_mlib_name_bam_mrep_counts.collect{ it[1] }\n    path saf from ch_mrep_macs_consensus_saf.collect()\n\n    output:\n    path '*featureCounts.txt' into ch_mrep_macs_consensus_counts\n    path '*featureCounts.txt.summary' into ch_mrep_macs_consensus_counts_mqc\n\n    script:\n    prefix = 'consensus_peaks.mRp.clN'\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    pe_params = params.single_end ? '' : '-p --donotsort'\n    \"\"\"\n    featureCounts \\\\\n        -F SAF \\\\\n        -O \\\\\n        --fracOverlap 0.2 \\\\\n        -T $task.cpus \\\\\n        $pe_params \\\\\n        -a $saf \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bam_files.join(' ')}\n    \"\"\"\n}", "\nprocess Tophat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/tophat2\", mode: 'link', overwrite: true\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from tophat2_reads\n    file index from tophat2_index.collect()\n    file gtf\n\n    output:\n    set val(sample_id), file(\"*_tophat2.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into tophat2_bam\n    file \"*_log.txt\" into tophat2_log\n    \n    when:\n    aligner == \"tophat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.rev)?(\\.\\d)?(\\.fa)?(\\.bt2)?$/\n    strand_info = params.stranded == \"no\" ? \"fr-unstranded\" : params.stranded == \"reverse\" ? \"fr-secondstrand\" : \"fr-firststrand\"\n    if (reads_single_end) {\n        \"\"\"\n        tophat  -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                $reads > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        tophat -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                ${reads[0]} ${reads[1]} > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    }\n}", "\nprocess concat_dbs {\n    publishDir \"${params.output_dir}\", overwrite: true\n\n    input:\n    file(all_dbs: \"db?\") from sample_dbs.collect()\n\n    output:\n    file(\"${output_sqlite}\") into plots_db\n\n    script:\n    output_sqlite = \"plots.sqlite\"\n    \"\"\"\n    python - ${all_dbs} <<E0F\nimport sys\nimport sqlite3\n\ndbs = sys.argv[1:]\n\n# setup new output db\noutput_db = \"${output_sqlite}\"\nconn = sqlite3.connect(output_db)\nconn.execute(\"CREATE TABLE TBL1(sampleID text, x text, y text, a text, baseplot blob, ggplot blob)\")\n\n# add each input db to the output db\nfor db in dbs:\n    conn.execute('''ATTACH '{0}' as dba'''.format(db))\n    conn.execute(\"BEGIN\")\n    for row in conn.execute('''SELECT * FROM dba.sqlite_master WHERE type=\"table\"'''):\n        combine_sql = \"INSERT INTO \"+ row[1] + \" SELECT * FROM dba.\" + row[1]\n        conn.execute(combine_sql)\n    conn.commit()\n    conn.execute(\"detach database dba\")\nE0F\n    \"\"\"\n}", "\nprocess MiXCR {\n  publishDir \"$params.outdir/$id\", mode: 'copy', overwrite: true\n  input:\n    tuple val(id), path(f1), path(f2)\n  output:\n    path 'analysis*'\n  script:\n  \"\"\"\n  mixcr -Xmx1G analyze shotgun \\\n        --receptor-type tcr \\\n        --species hs \\\n        --starting-material rna \\\n        --only-productive \\\n        --report analysis.report \\\n        $f1 $f2 analysis.$id\n  \"\"\"\n}"], "list_proc": ["lengfei5/atacseq_nf/MERGED_REP_CONSENSUS_COUNTS", "juneb4869/new_meripseqpipe/Tophat2Align", "stevekm/nextflow-demos/concat_dbs", "kojix2/simple_mixcr_nextflow/MiXCR"], "list_wf_names": ["juneb4869/new_meripseqpipe", "kojix2/simple_mixcr_nextflow", "lengfei5/atacseq_nf", "stevekm/nextflow-demos"]}, {"nb_reuse": 3, "tools": ["SAMtools", "RAxML-NG", "BWA"], "nb_own": 3, "list_own": ["ksumngs", "stevekm", "juneb4869"], "nb_wf": 3, "list_wf": ["nf-modules", "nextflow-pipeline-demo", "new_meripseqpipe"], "list_contrib": ["MillironX", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess bwa_mem {\n                                    \n    tag { \"${sample_ID}\" }\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'bwa/0.7.17'\n\n    input:\n    set val(sample_ID), file(fastq_R1_trim), file(fastq_R2_trim), file(ref_fa_bwa_dir) from samples_fastq_trimmed.combine(ref_fa_bwa_dir)\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.sam\") into samples_bwa_sam\n\n    script:\n    \"\"\"\n    bwa mem -M -v 1 -t \\${NSLOTS:-1} -R '@RG\\\\tID:${sample_ID}\\\\tSM:${sample_ID}\\\\tLB:${sample_ID}\\\\tPL:ILLUMINA' \"${ref_fa_bwa_dir}/genome.fa\" \"${fastq_R1_trim}\" \"${fastq_R2_trim}\" -o \"${sample_ID}.sam\"\n    \"\"\"\n}", "process RAXMLNG_SUPPORT {\n    tag \"$tree\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path tree\n    path bootstraps\n\n    output:\n    path \"*.raxml.support\", emit: support\n    path \"versions.yml\"   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        --support \\\\\n        --threads auto{${task.cpus}} \\\\\n        --tree ${tree} \\\\\n        --bs-trees ${bootstraps} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess BWAAlign{\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/bwa\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from bwa_reads\n    file index from bwa_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_bwa.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into bwa_bam\n    file \"*\" into bwa_result\n\n    when:\n    aligner == \"bwa\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.pac)?(\\.bwt)?(\\.ann)?(\\.amb)?(\\.sa)?(\\.fa)?$/\n    if (reads_single_end) {\n        \"\"\"\n        bwa aln -t ${task.cpus} \\\n                -f ${reads.baseName}.sai \\\n                $index_base \\\n                $reads\n        bwa samse \\\n                $index_base \\\n                ${reads.baseName}.sai \\\n                $reads  | \\\n            samtools view -@ ${task.cpus} -h -bS - > ${sample_name}_bwa.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        bwa aln -t ${task.cpus} \\\n                -f ${reads[0].baseName}.sai \\\n                $index_base \\\n                ${reads[0]}\n        bwa aln -t ${task.cpus} \\\n                -f ${reads[1].baseName}.sai \\\n                $index_base \\\n                ${reads[1]}\n        bwa sampe \\\n                $index_base \\\n                ${reads[0].baseName}.sai ${reads[1].baseName}.sai \\\n                ${reads[0]} ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -h -bS - > ${sample_name}_bwa.bam\n        \"\"\"\n    }\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/bwa_mem", "ksumngs/nf-modules/RAXMLNG_SUPPORT", "juneb4869/new_meripseqpipe/BWAAlign"], "list_wf_names": ["juneb4869/new_meripseqpipe", "stevekm/nextflow-pipeline-demo", "ksumngs/nf-modules"]}, {"nb_reuse": 4, "tools": ["SAMtools", "kraken2", "BUStools", "STAR"], "nb_own": 4, "list_own": ["lengfei5", "ksumngs", "stevekm", "juneb4869"], "nb_wf": 4, "list_wf": ["nf-modules", "nextflow-pipeline-demo", "nf_visium_kallisto", "new_meripseqpipe"], "list_contrib": ["MillironX", "lengfei5", "stevekm", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess bam_ra_rc_gatk {\n                                               \n                                                                                                                                            \n                                                                                                                                             \n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam_dd_ra_rc_gatk\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n\n    input:\n    set val(sample_ID), file(sample_bam), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(gatk_1000G_phase1_indels_vcf), file(mills_and_1000G_gold_standard_indels_vcf), file(dbsnp_ref_vcf) from samples_dd_bam_ref_gatk\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.ra.rc.bam\"), file(\"${sample_ID}.dd.ra.rc.bam.bai\") into samples_dd_ra_rc_bam, samples_dd_ra_rc_bam2, samples_dd_ra_rc_bam3\n    file \"${sample_ID}.intervals\"\n    file \"${sample_ID}.table1.txt\"\n    file \"${sample_ID}.table2.txt\"\n    file \"${sample_ID}.csv\"\n    file \"${sample_ID}.pdf\"\n\n    script:\n    \"\"\"\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T RealignerTargetCreator \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    -nt \\${NSLOTS:-1} \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.intervals\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T IndelRealigner \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --maxReadsForRealignment 50000 \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -targetIntervals \"${sample_ID}.intervals\" \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.dd.ra.bam\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.table1.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --out \"${sample_ID}.table2.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T AnalyzeCovariates \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -before \"${sample_ID}.table1.txt\" \\\n    -after \"${sample_ID}.table2.txt\" \\\n    -csv \"${sample_ID}.csv\" \\\n    -plots \"${sample_ID}.pdf\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T PrintReads \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.dd.ra.rc.bam\"\n\n    samtools index \"${sample_ID}.dd.ra.rc.bam\"\n    \"\"\"\n}", "\nprocess KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_high_memory'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*classified*\")  , emit: classified\n    tuple val(meta), path(\"*unclassified*\"), emit: unclassified\n    tuple val(meta), path(\"*.kraken.gz\")   , emit: kraken\n    tuple val(meta), path(\"*.kreport\")     , emit: kreport\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def pairedFlag = meta.single_end ? '' : '--paired'\n    def classifiedFlag = meta.single_end ? \"${prefix}_classified.fastq\" : \"${prefix}_classified#.fastq\"\n    def unclassifiedFlag = meta.single_end ? \"${prefix}_unclassified.fastq\" : \"${prefix}_unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n            --db ${db} \\\\\n            --threads ${task.cpus} \\\\\n            --classified-out ${classifiedFlag} \\\\\n            --unclassified-out ${unclassifiedFlag} \\\\\n            --report ${prefix}.kreport \\\\\n            ${pairedFlag} \\\\\n            ${args} \\\\\n            ${reads} \\\\\n        | gzip \\\\\n        > ${prefix}.kraken.gz\n\n    pigz -p${task.cpus} *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess StarAlign {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/star\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from star_reads\n    file star_index from star_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_star.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into star_bam\n    file \"*.final.out\" into star_log\n\n    when:\n    aligner == \"star\"\n\n    script:\n    gzip_cmd = gzip ? \"--readFilesCommand zcat\" : \"\"\n    if (reads_single_end) {\n        \"\"\"\n        STAR --runThreadN ${task.cpus} $gzip_cmd \\\n            --twopassMode Basic \\\n            --genomeDir $star_index \\\n            --readFilesIn $reads  \\\n            --outSAMtype BAM Unsorted \\\n            --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 \\\n            --outFilterIntronMotifs RemoveNoncanonical \\\n            --outFilterMultimapNmax 20 \\\n            --alignIntronMin 20 \\\n            --alignIntronMax 100000 \\\n            --alignMatesGapMax 1000000 \\\n            --outFileNamePrefix ${sample_name}  > ${sample_name}_log.txt\n        mv ${sample_name}Aligned.out.bam ${sample_name}_star.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        STAR --runThreadN ${task.cpus} $gzip_cmd \\\n            --twopassMode Basic \\\n            --genomeDir $star_index \\\n            --readFilesIn ${reads[0]} ${reads[1]}  \\\n            --outSAMtype BAM Unsorted \\\n            --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 \\\n            --outFilterIntronMotifs RemoveNoncanonical \\\n            --outFilterMultimapNmax 20 \\\n            --alignIntronMin 20 \\\n            --alignIntronMax 1000000 \\\n            --alignMatesGapMax 1000000 \\\n            --outFileNamePrefix ${sample_name} > ${sample_name}_log.txt\n        mv ${sample_name}Aligned.out.bam ${sample_name}_star.bam\n        \"\"\"\n    }\n}", "\nprocess corrsort {\n\n                                            \n    storeDir params.outdir\n\n    input:\n    file outbus from kallisto_bus_to_sort\n    file white from bc_wl_kal.collect()\n\n    output:\n    file \"${outbus}/output.cor.sort.bus\"\n    file outbus into kal_sort_to_count\n    file outbus into kal_sort_to_umi\n\n    when: params.protocol!='plate'\n\n    script:\n    \"\"\"\n    ml load bustools/0.40.0-foss-2018b\n    bustools correct -w $white -o ${outbus}/output.cor.bus ${outbus}/output.bus\n    bustools sort -o ${outbus}/output.cor.sort.bus -t 8 ${outbus}/output.cor.bus\n\n    \"\"\"\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/bam_ra_rc_gatk", "ksumngs/nf-modules/KRAKEN2", "juneb4869/new_meripseqpipe/StarAlign", "lengfei5/nf_visium_kallisto/corrsort"], "list_wf_names": ["juneb4869/new_meripseqpipe", "stevekm/nextflow-pipeline-demo", "ksumngs/nf-modules", "lengfei5/nf_visium_kallisto"]}, {"nb_reuse": 3, "tools": ["SAMtools", "BCFtools", "RAxML-NG"], "nb_own": 3, "list_own": ["ksumngs", "stevekm", "juneb4869"], "nb_wf": 3, "list_wf": ["nf-modules", "nextflow-pipeline-demo", "new_meripseqpipe"], "list_contrib": ["MillironX", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": ["process RAXMLNG_SEARCH {\n    tag \"$msa\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path msa\n\n    output:\n    path \"*.raxml.bestTree\", emit: best_tree\n    path \"versions.yml\"    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n\n    \"\"\"\n    raxml-ng \\\\\n        --threads auto{${task.cpus}} \\\\\n        --workers auto \\\\\n        --msa ${msa} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess lofreq {\n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/vcf_lofreq\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n    input:\n    set val(sample_ID), file(sample_bam), file(sample_bai), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(dbsnp_ref_vcf) from samples_dd_ra_rc_bam_ref_dbsnp\n\n    output:\n    file(\"${sample_ID}.vcf\")\n    file(\"${sample_ID}.norm.vcf\")\n    file(\"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\") into lofreq_annotations\n    file(\"${sample_ID}.eval.grp\")\n    val(sample_ID) into sample_lofreq_done\n\n    script:\n    \"\"\"\n    \"${params.lofreq_bin}\" call-parallel \\\n    --call-indels \\\n    --pp-threads \\${NSLOTS:-1} \\\n    --ref \"${ref_fasta}\" \\\n    --bed \"${targets_bed_file}\" \\\n    --out \"${sample_ID}.vcf\" \\\n    \"${sample_bam}\"\n\n    bgzip -c \"${sample_ID}.vcf\" > \"${sample_ID}.vcf.bgz\"\n\n    bcftools index \"${sample_ID}.vcf.bgz\"\n\n    bcftools norm \\\n    --multiallelics \\\n    -both \\\n    --output-type v \\\n    \"${sample_ID}.vcf.bgz\" | \\\n    bcftools norm \\\n    --fasta-ref \"${ref_fasta}\" \\\n    --output-type v - | \\\n    bcftools view \\\n    --exclude 'DP<5' \\\n    --output-type v >  \"${sample_ID}.norm.vcf\"\n\n    # annotate the vcf\n    annotate_vcf.sh \"${sample_ID}.norm.vcf\" \"${sample_ID}.norm\"\n\n    # add a column with the sample ID\n    paste_col.py -i \"${sample_ID}.norm.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\" -o \"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\" --header \"Sample\" -v \"${sample_ID}\" -d \"\\t\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T VariantEval \\\n    -R \"${ref_fasta}\" \\\n    -o \"${sample_ID}.eval.grp\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --eval \"${sample_ID}.norm.vcf\"\n\n    \"\"\"\n}", "\nprocess SortRename {\n    label 'sort'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/samtoolsSort/\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_id), file(bam_file), val(reads_single_end), val(gzip), val(input), val(group) from merge_bam_file\n\n    output:\n    set val(group), val(sample_id), file(\"*.bam\"), file(\"*.bai\") into sorted_bam\n    file \"*.{bam,bai}\" into rseqc_bam, bedgraph_bam, feacount_bam, cuffbam, peakquan_bam, diffpeak_bam, sng_bam\n    file \"*.bam\" into bam_results\n\n    script:\n    sample_name = sample_id + (input ? \".input_\" : \".ip_\") + group\n    output = sample_name + \".bam\"\n    mapq_cutoff = (params.mapq_cutoff).toInteger() \n    if (!params.skip_sort){\n        \"\"\"\n        if [ \"$mapq_cutoff\" -gt \"0\" ]; then\n            samtools view -hbq $mapq_cutoff $bam_file | samtools sort -@ ${task.cpus} -O BAM -o $output -\n        else\n            samtools sort -@ ${task.cpus} -O BAM -o $output $bam_file\n        fi\n        samtools index -@ ${task.cpus} $output\n        \"\"\"\n    } else {\n        \"\"\"\n        if [ \"$mapq_cutoff\" -gt \"0\" ]; then\n            samtools view -hbq $mapq_cutoff $bam_file > $output\n        else\n            mv $bam_file $output\n        fi\n        samtools index -@ ${task.cpus} $output\n        \"\"\"\n    }\n}"], "list_proc": ["ksumngs/nf-modules/RAXMLNG_SEARCH", "stevekm/nextflow-pipeline-demo/lofreq", "juneb4869/new_meripseqpipe/SortRename"], "list_wf_names": ["juneb4869/new_meripseqpipe", "stevekm/nextflow-pipeline-demo", "ksumngs/nf-modules"]}, {"nb_reuse": 4, "tools": ["SAMtools", "kraken2", "MultiQC", "Cutadapt"], "nb_own": 4, "list_own": ["lengfei5", "ksumngs", "stevekm", "juneb4869"], "nb_wf": 4, "list_wf": ["v-met", "nextflow-pipeline-demo", "smallRNA_nf", "new_meripseqpipe"], "list_contrib": ["MillironX", "lengfei5", "stevekm", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_high_memory'\n\n    conda (params.enable_conda ? 'bioconda::kraken2=2.1.2 conda-forge::pigz=2.6' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:87fc08d11968d081f3e8a37131c1f1f6715b6542-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*classified*\")  , emit: classified\n    tuple val(meta), path(\"*unclassified*\"), emit: unclassified\n    tuple val(meta), path(\"*.kraken.gz\")   , emit: kraken\n    tuple val(meta), path(\"*.kreport\")     , emit: kreport\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def pairedFlag = meta.single_end ? '' : '--paired'\n    def classifiedFlag = meta.single_end ? \"${prefix}_classified.fastq\" : \"${prefix}_classified#.fastq\"\n    def unclassifiedFlag = meta.single_end ? \"${prefix}_unclassified.fastq\" : \"${prefix}_unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n            --db ${db} \\\\\n            --threads ${task.cpus} \\\\\n            --classified-out ${classifiedFlag} \\\\\n            --unclassified-out ${unclassifiedFlag} \\\\\n            --report ${prefix}.kreport \\\\\n            ${pairedFlag} \\\\\n            ${args} \\\\\n            ${reads} \\\\\n        | gzip \\\\\n        > ${prefix}.kraken.gz\n\n    pigz -p${task.cpus} *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess cutadapt {\n    tag \"Channel: ${name}\"\n\n    publishDir \"${params.outdir}/cutadapt\", mode: 'copy', pattern: '*.err'\n\n    input:\n        set val(name), file(bam) from read_files\n\n    output:\n        set name, file(\"cutadapt.fastq\") into fastq_cutadapt\n\t      set name, file(\"cutadapt.${name}.err\") into stat_cutadapt\n\t      set name, file(\"cntTotal.txt\") into cnt_total\n\n    script:\n    \"\"\"\n        PYTHON_EGG_CACHE=`pwd` #cutadapt wants to write into home FIXME\n        export PYTHON_EGG_CACHE\n\n        samtools view -c ${bam} > cntTotal.txt\n        bamToFastq -i ${bam} -fq /dev/stdout |\\\n            cutadapt -e ${params.adapterER} -a ${params.adapter} -f fastq --overlap 5 --discard-untrimmed -o cutadapt.fastq - > cutadapt.${name}.err\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.output_dir}\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    executor \"local\"\n    module 'python/2.7.3'\n\n    input:\n    val(comparisonID) from mutect2_sampleIDs.mix(sample_gatk_hc_done)\n                                            .mix(sample_lofreq_done)\n                                            .collect()                                            \n    file(output_dir) from Channel.fromPath(\"${params.output_dir}\")\n\n    output:\n    file \"multiqc_report.html\" into email_files\n    file \"multiqc_data\"\n\n    script:\n    \"\"\"\n    export PS=\\${PS:-''} # needed for virtualenv bug\n    export PS1=\\${PS1:-''}\n    unset PYTHONPATH\n    source activate\n    multiqc \"${output_dir}\"\n    \"\"\"\n}", "\nprocess multiqc{\n    publishDir \"${params.outdir}/Report/QCReadsReport\" , mode: 'link', overwrite: true\n    \n    when:\n    !params.skip_qc && !params.skip_multiqc\n\n    input:\n    file arranged_qc from arranged_qc.collect()\n\n    output:\n    file \"multiqc*\" into multiqc_results\n\n    script:\n    \"\"\"\n    multiqc -n multiqc_report_$aligner .\n    \"\"\"\n}"], "list_proc": ["ksumngs/v-met/KRAKEN2", "lengfei5/smallRNA_nf/cutadapt", "stevekm/nextflow-pipeline-demo/multiqc", "juneb4869/new_meripseqpipe/multiqc"], "list_wf_names": ["juneb4869/new_meripseqpipe", "stevekm/nextflow-pipeline-demo", "lengfei5/smallRNA_nf", "ksumngs/v-met"]}, {"nb_reuse": 3, "tools": ["IMPACT_S", "Gene", "Distanced", "Flagser", "G-BLASTN", "fastafrombed", "GATK"], "nb_own": 3, "list_own": ["ksumngs", "stevekm", "juneb4869"], "nb_wf": 3, "list_wf": ["vep-annotation-nf", "new_meripseqpipe", "v-met"], "list_contrib": ["MillironX", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess MotifSearching {\n    label 'onecore_peak'\n    tag \"${bed_file.baseName}\"\n    publishDir \"${params.outdir}/m6AAnalysis/motif\", mode: 'link', overwrite: true\n    \n    input:\n    file bed_file from motif_collection\n    file chromsizesfile from chromsizesfile.collect()\n    file bed12 from bed12file.collect()\n    file fasta\n    file gtf\n\n    output:\n    file \"*_{dreme,homer}\" into motif_results, motif_results_for_report\n\n    when:\n    !params.skip_motif\n\n    script:\n    motif_file_dir = baseDir + \"/bin\"\n    bed_prefix = bed_file.baseName\n    length = params.motiflength\n    println LikeletUtils.print_purple(\"Motif analysis is going on by Homer\")\n    \"\"\"\n    # cp ${motif_file_dir}/m6A_motif.meme ./\n    sort -k5,5 -g ${bed_file} | awk 'FNR <= 2000{ print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$3}' > ${bed_prefix}.location\n    intersectBed -wo -a ${bed_prefix}.location -b $gtf | awk -v OFS=\"\\\\t\" '{print \\$1,\\$2,\\$3,\"*\",\"*\",\\$10}' | sort -k1,2 | uniq > ${bed_prefix}_bestpeaks.bed\n    fastaFromBed -name+ -split -s -fi $fasta -bed ${bed_prefix}_bestpeaks.bed > ${bed_prefix}_bestpeaks.fa\n    # ame -oc ${bed_prefix}_ame ${bed_prefix}_bestpeaks.fa m6A_motif.meme\n    shuffleBed -incl ${bed12} -seed 12345 -noOverlapping -i ${bed_prefix}_bestpeaks.bed -g ${chromsizesfile} > ${bed_prefix}_random_peak.bed\n    fastaFromBed -name+ -split -s -fi $fasta -bed ${bed_prefix}_random_peak.bed > ${bed_prefix}_random_peak.fa\n    findMotifs.pl ${bed_prefix}_bestpeaks.fa fasta ${bed_prefix}_homer -fasta ${bed_prefix}_random_peak.fa -p ${task.cpus} \\\n        -len $length -S 10 -rna -dumpFasta > ${bed_prefix}_homer_run.log 2>&1\n    \"\"\"\n}", "\nprocess vcf_to_tsv {\n    tag \"${sampleID}\"\n    publishDir \"${params.outputDir}/VEP/vcf_tsv\", mode: 'copy'\n\n    input:\n    set val(sampleID), file(vcf), file(refFasta), file(refFai), file(refDict) from vcf_annotated.combine(ref_fa2)\n        .combine(ref_fai2)\n        .combine(ref_dict2)\n\n    output:\n    set val(sampleID), file(\"${output_file}\"), file(\"csq_key.txt\") into tsv_annotations\n\n    script:\n    prefix = \"${sampleID}\"\n    output_file = \"${prefix}.vep.tsv\"\n    \"\"\"\n    gatk VariantsToTable \\\n    -R \"${refFasta}\" \\\n    --variant \"${vcf}\" \\\n    -F CHROM \\\n    -F POS \\\n    -F ID \\\n    -F REF \\\n    -F ALT \\\n    -F QUAL \\\n    -F FILTER \\\n    -F CSQ \\\n    --output \"${output_file}\"\n\n    grep '##INFO=<ID=CSQ' \"${vcf}\" | sed -e 's|\\\\(^.*Format: \\\\)\\\\(.*\\\\)\">\\$|\\\\2|g' | tr '|' '\\\\n' > csq_key.txt\n\n    ##INFO=<ID=CSQ,Number=.,Type=String,Description=\"Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID\">\n    \"\"\"\n}", "process BLAST_BLASTN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::blast=2.12.0' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/blast:2.12.0--pl5262h3289130_0' :\n        'quay.io/biocontainers/blast:2.12.0--pl5262h3289130_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path  db\n\n    output:\n    tuple val(meta), path('*.blastn.txt'), emit: txt\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    DB=`find -L ./ -name \"*.ndb\" | sed 's/.ndb//'`\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db \\$DB \\\\\n        -query $fasta \\\\\n        $args \\\\\n        -out ${prefix}.blastn.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        blast: \\$(blastn -version 2>&1 | sed 's/^.*blastn: //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["juneb4869/new_meripseqpipe/MotifSearching", "stevekm/vep-annotation-nf/vcf_to_tsv", "ksumngs/v-met/BLAST_BLASTN"], "list_wf_names": ["juneb4869/new_meripseqpipe", "stevekm/vep-annotation-nf", "ksumngs/v-met"]}, {"nb_reuse": 4, "tools": ["SAMtools", "FastQC", "VGE"], "nb_own": 4, "list_own": ["kviljoen", "subwaystation", "lengfei5", "juneb4869"], "nb_wf": 4, "list_wf": ["16S_quick_QC", "new_meripseqpipe", "pangenome", "smallRNA_nf"], "list_contrib": ["nf-core-bot", "subwaystation", "lengfei5", "Zethson", "kviljoen", "AndreaGuarracino", "heuermh", "juneb4869"], "nb_contrib": 8, "codes": ["\nprocess alignStat {\n\n  tag \"Channel: ${name}\"\n\n  input:\n  set name, file(bam) from bam_tailor2\n\n  output:\n  set name, file(\"tailorStat.txt\") into tailorStat\n\n  script:\n  \"\"\"\n  samtools sort -@ ${task.cpus} -n -m ${params.memPerCPUSort} -l 0 ${bam} | samtools view | cut -f 1  | uniq | wc -l > tailorStat.txt\n  \"\"\"\n}", "\nprocess CreateIGVjs {\n    publishDir \"${params.outdir}/Report\" , mode: 'link', overwrite: true,\n        saveAs: {filename ->\n                 if (filename.indexOf(\".html\") > 0)  \"Igv_js/$filename\"\n                 else if (filename.indexOf(\".pdf\") > 0)  \"Igv_js/$filename\"\n                 else \"Igv_js/$filename\"\n        }        \n    input:\n    file m6APipe_result from m6APipe_result\n    file fasta \n    file gtf\n    file formatted_designfile from formatted_designfile.collect()\n    file group_bed from group_merged_bed.collect()\n    file all_bed from all_merged_bed.collect()\n    file bedgraph from bedgraph_for_igv.collect()\n    \n    output:\n    file \"*\" into igv_js\n\n    script:    \n    igv_fasta = fasta.baseName.toString() + \".igv.fa\"\n    igv_gtf = gtf.baseName.toString() + \".igv.gtf\"\n    merged_allpeaks_igvfile = all_bed.baseName.toString() + \".igv.bed\"\n    \"\"\"\n    ls -l $fasta | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_fasta'\"}' | bash\n    ls -l $gtf | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_gtf'\"}' | bash\n    ls -l $m6APipe_result | awk '{print \"ln -s \"\\$11\" initial.m6APipe\"}' | bash\n    ls -l $group_bed $all_bed | awk '{sub(\".bed\\$\",\".igv.bed\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    ls -l $bedgraph | awk '{sub(\".bedgraph\\$\",\".igv.bedgraph\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    samtools faidx $igv_fasta\n    bash $baseDir/bin/create_IGV_js.sh $igv_fasta $igv_gtf $merged_allpeaks_igvfile $formatted_designfile\n    \"\"\"\n}", "\nprocess vg_deconstruct {\n  publishDir \"${params.outdir}/vg_deconstruct\", mode: \"${params.publish_dir_mode}\"\n\n  input:\n  path(graph)\n\n  output:\n  path(\"${graph}.*.vcf\")\n\n  \"\"\"\n  for s in \\$(echo \"${params.vcf_spec}\" | tr ',' ' ');\n  do\n    ref=\\$(echo \"\\$s\" | cut -f 1 -d:)\n    delim=\\$(echo \"\\$s\" | cut -f 2 -d:)\n    vcf=\"${graph}\".\\$(echo \\$ref | tr '/|' '_').vcf\n    vg deconstruct -P \\$ref -H \\$delim -e -a -t \"${task.cpus}\" \"${graph}\" > \\$vcf\n  done\n  \"\"\"\n}", "\nprocess runFastQC{\n    tag { \"${params.projectName}.rFQC.${sample}\" }\n    publishDir \"${out_dir}/${sample}\", mode: 'copy', overwrite: false\n\n    input:\n        set sample, file(in_fastq) from read_pair_p1\n\n    output:\n        file(\"${sample}_fastqc/*.zip\") into fastqc_files\n\n    \"\"\"\n    mkdir ${sample}_fastqc\n    fastqc --outdir ${sample}_fastqc \\\n    ${in_fastq.get(0)} \\\n    ${in_fastq.get(1)}\n    \"\"\"\n}"], "list_proc": ["lengfei5/smallRNA_nf/alignStat", "juneb4869/new_meripseqpipe/CreateIGVjs", "subwaystation/pangenome/vg_deconstruct", "kviljoen/16S_quick_QC/runFastQC"], "list_wf_names": ["juneb4869/new_meripseqpipe", "subwaystation/pangenome", "lengfei5/smallRNA_nf", "kviljoen/16S_quick_QC"]}, {"nb_reuse": 2, "tools": ["FastQC", "MultiQC"], "nb_own": 2, "list_own": ["rbpisupati", "junyu-boston"], "nb_wf": 2, "list_wf": ["nf-haplocaller", "dicerna-rnaseq"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "skrakau", "rbpisupati", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "mashehu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 50, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    label 'env_qual_small'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.9--pyh9f0ad1d_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.9--pyh9f0ad1d_0\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc/*')\n    path ('trimgalore/fastqc/*')\n    path ('trimgalore/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n    \n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}"], "list_proc": ["rbpisupati/nf-haplocaller/fastqc", "junyu-boston/dicerna-rnaseq/MULTIQC"], "list_wf_names": ["rbpisupati/nf-haplocaller", "junyu-boston/dicerna-rnaseq"]}, {"nb_reuse": 3, "tools": ["seqcluster", "HISAT2", "BWA", "SAMtools", "GATK"], "nb_own": 4, "list_own": ["rbpisupati", "lengfei5", "vincenthhu", "junyu-boston"], "nb_wf": 3, "list_wf": ["atacseq_nf", "nf-core-westest", "dicerna-rnaseq", "nf-repeatexplorer"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "jinmingda", "d4straub", "kviljoen", "na399", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "lengfei5", "Galithil", "pcantalupo", "olgabot", "sven1103", "skrakau", "drejom", "rbpisupati", "sofiahaglund", "drewjbeh", "ewels", "pditommaso", "FriederikeHanssen", "mashehu", "vincenthhu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 54, "codes": [" process BWA_INDEX {\n        tag \"$fasta\"\n        label 'process_high'\n        publishDir path: { params.save_reference ? \"${params.outdir}/genome\" : params.outdir },\n            saveAs: { params.save_reference ? it : null }, mode: params.publish_dir_mode\n\n        input:\n        path fasta from ch_fasta\n\n        output:\n        path 'BWAIndex' into ch_bwa_index\n\n        script:\n        \"\"\"\n        bwa index -a bwtsw $fasta\n        mkdir BWAIndex && mv ${fasta}* BWAIndex\n        \"\"\"\n    }", "process GATK4_GENOMICSDBIMPORT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(intervalfile), val(intervalval), path(wspace)\n    val run_intlist\n    val run_updatewspace\n    val input_map\n\n    output:\n    tuple val(meta), path(\"${prefix}\")      , optional:true, emit: genomicsdb\n    tuple val(meta), path(\"$updated_db\")    , optional:true, emit: updatedb\n    tuple val(meta), path(\"*.interval_list\"), optional:true, emit: intervallist\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n                                                     \n    inputs_command = input_map ? \"--sample-name-map ${vcf[0]}\" : \"${'-V ' + vcf.join(' -V ')}\"\n    dir_command = \"--genomicsdb-workspace-path ${prefix}\"\n    intervals_command = intervalfile ? \" -L ${intervalfile} \" : \" -L ${intervalval} \"\n\n                                                                                  \n    if (run_intlist) {\n        inputs_command = ''\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = \"--output-interval-list-to-file ${prefix}.interval_list\"\n    }\n\n                                                                                                                                        \n    if (run_updatewspace) {\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = ''\n        updated_db = wspace.toString()\n    }\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenomicsDBImport] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GenomicsDBImport \\\\\n        $inputs_command \\\\\n        $dir_command \\\\\n        $intervals_command \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0\"\n    }\n    \n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n    \n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path(\"*.log\"), emit: summary\n    path  \"*.version.txt\"         , emit: version\n\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $options.args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        echo $VERSION > ${software}.version.txt\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $options.args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        echo $VERSION > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess repExplorer {\n    tag \"$name\"\n    publishDir \"$params.outdir\", mode: 'copy'\n\n    input:\n    set val(name), file(inter_fastq) from interleaved_fastq\n\n    output:\n    file(\"re_${name}\") into explorer_out\n\n    script:\n    \"\"\"\n    ${params.REPEXPLORER_PATH}/seqclust -p \\\n    -c ${task.cpus} -r ${task.memory.toKilo()} \\\n    -v re_${name} $inter_fastq\n    \"\"\"\n}"], "list_proc": ["lengfei5/atacseq_nf/BWA_INDEX", "junyu-boston/dicerna-rnaseq/HISAT2_ALIGN", "rbpisupati/nf-repeatexplorer/repExplorer"], "list_wf_names": ["rbpisupati/nf-repeatexplorer", "lengfei5/atacseq_nf", "junyu-boston/dicerna-rnaseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["junyu-boston"], "nb_wf": 1, "list_wf": ["dicerna-rnaseq"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "mashehu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 49, "codes": ["\nprocess STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/star:2.6.1d--0\"\n    } else {\n        container \"quay.io/biocontainers/star:2.6.1d--0\"\n    }\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    def memory    = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    \"\"\"\n    mkdir star\n    STAR \\\\\n        --runMode genomeGenerate \\\\\n        --genomeDir star/ \\\\\n        --genomeFastaFiles $fasta \\\\\n        --sjdbGTFfile $gtf \\\\\n        --runThreadN $task.cpus \\\\\n        $memory \\\\\n        $options.args\n\n    STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["junyu-boston/dicerna-rnaseq/STAR_GENOMEGENERATE"], "list_wf_names": ["junyu-boston/dicerna-rnaseq"]}, {"nb_reuse": 4, "tools": ["SAMtools", "kallisto", "Salmon", "VCFtools"], "nb_own": 5, "list_own": ["lengfei5", "lifebit-ai", "redst4r", "vincenthhu", "junyu-boston"], "nb_wf": 4, "list_wf": ["nf-core-westest", "dicerna-rnaseq", "DeepVariant", "nf-10x-kallisto", "nf_visium_kallisto"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "PhilPalmer", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "lengfei5", "Galithil", "pcantalupo", "olgabot", "sven1103", "skrakau", "drejom", "pprieto", "sofiahaglund", "ewels", "Vlad-Dembrovskyi", "pditommaso", "redst4r", "FriederikeHanssen", "mashehu", "vincenthhu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "luisas", "vezzi", "mvanins", "mariach", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 57, "codes": [" process kallisto {\n                          \n                                                                    \n\n     input:\n                                                           \n     file(reads) from combined_flat\n     file index from kallisto_index.collect()\n\n     output:\n     file \"bus_output\" into kallisto_bus_to_sort\n     file \"kallisto.log\" into kallisto_log_for_multiqc\n\n     script:\n     \"\"\"\n     echo $index\n     kallisto bus \\\\\n         -i $index \\\\\n         -o bus_output/ \\\\\n         -x ${params.chemistry} \\\\\n         -t ${params.cpus} \\\\\n         $reads | tee kallisto.log\n     \"\"\"\n }", "\nprocess vcftools{\n  tag \"$vcf\"\n\n  container 'lifebitai/vcftools:latest'\n\n  input:\n  set val(bam),file(vcf) from postout\n  output:\n  file(\"*\") into vcfout\n\n  script:\n  \"\"\"\n  vcftools --vcf $vcf --TsTv-summary\n  vcftools --vcf $vcf --TsTv-by-count\n  vcftools --vcf $vcf --TsTv-by-qual\n  # remove rows containing 'inf' which breaks multiqc report\n  sed -i '/inf/d' out.TsTv.qual\n  \"\"\"\n}", "\nprocess pseudoalPlate {\n    storeDir params.outdir\n\n    input:\n    file index from transcriptome_index\n    file batchkal from batch_kal.collect()\n\n    output:\n    file params.samplename into kallisto_pseudo\n\n    when: params.protocol=='plate'\n\n    script:\n    \"\"\"\n    ml load kallisto/0.46.0-foss-2018b\n    kallisto pseudo -t 16 --quant \\\\\n        -i $index \\\\\n        -o ${params.samplename} \\\\\n        -b $batchkal\n\n    \"\"\"\n}", "\nprocess SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.4.0--hf69c8f4_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.4.0--hf69c8f4_0\"\n    }\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $options.args \\\\\n        -i salmon\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}", "process BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' :\n        'quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 \\\\\n        mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 -@ $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["redst4r/nf-10x-kallisto/kallisto", "lifebit-ai/DeepVariant/vcftools", "lengfei5/nf_visium_kallisto/pseudoalPlate", "junyu-boston/dicerna-rnaseq/SALMON_INDEX"], "list_wf_names": ["redst4r/nf-10x-kallisto", "junyu-boston/dicerna-rnaseq", "lengfei5/nf_visium_kallisto", "lifebit-ai/DeepVariant"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["junyu-boston"], "nb_wf": 1, "list_wf": ["dicerna-rnaseq"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "skrakau", "drejom", "sofiahaglund", "ewels", "pditommaso", "FriederikeHanssen", "mashehu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 49, "codes": ["\nprocess BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.29.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.29.2--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.29.2--hc088bd4_0\"\n    }\n    \n    input:\n    tuple val(meta), path(bam)\n    \n    output:\n    tuple val(meta), path(\"*.bedGraph\"), emit: bedgraph\n    path \"*.version.txt\"               , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = '-strand + -du'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = '-strand - -du'\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        $strandedness \\\\\n        -split \\\\\n        | bedtools sort > ${prefix}.bedGraph\n\n    bedtools --version | sed -e \"s/bedtools v//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["junyu-boston/dicerna-rnaseq/BEDTOOLS_GENOMECOV"], "list_wf_names": ["junyu-boston/dicerna-rnaseq"]}, {"nb_reuse": 3, "tools": ["FeatureCounts", "BUStools", "RNASEQR", "QualiMap", "GATK"], "nb_own": 4, "list_own": ["redst4r", "lengfei5", "vincenthhu", "junyu-boston"], "nb_wf": 3, "list_wf": ["nf-10x-kallisto", "nf-core-westest", "dicerna-rnaseq", "atacseq_nf"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "jinmingda", "marchoeppner", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "lengfei5", "pcantalupo", "olgabot", "sven1103", "skrakau", "drejom", "sofiahaglund", "drewjbeh", "ewels", "pditommaso", "redst4r", "FriederikeHanssen", "mashehu", "vincenthhu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 54, "codes": [" process bustools_count{\n                  \n     publishDir \"${params.outdir}/kallisto/bustools_counts\", mode: \"copy\"\n\n     input:\n     file bus from kallisto_corrected_sort_to_count\n     file t2g from kallisto_gene_map.collect()\n\n     output:\n     file \"${bus}_eqcount\"\n     file \"${bus}_genecount\"\n\n     script:\n     \"\"\"\n     mkdir -p ${bus}_eqcount\n     mkdir -p ${bus}_genecount\n     bustools count -o ${bus}_eqcount/tcc -g $t2g -e ${bus}/matrix.ec -t ${bus}/transcripts.txt ${bus}/output.corrected.sort.bus\n     bustools count -o ${bus}_genecount/gene -g $t2g -e ${bus}/matrix.ec -t ${bus}/transcripts.txt --genecounts ${bus}/output.corrected.sort.bus\n     \"\"\"\n }", "process GATK4_BASERECALIBRATOR {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n    path knownSites\n    path knownSites_tbi\n\n    output:\n    tuple val(meta), path(\"*.table\"), emit: table\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def intervalsCommand = intervals ? \"-L ${intervals}\" : \"\"\n    def sitesCommand = knownSites.collect{\"--known-sites ${it}\"}.join(' ')\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK BaseRecalibrator] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" BaseRecalibrator  \\\n        -R $fasta \\\n        -I $input \\\n        $sitesCommand \\\n        $intervalsCommand \\\n        --tmp-dir . \\\n        $args \\\n        -O ${prefix}.table\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1\"\n    } else {\n        container \"quay.io/biocontainers/qualimap:2.2.2d--1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n    \n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $options.args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MERGED_LIB_CONSENSUS_COUNTS {\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/macs/${PEAK_TYPE}/consensus\", mode: params.publish_dir_mode\n\n    when:\n    params.macs_gsize && (replicatesExist || multipleGroups) && !params.skip_consensus_peaks\n\n    input:\n    path bams from ch_mlib_name_bam_mlib_counts.collect{ it[1] }\n    path saf from ch_mlib_macs_consensus_saf.collect()\n\n    output:\n    path '*featureCounts.txt' into ch_mlib_macs_consensus_counts\n    path '*featureCounts.txt.summary' into ch_mlib_macs_consensus_counts_mqc\n\n    script:\n    prefix = 'consensus_peaks.mLb.clN'\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    pe_params = params.single_end ? '' : '-p --donotsort'\n    \"\"\"\n    featureCounts \\\\\n        -F SAF \\\\\n        -O \\\\\n        --fracOverlap 0.2 \\\\\n        -T $task.cpus \\\\\n        $pe_params \\\\\n        -a $saf \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bam_files.join(' ')}\n    \"\"\"\n}"], "list_proc": ["redst4r/nf-10x-kallisto/bustools_count", "junyu-boston/dicerna-rnaseq/QUALIMAP_RNASEQ", "lengfei5/atacseq_nf/MERGED_LIB_CONSENSUS_COUNTS"], "list_wf_names": ["lengfei5/atacseq_nf", "redst4r/nf-10x-kallisto", "junyu-boston/dicerna-rnaseq"]}, {"nb_reuse": 3, "tools": ["StringTie", "SAMtools", "BamTools", "FeatureCounts", "QualiMap"], "nb_own": 4, "list_own": ["raygozag", "lengfei5", "vincenthhu", "junyu-boston"], "nb_wf": 3, "list_wf": ["atacseq_nf", "nf-core-westest", "dicerna-rnaseq", "rnaseq"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "jinmingda", "d4straub", "kviljoen", "na399", "marchoeppner", "raygozag", "pranathivemuri", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "lengfei5", "Galithil", "pcantalupo", "olgabot", "sven1103", "skrakau", "drejom", "sofiahaglund", "drewjbeh", "ewels", "pditommaso", "FriederikeHanssen", "mashehu", "vincenthhu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 54, "codes": ["\nprocess MERGED_LIB_BAM_FILTER {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir path: \"${params.outdir}/bwa/mergedLibrary\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (params.single_end || params.save_align_intermeds) {\n                          if (filename.endsWith('.flagstat')) \"samtools_stats/$filename\"\n                          else if (filename.endsWith('.idxstats')) \"samtools_stats/$filename\"\n                          else if (filename.endsWith('.stats')) \"samtools_stats/$filename\"\n                          else if (filename.endsWith('.sorted.bam')) filename\n                          else if (filename.endsWith('.sorted.bam.bai')) filename\n                          else null\n                      }\n                }\n\n    input:\n    tuple val(name), path(bam) from ch_mlib_bam_filter\n    path bed from ch_genome_filter_regions.collect()\n    path bamtools_filter_config from ch_bamtools_filter_config\n\n    output:\n    tuple val(name), path('*.{bam,bam.bai}') into ch_mlib_filter_bam\n    tuple val(name), path('*.flagstat') into ch_mlib_filter_bam_flagstat\n    path '*.{idxstats,stats}' into ch_mlib_filter_bam_stats_mqc\n\n    script:\n    prefix = params.single_end ? \"${name}.mLb.clN\" : \"${name}.mLb.flT\"\n    filter_params = params.single_end ? '-F 0x004' : '-F 0x004 -F 0x0008 -f 0x001'\n    dup_params = params.keep_dups ? '' : '-F 0x0400'\n    multimap_params = params.keep_multi_map ? '' : '-q 1'\n    blacklist_params = params.blacklist ? \"-L $bed\" : ''\n    name_sort_bam = params.single_end ? '' : \"samtools sort -n -@ $task.cpus -o ${prefix}.bam -T $prefix ${prefix}.sorted.bam\"\n    \"\"\"\n    samtools view \\\\\n        $filter_params \\\\\n        $dup_params \\\\\n        $multimap_params \\\\\n        $blacklist_params \\\\\n        -b ${bam[0]} \\\\\n        | bamtools filter \\\\\n            -out ${prefix}.sorted.bam \\\\\n            -script $bamtools_filter_config\n\n    samtools index ${prefix}.sorted.bam\n    samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n    samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n    samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n\n    $name_sort_bam\n    \"\"\"\n}", "process QUALIMAP_BAMQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1' :\n        'quay.io/biocontainers/qualimap:2.2.2d--1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path gff\n    val use_gff\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def collect_pairs = meta.single_end ? '' : '--collect-overlap-pairs'\n    def memory     = task.memory.toGiga() + \"G\"\n    def regions = use_gff ? \"--gff $gff\" : ''\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        bamqc \\\\\n        $args \\\\\n        -bam $bam \\\\\n        $regions \\\\\n        -p $strandedness \\\\\n        $collect_pairs \\\\\n        -outdir $prefix \\\\\n        -nt $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qualimap: \\$(echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0' :\n        'quay.io/biocontainers/subread:2.0.1--hed695b0_0' }\"\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"versions.yml\"                                , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        subread: \\$( echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess STRINGTIE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::stringtie=2.1.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/stringtie:2.1.4--h7e0af3c_0\"\n    } else {\n        container \"quay.io/biocontainers/stringtie:2.1.4--h7e0af3c_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n    \n    output:\n    tuple val(meta), path(\"*.coverage.gtf\")   , emit: coverage_gtf\n    tuple val(meta), path(\"*.transcripts.gtf\"), emit: transcript_gtf\n    tuple val(meta), path(\"*.txt\")            , emit: abundance\n    tuple val(meta), path(\"*.ballgown\")       , emit: ballgown\n    path  \"*.version.txt\"                     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = '--fr'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = '--rf'\n    }\n    \"\"\"\n    stringtie \\\\\n        $bam \\\\\n        $strandedness \\\\\n        -G $gtf \\\\\n        -o ${prefix}.transcripts.gtf \\\\\n        -A ${prefix}.gene_abundance.txt \\\\\n        -C ${prefix}.coverage.gtf \\\\\n        -b ${prefix}.ballgown \\\\\n        $options.args\n\n    stringtie --version > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["lengfei5/atacseq_nf/MERGED_LIB_BAM_FILTER", "raygozag/rnaseq/SUBREAD_FEATURECOUNTS", "junyu-boston/dicerna-rnaseq/STRINGTIE"], "list_wf_names": ["junyu-boston/dicerna-rnaseq", "lengfei5/atacseq_nf", "raygozag/rnaseq"]}, {"nb_reuse": 4, "tools": ["FeatureCounts"], "nb_own": 4, "list_own": ["ray1919", "vibbits", "lauramble", "junyu-boston"], "nb_wf": 4, "list_wf": ["lRNA-Seq", "rnaseq-vizfada", "dicerna-rnaseq", "rnaseq-editing"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "lauramble", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "evanfloden", "skrakau", "drejom", "sofiahaglund", "ray1919", "ewels", "pditommaso", "abotzki", "FriederikeHanssen", "mashehu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "alex-botzki", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 54, "codes": ["\nprocess SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0\"\n    } else {\n        container \"quay.io/biocontainers/subread:2.0.1--hed695b0_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"*.version.txt\"                               , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0\"\n    } else {\n        container \"quay.io/biocontainers/subread:2.0.1--hed695b0_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n    \n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"*.version.txt\"                               , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0\"\n    } else {\n        container \"quay.io/biocontainers/subread:2.0.1--hed695b0_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"*.version.txt\"                               , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SUBREAD_FEATURECOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::subread=2.0.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/subread:2.0.1--hed695b0_0\"\n    } else {\n        container \"quay.io/biocontainers/subread:2.0.1--hed695b0_0\"\n    }\n\n    input:\n    tuple val(meta), path(bams), path(annotation)\n\n    output:\n    tuple val(meta), path(\"*featureCounts.txt\")        , emit: counts\n    tuple val(meta), path(\"*featureCounts.txt.summary\"), emit: summary\n    path \"*.version.txt\"                               , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-p'\n\n    def strandedness = 0\n    if (meta.strandedness == 'forward') {\n        strandedness = 1\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 2\n    }\n    \"\"\"\n    featureCounts \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -T $task.cpus \\\\\n        -a $annotation \\\\\n        -s $strandedness \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bams.join(' ')}\n\n    echo \\$(featureCounts -v 2>&1) | sed -e \"s/featureCounts v//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/SUBREAD_FEATURECOUNTS", "junyu-boston/dicerna-rnaseq/SUBREAD_FEATURECOUNTS", "vibbits/rnaseq-editing/SUBREAD_FEATURECOUNTS", "lauramble/rnaseq-vizfada/SUBREAD_FEATURECOUNTS"], "list_wf_names": ["vibbits/rnaseq-editing", "ray1919/lRNA-Seq", "junyu-boston/dicerna-rnaseq", "lauramble/rnaseq-vizfada"]}, {"nb_reuse": 5, "tools": ["MarkDuplicates (IP)", "seqtk", "Cutadapt", "STAR", "Sambamba", "FastQC", "Mgenome", "Picard"], "nb_own": 6, "list_own": ["ray1919", "viktorlj", "nibscbioinformatics", "lauramble", "letovesnoi", "junyu-boston"], "nb_wf": 5, "list_wf": ["rnaseq-vizfada", "dicerna-rnaseq", "clusterassembly", "nf-core-viralevo", "SarGet", "lRNA-Seq"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "lauramble", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "viktorlj", "pcantalupo", "olgabot", "sven1103", "evanfloden", "skrakau", "drejom", "sofiahaglund", "ray1919", "ewels", "pditommaso", "FriederikeHanssen", "mashehu", "jburos", "chuan-wang", "letovesnoi", "nf-core-bot", "robsyme", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "kaurravneet4123", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 55, "codes": ["\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n    \n    conda (params.enable_conda ? \"bioconda::picard=2.23.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.23.9--0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.23.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    \n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"*.version.txt\"                 , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.bam \\\\\n        METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt\n\n    echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d: > ${software}.version.txt\n    \"\"\"\n}", "process coverage_analysis_standard{\n    tag {idSample}\n\n    publishDir directoryMap.coverage, mode: 'link'\n\n    input:\n      set idPatient, idSample, file(bam), file(bai) from trimmed_StandardBAM\n      file(regions) from Channel.value(regionsFile)\n\n    output:\n      file(\"${idSample}.standard.coverage.txt\") into standard_coverage\n\n    script:\n    \"\"\"\n    sambamba depth region --filter 'mapping_quality > 20' -q 20 -T 10 -T 50 -T 100 -T 500 -T 1000 -L ${regions} -o ${idSample}.rawcoverage.txt ${bam}\n    ParseSamCoverage.py -i ${idSample}.rawcoverage.txt -o ${idSample}.standard.coverage.txt\n    \"\"\"\n}", "\nprocess FQ2FA {\n    tag \"${sample.id}, ${sample.type}\"\n    conda (params.enable_conda ? \"bioconda::seqtk\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(reads)\n\n    output:\n    tuple val(sample), path('*.fasta'), emit: reads_in_fasta\n\n    when:\n    reads.toString().endsWith('.fastq') or reads.toString().endsWith('.fq') or reads.toString().endsWith('.fastq.gz') or reads.toString().endsWith('.fq.gz')\n\n    script:\n    \"\"\"\n    basename=\\$(basename ${reads})\n    ext=\\${basename#*.}\n    filename=\\${basename%%.*}\n    seqtk seq -a ${reads} > \\${filename}.fasta\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'data/star/log', meta:meta, publish_by_meta:['id']) },\n        pattern: \"*.{out,tab}\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'temp/alignment/star', meta:meta, publish_by_meta:['id']) },\n        pattern: \"*.{bam,fastq.gz}\"\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.6.1d' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0'\n    } else {\n        container 'quay.io/biocontainers/star:2.7.9a--h9ee0642_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  '*.version.txt'                     , emit: version\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (options.args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (options.args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $options.args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.2' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/cutadapt:3.2--py38h0213d0e_0'\n    } else {\n        container 'quay.io/biocontainers/cutadapt:3.2--py38h0213d0e_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path adapterfile\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired   = meta.single_end ? \"-a file:${adapterfile} -g file:${adapterfile}\"  : \"-a file:${adapterfile} -A file:${adapterfile} -g file:${adapterfile} -G file:${adapterfile}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $options.args \\\\\n        $paired \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"\n}", "\nprocess RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0\"\n    }\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"                , emit: index\n    path \"rsem/*transcripts.fa\", emit: transcript_fasta\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def args     = options.args.tokenize()\n    if (args.contains('--star')) {\n        args.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $options.args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["junyu-boston/dicerna-rnaseq/PICARD_MARKDUPLICATES", "viktorlj/SarGet/coverage_analysis_standard", "letovesnoi/clusterassembly/FQ2FA", "ray1919/lRNA-Seq/STAR_ALIGN", "lauramble/rnaseq-vizfada/RSEM_PREPAREREFERENCE"], "list_wf_names": ["junyu-boston/dicerna-rnaseq", "viktorlj/SarGet", "ray1919/lRNA-Seq", "lauramble/rnaseq-vizfada", "letovesnoi/clusterassembly"]}, {"nb_reuse": 4, "tools": ["STAR", "preseq", "gffread"], "nb_own": 4, "list_own": ["ray1919", "vibbits", "lauramble", "junyu-boston"], "nb_wf": 4, "list_wf": ["rnaseq-vizfada", "rnaseq-editing", "dicerna-rnaseq", "lRNA-Seq"], "list_contrib": ["arontommi", "amayer21", "rfenouil", "abhi18av", "alneberg", "d4straub", "na399", "kviljoen", "marchoeppner", "pranathivemuri", "lauramble", "orionzhou", "senthil10", "rsuchecki", "aanil", "ppericard", "drpatelh", "lpantano", "silviamorins", "jemten", "matrulda", "Galithil", "pcantalupo", "olgabot", "sven1103", "evanfloden", "skrakau", "drejom", "sofiahaglund", "ray1919", "ewels", "abotzki", "pditommaso", "FriederikeHanssen", "mashehu", "jburos", "chuan-wang", "nf-core-bot", "robsyme", "alex-botzki", "veeravalli", "paulklemm", "maxulysse", "ggabernet", "colindaven", "apeltzer", "vezzi", "mvanins", "grst", "jun-wan", "jordwil", "BABS-STP1", "Hammarn", "zxl124"], "nb_contrib": 54, "codes": ["\nprocess STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n    }\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    def args     = options.args.tokenize()\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess GFFREAD {\n    tag \"$gff\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0\"\n    } else {\n        container \"quay.io/biocontainers/gffread:0.12.1--h8b12597_0\"\n    }\n\n    input:\n    path gff\n    \n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    gffread $gff $options.args -o ${gff.baseName}.gtf\n    echo \\$(gffread --version 2>&1) > ${software}.version.txt\n    \"\"\"\n}", "\nprocess STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.6.1d' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/star:2.6.1d--0'\n    } else {\n        container 'quay.io/biocontainers/star:2.6.1d--0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  '*.version.txt'                     , emit: version\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (options.args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (options.args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $options.args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1\"\n    } else {\n        container \"quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"*.version.txt\"                , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/STAR_GENOMEGENERATE", "junyu-boston/dicerna-rnaseq/GFFREAD", "lauramble/rnaseq-vizfada/STAR_ALIGN", "ray1919/lRNA-Seq/PRESEQ_LCEXTRAP"], "list_wf_names": ["vibbits/rnaseq-editing", "ray1919/lRNA-Seq", "junyu-boston/dicerna-rnaseq", "lauramble/rnaseq-vizfada"]}, {"nb_reuse": 1, "tools": ["BCFtools", "Bedops"], "nb_own": 1, "list_own": ["jvierstra"], "nb_wf": 1, "list_wf": ["nf-genotyping"], "list_contrib": ["jvierstra"], "nb_contrib": 1, "codes": ["\nprocess filter_variants {\n\ttag \"${indiv_id}:${cell_type}\"\n\tmodule ' bedops/2.4.40-typical'\n\n\tpublishDir params.outdir, mode: 'symlink'\n\n\tinput:\n\tset val(indiv_id), val(cell_type), val(hotspots_file) from INDIV_CELL_TYPE\n\t\n\tfile genotype_file from file(params.genotype_file)\n\tfile '*' from file(\"${params.genotype_file}.csi\")\n\t\n\tval min_DP from params.min_DP\n\tval min_AD from params.min_AD\n\tval min_GQ from params.min_GQ\n\n\toutput:\n\tfile(\"${indiv_id}_${cell_type}.bed.gz\")\n\tfile(\"${indiv_id}_${cell_type}.bed.gz.tbi\")\n\n\tscript:\n\t\"\"\"\n\tbcftools query \\\n\t\t-s ${indiv_id} \\\n\t\t-i'GT=\"alt\"' \\\n\t\t-f'%CHROM\\\\t%POS0\\\\t%POS\\\\t%CHROM:%POS:%REF:%ALT\\\\t%REF\\\\t%ALT\\\\t[%GT\\\\t%GQ\\\\t%DP\\\\t%AD{0}\\\\t%AD{1}]\\\\n' \\\n\t\t${genotype_file} \\\n\t| awk -v OFS=\"\\\\t\" \\\n\t\t-v min_GQ=${min_GQ} -v min_AD=${min_AD} -v min_DP=${min_DP}\\\n\t\t'\\$8<min_GQ { next; } \\$9<min_DP { next; }\\\n\t\t\t(\\$7==\"0/1\" || \\$7==\"1/0\" || \\$7==\"0|1\" || \\$7==\"1|0\") && (\\$10<min_AD || \\$11<min_AD) { \\\n\t\t\t\tnext; \\\n\t\t\t} \\\n\t\t\t{ print; }' \\\n\t| sort-bed - \\\n\t| grep -v chrX | grep -v chrY | grep -v chrM | grep -v _random | grep -v _alt | grep -v chrUn \\\n\t| bedops -e 1 - ${hotspots_file} \\\n\t| bgzip -c > ${indiv_id}_${cell_type}.bed.gz\n\n\ttabix -p bed ${indiv_id}_${cell_type}.bed.gz\n\t\"\"\"\n}"], "list_proc": ["jvierstra/nf-genotyping/filter_variants"], "list_wf_names": ["jvierstra/nf-genotyping"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jvierstra"], "nb_wf": 1, "list_wf": ["nf-genotyping"], "list_contrib": ["jvierstra"], "nb_contrib": 1, "codes": ["\nprocess merge_bamfiles {\n\ttag \"${indiv_id}\"\n\n\tcpus 2\n\n\tpublishDir params.outdir + '/merged', mode: 'symlink'\n\n\tinput:\n\tset val(indiv_id), val(bam_files) from SAMPLES_AGGREGATIONS_MERGE\n\n\toutput:\n\tset val(indiv_id), file(\"${indiv_id}.bam\"), file(\"${indiv_id}.bam.bai\") into INDIV_MERGED_FILES, INDIV_MERGED_LIST\n\n\tscript:\n\t\"\"\"\n\tsamtools merge -f -@${task.cpus} ${indiv_id}.bam ${bam_files}\n\tsamtools index ${indiv_id}.bam\n\t\"\"\"\n}"], "list_proc": ["jvierstra/nf-genotyping/merge_bamfiles"], "list_wf_names": ["jvierstra/nf-genotyping"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["jvierstra"], "nb_wf": 1, "list_wf": ["nf-genotyping"], "list_contrib": ["jvierstra"], "nb_contrib": 1, "codes": ["\nprocess call_genotypes {\n\ttag \"Region: ${region}\"\n\t\n\tscratch true\n\tcpus 2\n\n\tinput:\n\tfile genome_fasta from file(genome_fasta_file)\n\t\n\tfile dbsnp_file from file(params.dbsnp_file)\n\tfile dbsnp_index_file from file(\"${params.dbsnp_file}.tbi\")\n \t\n\tval min_DP from params.min_DP\n\tval min_SNPQ from params.min_SNPQ\n\tval min_GQ from params.min_GQ\n\tval hwe_cutoff from params.hwe_cutoff\n\n\tval region from GENOME_CHUNKS_MAP\n\tfile '*' from INDIV_MERGED_ALL_FILES.collect()\n\tfile 'sample_indiv_map.tsv' from INDIV_MERGED_SAMPLE_MAP_FILE \n\n\toutput:\n\tfile '*filtered.annotated.vcf.gz*' into GENOME_CHUNKS_VCF\n\n\tscript:\n\t\"\"\"\n\tcut -f1 sample_indiv_map.tsv > samples.txt\n\tcut -f2 sample_indiv_map.tsv > filelist.txt\n\n\tbcftools mpileup \\\n\t\t--regions ${region} \\\n\t\t--fasta-ref ${genome_fasta} \\\n\t\t--redo-BAQ \\\n\t\t--adjust-MQ 50 \\\n\t\t--gap-frac 0.05 \\\n\t\t--max-depth 10000 --max-idepth 200000 \\\n\t\t--annotate FORMAT/DP,FORMAT/AD \\\n\t\t--bam-list filelist.txt \\\n\t\t--output-type u \\\n\t| bcftools call \\\n\t\t--threads ${task.cpus} \\\n\t\t--keep-alts \\\n\t\t--multiallelic-caller \\\n\t\t--format-fields GQ \\\n\t\t--output-type v \\\n\t| bcftools filter \\\n\t\t-i\"INFO/DP>=${min_DP}\" \\\n\t\t--output-type z - \\\n\t> ${region}.vcf.gz\n\n\tbcftools index ${region}.vcf.gz\n\n\tbcftools reheader \\\n\t\t-s samples.txt \\\n\t \t${region}.vcf.gz \\\n\t| bcftools norm \\\n\t\t--threads ${task.cpus} \\\n\t\t--check-ref w \\\n\t\t-m - \\\n\t\t--fasta-ref ${genome_fasta} \\\n\t| bcftools filter \\\n\t\t-i\"QUAL>=${min_SNPQ} & FORMAT/GQ>=${min_GQ} & FORMAT/DP>=${min_DP}\" \\\n\t\t--SnpGap 3 --IndelGap 10 --set-GTs . \\\n\t| bcftools view \\\n\t\t-i'GT=\"alt\"' --trim-alt-alleles \\\n\t| bcftools annotate -x ^INFO/DP \\\n\t| bcftools +fill-tags -- -t all \\\n\t| bcftools filter --output-type z -e\"INFO/HWE<${hwe_cutoff}\" \\\n\t> ${region}.filtered.vcf.gz\n\n\tbcftools index ${region}.filtered.vcf.gz\n\n\tbcftools annotate \\\n\t\t-r ${region} \\\n\t\t-a ${dbsnp_file} \\\n\t\t--columns ID,CAF,TOPMED \\\n\t\t--output-type z \\\n  \t\t${region}.filtered.vcf.gz \\\n\t> ${region}.filtered.annotated.vcf.gz\n\n\tbcftools index ${region}.filtered.annotated.vcf.gz\t\t\n\t\"\"\"\n}"], "list_proc": ["jvierstra/nf-genotyping/call_genotypes"], "list_wf_names": ["jvierstra/nf-genotyping"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["jvierstra"], "nb_wf": 1, "list_wf": ["nf-genotyping"], "list_contrib": ["jvierstra"], "nb_contrib": 1, "codes": ["\nprocess merge_vcfs {\n\n\tscratch true\n\tpublishDir params.outdir + '/genotypes', mode: 'symlink'\n\n\tinput:\n\tfile '*' from GENOME_CHUNKS_VCF.collect()\n\tfile genome_fasta_ancestral from file(params.genome_ancestral_fasta_file)\n\n\toutput:\n\tfile 'all.filtered.vcf.gz*'\n\tset file('all.filtered.snps.annotated.vcf.gz'), file('all.filtered.snps.annotated.vcf.gz.csi') into FILTERED_SNPS_VCF\n    file \"plink.*\"\n    \n\tscript:\n\t\"\"\"\n\t# Concatenate files\n\tls *.filtered.annotated.vcf.gz > files.txt\n\tcat files.txt | tr \":-\" \"\\\\t\" | tr \".\" \"\\\\t\" | cut -f1-3 | paste - files.txt | sort-bed - | awk '{ print \\$NF; }' > mergelist.txt\n\n\tbcftools concat \\\n\t\t--output-type z \\\n\t\t-f mergelist.txt \\\n\t> all.filtered.vcf.gz\n\t\n\tbcftools index all.filtered.vcf.gz\n\t\n\t# Output only SNPs\n\tbcftools view \\\n\t\t-m2 -M2 -v snps \\\n\t\t--output-type z \\\n\t\tall.filtered.vcf.gz \\\n\t> all.filtered.snps.vcf.gz\n\t\n\tbcftools index all.filtered.snps.vcf.gz\t\n\n\t# Annotate ancestral allele\n\t\n\techo \"##INFO=<ID=AA,Number=1,Type=String,Description=\\\"Inferred ancestral allele -- EPO/PECAN alignments\\\">\" > header.txt\n\n\t# Contigs with ancestral allele information\n\tfaidx -i chromsizes ${genome_fasta_ancestral} | cut -f1 > chroms.txt\n\t\n\t# Get SNPs in BED-format; remove contigs with no FASTA sequence\n\tbcftools query  -f \"%CHROM\\t%POS0\\t%POS\\t%REF\\t%ALT\\n\" all.filtered.snps.vcf.gz \\\n\t| grep -w -f chroms.txt \\\n\t> all.filtered.snps.bed\n\n\t# Get ancestral allele from FASTA file and make a TABIX file\n\tfaidx -i transposed \\\n\t\t-b all.filtered.snps.bed \\\n\t\t${genome_fasta_ancestral} \\\n\t| paste - all.filtered.snps.bed\t\\\n\t| awk -v OFS=\"\\t\" '{ print \\$5, \\$7, \\$4; }' \\\n\t| bgzip -c > all.filtered.snps.ancestral.tab.gz\n\n\ttabix -b 2 -e -2 all.filtered.snps.ancestral.tab.gz\n\n\t# Annotate VCF\n\tbcftools annotate \\\n\t\t--output-type z \\\n\t\t-h header.txt \\\n\t\t-a all.filtered.snps.ancestral.tab.gz \\\n\t\t-c CHROM,POS,INFO/AA \\\n\t\tall.filtered.snps.vcf.gz \\\n\t> all.filtered.snps.annotated.vcf.gz\n\t\n\tbcftools index all.filtered.snps.annotated.vcf.gz\n    \n    \n    plink2 --make-bed \\\n    \t--output-chr chrM \\\n    \t--vcf all.filtered.snps.annotated.vcf.gz \\\n        --keep-allele-order \\\n    \t--snps-only \\\n        -allow-extra-chr \\\n    \t--out plink\n\n\t\"\"\"\n}"], "list_proc": ["jvierstra/nf-genotyping/merge_vcfs"], "list_wf_names": ["jvierstra/nf-genotyping"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["jvierstra"], "nb_wf": 1, "list_wf": ["nf-index"], "list_contrib": ["jvierstra"], "nb_contrib": 1, "codes": ["\nprocess merge_bamfiles {\n\ttag \"${indiv_id}:${cell_type}\"\n\n\tpublishDir params.outdir + '/merged', mode: 'symlink' \n\n\tcpus 2\n\n\tinput:\n\tset val(indiv_id), val(cell_type), val(bam_files) from SAMPLES_AGGREGATIONS_MERGE\n\n\toutput:\n\tset val(indiv_id), val(cell_type), file('*.bam'), file('*.bam.bai') into BAMS_MERGED_HOTSPOTS, BAMS_MERGED_COUNTS \n\n\tscript:\n\t\"\"\"\n\tsamtools merge -f -@${task.cpus} ${indiv_id}_${cell_type}.bam ${bam_files}\n\tsamtools index ${indiv_id}_${cell_type}.bam\n\t\"\"\"\n}"], "list_proc": ["jvierstra/nf-index/merge_bamfiles"], "list_wf_names": ["jvierstra/nf-index"]}, {"nb_reuse": 1, "tools": ["SAMtools", "4peaks"], "nb_own": 1, "list_own": ["jvierstra"], "nb_wf": 1, "list_wf": ["nf-index"], "list_contrib": ["jvierstra"], "nb_contrib": 1, "codes": ["\nprocess call_hotspots {\n\ttag \"${indiv_id}:${cell_type}\"\n\n\t                                       \n\tpublishDir params.outdir + '/hotspots', mode: 'symlink', pattern: \"*.starch\" \n\n\tmodule \"bedops/2.4.35-typical:modwt/1.0\"\n\n\t              \n\n\tinput:\n\tfile 'nuclear_chroms.txt' from file(\"${nuclear_chroms}\")\n\tfile 'mappable.bed' from file(\"${mappable}\")\n\tfile 'chrom_sizes.bed' from file(\"${chrom_sizes}\")\n\tfile 'centers.starch' from file(\"${centers}\")\n\n\tset val(indiv_id), val(cell_type), val(bam_file) from BAMS_HOTSPOTS\n\n\toutput:\n\tset val(indiv_id), val(cell_type), val(bam_file), file(\"${indiv_id}_${cell_type}.varw_peaks.fdr0.001.starch\") into PEAKS\n\tfile(\"${indiv_id}_${cell_type}.hotspots.fdr*.starch\")\n\n\tscript:\n\t\"\"\"\n\n\tTMPDIR=\\$(mktemp -d)\n\n\tsamtools view -H ${bam_file} > \\${TMPDIR}/header.txt\n\n\tcat nuclear_chroms.txt \\\n\t| xargs samtools view -b ${bam_file} \\\n\t| samtools reheader \\${TMPDIR}/header.txt - \\\n\t> \\${TMPDIR}/nuclear.bam\n\n\tPATH=/home/jvierstra/.local/src/hotspot2/bin:\\$PATH\n\tPATH=/home/jvierstra/.local/src/hotspot2/scripts:\\$PATH\n\n\thotspot2.sh -F 0.05 -f 0.05 -p varWidth_20_${indiv_id}_${cell_type} \\\n\t\t-M mappable.bed \\\n    \t-c chrom_sizes.bed \\\n    \t-C centers.starch \\\n    \t\\${TMPDIR}/nuclear.bam \\\n    \tpeaks\n\n\tcd peaks\n\n\thsmerge.sh -f 0.001 nuclear.allcalls.starch nuclear.hotspots.fdr0.001.starch\n\n\trm -f nuclear.varw_peaks.*\n\n\tdensity-peaks.bash \\\n\t\t\\${TMPDIR}\\\n\t\tvarWidth_20_${indiv_id}_${cell_type} \\\n\t\tnuclear.cutcounts.starch \\\n\t\tnuclear.hotspots.fdr0.001.starch \\\n\t\t../chrom_sizes.bed \\\n\t\tnuclear.varw_density.fdr0.001.starch \\\n\t\tnuclear.varw_peaks.fdr0.001.starch \\\n\t\t\\$(cat nuclear.cleavage.total)\n\n\t\tcp nuclear.varw_peaks.fdr0.001.starch ../${indiv_id}_${cell_type}.varw_peaks.fdr0.001.starch\n    \n    \tcp nuclear.hotspots.fdr0.05.starch ../${indiv_id}_${cell_type}.hotspots.fdr0.05.starch\n    \tcp nuclear.hotspots.fdr0.001.starch ../${indiv_id}_${cell_type}.hotspots.fdr0.001.starch\n\n\trm -rf \\${TMPDIR}\n\t\"\"\"\n}"], "list_proc": ["jvierstra/nf-index/call_hotspots"], "list_wf_names": ["jvierstra/nf-index"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["k-florek"], "nb_wf": 1, "list_wf": ["mirkwater"], "list_contrib": ["k-florek"], "nb_contrib": 1, "codes": ["\nprocess preProcessPacBioBAM {\n  tag \"${reads.baseName}\"\n\n  input:\n  set file(reads), file(reference), file(reference_gff) from pacbio_reads_reference\n\n  when:\n  params.pacbio\n\n  output:\n  tuple val(\"${reads.baseName}\"), file(\"*.fastq\"), file(reference), file(reference_gff) into fastq_reads_mapping, fastq_reads_pbaa\n  file(\"*fasta.fai\") into reference_genome_index_pbaa\n  file(\"*fastq.fai\") into read_index\n\n  script:\n  \"\"\"\n    samtools fastq ${reads} > ${reads.baseName}.fastq\n    samtools faidx ${reference}\n    samtools fqidx ${reads.baseName}.fastq\n  \"\"\"\n}"], "list_proc": ["k-florek/mirkwater/preProcessPacBioBAM"], "list_wf_names": ["k-florek/mirkwater"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["k-florek"], "nb_wf": 1, "list_wf": ["mirkwater"], "list_contrib": ["k-florek"], "nb_contrib": 1, "codes": ["\nprocess cluster_seq_alignment {\n  publishDir \"${params.outdir}/cluster_alignment\", mode: 'copy'\n\n  input:\n  file(clusters) from passed_clusters.collect()\n  file(reference) from  reference_genome_cluster\n\n  output:\n  file(\"cluster_alignment.fasta\") into cluster_alignment\n\n  script:\n  \"\"\"\n  cat ${clusters} > all.fa\n  mafft --6merpair --adjustdirection --addfragments all.fa ${reference} > cluster_alignment.fasta\n  \"\"\"\n}"], "list_proc": ["k-florek/mirkwater/cluster_seq_alignment"], "list_wf_names": ["k-florek/mirkwater"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["k-florek"], "nb_wf": 1, "list_wf": ["mirkwater"], "list_contrib": ["k-florek"], "nb_contrib": 1, "codes": ["\nprocess prep_variant_calls {\n  publishDir \"${params.outdir}/cluster_variants\", mode: 'copy'\n\n  input:\n  file(vcf) from cluster_variants\n\n  output:\n  file(\"*sample*.vcf\") into prep_variant_calls\n\n  script:\n  \"\"\"\n  bcftools annotate --rename-chrs <(printf '1\\tNC_045512.2') ${vcf} > cluster_variants_chrom.vcf\n  for sample in \\$(bcftools query -l cluster_variants_chrom.vcf); do\n    bcftools view -c1 -Ov -s \\$sample -o \\$sample.vcf cluster_variants_chrom.vcf\n  done\n  mv cluster_variants_chrom.vcf ${vcf}\n  \"\"\"\n}"], "list_proc": ["k-florek/mirkwater/prep_variant_calls"], "list_wf_names": ["k-florek/mirkwater"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["k-florek"], "nb_wf": 1, "list_wf": ["mirkwater"], "list_contrib": ["k-florek"], "nb_contrib": 1, "codes": ["\nprocess annotate_variants {\n  publishDir \"${params.outdir}/annotated_variants\", mode: 'copy'\n\n  input:\n  tuple file(vcf), file(compressed_database) from annotation_input\n\n  output:\n  file(vcf) into annotated_variants\n\n  script:\n  \"\"\"\n  tar -xzf ${compressed_database}\n  mv ${vcf} input.vcf\n  snpEff ann sc2 input.vcf > ${vcf}\n  \"\"\"\n}"], "list_proc": ["k-florek/mirkwater/annotate_variants"], "list_wf_names": ["k-florek/mirkwater"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["k-florek"], "nb_wf": 1, "list_wf": ["mirkwater"], "list_contrib": ["k-florek"], "nb_contrib": 1, "codes": ["\nprocess mapping_reads {\n  tag \"$name\"\n  publishDir \"${params.outdir}/mapped\", mode: 'copy'\n\n  input:\n  set val(name), file(reads), file(reference), file(reference_gff) from fastq_reads_mapping\n\n  output:\n  tuple name, file(\"*.sam\"), file(reference), file(reference_gff) into mapped_reads\n\n  script:\n  \"\"\"\n  minimap2 -ax splice:hq ${reference} ${reads} > ${name}.sam\n  \"\"\"\n}"], "list_proc": ["k-florek/mirkwater/mapping_reads"], "list_wf_names": ["k-florek/mirkwater"]}, {"nb_reuse": 1, "tools": ["SAMtools", "AIVAR"], "nb_own": 1, "list_own": ["k-florek"], "nb_wf": 1, "list_wf": ["mirkwater"], "list_contrib": ["k-florek"], "nb_contrib": 1, "codes": ["\nprocess ivar_variant_calling {\n  tag \"$name\"\n  publishDir \"${params.outdir}/variant_calls\", mode: 'copy', saveAs: {\"${name}.${file(it).getExtension()}\"}\n\n  memory {2.GB * task.attempt}\n  errorStrategy {'retry'}\n  maxRetries 2\n\n  input:\n  set val(name), file(reads), file(reference), file(annotation) from mapped_reads\n\n  output:\n  file(\"*.tsv\")\n\n  script:\n  \"\"\"\n    samtools view -b ${reads} | samtools sort > ${name}.bam\n    samtools mpileup -aa -A -d 0 -B -Q 0 --reference ${reference} ${name}.bam | ivar variants -p ${name} -q ${params.minquality} -t ${params.minfreq} -m ${params.minreaddepth} -r ${reference} -g ${annotation}\n  \"\"\"\n}"], "list_proc": ["k-florek/mirkwater/ivar_variant_calling"], "list_wf_names": ["k-florek/mirkwater"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Orphadata"], "nb_own": 2, "list_own": ["kaizhang", "katharily"], "nb_wf": 2, "list_wf": ["pe_stacks_pipeline", "scATAC-benchmark"], "list_contrib": ["kaizhang", "katharily"], "nb_contrib": 2, "codes": ["\nprocess dim_reduct_snapatac_2 {\n    publishDir 'result/reduced_dim/'\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val(\"SnapATAC2\"), val(data), path(\"${data.name}_snapatac2_reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env python3\n    import snapatac2 as snap\n    import numpy as np\n    adata = snap.read(\"${data.anndata}\")\n    snap.tl.spectral(adata, features=None)\n    np.savetxt(\"${data.name}_snapatac2_reduced_dim.tsv\", adata.obsm[\"X_spectral\"], delimiter=\"\\t\")\n    \"\"\"\n}", "\nprocess samtools_unmapped {\n    conda 'samtools'\n    publishDir BAM_DIR, mode: 'copy', pattern: '*.bam'\n    publishDir LOG_DIR, mode: 'copy', pattern: '*.log'\n    maxForks 2\n    input:\n        file mapping from ch_bowtie_mapping\n    output:\n        file '*.bam' into ch_samtools_unmapped_output\n        file '*.log' into ch_samtools_unmapped_log\n    script:\n    \"\"\"\n    samtools view --threads 6 -bh -F 4 ${mapping} > ${mapping.baseName}.bam 2> samtools_unmapped_${mapping.baseName}.log\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_snapatac_2", "katharily/pe_stacks_pipeline/samtools_unmapped"], "list_wf_names": ["katharily/pe_stacks_pipeline", "kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["Orphadata"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_snapatac_2_svd {\n    publishDir 'result/reduced_dim/'\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val(\"SnapATAC2_SVD\"), val(data), path(\"${data.name}_snapatac2_svd_reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env python3\n    import snapatac2 as snap\n    import numpy as np\n\n    adata = snap.read(\"${data.anndata}\")\n    snap.tl.laplacian(adata, features=None)\n    output = \"${data.name}_snapatac2_svd_reduced_dim.tsv\"\n    np.savetxt(output, adata.obsm[\"X_spectral\"], delimiter=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_snapatac_2_svd"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["Orphadata"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_snapatac_2_nystrom {\n    publishDir 'result/reduced_dim/'\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val(\"SnapATAC2\"), val(data), path(\"${data.name}_snapatac2_nystrom_${data.samplingFraction}_${data.randomSeed}_reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env python3\n    import snapatac2 as snap\n    import numpy as np\n\n    adata = snap.read(\"${data.anndata}\")\n\n    snap.tl.spectral(adata, features=None, chunk_size=500, sample_size=${data.samplingFraction}, random_state=${data.randomSeed})\n    output = \"${data.name}_snapatac2_nystrom_${data.samplingFraction}_${data.randomSeed}_reduced_dim.tsv\"\n    np.savetxt(output, adata.obsm[\"X_spectral\"], delimiter=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_snapatac_2_nystrom"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["Orphadata"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_snapatac_1 {\n    container 'kaizhang/snapatac:1.0'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val(\"SnapATAC-v1.0\"), val(data), path(\"reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"Matrix\")\n    library(\"anndata\")\n    set.seed(2022)\n    adata <- read_h5ad(\"${data.anndata}\")\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    x.sp <- SnapATAC::newSnap()\n    x.sp@bmat <- data\n    x.sp <- SnapATAC::makeBinary(x.sp, mat=\"bmat\")\n    x.sp <- SnapATAC::runDiffusionMaps(\n            obj=x.sp,\n            input.mat=\"bmat\", \n            num.eigs=$nDims\n        )\n    result <- SnapATAC:::weightDimReduct(x.sp@smat, 1:$nDims, weight.by.sd=F)\n    output <- \"reduced_dim.tsv\"\n    write.table(result, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_snapatac_1"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["Orphadata"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_snapatac_1_nystrom {\n    container 'kaizhang/snapatac:1.0'\n    publishDir 'result/reduced_dim/'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val(\"SnapATAC-v1.0\"), val(data), path(\"${data.name}_snapatac_nystrom_${data.samplingFraction}_${data.randomSeed}_reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"Matrix\")\n    library(\"anndata\")\n    set.seed($data.randomSeed)\n    adata <- read_h5ad(\"${data.anndata}\")\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    sample_size <- trunc(nrow(data) * $data.samplingFraction)\n    n_dim <- min(sample_size - 2, $nDims)\n    reference <- data[sample(nrow(data),size=sample_size, replace=F),]\n\n    x.ref <- SnapATAC::newSnap()\n    x.ref@bmat <- reference\n    x.ref <- SnapATAC::makeBinary(x.ref, mat=\"bmat\")\n    x.ref <- SnapATAC::runDiffusionMaps(\n        obj=x.ref,\n        input.mat=\"bmat\", \n        num.eigs=n_dim\n    )\n\n    x.sp <- SnapATAC::newSnap()\n    x.sp@bmat <- data\n    x.sp <- SnapATAC::makeBinary(x.sp, mat=\"bmat\")\n    x.sp <- SnapATAC::runDiffusionMapsExtension(\n\t      obj1 = x.ref,\n        obj2 = x.sp,\n        input.mat=\"bmat\"\n    )\n\n    result <- SnapATAC:::weightDimReduct(x.sp@smat, 1:n_dim, weight.by.sd=F)\n    output <- \"${data.name}_snapatac_nystrom_${data.samplingFraction}_${data.randomSeed}_reduced_dim.tsv\"\n    write.table(result, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_snapatac_1_nystrom"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 3, "tools": ["SEED", "Orphadata", "Bedops", "BEDTools"], "nb_own": 2, "list_own": ["kaizhang", "keoughkath"], "nb_wf": 2, "list_wf": ["AcceleratedRegionsNF", "scATAC-benchmark"], "list_contrib": ["kaizhang", "keoughkath"], "nb_contrib": 2, "codes": ["\nprocess syntenyFilterPhastCons {\n\ttag \"Synteny filtering phastCons from ${fname}\"\n\n\t                                                          \n\n\tinput:\n\tfile(phastcons) from phastConsScoreFiltered.filter{ it.size()>0 }\n\tfile(synteny_file_list) from syntenyFiles.collect()\n\n\toutput:\n\tfile(\"*.bed\") into phastConsSyntenyFiltered\n\n\tscript:\n\tchrom = phastcons.simpleName\n\tfname = phastcons.baseName.split('_')[0]\n\t\"\"\"\n\tbedops -i ${phastcons} ${synteny_file_list} > ${fname}_phastcons_synteny_filt.bed\n\t\"\"\"\n}", "\nprocess phastconsDupFilter {\n\ttag \"Filtering phastCons from ${fname} for repeats, duplicated regions, pseudogenes and selfChain\"\n\n\t                                                          \n\n\tinput:\n\tfile(phastcons) from phastConsSyntenyFiltered.filter{ it.size()>0 }\n\tfile(ar_filters) from arFilterFile\n\n\toutput:\n\tfile(\"*.bed\") into phastConsFiltered\n\n\tscript:\n\tchrom = phastcons.simpleName\n\tfname = phastcons.baseName.split('_')[0]\n\t\"\"\"\n\tbedtools subtract -a ${phastcons} -b ${ar_filters} | bedtools sort -i - | bedtools merge -d 0 -i - > ${fname}_phastcons_dup_filt.bed\n\t\"\"\"\n}", "\nprocess dim_reduct_archr_1 {\n    container 'kaizhang/archr:1.0.1'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val('ArchR (TF-logIDF)'), val(data), path(\"reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"ArchR\")\n    library(\"anndata\")\n    set.seed(2022)\n    adata <- read_h5ad(\"${data.anndata}\")\\$T\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    result <- ArchR:::.computeLSI(mat = data,\n        LSIMethod = 1,\n        scaleTo = 10^4,\n        nDimensions = $nDims,\n        binarize = TRUE, \n        outlierQuantiles = NULL,\n        seed = 1\n    )\n    output <- \"reduced_dim.tsv\"\n    write.table(result\\$matSVD, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["keoughkath/AcceleratedRegionsNF/syntenyFilterPhastCons", "keoughkath/AcceleratedRegionsNF/phastconsDupFilter", "kaizhang/scATAC-benchmark/dim_reduct_archr_1"], "list_wf_names": ["kaizhang/scATAC-benchmark", "keoughkath/AcceleratedRegionsNF"]}, {"nb_reuse": 2, "tools": ["SEED", "Orphadata", "PhyloPat"], "nb_own": 2, "list_own": ["kaizhang", "keoughkath"], "nb_wf": 2, "list_wf": ["AcceleratedRegionsNF", "scATAC-benchmark"], "list_contrib": ["kaizhang", "keoughkath"], "nb_contrib": 2, "codes": ["\nprocess accRegionsAutosomal {\n\ttag \"Identifying accelerated regions on ${window}\"\n\n\terrorStrategy 'retry'\n\tmaxRetries 3\n\n\tinput:\n\tset val(window), file(phastcons), file(species_maf) from phastConsSplitAutoMaf\n\n\toutput:\n\tstdout into phyloPResultsAuto\n\n\tscript:\n\t  \n\t                    \n\t  \n\tchrom = phastcons.simpleName\n\t\"\"\"\n\t${params.phast_path}./phyloP --features ${phastcons} --msa-format MAF --method LRT --mode ACC --subtree ${params.species_of_interest} -d ${params.random_seed} -g ${params.auto_neutral_model} ${species_maf}\n\t\"\"\"\n}", "\nprocess dim_reduct_archr_2 {\n    container 'kaizhang/archr:1.0.1'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val('ArchR (log(TF-IDF))'), val(data), path(\"reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"ArchR\")\n    library(\"anndata\")\n    set.seed(2022)\n    adata <- read_h5ad(\"${data.anndata}\")\\$T\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    result <- ArchR:::.computeLSI(mat = data,\n        LSIMethod = 2,\n        scaleTo = 10^4,\n        nDimensions = $nDims,\n        binarize = TRUE, \n        outlierQuantiles = NULL,\n        seed = 1\n    )\n    output <- \"reduced_dim.tsv\"\n    write.table(result\\$matSVD, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["keoughkath/AcceleratedRegionsNF/accRegionsAutosomal", "kaizhang/scATAC-benchmark/dim_reduct_archr_2"], "list_wf_names": ["kaizhang/scATAC-benchmark", "keoughkath/AcceleratedRegionsNF"]}, {"nb_reuse": 1, "tools": ["SEED", "Orphadata", "NetStart"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_archr_subsample {\n    container 'kaizhang/archr:1.0.1'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val('ArchR (log(TF-IDF))'), val(data), path(\"reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"ArchR\")\n    library(\"anndata\")\n    set.seed($data.randomSeed)\n    adata <- read_h5ad(\"${data.anndata}\")\\$T\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    sample_size <- trunc(ncol(data) * $data.samplingFraction)\n    n_dim <- min(sample_size - 2, $nDims)\n    reference_data <- data[, sample(ncol(data),size=sample_size, replace=F)]\n\n    ref <- ArchR:::.computeLSI(mat = reference_data,\n        LSIMethod = 2,\n        scaleTo = 10^4,\n        nDimensions = n_dim,\n        binarize = TRUE, \n        outlierQuantiles = NULL,\n        seed = 1\n    )\n    result <- ArchR:::.projectLSI(\n        mat = data,\n        LSI = ref,\n        returnModel = FALSE, \n        verbose = FALSE, \n        tstart = NULL,\n        logFile = NULL\n    )\n    output <- \"reduced_dim.tsv\"\n    write.table(result, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_archr_subsample"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["Orphadata", "CellAssign"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_cistopic {\n    memory '100 GB'\n    cpus 8\n    container 'kaizhang/cistopic:3.0'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val('cisTopic'), val(data), path('reduced_dim.tsv')\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"cisTopic\")\n    library(\"anndata\")\n    library(\"stringr\")\n    set.seed(0)\n    mk_chr_name <- function(x) {\n        r <- str_split(x, \"_|:|-\")[[1]]\n        return(paste(r[1], paste(r[2], r[3], sep=\"-\"), sep=\":\"))\n    }\n    adata <- read_h5ad(\"${data.anndata}\")\\$T\n    mat <- as(adata\\$X, \"CsparseMatrix\")\n    rownames(mat) <- sapply(adata\\$obs_names, mk_chr_name)\n    colnames(mat) <- adata\\$var_names\n    cisTopicObject <- createcisTopicObject(\n        mat,\n        project.name='cisTopic',\n        min.cells = 0,\n        min.regions = 0,\n        is.acc = 0,\n        keepCountsMatrix=F,\n    )\n\n    cisTopicObject <- runWarpLDAModels(cisTopicObject, topic=50,\n        seed=2022, nCores=8, addModels=FALSE\n    )\n    cisTopicObject <- selectModel(cisTopicObject, type='maximum')\n    cellassign <- modelMatSelection(cisTopicObject, 'cell', 'Probability')\n\n    write.table(t(cellassign), file=\"reduced_dim.tsv\", row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_cistopic"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Orphadata"], "nb_own": 2, "list_own": ["kaizhang", "kevbrick"], "nb_wf": 2, "list_wf": ["pipeIt", "scATAC-benchmark"], "list_contrib": ["kaizhang", "kevbrick"], "nb_contrib": 2, "codes": ["\nprocess pairsam {\n\n  tag { bam.name.replaceFirst(\".bt2.qSort.phased.bam\",\"\") }\n\n  input:\n  tuple(val(sample), val(rep), val(bio), path(bam))\n\n  output:\n  tuple(val({ (bio == 'NA') ? \"${sample}\" : \"${sample}_${bio}\"} ), path('*.sam.pairs.gz'), emit: pairs)\n  tuple(val({ (bio == 'NA') ? \"${sample}\" : \"${sample}_${bio}\"} ), path('*.stat.txt'), emit: rep)\n\n  script:\n  def initpairs   = bam.name.replaceFirst(\".bam\",\".initpairs.gz\")\n  def initpairsOK = bam.name.replaceFirst(\".bam\",\".initpairsOK\")\n  def pairs       = bam.name.replaceFirst(\".bam\",\".sam.pairs.gz\")\n  def pairstat    = bam.name.replaceFirst(\".bam\",\".sam.pairsam.stat.txt\")\n  def samcols     = (params.phased)?\"mapq,XA,MD\":\"mapq\"\n  \"\"\"\n  #!/bin/bash\n\n  # Classify Hi-C molecules as unmapped/single-sided/multimapped/chimeric/etc\n  # and output one line per read, containing the following, separated by \\\\v:\n  #  * true-flipped pairs\n  #  * read id\n  #  * type of a Hi-C molecule\n  #  * corresponding sam entries\n\n  samtools view -h ${bam} | pairtools parse -c ${params.fai} \\\n    --output-stats ${pairstat} \\\n    --add-columns ${samcols} \\\n    -o ${initpairs}\n\n  ## The output of pairtools parse does not account for missing fields. This messes\n  ## things up later. Here, we do a small cleanup, replacing missing fields with NA\n\n  ## First, get the header - use perl to prevent the need to go through whole file with grep\n  zcat ${initpairs} |perl -lane 'exit if (\\$_ !~ /^#/);print \\$_' >${initpairsOK}\n\n  ## Get the number of expected columns\n  ncols=`grep ^#columns ${initpairsOK} |perl -lane 'print \\$#F'`\n\n  ## Pad out gaps with NA\n  zcat ${initpairs}|grep -v ^# |\n      perl  -pe 's/\\\\t(?=\\\\t)/\\\\tNA/g' | ##This line does adjacent gaps\n      perl -lane 'print \\$_.(\"\\\\tNA\" x ('\\$ncols'-\\$#F-1))' >>${initpairsOK} ## This line does gaps at the end of the line\n\n  cat ${initpairsOK} | pairtools sort --nproc ${task.cpus} \\\n    --compress-program lz4c \\\n    --tmpdir ${TMPDIR} \\\n    --output ${pairs}\n  \"\"\"\n  }", "\nprocess dim_reduct_signac_3 {\n    container 'kaizhang/signac:1.5'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val('Signac (logTF-logIDF)'), val(data), path(\"reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"anndata\")\n    set.seed(2022)\n    adata <- read_h5ad(\"${data.anndata}\")\\$T\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    result <- Signac:::RunTFIDF.default(data,method = 3)\n    result <- Signac:::RunSVD.default(result, n = 50)\n\n    output <- \"reduced_dim.tsv\"\n    write.table(result@cell.embeddings, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kevbrick/pipeIt/pairsam", "kaizhang/scATAC-benchmark/dim_reduct_signac_3"], "list_wf_names": ["kevbrick/pipeIt", "kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["Orphadata"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess dim_reduct_signac_4 {\n    container 'kaizhang/signac:1.5'\n\n    input:\n      tuple val(data), val(nDims)\n    output:\n      tuple val('Signac (IDF)'), val(data), path(\"reduced_dim.tsv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(\"anndata\")\n    set.seed(2022)\n    adata <- read_h5ad(\"${data.anndata}\")\\$T\n    data <- as(adata\\$X, \"CsparseMatrix\")\n    result <- Signac:::RunTFIDF.default(data,method = 4)\n    result <- Signac:::RunSVD.default(result, n = 50)\n\n    output <- \"reduced_dim.tsv\"\n    write.table(result@cell.embeddings, file=output, row.names=F, col.names=F, sep=\"\\t\")\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/dim_reduct_signac_4"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["RDFScape", "Annot"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess plot_umap {\n    publishDir 'result/umap'\n    input:\n        val report\n    output:\n        file \"*.pdf\"\n\n    \"\"\"\n    #!/usr/bin/env python3\n    import seaborn as sns\n    import pandas as pd\n    import anndata as ad\n    import matplotlib.pyplot as plt\n    import numpy as np\n    data = pd.read_csv(\"${report}\", sep=\"\\t\")\n    data = data[data[\"sample_fraction\"] == 1]\n    dfs = []\n    for index, row in data.iterrows():\n        annot = ad.read(row['anndata']).obs[['cell_annotation']]\n        umap = np.loadtxt(row['UMAP'])\n        annot[\"UMAP1\"] = umap[:, 0]\n        annot[\"UMAP2\"] = umap[:, 1]\n        annot[\"dataset\"] = row[\"dataset\"]\n        annot[\"method\"] = row[\"algorithm\"]\n        dfs.append(annot)\n    df = pd.concat(dfs, ignore_index=False)\n    for name, group in df.groupby(\"dataset\"):\n        plot = sns.FacetGrid(\n            group,\n            col=\"method\",\n            col_wrap=4,\n            hue=\"cell_annotation\",\n            sharex=False,\n            sharey=False,\n        )\n        plot.map(sns.scatterplot, \"UMAP1\", \"UMAP2\", s=2)\n        plot.add_legend(markerscale=3)\n        plt.savefig(name + '.pdf')\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/plot_umap"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 1, "tools": ["pcaMethods"], "nb_own": 1, "list_own": ["kaizhang"], "nb_wf": 1, "list_wf": ["scATAC-benchmark"], "list_contrib": ["kaizhang"], "nb_contrib": 1, "codes": ["\nprocess plot_dim_reduct_report {\n    publishDir 'result'\n\n    input:\n        val report\n    output:\n        file \"*.pdf\"\n\n    \"\"\"\n    #!/usr/bin/env python3\n    import seaborn as sns\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    data = pd.read_csv(\"${report}\", sep=\"\\t\")\n\n    plot = sns.FacetGrid(\n        data[data[\"sample_fraction\"] == 1],\n        col=\"dataset\",\n        gridspec_kws={\"bottom\": 0.4}\n    )\n    plot.map(sns.barplot, \"algorithm\", \"ARI\")\n\n    plot.set_xticklabels(rotation=45, horizontalalignment='right')\n\n    plt.savefig('report.pdf')\n\n    methods = set(data[data[\"sample_fraction\"] < 1][\"algorithm\"])\n    plot = sns.FacetGrid(\n        data.loc[data[\"algorithm\"].isin(methods)],\n        col=\"dataset\",\n        hue=\"algorithm\",\n    )\n    plot.map(sns.lineplot, \"sample_fraction\", \"ARI\",\n        err_style=\"bars\",\n        ci=95,\n        markers=True,\n        marker=\"o\",\n        dashes=False,\n    )\n    plot.add_legend()\n    plt.savefig('subsample.pdf')\n    \"\"\"\n}"], "list_proc": ["kaizhang/scATAC-benchmark/plot_dim_reduct_report"], "list_wf_names": ["kaizhang/scATAC-benchmark"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC", "Minimap2"], "nb_own": 2, "list_own": ["katharily", "kevbrick"], "nb_wf": 2, "list_wf": ["pe_stacks_pipeline", "pipeIt"], "list_contrib": ["katharily", "kevbrick"], "nb_contrib": 2, "codes": ["\nprocess fastqc_bz2 {\n    conda 'fastqc'\n    publishDir FASTQC_DIR, mode: 'copy', pattern: '*.html'\n    publishDir FASTQC_DIR, mode: 'copy', pattern: '*.zip'\n    publishDir LOG_DIR,    mode: 'copy', pattern: '*.log'\n    maxForks 8\n    input:\n        file read from ch_fastqc_input.flatten()\n    output:\n        file '*.zip'  into ch_fastqc_bz2_zip\n        file '*.html' into ch_fastqc_bz2_html\n        file '*.log'  into ch_fastqc_bz2_log\n    script:\n        \"\"\"\n        mkdir -p ${FASTQC_DIR}\n        fastqc ${read} 2> fastqc_${read.simpleName}_raw.log\n        \"\"\"\n}", "process minimap2SR {\n  label 'aligner'\n\n  time { fq.size() < 1000000000 ? 0.5.hour : 1.hour * (1+fq.size()/1000000000 * task.attempt) }\n\n  input:\n  file(fq)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n  \"\"\"\n  minimap2 -ax sr \\\n    -t ${task.cpus} \\\n    ${params.genome_mm2idx} \\\n    ${fq} >tmp.sam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam \\\n            I=tmp.sam \\\n            O=${bam} \\\n            SO=coordinate \\\n\t\t\t\t\t\tTMP_DIR=\"\\$TMPDIR\" \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }"], "list_proc": ["katharily/pe_stacks_pipeline/fastqc_bz2", "kevbrick/pipeIt/minimap2SR"], "list_wf_names": ["katharily/pe_stacks_pipeline", "kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["katharily"], "nb_wf": 1, "list_wf": ["pe_stacks_pipeline"], "list_contrib": ["katharily"], "nb_contrib": 1, "codes": ["\nprocess fastqc_gz {\n    conda 'fastqc'\n    publishDir FASTQC_DIR, mode: 'copy', pattern: '*.html'\n    publishDir FASTQC_DIR, mode: 'copy', pattern: '*.zip'\n    publishDir LOG_DIR,    mode: 'copy', pattern: '*.log'\n    maxForks 8\n    input:\n        file read from ch_fastqc_gz_input.flatten()\n    output:\n        file '*.zip'  into ch_fastqc_gz_zip\n        file '*.html' into ch_fastqc_gz_html\n        file '*.log'  into ch_fastqc_gz_log\n    script:\n        \"\"\"\n        echo ${read}\n        mkdir -p ${FASTQC_DIR}\n        fastqc ${read} 2> fastqc_${read.simpleName}_raw.log\n        \"\"\"\n}"], "list_proc": ["katharily/pe_stacks_pipeline/fastqc_gz"], "list_wf_names": ["katharily/pe_stacks_pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["katharily"], "nb_wf": 1, "list_wf": ["pe_stacks_pipeline"], "list_contrib": ["katharily"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    conda 'multiqc'\n    publishDir MULTIQC_DIR, mode: 'copy', pattern: '*.html'\n    publishDir MULTIQC_DIR, mode: 'copy', pattern: 'multiqc_data/*'\n    publishDir LOG_DIR,     mode: 'copy', pattern: '*.log'\n    input:\n        file fastqc_zip_files from ch_fastqc_zip.collect()\n        file fastqc_zip_files_cleaned from ch_fastqc_zip_cleaned.collect()\n    output:\n        file '*.html' into ch_multiqc_report_output\n        file 'multiqc_data/*' into ch_multiqc_data_output\n        file '*.log' into ch_multiqc_log\n    when:\n        !params.mappingOnly\n    script:\n    \"\"\"\n    multiqc -f ${fastqc_zip_files} ${fastqc_zip_files_cleaned} 2> multiqc.log\n    \"\"\"\n}"], "list_proc": ["katharily/pe_stacks_pipeline/multiqc"], "list_wf_names": ["katharily/pe_stacks_pipeline"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["katharily"], "nb_wf": 1, "list_wf": ["pe_stacks_pipeline"], "list_contrib": ["katharily"], "nb_contrib": 1, "codes": ["\nprocess bowtie_mapping {\n    conda 'bowtie2'\n    publishDir SAM_DIR, mode: 'copy', pattern: '*.sam'\n    publishDir LOG_DIR, mode: 'copy', pattern: '*.log'\n    maxForks 8\n    input:\n        set id, file(reads), file(reference_file), file(indexes_a) from ch_bowtie_input.combine(ch_references_mapping.merge(ch_bowtie_index.collect().toList()))\n    output:\n        file '*.sam' into ch_bowtie_mapping\n        file '*.log' into ch_bowtie_mapping_log\n    shell:\n    '''\n    index_dir=\"$(basename !{reference_file} .fna.gz)\"\n    bowtie2 --threads 20 --very-sensitive-local -x $index_dir -1 !{reads[0]} -2 !{reads[1]} -S !{reads[0].simpleName}_${index_dir}_mapping.sam 2> bowtie_mapping_!{reads[0].simpleName}.log\n    '''\n}"], "list_proc": ["katharily/pe_stacks_pipeline/bowtie_mapping"], "list_wf_names": ["katharily/pe_stacks_pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["katharily"], "nb_wf": 1, "list_wf": ["pe_stacks_pipeline"], "list_contrib": ["katharily"], "nb_contrib": 1, "codes": ["\nprocess samtools_sort {\n    conda 'samtools'\n    publishDir SORTED_BAM_DIR, mode: 'copy', pattern: '*_sorted.bam'\n    publishDir LOG_DIR,        mode: 'copy', pattern: '*.log'\n    maxForks 2\n    input:\n        file bam_file from ch_samtools_unmapped_output\n    output:\n        file '*_sorted.bam' into ch_samtools_sorted_output\n        file '*.log'        into ch_samtools_sorted_log\n    script:\n    \"\"\"\n    mkdir -p ${SORTED_BAM_DIR}\n    samtools sort --threads 10 -o ${bam_file.baseName}_sorted.bam ${bam_file.baseName}.bam 2> samtools_sort_${bam_file.baseName}.log\n    \"\"\"\n}"], "list_proc": ["katharily/pe_stacks_pipeline/samtools_sort"], "list_wf_names": ["katharily/pe_stacks_pipeline"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["katharily"], "nb_wf": 1, "list_wf": ["pe_stacks_pipeline"], "list_contrib": ["katharily"], "nb_contrib": 1, "codes": ["\nprocess bowtie_index {\n    conda 'bowtie2'\n    publishDir INDEX_DIR, mode: 'copy', pattern: '*.bt2l'\n    publishDir INDEX_DIR, mode: 'copy', pattern: '*.log'\n    maxForks 24\n    input:\n        file reference_file from ch_references_index_input\n    output:\n        file '*.bt2l' into ch_bowtie_index_output\n        file '*.log'  into ch_bowtie_log\n    shell:\n    '''\n    mkdir -p !{INDEX_DIR}\n    reference_name=\"$(basename !{reference_file} .fna.gz)\"\n    bowtie2-build --threads 24 --large-index !{reference_file} $reference_name 2> bowtie_index_${reference_name}.log\n    '''\n}"], "list_proc": ["katharily/pe_stacks_pipeline/bowtie_index"], "list_wf_names": ["katharily/pe_stacks_pipeline"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["kazumaxneo"], "nb_wf": 1, "list_wf": ["Nextflow_test_code"], "list_contrib": ["kazumaxneo"], "nb_contrib": 1, "codes": ["\nprocess preprocessing {\n    cpus 2\n    tag \"$pair_id\"\n    input:\n    tuple val(pair_id), path(reads) from read_pairs_ch\n\n    output:\n    set pair_id, \"${pair_id}_QT_R1.fq.gz\" into read1_ch\n    set pair_id, \"${pair_id}_QT_R2.fq.gz\" into read2_ch\n\n    \"\"\"\n    fastp -i ${pair_id}_R1.fq.gz -I ${pair_id}_R2.fq.gz \\\n    -o ${pair_id}_QT_R1.fq.gz -O ${pair_id}_QT_R2.fq.gz \\\n    -h ${pair_id}_fastp.html -j ${pair_id}_fastp.json \\\n    -q 30 -n 5 -t 1 -T 1 -l 20 -w ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["kazumaxneo/Nextflow_test_code/preprocessing"], "list_wf_names": ["kazumaxneo/Nextflow_test_code"]}, {"nb_reuse": 1, "tools": ["breseq"], "nb_own": 1, "list_own": ["kazumaxneo"], "nb_wf": 1, "list_wf": ["Nextflow_test_code"], "list_contrib": ["kazumaxneo"], "nb_contrib": 1, "codes": ["\nprocess breseq {\n\n    tag \"$pair_id\"\n    publishDir params.outdir2, mode: 'copy'\n    cpus 4\n    input:\n    path gff from params.gff\n    tuple val(pair_id), path(reads1) from read1_ch\n    tuple val(pair_id), path(reads2) from read2_ch\n\n    output:\n    set pair_id, \"${pair_id}\"\n\n\n    \"\"\"\n    breseq -j ${task.cpus} -o ${pair_id} -r $gff $reads1 $reads2\n    rm -rf ${pair_id}/0* ${pair_id}/data\n    mv ${pair_id}/output/* ${pair_id}/\n    mv ${pair_id}/output.gd ${pair_id}/${pair_id}.gd\n    mv ${pair_id}/output.vcf ${pair_id}/${pair_id}.vcf\n    mv ${pair_id}/summary.html ${pair_id}/${pair_id}.html\n    mv ${pair_id}/summary.json ${pair_id}/${pair_id}.json\n    \"\"\"\n}"], "list_proc": ["kazumaxneo/Nextflow_test_code/breseq"], "list_wf_names": ["kazumaxneo/Nextflow_test_code"]}, {"nb_reuse": 1, "tools": ["SAMtools", "elPrep", "Minimap2"], "nb_own": 1, "list_own": ["kazumaxneo"], "nb_wf": 1, "list_wf": ["Nextflow_test_code"], "list_contrib": ["kazumaxneo"], "nb_contrib": 1, "codes": ["\nprocess mapping {\n\n    tag \"$pair_id\"\n    publishDir params.outdir2, mode: 'copy'\n    cpus 3\n    input:\n    path gff from params.gff\n    path fasta from params.fasta\n    tuple val(pair_id), path(reads1) from read1_ch\n    tuple val(pair_id), path(reads2) from read2_ch\n\n    output:\n    set pair_id, \"${pair_id}.bam\" into bam_ch\n    set pair_id, \"${pair_id}*flagstats\"\n\n    \"\"\"\n    minimap2 -ax sr -t ${task.cpus} -R \"@RG\\\\tID:${pair_id}\\\\tLB:Y\\\\tSM:${pair_id}\\\\tPL:ILLUMINA\" $fasta $reads1 $reads2 \\\n    | elprep filter /dev/stdin ${pair_id}.bam \\\n    --mark-duplicates \\\n    --remove-duplicates \\\n    --filter-mapping-quality 0 \\\n    --clean-sam \\\n    --nr-of-threads ${task.cpus} \\\n    --sorting-order coordinate \\\n    --filter-unmapped-reads-strict\n    samtools flagstats ${pair_id}.bam > ${pair_id}.bam_flagstats\n    \"\"\"\n}"], "list_proc": ["kazumaxneo/Nextflow_test_code/mapping"], "list_wf_names": ["kazumaxneo/Nextflow_test_code"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FreeBayes"], "nb_own": 1, "list_own": ["kazumaxneo"], "nb_wf": 1, "list_wf": ["Nextflow_test_code"], "list_contrib": ["kazumaxneo"], "nb_contrib": 1, "codes": ["\nprocess variantcall {\n\n    tag \"$pair_id\"\n    publishDir params.outdir4, mode: 'copy'\n    input:\n    path fasta from params.fasta\n    tuple val(pair_id), path(bamfile) from bam_ch\n    output:\n    set pair_id, \"${pair_id}.vcf*\"\n\n    \"\"\"\n    samtools index ${pair_id}.bam\n    freebayes -F 0.2 -u -p 2 -f $fasta $bamfile > ${pair_id}.vcf\n    bgzip ${pair_id}.vcf\n    tabix -p vcf ${pair_id}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["kazumaxneo/Nextflow_test_code/variantcall"], "list_wf_names": ["kazumaxneo/Nextflow_test_code"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["keng404"], "nb_wf": 1, "list_wf": ["nextflow_test"], "list_contrib": ["keng404"], "nb_contrib": 1, "codes": [" process remap_to_hla {\n        label 'process_medium'\n        publishDir \"${params.outdir}\", mode: params.publish_dir_mode, overwrite: false\n        input:\n        path(data_index) from params.base_index_path\n        set val(pattern), file(bams) from input_data\n        output:\n        set val(pattern), \"mapped_{1,2}.sorted.bam\" into fished_reads\n        file(\"mapped_{1,2}.sorted.bam.bai\") into fished_reads_bai\n        script:\n        def full_index = \"$data_index/$base_index_name\"\n        if (params.single_end)\n            \"\"\"\n            samtools bam2fq $bams > output_1.fastq\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index output_1.fastq > output_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -b1 output_1.bam > mapped_1.bam\n            samtools sort -@ ${task.cpus} -o mapped_1.sorted.bam mapped_1.bam\n            samtools index mapped_1.sorted.bam\n            \"\"\"\n        else\n            \"\"\"\n            samtools view -@ ${task.cpus} -h -f 0x40 $bams > output_1.bam\n            samtools view -@ ${task.cpus} -h -f 0x80 $bams > output_2.bam\n            samtools bam2fq output_1.bam > output_1.fastq\n            samtools bam2fq output_2.bam > output_2.fastq\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index output_1.fastq output_2.fastq > output.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x40 -b1 output.bam > mapped_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x80 -b1 output.bam > mapped_2.bam\n            samtools sort -@ ${task.cpus} -o mapped_1.sorted.bam mapped_1.bam \n            samtools sort -@ ${task.cpus} -o mapped_2.sorted.bam mapped_2.bam\n            samtools index mapped_1.sorted.bam mapped_1.sorted.bam.bai\n            samtools index mapped_2.sorted.bam mapped_2.sorted.bam.bai\n            \"\"\"\n    }"], "list_proc": ["keng404/nextflow_test/remap_to_hla"], "list_wf_names": ["keng404/nextflow_test"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["keng404"], "nb_wf": 1, "list_wf": ["nextflow_test"], "list_contrib": ["keng404"], "nb_contrib": 1, "codes": [" process pre_map_hla {\n        label 'process_medium'\n        publishDir \"${params.outdir}\", mode: params.publish_dir_mode, overwrite: false\n        input:\n        path(data_index) from params.base_index_path\n        set val(pattern), file(reads) from raw_reads\n\n        output:\n        set val(pattern), \"mapped_{1,2}.sorted.bam\" into fished_reads\n        file(\"mapped_{1,2}.sorted.bam.bai\") into fished_reads_bai\n        script:\n        def full_index = \"$data_index/$base_index_name\"\n        if (params.single_end)\n            \"\"\"\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index $reads > output_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -b1 output_1.bam > mapped_1.bam\n            samtools sort -@ ${task.cpus} -o mapped_1.sorted.bam mapped_1.bam \n            samtools index mapped_1.sorted.bam \n            \"\"\"\n        else\n            \"\"\"\n            yara_mapper -e 3 -t ${task.cpus} -f bam $full_index $reads > output.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x40 -b1 output.bam > mapped_1.bam\n            samtools view -@ ${task.cpus} -h -F 4 -f 0x80 -b1 output.bam > mapped_2.bam\n            samtools sort -@ ${task.cpus} -o mapped_1.sorted.bam mapped_1.bam\n            samtools sort -@ ${task.cpus} -o mapped_2.sorted.bam mapped_2.bam \n            samtools index mapped_1.sorted.bam mapped_1.sorted.bam.bai\n            samtools index mapped_2.sorted.bam mapped_2.sorted.bam.bai\n            \"\"\"\n    }"], "list_proc": ["keng404/nextflow_test/pre_map_hla"], "list_wf_names": ["keng404/nextflow_test"]}, {"nb_reuse": 1, "tools": ["PhyloPat"], "nb_own": 1, "list_own": ["keoughkath"], "nb_wf": 1, "list_wf": ["AcceleratedRegionsNF"], "list_contrib": ["keoughkath"], "nb_contrib": 1, "codes": ["\nprocess accRegionsNonAutosomal {\n\ttag \"Identifying accelerated regions on ${window}\"\n\n\terrorStrategy 'retry'\n\tmaxRetries 3\n\n\tinput:\n\tset val(window), file(phastcons), file(species_maf) from phastConsSplitNonAutoMaf\n\n\toutput:\n\tstdout into phyloPResultsNonAuto\n\n\tscript:\n\t  \n\t                    \n\t  \n\tchrom = phastcons.simpleName\n\t\"\"\"\n\t${params.phast_path}./phyloP --features ${phastcons} --msa-format MAF --method LRT --mode ACC --subtree ${params.species_of_interest} -d ${params.random_seed} -g ${params.nonauto_neutral_model}${chrom}.neutral.mod ${species_maf}\n\t\"\"\"\n\n}"], "list_proc": ["keoughkath/AcceleratedRegionsNF/accRegionsNonAutosomal"], "list_wf_names": ["keoughkath/AcceleratedRegionsNF"]}, {"nb_reuse": 4, "tools": ["SAMtools", "MultiQC", "FastQC", "Prokka", "STAR"], "nb_own": 4, "list_own": ["mpieva", "replikation", "kerimoff", "likelet"], "nb_wf": 4, "list_wf": ["docker_pipelines", "quicksand", "circPipe", "qtlquant"], "list_contrib": ["replikation", "merszym", "kerimoff", "dengshuang0116", "bioinformatist", "wjv", "weiqijin", "mult1fractal", "likelet"], "nb_contrib": 9, "codes": ["\nprocess filterUnmapped {\n    tag \"$rg\"\n\n    input:\n    set rg, 'input.bam' from filter_unmapped_in\n\n    output:\n    set rg, 'output.bam' into filter_unmapped_out\n\n    script:\n    \"\"\"\n    samtools view -b -u -F 4 -o output.bam input.bam\n    \"\"\"\n}", "process prokka {\n      publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.gff\"\n      publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.gbk\"\n      label 'prokka'\n    input:\n      tuple val(name), file(assembly) \n    output:\n      tuple val(name), file(\"${name}.gff\"), file(\"${name}.gbk\")\n    script:\n      \"\"\"\n    \tprokka --cpus ${task.cpus} --outdir output --prefix annotation ${assembly}\n      mv output/annotation.gff ${name}.gff\n      mv output/annotation.gbk ${name}.gbk\n      \"\"\"\n}", " process RUN_STAR{\n        tag \"$sampleID\"\n        publishDir \"${params.outdir}/Alignment/STAR\", mode: 'link', overwrite: true\n\n        input:\n        tuple val(sampleID),  file(query_file) from Fastpfiles_star\n        file index from starindex.collect()\n\n        output:\n        tuple val(sampleID),  file ('*.junction') into starfiles\n        file ('*.out') into Star_multiqc\n\n\n\n        shell:\n        if(params.skip_fastp){\n                if(params.singleEnd){\n                \"\"\"\n                STAR \\\n                --runThreadN ${task.cpus} \\\n                --chimSegmentMin 10 \\\n                --genomeDir ${star_run_index} \\\n                --readFilesIn ${query_file} \\\n                --readFilesCommand zcat \\\n                --outFileNamePrefix star_${sampleID}_ \\\n                --outSAMtype None\n                \"\"\"\n            }else{\n                \"\"\"\n                STAR \\\n                --runThreadN ${task.cpus} \\\n                --chimSegmentMin 10 \\\n                --genomeDir ${star_run_index} \\\n                --readFilesCommand zcat \\\n                --readFilesIn ${query_file[0]} ${query_file[1]} \\\n                --outFileNamePrefix star_${sampleID}_ \\\n                --outSAMtype None\n                \"\"\"\n            }\n        }else{\n            if(params.singleEnd){\n                \"\"\"\n                STAR \\\n                --runThreadN ${task.cpus} \\\n                --chimSegmentMin 10 \\\n                --genomeDir ${star_run_index} \\\n                --readFilesIn ${query_file} \\\n                --outFileNamePrefix star_${sampleID}_ \\\n                --outSAMtype None\n                \"\"\"\n            }else{\n                \"\"\"\n                STAR \\\n                --runThreadN ${task.cpus} \\\n                --chimSegmentMin 10 \\\n                --genomeDir ${star_run_index} \\\n                --readFilesIn ${query_file[0]} ${query_file[1]} \\\n                --outFileNamePrefix star_${sampleID}_ \\\n                --outSAMtype None\n                \"\"\"\n            }   \n        }\n        \n\n    }", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["mpieva/quicksand/filterUnmapped", "replikation/docker_pipelines/prokka", "likelet/circPipe/RUN_STAR", "kerimoff/qtlquant/get_software_versions"], "list_wf_names": ["replikation/docker_pipelines", "mpieva/quicksand", "kerimoff/qtlquant", "likelet/circPipe"]}, {"nb_reuse": 12, "tools": ["Flye", "SAMtools", "sourmash", "FreeBayes", "MultiQC", "Bowtie", "DEPTH", "SAMBLASTER", "SyConn", "fastPHASE"], "nb_own": 10, "list_own": ["sripaladugu", "replikation", "marcodelapierre", "mpieva", "kerimoff", "quinlan-lab", "zamanianlab", "vpeddu", "likelet", "stevekm"], "nb_wf": 12, "list_wf": ["quicksand", "nextflow-demos", "nextflow-pipeline-demo", "qtlquant", "preon", "illumina-nf", "demo-shpc-nf", "docker_pipelines", "germline_somatic", "circPipe", "ev-meta", "Core_RNAseq-nf"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "kerimoff", "davidmasp", "replikation", "merszym", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "chenthorn", "mult1fractal", "stevekm", "lescai", "szilvajuhos", "pditommaso", "FriederikeHanssen", "wjv", "likelet", "nf-core-bot", "marcodelapierre", "maxulysse", "ggabernet", "apeltzer", "mzamanian", "weiqijin", "brwnj", "dengshuang0116", "bioinformatist", "vpeddu", "adrlar", "vsoch"], "nb_contrib": 35, "codes": ["\nprocess sam_post_map_refs {\n  tag \"${dir}/${name}/${seqid}\"\n  publishDir \"${dir}/${params.outprefix}${name}${params.hash_cascade}/${seqid}\", mode: 'copy'\n                                                                                                                                           \n\n  input:\n  tuple val(dir), val(name), val(seqid), path('mapped_refseq_unsorted.sam')\n\n  output:\n                                                               \n  tuple val(seqid), val(dir), val(name), path('mapped_refseq.bam'), path('mapped_refseq.bam.bai'), emit: bam\n  tuple val(dir), val(name), val(seqid), path('depth_refseq.dat'), emit: depth\n\n  script:\n  \"\"\"\n  samtools \\\n    view -b -o mapped_refseq_unsorted.bam \\\n    mapped_refseq_unsorted.sam\n\n  samtools \\\n    sort -o mapped_refseq.bam \\\n    mapped_refseq_unsorted.bam\n\n  samtools \\\n    index mapped_refseq.bam\n\n  samtools \\\n    depth -aa mapped_refseq.bam \\\n    >depth_refseq.dat\n  \"\"\"\n}", "process sourmashmeta {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"metagenomic-composition.txt\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(fasta) \n      file(database) \n    output:\n      tuple val(name), file(\"metagenomic-composition.txt\")\n    script:\n      if (params.fasta)\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o ${name}.sig \n      sourmash gather  ${name}.sig ${database} --ignore-abundance -o metagenomic-composition.txt\n      \"\"\"\n      else if (params.fastq)\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 --track-abundance ${fasta} -o ${name}.sig \n      sourmash gather  ${name}.sig ${database} -o metagenomic-composition.txt\n      \"\"\"\n}", " process RUN_Find_circ{\n        tag \"$sampleID\"\n        publishDir \"${params.outdir}/circRNA_Identification/Find_circ\", mode: 'copy', overwrite: true\n\n        input:\n        tuple val(sampleID), file (query_file) from Bowtie2files\n        file genomefile\n        file index from Bowtie2index_fc.collect()\n\n        output:\n        tuple val(sampleID),file ('*splice_sites.bed') into find_circfiles\n\n\n\n        shell:\n        \"\"\"     \n         source activate find_circ\n        unmapped2anchors.py ${query_file} \\\n        | gzip \\\n        > find_circ_${sampleID}_anchors.qfa.gz\n        \n        bowtie2 \\\n            -p ${task.cpus} \\\n            --reorder \\\n            --mm \\\n            --score-min=C,-15,0 \\\n            -q \\\n            -x ${bowtie2_run_index} \\\n            -U find_circ_${sampleID}_anchors.qfa.gz \\\n        | find_circ.py \\\n            -G ${genomefile} \\\n            -p ${sampleID}_ \\\n            -s find_circ_${sampleID}_stats.sites.log \\\n            -n find_circ \\\n            -R find_circ_${sampleID}_spliced_reads.fa \\\n            > find_circ_${sampleID}_splice_sites.bed   \n\n        # remove temp file for reduce the usage of the disk .\n        rm find_circ_${sampleID}_spliced_reads.fa \n        rm find_circ_${sampleID}_anchors.qfa.gz\n        \"\"\"\n    }", "\nprocess MetaFlye { \npublishDir \"${params.OUTPUT}/MetaFlye/${base}\", mode: 'symlink', overwrite: true\ncontainer \"quay.io/biocontainers/flye:2.9--py27h6a42192_0\"\nbeforeScript 'chmod o+rw .'\nerrorStrategy 'ignore'\ncpus 16\ninput: \n    tuple val(base), file(unassigned_fastq)\noutput: \n    tuple val(\"${base}\"), file(\"${base}.flye.fasta.gz\")\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \n\nflye --nano-corr ${unassigned_fastq} \\\n    --out-dir ${base}.flye \\\n    -t ${task.cpus} \\\n    --meta \n\nif [[ -f ${base}.flye/assembly.fasta ]]\nthen\n    echo \"flye assembled reads\"\n    mv ${base}.fly e/assembly.fasta ${base}.flye.fasta\nelse\n    echo \"flye did not assemble reads\" \n    mv ${unassigned_fastq} ${base}.flye.fasta\nfi\n\nmv ${base}.flye/assembly.fasta ${base}.flye.fasta\n\ngzip ${base}.flye.fasta\n\"\"\"\n}", "\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess update_dbs {\n    tag \"${name}\"\n    publishDir \"${params.output_dir}/${name}\", overwrite: true\n\n    input:\n    set val(name), val(url), file(pic), file(db) from people_db_files\n\n    output:\n    file(\"${db}\") into people_updated_dbs\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n    import sqlite3\n    conn = sqlite3.connect(\"${db}\")\n\n    # insert the image file into the database, with metadata\n    with open(\"${pic}\", 'rb') as input_file:\n            blob = input_file.read()\n            sql = '''\n            INSERT INTO TBL1 (NAME, URL, FILENAME, PIC) VALUES(?, ?, ?, ?);\n            '''\n            conn.execute(sql,[\"${name}\", \"${url}\", \"${pic}\", sqlite3.Binary(blob)])\n            conn.commit()\n    conn.close()\n    \"\"\"\n}", "\nprocess mapBwa {\n    publishDir 'out', mode: 'copy', saveAs: {out_bam}, pattern: '*.bam'\n    tag \"$rg:$family:$species\"\n\n    input:\n    set rg, order, family, species, \"input.bam\", genomes from pre_bwa\n\n    output:\n    set order, family, rg, species, 'output.bam', taxon into mapped_bam\n    set order, family, rg, species, stdout into (mapped_count, mapping_data)\n\n    script:\n    if(params.taxlvl == 'o'){\n    taxon = order\n    } else {\n    taxon = family\n    }\n    if(params.byrg){\n        out_bam = \"${rg}/${taxon}/aligned/${family}.${species}.bam\"\n    } else {\n        out_bam = \"${taxon}/aligned/${rg}.${family}.${species}.bam\"\n    }\n    \"\"\"\n    samtools sort -n -l0 input.bam \\\n    | $params.bwa bam2bam -g ${genomes}/$family/\\\"${species}.fasta\\\"  -n 0.01 -o 2 -l 16500 --only-aligned - \\\n    | samtools view -b -u -q $params.quality \\\n    | samtools sort -l $params.level -o output.bam\n    samtools view -c output.bam\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess bam_ra_rc_gatk {\n                                               \n                                                                                                                                            \n                                                                                                                                             \n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam_dd_ra_rc_gatk\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n\n    input:\n    set val(sample_ID), file(sample_bam), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(gatk_1000G_phase1_indels_vcf), file(mills_and_1000G_gold_standard_indels_vcf), file(dbsnp_ref_vcf) from samples_dd_bam_ref_gatk\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.ra.rc.bam\"), file(\"${sample_ID}.dd.ra.rc.bam.bai\") into samples_dd_ra_rc_bam, samples_dd_ra_rc_bam2, samples_dd_ra_rc_bam3\n    file \"${sample_ID}.intervals\"\n    file \"${sample_ID}.table1.txt\"\n    file \"${sample_ID}.table2.txt\"\n    file \"${sample_ID}.csv\"\n    file \"${sample_ID}.pdf\"\n\n    script:\n    \"\"\"\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T RealignerTargetCreator \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    -nt \\${NSLOTS:-1} \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.intervals\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T IndelRealigner \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --maxReadsForRealignment 50000 \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -targetIntervals \"${sample_ID}.intervals\" \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.dd.ra.bam\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.table1.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --out \"${sample_ID}.table2.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T AnalyzeCovariates \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -before \"${sample_ID}.table1.txt\" \\\n    -after \"${sample_ID}.table2.txt\" \\\n    -csv \"${sample_ID}.csv\" \\\n    -plots \"${sample_ID}.pdf\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T PrintReads \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.dd.ra.rc.bam\"\n\n    samtools index \"${sample_ID}.dd.ra.rc.bam\"\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n    \n    input:\n    file('data*/*') from quant_ch.mix(fastqc_ch).collect()\n    file(config) from multiqc_file\n\n    output:\n    file('multiqc_report.html') optional true\n\n    script:\n    \"\"\"\n    cp $config/* .\n    echo \"custom_logo: \\$PWD/logo.png\" >> multiqc_config.yaml\n    multiqc -v .\n    \"\"\"\n}", "\nprocess trim_reads {\n\n   cpus large_core\n   tag { id }\n   publishDir \"${output}/trim_stats/\", mode: 'copy', pattern: '*.html'\n   publishDir \"${output}/trim_stats/\", mode: 'copy', pattern: '*.json'\n\n   input:\n       tuple val(id), file(reads) from fqs\n\n   output:\n       tuple id_out, file(\"${id_out}.fq.gz\") into trimmed_fqs\n       tuple file(\"*.html\"), file(\"*.json\")  into trim_log\n\n  script:\n      id_out = id.replace('.fastq.gz', '')\n\n   \"\"\"\n       fastp -i $reads -o ${id_out}.fq.gz -y -l 15 -h ${id_out}.html -j ${id_out}.json\n   \"\"\"\n}", "\nprocess run_freebayes {\n    tag \"$interval\"\n\n    input:\n    file(aln) from alignments_ch.collect()\n    file(idx) from alignment_indexes_ch.collect()\n    each interval from intervals_ch\n    file(fasta)\n    file(faidx)\n\n    output:\n                                               \n    file(\"${params.project}_${interval.replaceAll(~/\\:|\\-|\\*/, \"_\")}.vcf.gz\") into (unannotated_ch, vcf_ch)\n\n    script:\n    \"\"\"\n    freebayes \\\n        --fasta-reference ${fasta} \\\n        ${params.freebayesoptions} \\\n        --region ${interval} \\\n        ${aln.collect { \"--bam $it\" }.join(\" \")} \\\n        | bgzip -c > ${params.project}_${interval.replaceAll(~/\\:|\\-|\\*/, \"_\")}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["marcodelapierre/illumina-nf/sam_post_map_refs", "replikation/docker_pipelines/sourmashmeta", "likelet/circPipe/RUN_Find_circ", "vpeddu/ev-meta/MetaFlye", "sripaladugu/germline_somatic/GroupReadsByUmi", "stevekm/nextflow-demos/update_dbs", "mpieva/quicksand/mapBwa", "kerimoff/qtlquant/multiqc", "stevekm/nextflow-pipeline-demo/bam_ra_rc_gatk", "marcodelapierre/demo-shpc-nf/multiqc", "zamanianlab/Core_RNAseq-nf/trim_reads", "quinlan-lab/preon/run_freebayes"], "list_wf_names": ["replikation/docker_pipelines", "sripaladugu/germline_somatic", "marcodelapierre/illumina-nf", "vpeddu/ev-meta", "mpieva/quicksand", "kerimoff/qtlquant", "stevekm/nextflow-pipeline-demo", "zamanianlab/Core_RNAseq-nf", "likelet/circPipe", "stevekm/nextflow-demos", "marcodelapierre/demo-shpc-nf", "quinlan-lab/preon"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess fastqC {\n  tag {\"${params.name}\"}\n\n  publishDir \"${params.outdir}/reports\",  mode: 'copy', overwrite: true\n\n  input:\n  path(fq)\n\n  output:\n  path('*fastqc_report.*', emit: rep)\n\n  when:\n  !(\"${workflow.scriptName}\" =~ /commit/)\n\n  script:\n  \"\"\"\n  for f in *fastq*; do\n    if [[ \"\\$f\" =~ \".gz\" ]]; then\n      zcat \\$f |head -n 10000000 >subset.fastq\n      name=\\${f/.(fastq|fq).gz/}\n    else\n      head -n 10000000 \\$f >subset.fastq\n      name=\\${f/.(fastq|fq).gz/}\n    fi\n\n    fastqc -t ${task.cpus} subset.fastq\n\n    mv subset_fastqc.html \\$name\".fastqc_report.html\"\n    mv subset_fastqc.zip  \\$name\".fastqc_report.zip\"\n  done\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/fastqC"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bwa4D {\n  tag { fq1 }\n\n  publishDir \"${params.outdir}/bam\", mode: 'copy', overwrite: true, pattern: '*bam', enabled: params.saveBAM\n\n  input:\n  tuple(val(id), val(sample), val(rep), val(bio), path(fq1), path(fq2))\n\n  output:\n  tuple(val(sample), val(rep), val(bio), path('*BWAinit.bam'))\n\n  script:\n  def nm = fq1.name.replaceAll(\"R1.(\\\\d+).fastq.gz\",\"\\$1\")\n                                                        \n  \"\"\"\n  #ln -s ${fq1} fastq1.gz\n  #ln -s ${fq2} fastq2.gz\n\n  bwa mem \\\n    -t ${task.cpus} \\\n    -SP5M \\\n    ${params.bwaidx} \\\n    ${fq1} ${fq2} | samtools view -Shb -o ${nm}.BWAinit.bam -\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bwa4D"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bowtie2_end2end {\n  tag { fq1 }\n\n  input:\n  tuple(val(id), val(sample), val(rep), val(bio), path(fq1), path(fq2))\n  val(trim)\n  val(removeUnaligned)\n\n  output:\n  tuple(val(sample), val(rep), val(bio), val(fq1.name), val('R1'), path('*R1*endtoend.bam'), emit: bamR1)\n  tuple(val(sample), val(rep), val(bio), val(fq1.name), val('R2'), path('*R2*endtoend.bam'), emit: bamR2)\n  tuple(val(sample), val(rep), val(bio), val(fq1.name), val('R1'), path('*R1*.unmap.fastq'), emit: unmappedR1)\n  tuple(val(sample), val(rep), val(bio), val(fq1.name), val('R2'), path('*R2*.unmap.fastq'), emit: unmappedR2)\n\n  script:\n  def nm = fq1.name.replaceAll(\"R1.(\\\\d+).fastq\",\"\\$1\")\n  \"\"\"\n  if [ \"${trim}\" -gt 0 ]; then\n    zcat ${fq1} |perl -lane 'chomp; \\$l++; if (\\$l == 1 || \\$l == 3){\\$out = \\$_}else{\\$out = substr(\\$_,1,${trim})}; print \\$out; \\$l=(\\$l==4)?0:\\$l' >R1.fastq\n    zcat ${fq2} |perl -lane 'chomp; \\$l++; if (\\$l == 1 || \\$l == 3){\\$out = \\$_}else{\\$out = substr(\\$_,1,${trim})}; print \\$out; \\$l=(\\$l==4)?0:\\$l' >R2.fastq\n    name=\"${nm}_BOWTIE_trim${trim}\"\n  else\n    ln -s ${fq1} R1.fastq.gz\n    ln -s ${fq2} R2.fastq.gz\n    name=\"${nm}_BOWTIE${trim}\"\n  fi\n\n  if [ ${removeUnaligned} -eq 0 ]; then\n    bowtie2 --rg-id BMG --rg SM:bowtiealn \\\\\n        \t--very-sensitive -L 30 --score-min L,-0.6,-0.2 --end-to-end --reorder \\\\\n          -p ${task.cpus} \\\\\n          -x ${params.bt2idx} \\\\\n          --un \\${name}\".bowtie.R1.unmap.fastq\" \\\\\n        \t-U R1.fastq  | samtools view -bS - > \\${name}\".R1.bowtie_endtoend.bam\"\n\n    bowtie2 --rg-id BMG --rg SM:bowtiealn \\\\\n        \t--very-sensitive -L 30 --score-min L,-0.6,-0.2 --end-to-end --reorder \\\\\n          -p ${task.cpus} \\\\\n          -x ${params.bt2idx} \\\\\n          --un \\${name}\".bowtie.R2.unmap.fastq\" \\\\\n        \t-U R2.fastq  | samtools view -bS - > \\${name}\".R2.bowtie_endtoend.bam\"\n  else\n    bowtie2 --rg-id BMG --rg SM:bowtiealn \\\\\n          --very-sensitive -L 30 --score-min L,-0.6,-0.2 --end-to-end --reorder \\\\\n          -p ${task.cpus} \\\\\n          -x ${params.bt2idx} \\\\\n          --un \\${name}\".bowtie.R1.unmap.fastq\" \\\\\n          -U R1.fastq  | samtools view -F 4 -bS - > \\${name}\".R1.bowtie_endtoend.bam\"\n\n    bowtie2 --rg-id BMG --rg SM:bowtiealn \\\\\n          --very-sensitive -L 30 --score-min L,-0.6,-0.2 --end-to-end --reorder \\\\\n          -p ${task.cpus} \\\\\n          -x ${params.bt2idx} \\\\\n          --un \\${name}\".bowtie.R2.unmap.fastq\" \\\\\n          -U R2.fastq  | samtools view -F 4 -bS - > \\${name}\".R2.bowtie_endtoend.bam\"\n  fi\n  rm R1.fastq R2.fastq\n\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bowtie2_end2end"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bowtie2_on_trimmed_reads {\n\n  input:\n  tuple(val(sample), val(rep), val(bio), val(name), val(read), path(fq))\n\n  output:\n  tuple(val(sample), val(rep), val(bio), val(name), val(read), path(\"*bam\"))\n\n  script:\n  def bam=fq.name.replaceFirst('trimmed.fastq.*','trimmed.bam')\n  \"\"\"\n  bowtie2 --rg-id BMG --rg SM:${bam} \\\n          --very-sensitive -L 20 --score-min L,-0.6,-0.2 --end-to-end --reorder \\\n          -p ${task.cpus} \\\n          -x ${params.bt2idx} \\\n          -U ${fq} | samtools view -bS - > ${bam}\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bowtie2_on_trimmed_reads"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bowtie2_mergeR1R2{\n\n  input:\n  tuple(val(sample), val(rep), val(bio), val(name), val(read), path(bams))\n\n  output:\n  tuple(val(sample), val(rep), val(bio), val(name), path(\"*merged.bam\"), emit: bam)\n\n  script:\n  def nm = name.replaceAll(\"R1.(\\\\d+).fastq.*\",\"\\$1\")\n  def outbam=\"${nm}.${read}.merged.bam\"\n  \"\"\"\n  samtools merge -@ ${task.cpus} \\\n               -f tmp.bam \\\n             ${bams[0]} ${bams[1]}\n\n  samtools sort -@ ${task.cpus} -m 800M \\\n  \t              -n -T \\$TMPDIR \\\n                  -o ${outbam} \\\n                  tmp.bam\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bowtie2_mergeR1R2"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess markAlleleOfOrigin {\n  tag { bam.name.replaceFirst(\".bt2.qSort.bam\",\"\") }\n\n  publishDir \"${params.outdir}/bam\",     mode: 'copy', overwrite: true, pattern: \"*phased.bam\", enabled: params.saveBAM\n                                                                                                           \n\n  input:\n  tuple(val(sample), val(rep), val(bio), path(bam))\n\n  output:\n  tuple(val(sample), val(rep), val(bio), path('*bam'), emit: bam)\n  tuple(val(\"${sample}_${bio}\"), path('*markallelicstatus.txt'), emit: rep)\n  tuple(val(\"${sample}_${bio}\"), path('*allelic_stats.txt'), emit: report)\n\n  script:\n  def outbam = bam.name.replaceFirst(\".bam\",\".phased.bam\")\n  def tmprep = bam.name.replaceFirst(\".bam\",\".phased.allelstat\")\n  def outrep = bam.name.replaceFirst(\".bam\",\".phased.markallelicstatus.txt\")\n  def outmqc = bam.name.replaceFirst(\".bam\",\".allelic_stats.txt\")\n  \"\"\"\n  echo \"${sample}${rep}${bio}\"\n\n  ##Bugs in reporting - use custom script instead\n  ##/HiC-Pro_3.0.0/scripts/markAllelicStatus.py -i ${bam} -s ${params.vcf} -r |samtools view -Shb - > ${outbam}\n\n  python /usr/local/hicpro/scripts/markAllelicStatus.py -i ${bam} -s ${params.vcf} -r -o ${outbam}\n\n  grep    allelic_status ${tmprep}                      >${outmqc}\n  grep -v allelic_status ${tmprep} |grep -v FOR-MULTIQC >${outrep}\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/markAlleleOfOrigin"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess pairs_to_bam {\n\n  tag { grp }\n\n  input:\n  tuple(val(id), val(grp), path(pairsGZ))\n\n  output:\n  tuple(val(id), val(grp), path('*.bam'))\n\n  script:\n  def bam=pairsGZ.name.replaceFirst(\"pairs.gz\",\"bam\")\n  \"\"\"\n  #!/bin/bash\n  pairtools split ${pairsGZ} --output-pairs pairs.gz --output-sam pairs.sam\n  samtools view -Shb pairs.sam >${bam}\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/pairs_to_bam"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess multiqc{\n  tag { reports[0] }\n\n  publishDir \"${params.outdir}/multiqc\", mode: 'copy', overwrite: true\n\n  input:\n  path(reports)\n\n  output:\n  path('*multiqc*')\n\n  script:\n  \"\"\"\n  #!/bin/bash\n  nm=`ls *.pairtools_report.txt`\n  name=\\${nm/.all.pairtools_report.txt/.multiqc}\n\n  for z in *zip; do\n    cp \\$z report.zip\n    unzip report.zip\n  done\n\n  multiqc -n \\$name .\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/multiqc"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["process ssdsAlign {\n\n  time { fqR1.size()<300000000? 3.hour : 3.hour * fqR1.size()/300000000 * task.attempt }\n\n  tag {fqR1}\n\n  input:\n  tuple file(fqR1),file(fqR2)\n\n  output:\n  path('*_split_*bam')\n\n  script:\n                              \n  def nm = new Random().with {(1..12).collect {(('a'..'z')).join()[ nextInt((('a'..'z')).join().length())]}.join()}\n  def tmp = 'tmp_' + nm\n  def tmpR1fq = 'tmp_' + nm + '.R1.fq'\n  def tmpR2fq = 'tmp_' + nm + '.R2.fq'\n  def tmpR1sai = 'tmp_' + nm + '.R1.sai'\n  def tmpR2sai = 'tmp_' + nm + '.R2.sai'\n  def bamOut = params.name + '_split_' + nm + '.bam'\n  def bamAll = 'all_' + nm + '.bam'\n\n  println(\"R1 len: ${params.r1Len}\")\n\n  if (params.original){\n    \"\"\"\n\t  fastx_trimmer -Q 33 -f 1 -l ${params.r1Len} -i ${fqR1} -o ${tmpR1fq}\n\t  fastx_trimmer -Q 33 -f 1 -l ${params.r2Len} -i ${fqR2} -o ${tmpR2fq}\n\n\t  bwa_0.7.12 aln \\\n\t    -t ${task.cpus} \\\n\t    ${params.genome_bwaidx} \\\n\t    ${tmpR1fq} >${tmpR1sai}\n\n\t  bwa_ra_0.7.12 aln \\\n\t    -t ${task.cpus} \\\n\t    ${params.genome_bwaidx} \\\n\t    ${tmpR2fq} >${tmpR2sai}\n\n\t  bwa_0.7.12 sampe \\\n\t    ${params.genome_bwaidx} \\\n\t    ${tmpR1sai} ${tmpR2sai} \\\n\t    ${tmpR1fq} ${tmpR2fq} >${tmp}\".unsorted.sam\"\n\n\t  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SamFormatConverter \\\n                 I=${tmp}.unsorted.sam \\\n                 O=${tmp}.unsorted.tmpbam \\\n                 TMP_DIR=\"\\$TMPDIR\" \\\n                 VALIDATION_STRINGENCY=LENIENT\n\n\t  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam \\\n                 I=${tmp}.unsorted.tmpbam \\\n                 O=${bamOut} \\\n                 SO=coordinate \\\n                 TMP_DIR=\"\\$TMPDIR\" \\\n                 VALIDATION_STRINGENCY=LENIENT\n\n    samtools index ${bamOut}\n    \"\"\"\n  }else{\n    \"\"\"\n    fqLen=`cat ${fqR1} |sed -n '2~4p' |perl -lane 'print length(\\$_)' |head -n 10000 |sort -k1n,1n |tail -n1`\n\n    if [ \\$fqLen -ge 99 ]; then\n      echo \"FASTQ Max Length = \\$fqLen : Using minimap2 for initial alignment ...\"\n      minimap2 -ax sr -I 12g \\\n        -t ${task.cpus} \\\n        ${params.genome_mm2idx} \\\n        ${fqR1} ${fqR2} >${tmp}.unsorted.sam\n    else\n      echo \"FASTQ Max Length = \\$fqLen : Using bwa mem for initial alignment ...\"\n      bwa_0.7.12 mem \\\n        -t ${task.cpus} \\\n        ${params.genome_bwaidx} \\\n        ${fqR1} ${fqR2} >${tmp}.unsorted.sam\n    fi\n\n    java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SamFormatConverter \\\n              I=${tmp}.unsorted.sam \\\n              O=${tmp}.unsorted.tmpbam \\\n              TMP_DIR=\"\\$TMPDIR\" \\\n              VALIDATION_STRINGENCY=LENIENT\n\n    java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam \\\n              I=${tmp}.unsorted.tmpbam \\\n              O=${tmp}.Qsorted.sam \\\n              SO=queryname \\\n              TMP_DIR=\"\\$TMPDIR\" \\\n              VALIDATION_STRINGENCY=LENIENT\n\n    flagReadPairsWithMultipleSupplementaryMappings.pl ${tmp}.Qsorted.sam ${tmp}.ok.sam\n\n    java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam \\\n              I=${tmp}.ok.sam \\\n              O=${bamAll} \\\n              SO=coordinate \\\n              TMP_DIR=\"\\$TMPDIR\" \\\n              VALIDATION_STRINGENCY=LENIENT\n\n    samtools index ${bamAll}\n\n    samtools view -hb ${bamAll} >${bamOut}\n    samtools index ${bamOut}\n    \"\"\"\n    }\n  }"], "list_proc": ["kevbrick/pipeIt/ssdsAlign"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess mergeBAMssds {\n\n  publishDir \"${params.outdir}/bam\",     mode: 'copy', overwrite: true, pattern: '*unparsed.bam*'\n  publishDir \"${params.outdir}/reports\", mode: 'copy', overwrite: true, pattern: '*MDmetrics.txt*'\n\n  tag {params.name}\n\n  input:\n  path(bams)\n\n  output:\n  tuple path('*unparsed.bam'),path('*unparsed.bam.bai'), emit: bam\n  tuple path('*ignments.bam'),path('*ignments.bam.bai'), emit: suppbam, optional: true\n  path('*MDmetrics.txt', emit: mdmetrics)\n\n  script:\n                              \n  def input_args = bams.collect{ \"I=$it\" }.join(\" \")\n  def name = \"${params.name}.${params.genome}\"\n  \"\"\"\n  java -jar -Xmx32g \\$PICARDJAR MergeSamFiles \\\n                 ${input_args} \\\n                 O=allREADS.bam \\\n                 AS=true \\\n                 SO=coordinate \\\n                 TMP_DIR=\"\\$TMPDIR\" \\\n                 VALIDATION_STRINGENCY=LENIENT\n\n  java -jar -Xmx32g \\$PICARDJAR MarkDuplicatesWithMateCigar \\\n               I=allREADS.bam \\\n               O=${name}.allReads.bam \\\n               PG=Picard2.9.2_MarkDuplicates \\\n               M=${name}.MDmetrics.txt \\\n               MINIMUM_DISTANCE=400 \\\n               TMP_DIR=\"\\$TMPDIR\" \\\n\t\t   CREATE_INDEX=false \\\n\t\t   ASSUME_SORT_ORDER=coordinate \\\n       VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${name}.allReads.bam\n\n  samtools view -F 2048 -hb ${name}.allReads.bam > ${name}.SSDSunparsed.bam\n  samtools index ${name}.SSDSunparsed.bam\n\n  #samtools view -f 2 -hb ${name}.SSDSall.bam >${name}.SSDSunparsed_andPaired.bam\n  #samtools index ${name}.SSDSunparsed_andPaired.bam\n\n  samtools view -f 2048 -hb ${name}.allReads.bam > ${name}.SSDSunparsed.suppAlignments.bam\n  samtools index ${name}.SSDSunparsed.suppAlignments.bam\n\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/mergeBAMssds"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess parseITRs {\n\n  tag {bam}\n\n  cpus 4\n  memory '12 GB'\n  time { 6.hour * task.attempt}\n\n  errorStrategy 'retry'\n  maxRetries 1\n\n  input:\n  path(bam)\n\n  output:\n  path('*.bam', emit: pBam)\n  path('*.bai', emit: pBai)\n  path('*.bed', emit: pBed)\n\n  script:\n  def parseScript = params.original ? 'ITR_id_v3.pl' : 'ITR_id_v3_long.pl';\n  def name = bam.name.replaceFirst(\".bam\",\".pairs.bam\")\n  \"\"\"\n  samtools view -f 2 -hb ${bam} >${name}\n  samtools index ${name}\n\n  ${parseScript} ${name} ${params.genome} 2>/dev/null\n\n  sort -k1,1 -k2n,2n -k3n,3n -k4,4 -k5,5 -k6,6 ${name}.ssDNA_type1.bed  -o ${name}.ssDNA_type1.bed\n  sort -k1,1 -k2n,2n -k3n,3n -k4,4 -k5,5 -k6,6 ${name}.ssDNA_type2.bed  -o ${name}.ssDNA_type2.bed\n  sort -k1,1 -k2n,2n -k3n,3n -k4,4 -k5,5 -k6,6 ${name}.dsDNA.bed        -o ${name}.dsDNA.bed\n  sort -k1,1 -k2n,2n -k3n,3n -k4,4 -k5,5 -k6,6 ${name}.dsDNA_strict.bed -o ${name}.dsDNA_strict.bed\n  sort -k1,1 -k2n,2n -k3n,3n -k4,4 -k5,5 -k6,6 ${name}.unclassified.bed -o ${name}.unclassified.bed\n\n  samtools view -H ${name}   >header.txt\n  echo -e '@PG\\\\tID:SSDSpipeline PN:SSDSpipeline VN:2.0_nextflowDSL2' >>header.txt\n\n  cat header.txt ${name}.ssDNA_type1.sam  >${name}.ssDNA_type1.RH.sam\n  cat header.txt ${name}.ssDNA_type2.sam  >${name}.ssDNA_type2.RH.sam\n  cat header.txt ${name}.dsDNA.sam        >${name}.dsDNA.RH.sam\n  cat header.txt ${name}.dsDNA_strict.sam >${name}.dsDNA_strict.RH.sam\n  cat header.txt ${name}.unclassified.sam >${name}.unclassified.RH.sam\n\n  samtools view -Shb ${name}.ssDNA_type1.RH.sam  >${name}.ssDNA_type1.US.bam\n  samtools view -Shb ${name}.ssDNA_type2.RH.sam  >${name}.ssDNA_type2.US.bam\n  samtools view -Shb ${name}.dsDNA.RH.sam        >${name}.dsDNA.US.bam\n  samtools view -Shb ${name}.dsDNA_strict.RH.sam >${name}.dsDNA_strict.US.bam\n  samtools view -Shb ${name}.unclassified.RH.sam >${name}.unclassified.US.bam\n\n  java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${name}.ssDNA_type1.US.bam  O=${name}.ssDNA_type1.bam  SO=coordinate VALIDATION_STRINGENCY=LENIENT\n  java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${name}.ssDNA_type2.US.bam  O=${name}.ssDNA_type2.bam  SO=coordinate VALIDATION_STRINGENCY=LENIENT\n  java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${name}.dsDNA.US.bam        O=${name}.dsDNA.bam        SO=coordinate VALIDATION_STRINGENCY=LENIENT\n  java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${name}.dsDNA_strict.US.bam O=${name}.dsDNA_strict.bam SO=coordinate VALIDATION_STRINGENCY=LENIENT\n  java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${name}.unclassified.US.bam O=${name}.unclassified.bam SO=coordinate VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${name}.ssDNA_type1.bam\n  samtools index ${name}.ssDNA_type2.bam\n  samtools index ${name}.dsDNA.bam\n  samtools index ${name}.dsDNA_strict.bam\n  samtools index ${name}.unclassified.bam\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/parseITRs"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess gatherITROutputs {\n\n  publishDir \"${params.outdir}/reports\", mode: 'copy', overwrite: true, pattern: '*txt'\n  publishDir \"${params.outdir}/bam\",     mode: 'copy', overwrite: true, pattern: '*bam*'\n  publishDir \"${params.outdir}/bed\",     mode: 'copy', overwrite: true, pattern: '*bed*'\n\n  tag {bam}\n\n  time { bam.size() < 100 ? 2.hour : 2.hour * (1 + bam.size()/100 * task.attempt) }\n\n  input:\n  path(bam)\n  path(bed)\n  val(type)\n\n  output:\n  tuple(path('*bam'), path('*bai'), emit: bambai)\n  tuple(path('*bam'), path('*bai'), path('*bed'), path('*metrics.txt'), emit: itrFinal)\n  path('*.txt*', emit: report)\n  path('*.bam*', emit: bam)\n  path('*.bed',  emit: bed)\n\n  script:\n  def name=\"${params.name}.${params.genome}.${type}\"\n  \"\"\"\n  java -jar -Xmx8g \\$PICARDJAR MergeSamFiles TMP_DIR=\"\\$TMPDIR\" O=${name}.US.BAM `ls *${type}.bam | sed 's/.*\\$/I=& /'`\n  java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${name}.US.BAM   O=${name}.S.BAM   SO=coordinate VALIDATION_STRINGENCY=LENIENT\n  java -jar -Xmx8g \\$PICARDJAR MarkDuplicatesWithMateCigar TMP_DIR=\"\\$TMPDIR\" I=${name}.S.BAM  O=${name}.bam  PG=Picard2.9.2_MarkDuplicates M=${name}.MDmetrics.txt  CREATE_INDEX=false VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${name}.bam\n\n  sort -k1,1 -k2n,2n -k3n,3n -k4,4 -k5,5 -k6,6 -m *${type}.bed  >${name}.bed\n  rm -f ${name}.US.BAM\n  rm -f ${name}.S.BAM\n    \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/gatherITROutputs"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 2, "tools": ["bedGraphToBigWig", "Bowtie", "BEDTools"], "nb_own": 2, "list_own": ["kingzhuky", "kevbrick"], "nb_wf": 2, "list_wf": ["meripseqpipe", "pipeIt"], "list_contrib": ["kingzhuky", "kevbrick", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess makeFRBWssds {\n\n  publishDir \"${params.outdir}/bigwig\", mode: 'copy', overwrite: true, pattern: '*bigwig*'\n\n  cpus 2\n  memory '12 GB'\n\n  tag {bam}\n  time { bam.size()< 2000000000 ? 1.hour * task.attempt : 1.hour * bam.size()/2000000000 * task.attempt }\n\n  errorStrategy { 'retry' }\n  maxRetries 1\n\n  input:\n  tuple path(bam), path(idx)\n\n  output:\n  path('*.bigwig', emit: bw)\n\n  script:\n  iName = bam.name.replaceAll(/.bam/,\".out\")\n\n  \"\"\"\n  if [[ `samtools view -F 4 ${bam} |head -n 5001 |wc -l` -gt 5000 ]]; then\n    idx=\"${params.genome_fai}\";\n\n    bedtools makewindows  -g \\$idx -w 1000 -s 100 |sort -k1,1 -k2n,2n |perl -lane 'print \\$_ if ((\\$F[2]-\\$F[1]) == 1000)' >win.bed\n    java -jar -Xmx8g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" I=${bam} O=qSort.bam SO=queryname VALIDATION_STRINGENCY=LENIENT\n    bedtools bamtobed -mate1 -i qSort.bam -bedpe >frags.bedpe\n\n    perl -lane 'print join(\"\\\\t\", \\$F[0], \\$F[1], \\$F[5], @F[6..8]) if (\\$F[8] eq \"+\")' frags.bedpe |sort -k1,1 -k2n,2n >frags.POS.bed\n    perl -lane 'print join(\"\\\\t\", \\$F[0], \\$F[4], \\$F[2], @F[6..8]) if (\\$F[8] eq \"-\")' frags.bedpe |sort -k1,1 -k2n,2n >frags.NEG.bed\n\n    mapBed -a win.bed -b frags.POS.bed -c 1 -o count |perl -lane 'use Math::Round; \\$p=round((\\$F[1]+\\$F[2])/2); print join(\"\\\\t\",\\$F[0],\\$p-49,\\$p+50,\\$F[3])' >pos.bg\n    mapBed -a win.bed -b frags.NEG.bed -c 1 -o count |perl -lane 'use Math::Round; \\$p=round((\\$F[1]+\\$F[2])/2); print join(\"\\\\t\",\\$F[0],\\$p-49,\\$p+50,\\$F[3])' >neg.bg\n\n    paste pos.bg neg.bg |perl -lane '\\$F[3]+=0.5; \\$F[7]+=0.5; \\$v=(\\$F[3]/\\$F[7]); print join(\"\\\\t\",@F[0..2],(log(\\$v)/log(2)))' >fr.bg\n    paste pos.bg neg.bg |perl -lane '\\$v=(\\$F[3]+\\$F[7]); print join(\"\\\\t\",@F[0..2],\\$v)' >tot.bg\n\n    bedGraphToBigWig pos.bg \\$idx ${iName}.F.bigwig\n    bedGraphToBigWig neg.bg \\$idx ${iName}.R.bigwig\n    bedGraphToBigWig fr.bg  \\$idx ${iName}.FR.bigwig\n    bedGraphToBigWig tot.bg \\$idx ${iName}.Tot.bigwig\n  else\n    touch \"EMPTY_TOOFEWREADS_${iName}.F.bigwig\n    touch \"EMPTY_TOOFEWREADS_${iName}.R.bigwig\n    touch \"EMPTY_TOOFEWREADS_${iName}.FR.bigwig\n    touch \"EMPTY_TOOFEWREADS_${iName}.Tot.bigwig\n  fi\n  \"\"\"\n  }", " process MakeTophat2Index {\n        label 'build_index'\n        tag \"tophat2_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\": params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n\n        output:\n        file \"Tophat2Index/*\" into tophat2_index\n\n        when:\n        aligner == \"tophat2\"\n\n        script:\n        tophat2_index = \"Tophat2Index/\" + fasta.baseName.toString()\n        \"\"\"\n        mkdir Tophat2Index\n        ln $fasta Tophat2Index\n        bowtie2-build -f $fasta $tophat2_index\n        \"\"\"\n    }"], "list_proc": ["kevbrick/pipeIt/makeFRBWssds", "kingzhuky/meripseqpipe/MakeTophat2Index"], "list_wf_names": ["kevbrick/pipeIt", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess multiQCssds {\n\n  publishDir \"${params.outdir}/reports\", mode: 'copy', overwrite: true\n\n  tag {reports}\n\n  input:\n  path(reports)\n\n  output:\n  path('*ultiQC*', emit: mqcReport)\n\n  script:\n  \"\"\"\n  multiqc -f -m ssds -n ${params.name}.multiQC .\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/multiQCssds"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["meme_fimo", "BEDTools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["process getB6xCASThotspots {\n\n  cpus 2\n  memory '4g'\n\n  time { 1.hour * task.attempt }\n\n  errorStrategy { 'retry' }\n  maxRetries 1\n\n  input:\n  path(mm10FA)\n  path(mm10IDX)\n  path(mm10w1ks100)\n\n  output:\n\tpath('*.oneMotif.500bp.bed', emit: hotspotOneMotif500bp)\n\tpath('*_maleHS.3Kb.bedgraph', emit: hotspotsBG3Kbp)\n\tpath('B6_maleHS.3Kb.bed', emit: hotspotsBED3Kbp)\n\tpath('*oneMotif.500bp.bed', emit: hotspotsOneMotif500bp)\n\tpath('*oneMotif.3Kb.bed', emit: hotspotsOneMotif3Kb)\n  path('B6_maleHS.1bp.bedgraph', emit: hotspotBG1bp)\n  path('B6_maleHS.500bp.bedgraph', emit: hotspotBG500bp)\n  path('B6_maleHS.1Kb.bed', emit: hotspotBED)\n\tpath('CST_maleHS.1bp.bedgraph', emit: cstHotspotBG1bp)\n  path('CST_maleHS.500bp.bedgraph', emit: cstHotspotBG500bp)\n\tpath('CST_maleHS.3Kb.bedgraph', emit: cstHotspotBG3Kbp)\n  path('CST_maleHS.1Kb.bed', emit: cstHotspotBED)\n\tpath('B6xCST.heat.bedgraph', emit: b6xcst_HeatBG)\n\tpath('B6xCST.bias.bedgraph', emit: b6xcst_BiasBG)\n  path('B6xCST.details.tab', emit: b6xcst_details)\n\tpath('B6xCST_maleHS.3Kb.bed', emit: hotspotsBEDBxC3Kbp)\n\n  script:\n  \"\"\"\n\t## get B6 hotspots\n  wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2664nnn/GSM2664275/suppl/GSM2664275_Testis_SSDS_T1.DSBhotspots.bedgraph.gz\n  gunzip -c GSM2664275_Testis_SSDS_T1.DSBhotspots.bedgraph.gz |cut -f1-3,6 |grep -P \\'^chr[0-9]+\\' >B6_maleHS.bedgraph\n\n \tbedtools slop -l -0.5 -r -0.5 -pct -i B6_maleHS.bedgraph -g ${mm10IDX} |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >B6_maleHS.1bp.bedgraph\n  cut -f1-3 B6_maleHS.1bp.bedgraph                                                                                       >B6_maleHS.1bp.bed\n\n \tbedtools slop -l 250  -r 250       -i B6_maleHS.1bp.bedgraph          -g ${mm10IDX}            >B6_maleHS.500bp.bedgraph\n\tbedtools slop -l 1500 -r 1500      -i B6_maleHS.1bp.bedgraph          -g ${mm10IDX}            >B6_maleHS.3Kb.bedgraph\n\tbedtools slop -l 1500 -r 1500      -i B6_maleHS.1bp.bedgraph          -g ${mm10IDX} |cut -f1-3 >B6_maleHS.3Kb.bed\n\n\tbedtools slop -l 500 -r 500      -i B6_maleHS.1bp.bedgraph            -g ${mm10IDX} |cut -f1-3 >B6_maleHS.1Kb.bed\n\n  perl -lane \\'\\$nm=join(\"_\",@F[0..2]); print join(\"\\\\t\",@F[0..2],\\$nm,\\$nm,\\$F[3])' B6_maleHS.500bp.bedgraph >B6HS500forFIMO.bed\n  bedtools getfasta -fi ${mm10FA} -bed B6HS500forFIMO.bed -name -fo B6_maleHS.500bp.fa\n\n\trm -rf fimo\n  fimo --max-stored-scores 1000000 --thresh 1e-3 --o fimo1 ${params.accessorydir}/PRDM9motif/PRBS_B6.MEMEv4.pwm B6_maleHS.500bp.fa\n\n  perl ${params.codedir}/getHotspotsWithSingleMotif.pl --fimo  ./fimo1/fimo.tsv --w 250 --out B6.oneMotif.500bp.bed\n\tbedtools slop -l -0.5 -r -0.5 -pct -i B6.oneMotif.500bp.bed -g ${mm10IDX} >B6.oneMotif.1bp.bed\n\tbedtools slop -l 1500 -r 1500 -pct -i B6.oneMotif.1bp.bed   -g ${mm10IDX} >B6.oneMotif.3Kb.bed\n\n\t## get CST hotspots\n  wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM1954nnn/GSM1954846/suppl/GSM1954846%5FCAST%5Fhotspots%2Etab%2Egz\n  gunzip -c GSM1954846_CAST_hotspots.tab.gz |cut -f1-3,4 |grep -P \\'^chr[0-9]+\\' >CST_maleHS.bedgraph\n\n \tbedtools slop -l -0.5 -r -0.5 -pct -i CST_maleHS.bedgraph -g ${mm10IDX} |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >CST_maleHS.1bp.bedgraph\n  cut -f1-3 CST_maleHS.1bp.bedgraph                                                                                       >CST_maleHS.1bp.bed\n\n \tbedtools slop -l 250  -r 250       -i CST_maleHS.1bp.bedgraph          -g ${mm10IDX}            >CST_maleHS.500bp.bedgraph\n\tbedtools slop -l 1500 -r 1500      -i CST_maleHS.1bp.bedgraph          -g ${mm10IDX}            >CST_maleHS.3Kb.bedgraph\n\n\tbedtools slop -l 500 -r 500      -i CST_maleHS.1bp.bedgraph            -g ${mm10IDX} |cut -f1-3 >CST_maleHS.1Kb.bed\n\n  perl -lane \\'\\$nm=join(\"_\",@F[0..2]); print join(\"\\\\t\",@F[0..2],\\$nm,\\$nm,\\$F[3])' CST_maleHS.500bp.bedgraph >CSTHS500forFIMO.bed\n  bedtools getfasta -fi ${mm10FA} -bed CSTHS500forFIMO.bed -name -fo CST_maleHS.500bp.fa\n\n\trm -rf fimo\n  fimo --max-stored-scores 1000000 --thresh 1e-3 --o fimo2 ${params.accessorydir}/PRDM9motif/PRBS_CST.MEMEv4.pwm CST_maleHS.500bp.fa\n\n  perl ${params.codedir}/getHotspotsWithSingleMotif.pl --fimo  ./fimo2/fimo.tsv --w 250 --out CST.oneMotif.500bp.bed\n\tbedtools slop -l -0.5 -r -0.5 -pct -i CST.oneMotif.500bp.bed -g ${mm10IDX} >CST.oneMotif.1bp.bed\n\tbedtools slop -l 1500 -r 1500 -pct -i CST.oneMotif.1bp.bed   -g ${mm10IDX} >CST.oneMotif.3Kb.bed\n\n\t## get CSTXb6 hotspots\n  wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2049nnn/GSM2049312/suppl/GSM2049312%5Fdmc1hotspots%5FB6CASTF1%2EPRDM9bc%2Etxt%2Egz\n\n\tgunzip -c GSM2049312_dmc1hotspots_B6CASTF1.PRDM9bc.txt.gz |grep -v heat |perl -lane 'print \"chr\".join(\"\\\\t\",\\$F[0],\\$F[1]-500,\\$F[1]+500,\\$F[2])' >B6xCST.heat.bedgraph\n\tgunzip -c GSM2049312_dmc1hotspots_B6CASTF1.PRDM9bc.txt.gz |grep -v heat |perl -lane 'print \"chr\".join(\"\\\\t\",\\$F[0],\\$F[1]-500,\\$F[1]+500,(\\$F[3] == NA?\"0.5\":\\$F[3]))' >B6xCST.bias.bedgraph\n  gunzip -c GSM2049312_dmc1hotspots_B6CASTF1.PRDM9bc.txt.gz |grep -v heat |perl -lane 'print \"chr\".join(\"\\\\t\",\\$F[0],\\$F[1]-500,\\$F[1]+500,\\$F[2],\\$F[3])' >B6xCST.bg\n\n\tbedtools slop -l -0.5 -r -0.5 -pct -i B6xCST.bg -g ${mm10IDX} |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >B6xCST_maleHS.1bp.bedgraph\n\n \tbedtools slop -l 250  -r 250       -i B6xCST_maleHS.1bp.bedgraph          -g ${mm10IDX}            >B6xCST_maleHS.500bp.bedgraph\n\tbedtools slop -l 1500 -r 1500      -i B6xCST_maleHS.1bp.bedgraph          -g ${mm10IDX}            >B6xCST_maleHS.3Kb.bedgraph\n\tbedtools slop -l 1500 -r 1500      -i B6xCST_maleHS.1bp.bedgraph          -g ${mm10IDX} |cut -f1-3 >B6xCST_maleHS.3Kb.bed\n\n  intersectBed -a B6xCST.bg -b CST_maleHS.1Kb.bed -c >C1.bg\n  intersectBed -a C1.bg     -b B6_maleHS.1Kb.bed -c   >C2.bg\n\n  cat C2.bg |perl -lane '\\$type = \"\"; \\$type = \"CST\" if ((\\$F[5] > 0 && \\$F[6] == 0) || (\\$F[5] == 0 && \\$F[6] == 0 && (\\$F[4] > 0.75))); \\$type = \"B6\" if ((\\$F[5] == 0 && \\$F[6] > 0) || (\\$F[5] == 0 && \\$F[6] == 0 && (\\$F[4] < 0.25))); \\$type = \"Ambiguous\" if (not \\$type); print join(\"\\\\t\",@F,\\$type)' |sort -k1,1 -k2n,2n -k3n,3n >B6CST_all.tab\n\n  grep CST B6CST_all.tab |cut -f1-5 >B6xCST.CSTHS.tab\n  grep B6 B6CST_all.tab  |cut -f1-5 >B6xCST.B6HS.tab\n\n\tbedtools slop -l -0.5 -r -0.5 -pct -i B6xCST.CSTHS.tab -g ${mm10IDX} |cut -f1-3 |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >B6xCST.CSTHS.1bp.bedgraph\n\tbedtools slop -l -0.5 -r -0.5 -pct -i B6xCST.B6HS.tab   -g ${mm10IDX} |cut -f1-3 |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >B6xCST.B6HS.1bp.bedgraph\n\n\tbedtools slop -l 250  -r 250       -i B6xCST.CSTHS.1bp.bedgraph          -g ${mm10IDX}            >B6xCST.CSTHS.500bp.bedgraph\n\tbedtools slop -l 250  -r 250       -i B6xCST.B6HS.1bp.bedgraph            -g ${mm10IDX}            >B6xCST.B6HS.500bp.bedgraph\n\n  paste B6xCST.bias.bedgraph B6xCST.heat.bedgraph |cut -f1-4,8 >B6xCST.heatbias.tmp\n  intersectBed -a B6xCST.bias.bedgraph -b B6xCST.B6HS.1bp.bedgraph  -c |cut -f5 >likely_b6_defined.tab\n  intersectBed -a B6xCST.bias.bedgraph -b B6xCST.CSTHS.1bp.bedgraph -c |cut -f5 >likely_cast_defined.tab\n\n  echo -e \"cs\\\\tfrom\\\\tto\\\\tbias\\\\theat\\\\tB6\\\\tCAST\"                       >B6xCST.details.tab\n  paste B6xCST.heatbias.tmp likely_b6_defined.tab likely_cast_defined.tab >>B6xCST.details.tab\n\n  perl -lane \\'\\$nm=join(\"_\",@F[0..2]); print join(\"\\\\t\",@F[0..2],\\$nm,\\$nm,\\$F[3])' B6xCST.CSTHS.500bp.bedgraph >BxC_CSTHS500forFIMO.bed\n\tperl -lane \\'\\$nm=join(\"_\",@F[0..2]); print join(\"\\\\t\",@F[0..2],\\$nm,\\$nm,\\$F[3])' B6xCST.B6HS.500bp.bedgraph   >BxC_B6HS500forFIMO.bed\n\n\tbedtools getfasta -fi ${mm10FA} -bed BxC_CSTHS500forFIMO.bed -name -fo BxC_CST_maleHS.500bp.fa\n\tbedtools getfasta -fi ${mm10FA} -bed BxC_B6HS500forFIMO.bed   -name -fo BxC_B6_maleHS.500bp.fa\n\n  rm -rf fimo\n\tfimo --max-stored-scores 1000000 --thresh 1e-3 --o fimo3 ${params.accessorydir}/PRDM9motif/PRBS_CST.MEMEv4.pwm BxC_CST_maleHS.500bp.fa\n\tperl ${params.codedir}/getHotspotsWithSingleMotif.pl --fimo  ./fimo3/fimo.tsv --w 250 --out B6xCST_CSTHS.oneMotif.500bp.bed\n\tbedtools slop -l -0.5 -r -0.5 -pct -i B6xCST_CSTHS.oneMotif.500bp.bed -g ${mm10IDX} >B6xCST_CSTHS.oneMotif.1bp.bed\n\tbedtools slop -l 1500 -r 1500 -pct -i B6xCST_CSTHS.oneMotif.1bp.bed   -g ${mm10IDX} >B6xCST_CSTHS.oneMotif.3Kb.bed\n\n  rm -rf fimo\n\tfimo --max-stored-scores 1000000 --thresh 1e-3 --o fimo4 ${params.accessorydir}/PRDM9motif/PRBS_B6.MEMEv4.pwm BxC_B6_maleHS.500bp.fa\n\tperl ${params.codedir}/getHotspotsWithSingleMotif.pl --fimo  ./fimo4/fimo.tsv --w 250 --out B6xCST_B6HS.oneMotif.500bp.bed\n\tbedtools slop -l -0.5 -r -0.5 -pct -i B6xCST_B6HS.oneMotif.500bp.bed -g ${mm10IDX} >B6xCST_B6HS.oneMotif.1bp.bed\n\tbedtools slop -l 1500 -r 1500 -pct -i B6xCST_B6HS.oneMotif.1bp.bed   -g ${mm10IDX} >B6xCST_B6HS.oneMotif.3Kb.bed\n\n\tsort -k1,1 -k2n,2n -k3n,3n B6xCST_CSTHS.oneMotif.500bp.bed B6xCST_B6HS.oneMotif.500bp.bed >B6xCST.oneMotif.500bp.bed\n\tsort -k1,1 -k2n,2n -k3n,3n B6xCST_CSTHS.oneMotif.3Kb.bed B6xCST_B6HS.oneMotif.3Kb.bed     >B6xCST.oneMotif.3Kb.bed\n\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/getB6xCASThotspots"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess getGENCODE {\n  cpus 2\n  memory '4g'\n  time { 1.hour * task.attempt }\n\n  errorStrategy { 'retry' }\n  maxRetries 1\n\n                        \n                             \n                     \n                        \n                      \n\n  input:\n  path(mm10FA)\n  path(mm10IDX)\n  path(mm10w1ks100)\n\n  output:\n  path('gencodeTSS.1Kb.bed', emit: tssBEDa)\n  path('gencodeTES.1Kb.bed', emit: gencodeTESBED)\n  path('gencodeGene.bed', emit: gencodeGeneBED)\n\tpath('refseqTSS.bed', emit: refseqTSSb)\n\n  script:\n  \"\"\"\n  ##GENCODE\n  wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M20/gencode.vM20.annotation.gtf.gz\n  ##TSS\n  perl ${params.codedir}/gencodeGTFtoTSS.pl gencode.vM20.annotation.gtf.gz |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >gencodeTSS.1bp.noMerge.bed\n\n  bedtools slop -l 500 -r 500   -i gencodeTSS.1bp.noMerge.bed -g ${mm10IDX} >gencodeTSS.1Kb.noMerge.bed\n  mergeBed -i gencodeTSS.1Kb.noMerge.bed -c 4,5,6 -o distinct,distinct,distinct |grep -vP '([\\\\+\\\\-],[\\\\+\\\\-])' |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX}            >gencodeTSS.1Kb.bed\n  mergeBed -i gencodeTSS.1Kb.noMerge.bed -c 4,5,6 -o distinct,distinct,distinct |grep -vP '([\\\\+\\\\-],[\\\\+\\\\-])' |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} |cut -f1-3 >gencodeTSS.1Kb3Col.bed\n\n  cat gencodeTSS.1KbDets.bed |perl -lane \\'print join(\"\\\\t\",@F[0..2],0,@F[4..5])\\'                           >gencodeTSS.1Kb.forMerge.bed\n\n  ##TES\n  perl ${params.codedir}/gencodeGTFtoTES.pl gencode.vM20.annotation.gtf.gz |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} |grep -P \"\\\\s+\" >gencodeTES.1bp.noMerge.bed\n\n  bedtools slop -l 500 -r 500   -i gencodeTES.1bp.noMerge.bed -g ${mm10IDX} >gencodeTES.1Kb.noMerge.bed\n  mergeBed -i gencodeTES.1Kb.noMerge.bed -c 4,5,6 -o distinct,distinct,first |\\\n                                          ${params.codedir}/sortBEDByFAI.pl - \\\n                                          ${mm10IDX} >gencodeTES.1Kb.bed\n\n  ##GENES\n  perl ${params.codedir}/gencodeGTFtoCDS.pl gencode.vM20.annotation.gtf.gz |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >gencodeGene.noMerge.bed\n  mergeBed -s -i gencodeGene.noMerge.bed -c 4,5,6 \\\n           -o distinct,distinct,first |${params.codedir}/sortBEDByFAI.pl - \\\n          ${mm10IDX} |grep -P \"\\\\s+\" >gencodeGene.bed\n\n\t##REFSEQ - move to annotation section later\n\twget http://hgdownload.soe.ucsc.edu/goldenPath/mm10/database/ncbiRefSeqCurated.txt.gz\n\tperl ${params.codedir}/parseRefSeq.pl ncbiRefSeqCurated.txt.gz TSS  |${params.codedir}/sortBEDByFAI.pl - ${mm10IDX} >refseqTSS.bed\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/getGENCODE"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["LiftOver"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess getMouseMeiosisChromatinMods {\n  cpus 2\n  memory '4g'\n  time { 1.hour * task.attempt }\n\n  errorStrategy { 'retry' }\n  maxRetries 1\n\n                        \n                             \n                     \n                        \n                      \n\n  input:\n  path(mm10FA)\n  path(mm10IDX)\n  path(mm10w1ks100)\n\n  output:\n\tpath('Zygotene_H3K4me3.peaks.bed', emit: h3k4m3GL)\n\tpath('Zygotene_H3K4me3.peaks.bedgraph', emit: h3k4m3GLBG)\n\tpath('H3K36m3_B6.bedgraph', emit: h3k36m3B6)\n\tpath('H3K36m3_B6Spo11KO.bedgraph', emit: h3k36m3B6Spo11)\n\n  script:\n  \"\"\"\n\t## ZYGO H3K4me3 Peaks\n\twget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3734nnn/GSM3734414/suppl/GSM3734414%5FZY%2ER1%2EH3K4me3%2Epeaks%2Ebed%2Egz\n\tgunzip -c GSM3734414_ZY.R1.H3K4me3.peaks.bed.gz |cut -f1-3 >Zygotene_H3K4me3.peaks.bed\n\n\twget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3734nnn/GSM3734414/suppl/GSM3734414_ZY.R1.H3K4me3.monoCorrected.ws25bp.bigwig\n\tbigWigToBedGraph GSM3734414_ZY.R1.H3K4me3.monoCorrected.ws25bp.bigwig GSM3734414_ZY.R1.H3K4me3.monoCorrected.ws25bp.bedgraph\n\n\tmapBed -a Zygotene_H3K4me3.peaks.bed -b GSM3734414_ZY.R1.H3K4me3.monoCorrected.ws25bp.bedgraph -c 4 -o sum |sortBEDByFAI.pl - ${mm10IDX} >Zygotene_H3K4me3.peaks.bedgraph\n\n\t## H3K36m3 Peaks and PRDM9 ChIP-Seq data\n\twget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE93nnn/GSE93955/suppl/GSE93955_CHIP_H3K36me3_B6_coverage.bw\n\twget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE93nnn/GSE93955/suppl/GSE93955_CHIP_H3K36me3_B6_spo11_coverage.bw\n\twget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE93nnn/GSE93955/suppl/GSE93955_CHIP_PRDM9_B6_peaks.bed.gz\n\tbigWigToBedGraph GSE93955_CHIP_H3K36me3_B6_coverage.bw       GSE93955_CHIP_H3K36me3_B6_coverage.mm9.bedgraph\n\tbigWigToBedGraph GSE93955_CHIP_H3K36me3_B6_spo11_coverage.bw GSE93955_CHIP_H3K36me3_B6_spo11_coverage.mm9.bedgraph\n\n\t## GET LIFTOVER CHAIN FILE\n\twget --timestamping ftp://hgdownload.cse.ucsc.edu/goldenPath/mm9/liftOver/mm9ToMm10.over.chain.gz   -O mm9ToMm10.over.chain.gz\n\n\tliftOver GSE93955_CHIP_H3K36me3_B6_coverage.mm9.bedgraph        mm9ToMm10.over.chain.gz  H3K36m3_B6.bgtmp na\n\tliftOver GSE93955_CHIP_H3K36me3_B6_spo11_coverage.mm9.bedgraph  mm9ToMm10.over.chain.gz  H3K36m3_B6Spo11KO.bgtmp na\n\n  cat H3K36m3_B6.bgtmp        |sortBEDByFAI.pl - ${mm10IDX} >H3K36m3_B6.bedgraph\n\tcat H3K36m3_B6Spo11KO.bgtmp |sortBEDByFAI.pl - ${mm10IDX} >H3K36m3_B6Spo11KO.bedgraph\n\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/getMouseMeiosisChromatinMods"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 4, "tools": ["Bedops", "SAMtools", "Minimap2", "MultiQC", "Bowtie"], "nb_own": 3, "list_own": ["kingzhuky", "khigashi1987", "kevbrick"], "nb_wf": 3, "list_wf": ["meripseqpipe", "CUTRUN_Nextflow", "pipeIt"], "list_contrib": ["kingzhuky", "khigashi1987", "kevbrick", "juneb4869"], "nb_contrib": 4, "codes": [" process MakeTophat2Index {\n        label 'build_index'\n        tag \"tophat2_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\": params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n\n        output:\n        file \"Tophat2Index/*\" into tophat2_index\n\n        when:\n        aligner == \"tophat2\"\n\n        script:\n        tophat2_index = \"Tophat2Index/\" + fasta.baseName.toString()\n        \"\"\"\n        mkdir Tophat2Index\n        ln $fasta Tophat2Index\n        bowtie2-build -f $fasta $tophat2_index\n        \"\"\"\n    }", " process FILTER_SEACR_PEAKS {\n        tag \"$name\"\n        publishDir \"${params.outdir}/seacr\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(name), path(peaks) from ch_seacr_peaks\n        path(bed) from ch_genome_filter_regions_seacr.collect()\n\n        output:\n        tuple val(name), path('*.filtered.narrowPeak') into ch_filtered_seacr_peaks,\n                                                     ch_filtered_seacr_peaks_annot\n\n        \"\"\"\n        cat ${peaks} | \\\n          grep -v -e \"chrM\" | \\\n          sort-bed - | \\\n          bedops -n 1 - ${bed} > ${name}.${params.seacr_mode}.filtered.narrowPeak\n        \"\"\"\n    }", "\nprocess MULTIQC {\n    tag \"$name\"\n    publishDir \"${params.outdir}/multiqc/\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_multiqc && params.macs_gsize && params.blacklist\n\n    input:\n    path (multiqc_config) from ch_multiqc_config\n    path workflow_summary from ch_workflow_summary.collectFile(name: 'workflow_summary_mqc.yaml')\n    path ('fastqc/*') from ch_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trim_1st/fastqc/*') from ch_trim1st_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trim_2nd/fastqc/*') from ch_trim2nd_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('alignment/*') from ch_flagstat_mqc.collect()\n    path ('alignment/picard_metrics/*') from ch_picard_metrics_mqc.collect()\n    path ('macs/*') from ch_macs_mqc.collect().ifEmpty([])\n    path ('deeptools/*') from ch_plotprofile_mqc.collect().ifEmpty([])\n\n    output:\n    path '*multiqc_report.html'\n    path '*_data'\n\n    script:\n    rtitle = \"--title \\\"$params.name\\\"\"\n    rfilename = \"--filename \" + params.name + \"_multiqc_report\"\n    \"\"\"\n    multiqc . -f $rtitle $rfilename\n    \"\"\"\n}", "\nprocess minimap2PE {\n  label 'aligner'\n\n  time { fq1.size() < 3.GB ? 2.hour : 2.hour + 1.hour * (fq1.size()/3000000000) * task.attempt }\n\n  input:\n  tuple path(fq1), path(fq2)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq1.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n\n  \"\"\"\n  minimap2 -ax sr \\\n    -t ${task.cpus} \\\n    ${params.genome_mm2idx} \\\n    ${fq1} ${fq2} >tmp.sam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n            I=tmp.sam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }"], "list_proc": ["kingzhuky/meripseqpipe/MakeTophat2Index", "khigashi1987/CUTRUN_Nextflow/FILTER_SEACR_PEAKS", "khigashi1987/CUTRUN_Nextflow/MULTIQC", "kevbrick/pipeIt/minimap2PE"], "list_wf_names": ["kevbrick/pipeIt", "khigashi1987/CUTRUN_Nextflow", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 3, "tools": ["SAMtools", "BWA", "MultiQC"], "nb_own": 3, "list_own": ["kingzhuky", "khigashi1987", "kevbrick"], "nb_wf": 3, "list_wf": ["meripseqpipe", "CUTRUN_Nextflow", "pipeIt"], "list_contrib": ["kingzhuky", "khigashi1987", "kevbrick", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess MULTIQC {\n    tag \"$name\"\n    publishDir \"${params.outdir}/multiqc/\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_multiqc && params.macs_gsize && params.blacklist\n\n    input:\n    path (multiqc_config) from ch_multiqc_config\n    path workflow_summary from ch_workflow_summary.collectFile(name: 'workflow_summary_mqc.yaml')\n    path ('fastqc/*') from ch_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trim_1st/fastqc/*') from ch_trim1st_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('trim_2nd/fastqc/*') from ch_trim2nd_fastqc_reports_mqc.collect().ifEmpty([])\n    path ('alignment/*') from ch_flagstat_mqc.collect()\n    path ('alignment/picard_metrics/*') from ch_picard_metrics_mqc.collect()\n    path ('macs/*') from ch_macs_mqc.collect().ifEmpty([])\n    path ('deeptools/*') from ch_plotprofile_mqc.collect().ifEmpty([])\n\n    output:\n    path '*multiqc_report.html'\n    path '*_data'\n\n    script:\n    rtitle = \"--title \\\"$params.name\\\"\"\n    rfilename = \"--filename \" + params.name + \"_multiqc_report\"\n    \"\"\"\n    multiqc . -f $rtitle $rfilename\n    \"\"\"\n}", "\nprocess makechromesize {\n    label 'build_index'\n    tag \"gtf2bed12\"\n    publishDir path: { params.saveReference ? \"${params.outdir}/Genome/reference_genome\" : params.outdir },\n                saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n    when:\n    true\n\n    input:\n    file fasta\n    \n    output:\n    file \"chromsizes.file\" into chromsizesfile\n\n    shell:      \n    \"\"\"\n    samtools faidx ${fasta}\n    cut -f1,2 ${fasta}.fai > chromsizes.file\n    \"\"\"\n}", "\nprocess bwaAlnSR {\n  label 'aligner'\n\n  time { fq.size() < 1.GB ? 2.hour : 2.hour + 3.hour * (fq.size()/3000000000) * task.attempt }\n\n  tag { fq }\n\n  input:\n  path(fq)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n  \"\"\"\n  bwa aln \\\n    -t ${task.cpus} \\\n    ${params.genome_bwaidx} \\\n    ${fq} >r1.sai\n\n  bwa samse \\\n    ${params.genome_bwaidx} \\\n    r1.sai ${fq} >tmp.sam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam  TMP_DIR=\"\\$TMPDIR\" \\\n            I=tmp.sam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/MULTIQC", "kingzhuky/meripseqpipe/makechromesize", "kevbrick/pipeIt/bwaAlnSR"], "list_wf_names": ["kevbrick/pipeIt", "khigashi1987/CUTRUN_Nextflow", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bwaAlnPE {\n  label 'aligner'\n\n  time { fq1.size() < 1.GB ? 2.hour : 2.hour + 3.hour * (fq1.size()/3000000000) * task.attempt }\n\n  tag { fq1 }\n\n  input:\n  tuple path(fq1), path(fq2)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq1.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n  \"\"\"\n  bwa aln \\\n    -t ${task.cpus} \\\n    ${params.genome_bwaidx} \\\n    ${fq1} >r1.sai\n\n  bwa aln \\\n    -t ${task.cpus} \\\n    ${params.genome_bwaidx} \\\n    ${fq2} >r2.sai\n\n  bwa sampe \\\n    ${params.genome_bwaidx} \\\n    r1.sai r2.sai \\\n    ${fq1} ${fq2} >tmp.sam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n            I=tmp.sam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bwaAlnPE"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 4, "tools": ["SAMtools", "STAR", "fastPHASE", "BWA"], "nb_own": 2, "list_own": ["kingzhuky", "kevbrick"], "nb_wf": 2, "list_wf": ["meripseqpipe", "pipeIt"], "list_contrib": ["kingzhuky", "kevbrick", "juneb4869"], "nb_contrib": 3, "codes": [" process MakeStarIndex {\n        label 'build_index'\n        tag \"star_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n        file gtf\n\n        output:\n        file \"StarIndex\" into star_index\n\n        when:\n        aligner == \"star\"\n\n        script:\n        readLength = 50\n        overhang = readLength - 1\n        \"\"\"\n        mkdir StarIndex\n        STAR --runThreadN ${task.cpus} \\\n        --runMode genomeGenerate \\\n        --genomeDir StarIndex \\\n        --genomeFastaFiles $fasta \\\n        --sjdbGTFfile $gtf \\\n        --sjdbOverhang $overhang \\\n\t--limitGenomeGenerateRAM 36000000000\n        \"\"\"\n    }", "\nprocess bwaMemSR {\n  label 'aligner'\n\n  time { fq.size() < 3.GB ? 2.hour : 2.hour + 3.hour * (fq.size()/3000000000) * task.attempt }\n\n  tag { fq }\n\n  input:\n  path(fq)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n  \"\"\"\n  bwa mem \\\n    -t ${task.cpus} \\\n    ${params.genome_bwaidx} ${fq} >tmp.sam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n            I=tmp.sam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }", "\nprocess Fastp{\n    label 'fastp'\n    tag \"$sample_name\"\n                            \n    publishDir path: { params.skip_fastp ? params.outdir : \"${params.outdir}/QC/fastp\" },\n             saveAs: { params.skip_fastp ? null : it }, mode: 'link'\n        \n    input:\n    set val(sample_id), file(reads), val(reads_single_end), val(gzip), val(input), val(group) from raw_fastq\n\n    output:\n    set val(sample_name), file(\"*_aligners.fastq*\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into fastqc_reads, fastp_reads\n    file \"*\" into fastp_results\n\n    when:\n    aligner != \"none\"\n\n    shell:\n    skip_fastp = params.skip_fastp\n    if ( reads_single_end ){\n        filename = reads.toString() - ~/(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners = sample_name + \"_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then\n            fastp -i ${reads} -o ${add_aligners} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads} ${add_aligners}\n        fi\n        \"\"\"\n    } else {\n        filename = reads[0].toString() - ~/(_R[0-9])?(_[0-9])?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners_1 = sample_name + \"_1_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        add_aligners_2 = sample_name + \"_2_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then  \n            fastp -i ${reads[0]} -o ${add_aligners_1} -I ${reads[1]} -O ${add_aligners_2} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads[0]} ${add_aligners_1}\n            mv ${reads[1]} ${add_aligners_2}\n        fi\n        \"\"\"\n    } \n}", " process MakeBWAIndex {\n        label 'build_index'\n        tag \"bwa_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta\n\n        output:\n        file \"BWAIndex/*\" into bwa_index\n\n        when:\n        aligner == \"bwa\"\n     \n        script:\n        \"\"\"\n        mkdir BWAIndex\n        cd BWAIndex/\n        bwa index -p ${fasta.baseName} -a bwtsw ../$fasta\n        cd ../\n        \"\"\"\n    }"], "list_proc": ["kingzhuky/meripseqpipe/MakeStarIndex", "kevbrick/pipeIt/bwaMemSR", "kingzhuky/meripseqpipe/Fastp", "kingzhuky/meripseqpipe/MakeBWAIndex"], "list_wf_names": ["kevbrick/pipeIt", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FastQC", "BWA", "fastPHASE"], "nb_own": 2, "list_own": ["kingzhuky", "kevbrick"], "nb_wf": 2, "list_wf": ["meripseqpipe", "pipeIt"], "list_contrib": ["kingzhuky", "kevbrick", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess Fastp{\n    label 'fastp'\n    tag \"$sample_name\"\n                            \n    publishDir path: { params.skip_fastp ? params.outdir : \"${params.outdir}/QC/fastp\" },\n             saveAs: { params.skip_fastp ? null : it }, mode: 'link'\n        \n    input:\n    set val(sample_id), file(reads), val(reads_single_end), val(gzip), val(input), val(group) from raw_fastq\n\n    output:\n    set val(sample_name), file(\"*_aligners.fastq*\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into fastqc_reads, fastp_reads\n    file \"*\" into fastp_results\n\n    when:\n    aligner != \"none\"\n\n    shell:\n    skip_fastp = params.skip_fastp\n    if ( reads_single_end ){\n        filename = reads.toString() - ~/(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners = sample_name + \"_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then\n            fastp -i ${reads} -o ${add_aligners} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads} ${add_aligners}\n        fi\n        \"\"\"\n    } else {\n        filename = reads[0].toString() - ~/(_R[0-9])?(_[0-9])?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners_1 = sample_name + \"_1_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        add_aligners_2 = sample_name + \"_2_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then  \n            fastp -i ${reads[0]} -o ${add_aligners_1} -I ${reads[1]} -O ${add_aligners_2} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads[0]} ${add_aligners_1}\n            mv ${reads[1]} ${add_aligners_2}\n        fi\n        \"\"\"\n    } \n}", "\nprocess bwaMemPE {\n  label 'aligner'\n\n  time { fq1.size() < 3.GB ? 2.hour : 2.hour + 3.hour * (fq1.size()/3000000000) * task.attempt }\n\n  tag { fq1 }\n\n  input:\n  tuple path(fq1), path(fq2)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq1.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n\n  \"\"\"\n  bwa mem \\\n    -t ${task.cpus} \\\n    ${params.genome_bwaidx} \\\n    ${fq1} ${fq2} >tmp.sam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n            I=tmp.sam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }", "\nprocess Fastqc{\n    tag \"$sample_name\"\n    publishDir path: { params.skip_fastqc ? params.outdir : \"${params.outdir}/QC\" },\n             saveAs: { params.skip_fastqc ? null : it }, mode: 'link'\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from fastqc_reads\n\n    output:\n    file \"fastqc/*\" into fastqc_results\n\n    when:\n    aligner != \"none\" && !params.skip_fastqc\n\n    shell:\n    skip_fastqc = params.skip_fastqc\n    if ( reads_single_end){\n        \"\"\"\n        mkdir fastqc\n        fastqc -o fastqc --noextract ${reads}\n        \"\"\"       \n    } else {\n        \"\"\"\n        mkdir fastqc   \n        fastqc -o fastqc --noextract ${reads[0]}\n        fastqc -o fastqc --noextract ${reads[1]}\n        \"\"\"      \n    }\n}"], "list_proc": ["kingzhuky/meripseqpipe/Fastp", "kevbrick/pipeIt/bwaMemPE", "kingzhuky/meripseqpipe/Fastqc"], "list_wf_names": ["kevbrick/pipeIt", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie", "HISAT2"], "nb_own": 2, "list_own": ["kingzhuky", "kevbrick"], "nb_wf": 2, "list_wf": ["meripseqpipe", "pipeIt"], "list_contrib": ["kingzhuky", "kevbrick", "juneb4869"], "nb_contrib": 3, "codes": [" process FilterrRNA {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/rRNA_dup\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from rRNA_reads\n    file index from rRNA_index.collect()\n\n    output:\n    set val(sample_name), file(\"*.fastq.gz\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into tophat2_reads, hisat2_reads, bwa_reads, star_reads\n    file \"*_summary.txt\" into rRNA_log\n\n    when:\n    params.rRNA_fasta && !params.skip_filterrRNA\n\n    script:\n    gzip = true\n    index_base = index[0].toString() - ~/(\\.exon)?(\\.\\d)?(\\.fa)?(\\.gtf)?(\\.ht2)?$/\n    if (reads_single_end) {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-gz ${sample_id}.fastq.gz \\\n            -x $index_base \\\n            -U $reads | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        \"\"\"\n    } else {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-conc-gz ${sample_name}_fastq.gz \\\n            -x $index_base \\\n            -1 ${reads[0]} -2 ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        mv ${sample_name}_fastq.1.gz ${sample_name}_1.fastq.gz\n        mv ${sample_name}_fastq.2.gz ${sample_name}_2.fastq.gz\n        \"\"\"\n    }\n    }", "\nprocess bowtie2SR {\n  label 'aligner'\n\n  time { fq.size() < 3.GB ? 2.hour : 2.hour + 1.hour * (fq.size()/3000000000) * task.attempt }\n\n  tag { fq }\n\n  input:\n  file(fq)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n  \"\"\"\n  bowtie2 -x ${params.genome_bt2idx} -1 ${fq} -b init.tmpbam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n            I=init.tmpbam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }"], "list_proc": ["kingzhuky/meripseqpipe/FilterrRNA", "kevbrick/pipeIt/bowtie2SR"], "list_wf_names": ["kevbrick/pipeIt", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bowtie2PE {\n  label 'aligner'\n\n  time { fq1.size() < 3.GB ? 2.hour : 2.hour + 1.hour * (fq1.size()/3000000000) * task.attempt }\n\n  tag { fq1 }\n\n  input:\n  tuple path(fq1), path(fq2)\n\n  output:\n  tuple path('*.bam'), path('*.bai'), emit: bam\n\n  script:\n  def bam   = fq1.name.replaceFirst(\"(.R1.fastq|.R1.fastq.gz|.fastq.gz|.fastq)\",\".bam\")\n  \"\"\"\n  bowtie2 -x ${params.genome_bt2idx} -1 ${fq1} -2 ${fq2} -b init.tmpbam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n            I=init.tmpbam \\\n            O=${bam} \\\n            SO=coordinate \\\n            VALIDATION_STRINGENCY=LENIENT\n\n  samtools index ${bam}\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bowtie2PE"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess mergeBAM {\n  label 'mergeBAM'\n\n  publishDir \"${params.outdir}/bam\",     mode: 'copy', overwrite: true, pattern: '*bam*'\n  publishDir \"${params.outdir}/reports\", mode: 'copy', overwrite: true, pattern: '*txt'\n\n  time { bams[0].size() < 200000000 ? 6.hour : bams[0].size()/200000000 * task.attempt * 6.hour }\n\n  tag { bams }\n  input:\n  path(bams)\n\n  output:\n  tuple(path('*.bam'),path('*.bai'), emit: bam)\n  path('*MDmetrics.txt', emit: mdreport)\n\n  script:\n                              \n  def input_args = bams.findAll{ it =~ \".bam\\$\" }.collect{ \"I=$it\" }.join(\" \")\n  def name = \"${params.name}.${params.genome}\"\n  \"\"\"\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MergeSamFiles TMP_DIR=\"\\$TMPDIR\" \\\n                 ${input_args} \\\n                 O=merged.tmpbam \\\n                 AS=false \\\n                 SO=coordinate \\\n                 VALIDATION_STRINGENCY=LENIENT\n\n  if [[ `samtools view -h merged.bam |head -n 100000 |samtools view -f 2 ` ]]; then\n\t  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MarkDuplicatesWithMateCigar TMP_DIR=\"\\$TMPDIR\" \\\n                   I=merged.tmpbam \\\n                   O=${name}.bam \\\n                   PG=Picard2.9.2_MarkDuplicatesWithMateCigar \\\n                   M=${name}.MDmetrics.txt \\\n                   MINIMUM_DISTANCE=400 \\\n\t\t\t     CREATE_INDEX=false \\\n\t\t\t     ASSUME_SORT_ORDER=coordinate \\\n           VALIDATION_STRINGENCY=LENIENT\n  else\n    java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MarkDuplicates TMP_DIR=\"\\$TMPDIR\" \\\n                   I=merged.tmpbam \\\n                   O=${name}.bam \\\n                   PG=Picard2.9.2_MarkDuplicates \\\n                   M=${name}.MDmetrics.txt \\\n\t\t\t     CREATE_INDEX=false \\\n\t\t\t     ASSUME_SORT_ORDER=coordinate \\\n           VALIDATION_STRINGENCY=LENIENT\n  fi\n\n  samtools index ${name}.bam\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/mergeBAM"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess getPicardMetrics {\n\n  publishDir \"${params.outdir}/reports\",  mode: 'copy', overwrite: true\n\n  time { bam.size()< 200000000 ? 2.hour * task.attempt: 2.hour + 2.hour * bam.size()/200000000 * task.attempt }\n\n  tag {bam}\n\n  input:\n  tuple path(bam), path(bai)\n  val(srpe)\n\n  output:\n  path('*Metrics*', emit: report)\n\n  script:\n  def name=bam[0].name.replaceFirst(\".bam\",\"\")\n  def picardMem=task.memory.toGiga() - 2\n\n  if (srpe == 'PE'){\n\t  \"\"\"\n    picard -Xmx${picardMem}g CreateSequenceDictionary \\\n\t      R=${params.genome_fasta} \\\n\t      O=genome.dict \\\n\t      TMP_DIR=\\$TMPDIR \\\n\t      VALIDATION_STRINGENCY=LENIENT\n\n    picard -Xmx${picardMem}g CollectAlignmentSummaryMetrics \\\n\t    VALIDATION_STRINGENCY=LENIENT \\\n\t    REFERENCE_SEQUENCE=${params.genome_fasta} \\\n\t    I=${bam} \\\n\t    O=${name}.AlignmentSummary.picardMetrics.tab \\\n\t    TMP_DIR=\\$TMPDIR\n\n    picard -Xmx${picardMem}g CollectInsertSizeMetrics \\\n\t      VALIDATION_STRINGENCY=LENIENT \\\n\t      I=${bam} \\\n\t      O=${name}.InsertSize.picardMetrics.tab \\\n\t      H=${name}.InsertSize.picardMetrics.pdf \\\n\t      M=0.5 \\\n\t      TMP_DIR=\\$TMPDIR\n\n    picard -Xmx${picardMem}g MeanQualityByCycle \\\n\t      VALIDATION_STRINGENCY=LENIENT \\\n\t      I=${bam} \\\n\t      O=${name}.QByCycle.picardMetrics.tab \\\n\t      TMP_DIR=\\$TMPDIR \\\n\t      CHART=${name}.QByCycle.picardMetrics.pdf\n\n    picard -Xmx${picardMem}g QualityScoreDistribution \\\n\t      VALIDATION_STRINGENCY=LENIENT \\\n\t      I=${bam} \\\n\t      O=${name}.QScoreDist.picardMetrics.tab \\\n\t      TMP_DIR=\\$TMPDIR \\\n\t      CHART=${name}.QScoreDist.picardMetrics.pdf\n\t  \"\"\"\n  }else{\n\t  \"\"\"\n    picard -Xmx${picardMem}g CreateSequenceDictionary \\\n\t      R=${params.genome_fasta} \\\n\t      O=genome.dict \\\n\t      TMP_DIR=\\$TMPDIR \\\n\t      VALIDATION_STRINGENCY=LENIENT\n\n    picard -Xmx${picardMem}g MeanQualityByCycle \\\n\t      VALIDATION_STRINGENCY=LENIENT \\\n\t      I=${bam} \\\n\t      O=${name}.QByCycle.picardMetrics.tab \\\n\t      TMP_DIR=\\$TMPDIR \\\n\t      CHART=${name}.QByCycle.picardMetrics.pdf\n\n    picard -Xmx${picardMem}g QualityScoreDistribution \\\n\t      VALIDATION_STRINGENCY=LENIENT \\\n\t      I=${bam} \\\n\t      O=${name}.QScoreDist.picardMetrics.tab \\\n\t      TMP_DIR=\\$TMPDIR \\\n\t      CHART=${name}.QScoreDist.picardMetrics.pdf\n\t  \"\"\"\n  }\n  }"], "list_proc": ["kevbrick/pipeIt/getPicardMetrics"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["bedGraphToBigWig", "HiFiX"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess bamToBW {\n\n  publishDir \"${params.outdir}/bigwig\",  mode: 'copy', overwrite: true, pattern: '*bigwig'\n\n  time { bam.size()< 1000000000 ? 0.5.hour * task.attempt: 1.hour * bam.size()/1000000000 * task.attempt }\n\n\ttag { bam }\n\n  input:\n  tuple path(bam), path(bai)\n\n  output:\n  path('*bigwig', emit: bw)\n\n  script:\n  def name=bam.name.replaceFirst(\".bam\",\"\")\n  \"\"\"\n  if [[ `samtools view -F 4 ${bam} |head -n 5001 |wc -l` -gt 5000 ]]; then\n    genomeCoverageBed -ibam ${bam} -bg >${name}.tmpbg\n    ## SORT BY INDEX TO ASSURE WE ONLY HAVE CHROMOSOMES MENTIONED IN THE INDEX THEN DO A LINUX SORT\n    ## THIS IS A BIT ASSWAYS; FIX LATER\n    sortBEDByFAI.pl ${name}.tmpbg ${params.genome_fai} |sort -k1,1 -k2n,2n -k3n,3n >${name}.bedgraph\n    bedGraphToBigWig ${name}.bedgraph ${params.genome_fai} ${name}.bedtools.bigwig\n  else\n    touch EMPTY_TOOFEWREADS_${name}.bedtools.bigwig\n  fi\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/bamToBW"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools", "plotcoverage"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess makeDeeptoolsBW {\n\n  publishDir \"${params.outdir}/bigwig\",  mode: 'copy', overwrite: true, pattern: '*bigwig'\n  publishDir \"${params.outdir}/plots\",   mode: 'copy', overwrite: true, pattern: '*png'\n  publishDir \"${params.outdir}/reports\", mode: 'copy', overwrite: true, pattern: '*tab'\n\n  time { bam.size()< 500000000 ? 1.hour : 1.hour + 1.hour * bam.size()/500000000 * task.attempt }\n\n  tag {bam}\n\n  input:\n  tuple(path(bam), path(bai))\n\n  output:\n  path('*bigwig', emit: bw)\n  path('*png',    emit: png)\n  path('*tab',    emit: tab)\n\n  script:\n  def name            = bam.name.replaceAll(/.bam/,'')\n  \"\"\"\n  if [[ `samtools view -F 4 ${bam} |head -n 5001 |wc -l` -gt 5000 ]]; then\n\n   ## Added KB 2003-03-19:\n   ## Split BAM by chromosome\n   for cs in `samtools idxstats ${bam} |cut -f1 |grep -vP '\\\\*'`; do\n     csbam=\\$cs\".bam\"\n     csBW=\\$cs\".ALL.BW\"\n     csBWND=\\$cs\".ND.BW\"\n     cswig=\\$cs\".ALL.wig\"\n     cswigND=\\$cs\".ND.wig\"\n\n     if [ `samtools view -F 4 ${bam} \\$cs |head -n 10 |wc -l` -eq 10 ]; then\n       samtools view -F 4 -hb ${bam} \\$cs >\\$csbam\n       samtools index \\$csbam\n\n       bamCoverage --bam \\$csbam --binSize 150 --normalizeUsing RPKM \\\n         -p max -v -o \\$csBW\n       bigWigToWig \\$csBW \\$cswig\n       grep -w \\$cs \\$cswig >>all.wig\n\n       bamCoverage --bam \\$csbam --binSize 150 --normalizeUsing RPKM \\\n           --ignoreDuplicates -p max -v -o \\$csBWND\n       bigWigToWig \\$csBWND \\$cswigND\n       grep -w \\$cs \\$cswigND >>ND.wig\n     fi\n   done\n\n   wigToBigWig all.wig ${params.genome_fai} ${name}.deeptools.150bp.RPKM.bigwig\n   wigToBigWig ND.wig  ${params.genome_fai} ${name}.deeptools.150bp.RPKM.noDups.bigwig\n\n   plotCoverage --bamfiles ${bam} --numberOfProcessors ${task.cpus} \\\n     -o ${name}.deeptools.coveragePlot.png\n\n   plotFingerprint --bamfiles ${bam} --labels ${bam} --numberOfProcessors ${task.cpus} \\\n     --minMappingQuality 30 --skipZeros \\\n     --plotFile ${name}.deeptools.fingerprints.png \\\n     --outRawCounts ${name}.deeptools.fingerprints.tab\n\n  else\n    touch EMPTY_TOOFEWREADS_${name}.bigwig\n    touch EMPTY_TOOFEWREADS_${name}.deeptools.coveragePlot.png\n    touch EMPTY_TOOFEWREADS_${name}.deeptools.fingerprints.png\n    touch EMPTY_TOOFEWREADS_${name}.deeptools.fingerprints.tab\n  fi\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/makeDeeptoolsBW"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess samStats {\n\n  publishDir \"${params.outdir}/reports\",  mode: 'copy', overwrite: true\n\n  time { bam.size()< 10000000000 ? 1.hour : 1.hour * bam.size()/10000000000 * task.attempt }\n\n  tag {bam}\n\n  input:\n  tuple path(bam),  path(idx)\n\n  output:\n  path('*stats.tab', emit: report)\n\n  script:\n  iStat = bam.name.replaceAll(/.bam/,\".idxstats.tab\")\n  sStat = bam.name.replaceAll(/.bam/,\".samstats.tab\")\n  \"\"\"\n  samtools idxstats ${bam} >${iStat}\n  samtools stats ${bam} >${sStat}\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/samStats"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["bedGraphToBigWig", "BEDTools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": [" process makeFRBW {\n    publishDir \"${params.outdir}/bigwig\",  mode: 'copy', overwrite: true, pattern: '*bigwig'\n\n    cpus 2\n    memory 6\n\n    time { bam.size()< 5000000000 ? 1.hour : 1.hour * bam.size()/5000000000 * task.attempt }\n\n    errorStrategy { 'retry' }\n    maxRetries 1\n\n    input:\n    tuple path(bam), path(idx)\n\n    output:\n    path('*.bigwig', emit: bw)\n\n    script:\n    iName = bam.name.replaceAll(/.bam/,\".out\")\n\n    \"\"\"\n    if [[ `samtools view -F 4 ${bam} |head -n 5001 |wc -l` -gt 5000 ]]; then\n      idx=\"${params.genome_fai}\";\n\n      bedtools makewindows  -g \\$idx -w 1000 -s 100 |sort -k1,1 -k2n,2n |perl -lane 'print \\$_ if ((\\$F[2]-\\$F[1]) == 1000)' >win.bed\n      java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam I=${bam} O=qSort.bam SO=queryname VALIDATION_STRINGENCY=LENIENT TMP_DIR=\"\\$TMPDIR\"\n      bedtools bamtobed -mate1 -i qSort.bam -bedpe >frags.bedpe\n\n      perl -lane 'print join(\"\\\\t\", \\$F[0], \\$F[1], \\$F[5], @F[6..8]) if (\\$F[8] eq \"+\")' frags.bedpe |sort -k1,1 -k2n,2n >frags.POS.bed\n      perl -lane 'print join(\"\\\\t\", \\$F[0], \\$F[4], \\$F[2], @F[6..8]) if (\\$F[8] eq \"-\")' frags.bedpe |sort -k1,1 -k2n,2n >frags.NEG.bed\n\n      mapBed -a win.bed -b frags.POS.bed -c 1 -o count |perl -lane 'use Math::Round; \\$p=round((\\$F[1]+\\$F[2])/2); print join(\"\\\\t\",\\$F[0],\\$p-49,\\$p+50,\\$F[3])' >pos.bg\n      mapBed -a win.bed -b frags.NEG.bed -c 1 -o count |perl -lane 'use Math::Round; \\$p=round((\\$F[1]+\\$F[2])/2); print join(\"\\\\t\",\\$F[0],\\$p-49,\\$p+50,\\$F[3])' >neg.bg\n\n      paste pos.bg neg.bg |perl -lane '\\$F[3]+=0.5; \\$F[7]+=0.5; \\$v=(\\$F[3]/\\$F[7]); print join(\"\\\\t\",@F[0..2],(log(\\$v)/log(2)))' >fr.bg\n      paste pos.bg neg.bg |perl -lane '\\$v=(\\$F[3]+\\$F[7]); print join(\"\\\\t\",@F[0..2],\\$v)' >tot.bg\n\n      bedGraphToBigWig pos.bg \\$idx ${iName}.F.bigwig\n      bedGraphToBigWig neg.bg \\$idx ${iName}.F.bigwig\n      bedGraphToBigWig fr.bg  \\$idx ${iName}.FR.bigwig\n      bedGraphToBigWig tot.bg \\$idx ${iName}.Tot.bigwig\n    else\n      touch \"EMPTY_TOOFEWREADS_${iName}.F.bigwig\n      touch \"EMPTY_TOOFEWREADS_${iName}.R.bigwig\n      touch \"EMPTY_TOOFEWREADS_${iName}.FR.bigwig\n      touch \"EMPTY_TOOFEWREADS_${iName}.Tot.bigwig\n    fi\n    \"\"\"\n    }"], "list_proc": ["kevbrick/pipeIt/makeFRBW"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess mergeBAMv2 {\n\n  label 'mergeBAM'\n\n  time { 5.hour * task.attempt }\n\n  tag { \"${name} : ${bams.size()}\" }\n\n  input:\n  tuple(val(name), path(bams))\n\n  output:\n  tuple(val(name), path('*.bam'), path('*.bai'), emit: bam)\n\n  script:\n                              \n                                                                                \n  \"\"\"\n  ## STUPID PAIRTOOLS CAN ADD DUPLICATE PG FIELDS TO BAM\n  ## THIS FIXES THE ISSUE (WHICH KILLS PICARD)\n  for bam in *.bam; do\n    newbam=\\${bam/bam/okheader.tmpbam}\n    samtools view -h \\$bam |perl -lane 'if (\\$_ =~/ID:(\\\\S+)/){\\$prog=\\$1; if(\\$PG{\\$prog}++){\\$_ =~ s/ID:(\\\\S+)/ID:\\$1\\\\.\\$PG{\\$prog}/}}; if (\\$_ =~/PN:(\\\\S+)/){\\$prog=\\$1; if(\\$PN{\\$prog}++){\\$_ =~ s/PN:(\\\\S+)/PN:\\$1\\\\.\\$PN{\\$prog}/}};print \\$_;' |samtools view -Shb - >\\$newbam\n    #samtools view -h \\$bam |perl -lane 'if (\\$_ =~/ID:(\\\\S+)/){\\$prog=\\$1; if(\\$PG{\\$prog}++){\\$_ =~ s/ID:(\\\\S+)/ID:\\$1\\\\.\\$PG{\\$prog}/}}; if (\\$_ =~/PN:(\\\\S+)/){\\$prog=\\$1; if(\\$PN{\\$prog}++){\\$_ =~ s/PN:(\\\\S+)/PN:\\$1\\\\.\\$PN{\\$prog}/}}; print \\$_;' |samtools view -Shb - >S3ok.bam\n  done\n\n  input_args=`ls *okheader.tmpbam |perl -pi -e 's/\\\\n/ /g' |perl -pi -e 's/(\\\\S+)/I=\\$1/g'`\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MergeSamFiles TMP_DIR=\"\\$TMPDIR\" \\\n                 \\$input_args \\\n                 O=merged.tmpbam \\\n                 AS=false \\\n                 VALIDATION_STRINGENCY=LENIENT\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam TMP_DIR=\"\\$TMPDIR\" \\\n                I=merged.tmpbam \\\n                O=${name}.bam \\\n                SO=coordinate \\\n                VALIDATION_STRINGENCY=LENIENT\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR BuildBamIndex \\\n                I=${name}.bam VALIDATION_STRINGENCY=LENIENT\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/mergeBAMv2"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess markBAMduplicates {\n\n  label 'mergeBAM'\n\n  publishDir \"${params.outdir}/bam\",     mode: 'copy', overwrite: true, pattern: '*bam*'\n  publishDir \"${params.outdir}/reports\", mode: 'copy', overwrite: true, pattern: '*txt'\n\n  time { bam.size() < 2000000000 ? 6.hour : bam.size()/2000000000 * task.attempt * 6.hour }\n\n  tag { bam }\n\n  input:\n  tuple(val(name), path(bam), path(bai))\n\n  output:\n  tuple(val(name), path('*.bam'), path('*.bai'), emit: bam)\n  path('*MDmetrics.txt', emit: mdreport)\n\n  script:\n  \"\"\"\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MergeSamFiles TMP_DIR=\"\\$TMPDIR\" \\\n                 I=${bam} \\\n                 O=merged.tmpbam \\\n                 AS=true \\\n                 SO=coordinate \\\n                 VALIDATION_STRINGENCY=LENIENT\n\n  if [[ `samtools view -h merged.bam |head -n 100000 |samtools view -f 2 ` ]]; then\n\t  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MarkDuplicatesWithMateCigar TMP_DIR=\"\\$TMPDIR\" \\\n                   I=merged.tmpbam \\\n                   O=${name}.MD.bam \\\n                   PG=Picard_MarkDuplicatesWithMateCigar \\\n                   M=${name}.MDmetrics.txt \\\n                   MINIMUM_DISTANCE=400 \\\n\t\t\t     CREATE_INDEX=false \\\n\t\t\t     ASSUME_SORT_ORDER=coordinate \\\n           VALIDATION_STRINGENCY=LENIENT\n  else\n    java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MarkDuplicates TMP_DIR=\"\\$TMPDIR\" \\\n                   I=merged.tmpbam \\\n                   O=${name}.MD.bam \\\n                   PG=Picard_MarkDuplicates \\\n                   M=${name}.MDmetrics.txt \\\n\t\t\t     CREATE_INDEX=false \\\n\t\t\t     ASSUME_SORT_ORDER=coordinate \\\n           VALIDATION_STRINGENCY=LENIENT\n  fi\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR BuildBamIndex \\\n                I=${name}.MD.bam VALIDATION_STRINGENCY=LENIENT\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/markBAMduplicates"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["process getCoverage {\n\n  tag { chrom }\n\n  input:\n  tuple(path(bam), path(bai))\n  path(gctab)\n  val(chrom)\n\n  output:\n  path(\"*.w*0k.bedgraph\", emit: bg)\n  path(\"*.GCdata.tab\",    emit: gcData)\n\n  script:\n  \n  def win101=\"${chrom}.win101ALL.bed\"\n  \"\"\"\n  tbam=\"${chrom}.tmp.bam\"\n  s1bam=\"${chrom}.s1.bam\"\n  s2bam=\"${chrom}.s2.bam\"\n  s3bam=\"${chrom}.s3.bam\"\n  s4bam=\"${chrom}.s4.bam\"\n  cbam=\"${chrom}.bam\"\n\n  samtools view -hb -q 30 ${bam} ${chrom} >\\$tbam\n  samtools index \\$tbam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam I=\\$tbam O=\\$s1bam VALIDATION_STRINGENCY=LENIENT SO=queryname TMP_DIR=\"\\$TMPDIR\"\n\n  nPE=`samtools view -h \\$tbam |head -n 100000 |samtools view -c -f 1 -S /dev/stdin`\n\n  if [ \"\\$nPE\" -eq \"0\" ]; then\n    ln -s \\$s1bam \\$s3bam\n  else\n    java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR FixMateInformation I=\\$s1bam O=\\$s2bam VALIDATION_STRINGENCY=LENIENT SO=queryname AS=true TMP_DIR=\"\\$TMPDIR\"\n    samtools view -hb -f 2 \\$s2bam >\\$s3bam\n  fi\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MarkDuplicates I=\\$s3bam O=\\$s4bam VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=true M=metrics.tab TMP_DIR=\"\\$TMPDIR\"\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR SortSam I=\\$s4bam O=\\$cbam VALIDATION_STRINGENCY=LENIENT SO=coordinate TMP_DIR=\"\\$TMPDIR\"\n  samtools index \\$cbam\n\n  readLen=`perl ${params.rtData}/scripts/getClosestReadLength.pl \\$tbam 50,150`\n  #win101=\"${params.rtData}/genomewin/${chrom}.win101.bed\"\n  #ps101=\"${params.rtData}/genomewin/${chrom}.psr\"\\$readLen\".tab\"\n  #gc101=\"${params.rtData}/genomewin/${chrom}.win101.GC.tab\"\n\n  ### NEW ###\n  grep -w ${chrom} ${params.genome_fai} >chrom.fai\n  \n  bedtools makewindows -g chrom.fai -w 101 -s 1 |\n    perl -lane 'print join(\"\\\\t\",@F) unless ((\\$F[2]-\\$F[1]) != 101)' >${win101}\n    \n  bedtools nuc -fi ${params.genome_fasta} -bed ${win101} -C | grep ^chr | \\\n                   cut -f1-3,5 | perl -M\"Math::Round\" -pi -e 's/(0\\\\.\\\\d\\\\d+)/round(\\$1*100)/e' |cut -f4 >GC.tab\n  \n  cp \"${params.rtData}/mapability/${params.genome}/${chrom}.mapability_\"\\$readLen\"bp.tab.gz\" ./mapability.txt.gz\n  gunzip mapability.txt.gz\n      \n  intersectBed -a ${win101} -b \\$cbam -c -sorted -g ${params.genome_fai} |perl -pi -e 's/\\\\./0/g' >${chrom}.cover.tab\n\n  paste ${chrom}.cover.tab mapability.txt GC.tab |perl -pi -e 's/\\\\s\\\\./0(\\\\s|\\$)/g' >${chrom}.tmp\n  \n  intersectBed -v -sorted -a ${chrom}.tmp -b ${params.rtData}/blacklist/${params.genome}.blacklist.bed >${chrom}.tab\n  ### END NEW ###\n  \n  ## DO GC NORMALIZATION\n  expectedSim=`echo \\$readLen + 100 |bc`\n  mapabilityLowerLim=`echo \"(\\$readLen + 100)*0.66\" |bc`\n  mapabilityUpperLim=`echo \"(\\$readLen + 100)*1.2\" |bc`\n\n  perl ${params.rtData}/scripts/normalizeBy2NGC.pl ${gctab} ${chrom}.tab \\$expectedSim\n\n  shuf GCData.tab |head -n 1000000 |sort -k1rn,1rn >${chrom}.GCdata.tab\n\n  perl -lane 'print join(\"\\\\t\",@F[0..2],\\$F[4]) if (\\$F[4] > '\\$mapabilityLowerLim' && \\$F[4] <  '\\$mapabilityUpperLim' )' ${chrom}.tab >${chrom}.mapability.tab\n  nCov=`echo \"500000 * ${params.covPC} /100\" |bc`\n  \n  bedtools makewindows -g chrom.fai -w 500000 -s 50000  | perl -lane \\'print join(\"\\\\t\",@F) unless ((\\$F[2]-\\$F[1]) != 500000)\\' >win500k50k.init.bed\n  bedtools makewindows -g chrom.fai -w 500000 -s 100000 | perl -lane \\'print join(\"\\\\t\",@F) unless ((\\$F[2]-\\$F[1]) != 500000)\\' >win500k100k.init.bed\n\n  mapBed -a win500k50k.init.bed  -b ${chrom}.mapability.tab -c 4 -o count |perl -lane 'print join(\"\\\\t\",@F[0..2]) if (\\$F[3] > int(500000*'${params.covPC}'/100))' >win500k50k.bed\n  mapBed -a win500k100k.init.bed -b ${chrom}.mapability.tab -c 4 -o count |perl -lane 'print join(\"\\\\t\",@F[0..2]) if (\\$F[3] > int(500000*'${params.covPC}'/100))' >win500k100k.bed\n\n  mapBed -a win500k50k.bed  -b ${chrom}.GCcorrected.bedgraph -c 4,4 -o sum,count |perl -M\"Math::Round\" -lane '\\$F[3] = 0 if (\\$F[3] eq \".\" );  \\$val=\\$F[4]?\\$F[3]/\\$F[4]:0; print join(\"\\\\t\",\\$F[0],\\$F[1]+250000-25000,\\$F[1]+250000+24999,round(\\$val*100)/100) if (\\$val)' >${chrom}.win500k50k.tmp\n  mapBed -a win500k100k.bed -b ${chrom}.GCcorrected.bedgraph -c 4,4 -o sum,count |perl -M\"Math::Round\" -lane '\\$F[3] = 0 if (\\$F[3] eq \".\" );  \\$val=\\$F[4]?\\$F[3]/\\$F[4]:0; print join(\"\\\\t\",\\$F[0],\\$F[1]+250000-50000,\\$F[1]+250000+49999,round(\\$val*100)/100) if (\\$val)' >${chrom}.win500k100k.tmp\n\n  intersectBed -c -sorted -a ${chrom}.win500k50k.tmp  -b ${params.rtData}/blacklist/${params.genome}.blacklist.bed  >${chrom}.w500ks50k.bedgraph\n  intersectBed -c -sorted -a ${chrom}.win500k100k.tmp -b ${params.rtData}/blacklist/${params.genome}.blacklist.bed  >${chrom}.w500ks100k.bedgraph\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/getCoverage"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["kevbrick"], "nb_wf": 1, "list_wf": ["pipeIt"], "list_contrib": ["kevbrick"], "nb_contrib": 1, "codes": ["\nprocess generateMpileup {\n\n  tag { chrom }\n\n                                                             \n\n  input:\n  tuple(path(bam),path(bai))\n  val(chrom)\n\n  output:\n                                  \n  path(\"*.DS.tab\",  emit: dsGC)\n\n  script:\n  def nregion;\n  if (params.test & !params.fullChromTest){\n    nregion=\":1-15000000\"\n  }else{\n    nregion=\"\"\n  }\n\n  \"\"\"\n  tbam=\"${chrom}.tmp.bam\"\n  cbam=\"${chrom}.bam\"\n\n  samtools view -hb ${bam} ${chrom} >\\$tbam\n  samtools index \\$tbam\n\n  java -jar -Xmx${task.memory.toGiga()}g \\$PICARDJAR MarkDuplicates I=\\$tbam O=\\$cbam \\\n                     VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=true AS=true M=metrics.tab TMP_DIR=\"\\$TMPDIR\"\n  samtools index \\$cbam\n\n  readLen=`perl ${params.accessoryDir}/rtSeq/scripts/getClosestReadLength.pl \\$tbam 50,150`\n  pseudoReadDir=${params.pseudoReadBase}\"/\"\\$readLen\"bpReads_1bpStep/\"\n  \n  genomeCoverageBed -ibam \\$cbam -d -g >coverage.gcb.tab\n  \n  samtools mpileup -r ${chrom}${nregion} -f ${params.genome_mask_fa} -q 31 \\\n                                   \\$pseudoReadDir/${chrom}.bam \\$cbam \\\n                                   |perl -lane \\'print join(\"\\\\t\",\\$F[0],\\$F[1]-1,\\$F[1],\\$F[3],\\$F[6],(\\$F[2] =~ /[GATC]/?1:0),(\\$F[2] =~ /[GC]/?1:0))\\' \\\n                                   >${chrom}.mpu\n\n  grep -w ${chrom} ${params.genome_fai} >idx.fai\n  bedtools makewindows -g idx.fai -w 101 -s 1 | intersectBed -v -sorted -a - -b ${params.rtData}/blacklist/${params.genome}.blacklist.bed \\\n                                              |perl -lane \\'print join(\"\\\\t\",@F) unless ((\\$F[2]-\\$F[1]) != 101)\\' >win101.bed\n\n  nwin=`cat win101.bed |wc -l`\n  nDS=`printf %1.0f \\$(echo \"\\$nwin*0.02\" |bc)`\n  \n  mapBed -a win101.bed -b ${chrom}.mpu -c 4,5,6,7 -o sum,sum,sum,sum |perl -pi -e 's/\\\\./NA/g' >${chrom}.ALL.tab\n\n  ##Criteria :              NOT NA         && >0 cover &&   Fully mapable  &&  no repeat DNA\n  perl -lane 'print \\$_ if (\\$F[4] !~ /NA/ &&  \\$F[4]  && \\$F[3] == ('\\$readLen' * 101) && \\$F[5] eq \"101\")' ${chrom}.ALL.tab >ok.tab\n\n  shuf ok.tab |head -n \\$nDS |sort -k1,1 -k2n,2n >${chrom}.DS.tab\n  \"\"\"\n  }"], "list_proc": ["kevbrick/pipeIt/generateMpileup"], "list_wf_names": ["kevbrick/pipeIt"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["kevinpryan"], "nb_wf": 1, "list_wf": ["rtp_workshop"], "list_contrib": ["kevinpryan"], "nb_contrib": 1, "codes": ["\nprocess FASTQC{\n    tag \"{base}\"\n    publishDir \"${params.outdir}/quality_control/fastqc\", mode: 'copy',\n        saveAs: { params.save_qc_intermediates ? \"fastqc/${it}\" : null }\n\n    input:\n    tuple val(base), file(reads) from ch_qc_reads\n\n    output:\n    file(\"*.{html,zip}\") into ch_multiqc\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["kevinpryan/rtp_workshop/FASTQC"], "list_wf_names": ["kevinpryan/rtp_workshop"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["kevinpryan"], "nb_wf": 1, "list_wf": ["rtp_workshop"], "list_contrib": ["kevinpryan"], "nb_contrib": 1, "codes": ["\nprocess INDEX{\n    publishDir \"${params.outdir}/index\", mode: 'copy'\n\n    input:\n    file(transcriptome) from ch_transcriptome\n\n    output:\n    file(\"${transcriptome.baseName}.idx\") into index_created\n\n    script:\n    \"\"\"\n    kallisto index -i \"${transcriptome.baseName}.idx\" $transcriptome\n    \"\"\"\n}"], "list_proc": ["kevinpryan/rtp_workshop/INDEX"], "list_wf_names": ["kevinpryan/rtp_workshop"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["kevinpryan"], "nb_wf": 1, "list_wf": ["rtp_workshop"], "list_contrib": ["kevinpryan"], "nb_contrib": 1, "codes": ["\nprocess KALLISTO_QUANT{\n    publishDir \"${params.outdir}/quantification\", mode: 'copy'\n\n    input:\n    file(index) from index_created\n    tuple val(base), file(reads) from ch_raw_reads\n\n    output:\n                                                                                   \n    file(\"${base}.kallisto.log\") into kallisto_logs\n    script:\n    \"\"\"\n    kallisto quant -i  $index -o ${base} $reads &> ${base}.kallisto.log\n    \"\"\"\n\n}"], "list_proc": ["kevinpryan/rtp_workshop/KALLISTO_QUANT"], "list_wf_names": ["kevinpryan/rtp_workshop"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["kevinpryan"], "nb_wf": 1, "list_wf": ["rtp_workshop"], "list_contrib": ["kevinpryan"], "nb_contrib": 1, "codes": ["\nprocess MULTIQC{\n    publishDir \"${params.outdir}/quality_control/multiqc\", mode: 'copy'\n\n    input:\n    file(htmls) from ch_multiqc.collect()\n    file(kallisto_logs) from kallisto_logs.collect()\n\n    output:\n    file(\"*.html\") into ch_out\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["kevinpryan/rtp_workshop/MULTIQC"], "list_wf_names": ["kevinpryan/rtp_workshop"]}, {"nb_reuse": 1, "tools": ["FastQC", "Trimmomatic"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": ["\nprocess TRIM_1STROUND {\n    tag \"$name\"\n    publishDir \"${params.outdir}/trim_1st\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                    if (filename.endsWith('.html')) \"fastqc/$filename\"\n                    else if (filename.endsWith('.zip')) \"fastqc/zips/$filename\"\n                    else filename\n            }\n\n    input:\n    tuple val(name), path(reads) from ch_raw_reads_trim\n    path adapter from ch_adapter\n\n    output:\n    tuple val(name), path('*.paired.fastq.gz') into ch_trimmed_1st\n    path '*.{zip,html}' into ch_trim1st_fastqc_reports_mqc\n\n    \"\"\"\n    [ ! -f  ${name}_1.fastq.gz ] && ln -s ${reads[0]} ${name}_1.fastq.gz\n    [ ! -f  ${name}_2.fastq.gz ] && ln -s ${reads[1]} ${name}_2.fastq.gz\n    trimmomatic PE -threads 1 -phred33 \\\n      ${name}_1.fastq.gz ${name}_2.fastq.gz \\\n      ${name}_1.paired.fastq.gz ${name}_1.unpaired.fastq.gz \\\n      ${name}_2.paired.fastq.gz ${name}_2.unpaired.fastq.gz \\\n      ILLUMINACLIP:${adapter}:2:15:4:4:true \\\n      LEADING:20 \\\n      TRAILING:20 \\\n      SLIDINGWINDOW:4:15 \\\n      MINLEN:25\n    fastqc -q -t $task.cpus ${name}_1.paired.fastq.gz\n    fastqc -q -t $task.cpus ${name}_2.paired.fastq.gz\n    \"\"\"\n}"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/TRIM_1STROUND"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": ["\nprocess TRIM_2NDROUND {\n    tag \"$name\"\n    publishDir \"${params.outdir}/trim_2nd\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                    if (filename.endsWith('.html')) \"fastqc/$filename\"\n                    else if (filename.endsWith('.zip')) \"fastqc/zips/$filename\"\n                    else filename\n            }\n\n    input:\n    tuple val(name), path(reads) from ch_trimmed_1st\n\n    output:\n    tuple val(name), path('*.paired.trimmed.fastq.gz') into ch_trimmed_2nd\n    path '*.{zip,html}' into ch_trim2nd_fastqc_reports_mqc\n\n    \"\"\"\n    kseq_test ${reads[0]} ${params.seq_len} ${name}_1.paired.trimmed.fastq.gz\n    kseq_test ${reads[1]} ${params.seq_len} ${name}_2.paired.trimmed.fastq.gz\n    fastqc -q -t $task.cpus ${name}_1.paired.trimmed.fastq.gz\n    fastqc -q -t $task.cpus ${name}_2.paired.trimmed.fastq.gz\n    \"\"\"\n}"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/TRIM_2NDROUND"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": ["\nprocess BOWTIE2 {\n    tag \"$name\"\n    publishDir \"${params.outdir}/bowtie2_aln\", mode: params.publish_dir_mode\n\n    input:\n    tuple val(name), path(reads) from ch_trimmed_2nd\n    path index from ch_bt2_index.collect()\n\n    output:\n    tuple val(name), path('*.bam') into ch_bt2\n\n    \"\"\"\n    bowtie2 -p 2 --dovetail --phred33 -x ${index}/${bt2_base} \\\n      -1 ${reads[0]} \\\n      -2 ${reads[1]} \\\n    | samtools view -bS - -o ${name}.bam \\\n    \"\"\"\n}"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/BOWTIE2"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": ["\nprocess SORT_BAM {\n    tag \"$name\"\n    if (params.save_align_intermeds) {\n        publishDir \"${params.outdir}/sorted_bam\", mode: params.publish_dir_mode\n    }\n\n    input:\n    tuple val(name), path(bam) from ch_bt2\n\n    output:\n    tuple val(name), path('*.sorted.bam') into ch_sorted_bam\n\n    \"\"\"\n    samtools view -bh -f 3 -F 4 -F 8 ${bam} > ${name}.filter_unmapped.bam\n    java -jar ${params.picard_jar} SortSam INPUT=${name}.filter_unmapped.bam \\\n      OUTPUT=${name}.sorted.bam \\\n      SORT_ORDER=coordinate \\\n      VALIDATION_STRINGENCY=SILENT\n    \"\"\"\n}"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/SORT_BAM"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": [" process DEDUP {\n      tag \"$name\"\n      publishDir \"${params.outdir}/remove_duplicates\", mode: params.publish_dir_mode\n\n      input:\n      tuple val(name), path(bam) from ch_sorted_bam\n\n      output:\n      tuple val(name), path('*.sorted.dedup.bam') into ch_dedup_bam\n      path '*.txt' into ch_picard_metrics_mqc\n\n      \"\"\"\n      java -jar ${params.picard_jar} MarkDuplicates INPUT=${bam} \\\n        OUTPUT=${name}.sorted.marked.bam \\\n        VALIDATION_STRINGENCY=SILENT \\\n        METRICS_FILE=metrics.${name}.txt\n      samtools view -bh -F 1024 ${name}.sorted.marked.bam > ${name}.sorted.dedup.bam\n      \"\"\"\n    }"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/DEDUP"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": [" process FILTER120 {\n        tag \"$name\"\n        publishDir \"${params.outdir}/filter120\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(name), path(bam) from ch_dedup_bam\n\n        output:\n        tuple val(name), path('*.sorted.dedup.filtered.{bam,bam.bai}') into ch_filtered_bam\n\n        \"\"\"\n        samtools view -h ${bam} \\\n          |LC_ALL=C awk -f /cutruntools/filter_below.awk \\\n          |samtools view -Sb - -o ${name}.sorted.dedup.filtered.bam\n        samtools index ${name}.sorted.dedup.filtered.bam\n        \"\"\"\n    }"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/FILTER120"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": ["\nprocess BEDGRAPH {\n    tag \"$name\"\n    publishDir \"${params.outdir}/bedgraph\", mode: params.publish_dir_mode\n\n    input:\n    tuple val(name), path(bam) from ch_filtered_bam_bedgraph\n    path sizes from ch_genome_sizes_bedgraph.collect()\n\n    output:\n    tuple val(name), path('*.bedgraph') into ch_bedgraph_seacr\n    path '*.bedgraph'\n\n    \"\"\"\n    bedtools bamtobed -bedpe -i ${bam[0]} > ${name}.bed\n    awk '\\$1==\\$4 && \\$6-\\$2 < 1000 {print \\$0}' ${name}.bed > ${name}.clean.bed\n    cut -f 1,2,6 ${name}.clean.bed | sort -k1,1 -k2,2n -k3,3n > ${name}.fragments.bed\n    bedtools genomecov -bg -i ${name}.fragments.bed -g ${sizes} > ${name}.fragments.bedgraph\n    \"\"\"\n}"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/BEDGRAPH"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 1, "tools": ["Bedops"], "nb_own": 1, "list_own": ["khigashi1987"], "nb_wf": 1, "list_wf": ["CUTRUN_Nextflow"], "list_contrib": ["khigashi1987"], "nb_contrib": 1, "codes": [" process FILTER_MACS_PEAKS {\n        tag \"$name\"\n        publishDir \"${params.outdir}/macs\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(name), path(peaks), path(flagstat), path(bam) from ch_macs_peaks.join(ch_flagstat_macs, by: [0]).join(ch_filtered_bam_filter_macs, by:[0])\n        path(bed) from ch_genome_filter_regions_macs.collect()\n        path peak_count_header from ch_peak_count_header\n        path frip_score_header from ch_frip_score_header\n\n\n        output:\n        tuple val(name), path('*.filtered.narrowPeak') into ch_filtered_macs_peaks_qc,\n                                                            ch_filtered_macs_peaks_annot\n        path '*_mqc.tsv' into ch_macs_mqc\n\n        \"\"\"\n        cat ${peaks} | \\\n          grep -v -e \"chrM\" | \\\n          sort-bed - | \\\n          bedops -n 1 - ${bed} > ${name}.filtered.narrowPeak\n\n        cat ${name}.filtered.narrowPeak | \\\n            wc -l | \\\n            awk -v OFS='\\t' '{ print \"${name}\", \\$1 }' | \\\n            cat $peak_count_header - > ${name}_peaks.count_mqc.tsv\n        READS_IN_PEAKS=\\$(intersectBed -a ${bam[0]} -b ${name}.filtered.narrowPeak -bed -c -f 0.20 | awk -F '\\t' '{sum += \\$NF} END {print sum}')\n        grep 'mapped (' $flagstat | awk -v a=\"\\$READS_IN_PEAKS\" -v OFS='\\t' '{print \"${name}\", a/\\$1}' | \\\n            cat $frip_score_header - \\\n            > ${name}_peaks.FRiP_mqc.tsv\n        \"\"\"\n    }"], "list_proc": ["khigashi1987/CUTRUN_Nextflow/FILTER_MACS_PEAKS"], "list_wf_names": ["khigashi1987/CUTRUN_Nextflow"]}, {"nb_reuse": 3, "tools": ["kallisto", "BWA", "TopHat"], "nb_own": 3, "list_own": ["kingzhuky", "lengfei5", "stevekm"], "nb_wf": 3, "list_wf": ["meripseqpipe", "nf_visium_kallisto", "nextflow-pipeline-demo"], "list_contrib": ["kingzhuky", "lengfei5", "stevekm", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess bwa_mem {\n                                    \n    tag { \"${sample_ID}\" }\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'bwa/0.7.17'\n\n    input:\n    set val(sample_ID), file(fastq_R1_trim), file(fastq_R2_trim), file(ref_fa_bwa_dir) from samples_fastq_trimmed.combine(ref_fa_bwa_dir)\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.sam\") into samples_bwa_sam\n\n    script:\n    \"\"\"\n    bwa mem -M -v 1 -t \\${NSLOTS:-1} -R '@RG\\\\tID:${sample_ID}\\\\tSM:${sample_ID}\\\\tLB:${sample_ID}\\\\tPL:ILLUMINA' \"${ref_fa_bwa_dir}/genome.fa\" \"${fastq_R1_trim}\" \"${fastq_R2_trim}\" -o \"${sample_ID}.sam\"\n    \"\"\"\n}", "\nprocess pseudoalPlate {\n    storeDir params.outdir\n\n    input:\n    file index from transcriptome_index\n    file batchkal from batch_kal.collect()\n\n    output:\n    file params.samplename into kallisto_pseudo\n\n    when: params.protocol=='plate'\n\n    script:\n    \"\"\"\n    ml load kallisto/0.46.0-foss-2018b\n    kallisto pseudo -t 16 --quant \\\\\n        -i $index \\\\\n        -o ${params.samplename} \\\\\n        -b $batchkal\n\n    \"\"\"\n}", "\nprocess Tophat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/tophat2\", mode: 'link', overwrite: true\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from tophat2_reads\n    file index from tophat2_index.collect()\n    file gtf\n\n    output:\n    set val(sample_id), file(\"*_tophat2.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into tophat2_bam\n    file \"*_log.txt\" into tophat2_log\n    \n    when:\n    aligner == \"tophat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.rev)?(\\.\\d)?(\\.fa)?(\\.bt2)?$/\n    strand_info = params.stranded == \"no\" ? \"fr-unstranded\" : params.stranded == \"reverse\" ? \"fr-secondstrand\" : \"fr-firststrand\"\n    if (reads_single_end) {\n        \"\"\"\n        tophat  -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                $reads > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        tophat -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                ${reads[0]} ${reads[1]} > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    }\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/bwa_mem", "lengfei5/nf_visium_kallisto/pseudoalPlate", "kingzhuky/meripseqpipe/Tophat2Align"], "list_wf_names": ["stevekm/nextflow-pipeline-demo", "kingzhuky/meripseqpipe", "lengfei5/nf_visium_kallisto"]}, {"nb_reuse": 3, "tools": ["SAMtools", "HISAT2", "BUStools"], "nb_own": 3, "list_own": ["kingzhuky", "lengfei5", "stevekm"], "nb_wf": 3, "list_wf": ["meripseqpipe", "nf_visium_kallisto", "nextflow-pipeline-demo"], "list_contrib": ["kingzhuky", "lengfei5", "stevekm", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess bam_ra_rc_gatk {\n                                               \n                                                                                                                                            \n                                                                                                                                             \n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam_dd_ra_rc_gatk\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n\n    input:\n    set val(sample_ID), file(sample_bam), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(gatk_1000G_phase1_indels_vcf), file(mills_and_1000G_gold_standard_indels_vcf), file(dbsnp_ref_vcf) from samples_dd_bam_ref_gatk\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.ra.rc.bam\"), file(\"${sample_ID}.dd.ra.rc.bam.bai\") into samples_dd_ra_rc_bam, samples_dd_ra_rc_bam2, samples_dd_ra_rc_bam3\n    file \"${sample_ID}.intervals\"\n    file \"${sample_ID}.table1.txt\"\n    file \"${sample_ID}.table2.txt\"\n    file \"${sample_ID}.csv\"\n    file \"${sample_ID}.pdf\"\n\n    script:\n    \"\"\"\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T RealignerTargetCreator \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    -nt \\${NSLOTS:-1} \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.intervals\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T IndelRealigner \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --maxReadsForRealignment 50000 \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -targetIntervals \"${sample_ID}.intervals\" \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.dd.ra.bam\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.table1.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --out \"${sample_ID}.table2.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T AnalyzeCovariates \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -before \"${sample_ID}.table1.txt\" \\\n    -after \"${sample_ID}.table2.txt\" \\\n    -csv \"${sample_ID}.csv\" \\\n    -plots \"${sample_ID}.pdf\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T PrintReads \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.dd.ra.rc.bam\"\n\n    samtools index \"${sample_ID}.dd.ra.rc.bam\"\n    \"\"\"\n}", "\nprocess Hisat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/hisat2\", mode: 'link', overwrite: true\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from hisat2_reads\n    file index from hisat2_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_hisat2.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into hisat2_bam\n    file \"*_summary.txt\" into hisat2_log\n\n    when:\n    aligner == \"hisat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.exon)?(\\.\\d)?(\\.fa)?(\\.gtf)?(\\.ht2)?$/\n    if (reads_single_end) {\n        \"\"\"\n        hisat2  --summary-file ${sample_name}_hisat2_summary.txt\\\n                -p ${task.cpus} --dta \\\n                -x $index_base \\\n                -U $reads | \\\n                samtools view -@ ${task.cpus} -hbS - > ${sample_name}_hisat2.bam \n        \"\"\"\n    } else {\n        \"\"\"\n        hisat2  --summary-file ${sample_name}_hisat2_summary.txt \\\n                -p ${task.cpus} --dta \\\n                -x $index_base \\\n                -1 ${reads[0]} -2 ${reads[1]} | \\\n                samtools view -@ ${task.cpus} -hbS - > ${sample_name}_hisat2.bam\n        \"\"\"\n    }\n}", "\nprocess corrsort {\n\n                                            \n    storeDir params.outdir\n\n    input:\n    file outbus from kallisto_bus_to_sort\n    file white from bc_wl_kal.collect()\n\n    output:\n    file \"${outbus}/output.cor.sort.bus\"\n    file outbus into kal_sort_to_count\n    file outbus into kal_sort_to_umi\n\n    when: params.protocol!='plate'\n\n    script:\n    \"\"\"\n    ml load bustools/0.40.0-foss-2018b\n    bustools correct -w $white -o ${outbus}/output.cor.bus ${outbus}/output.bus\n    bustools sort -o ${outbus}/output.cor.sort.bus -t 8 ${outbus}/output.cor.bus\n\n    \"\"\"\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/bam_ra_rc_gatk", "kingzhuky/meripseqpipe/Hisat2Align", "lengfei5/nf_visium_kallisto/corrsort"], "list_wf_names": ["stevekm/nextflow-pipeline-demo", "kingzhuky/meripseqpipe", "lengfei5/nf_visium_kallisto"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools", "BWA"], "nb_own": 2, "list_own": ["kingzhuky", "stevekm"], "nb_wf": 2, "list_wf": ["meripseqpipe", "nextflow-pipeline-demo"], "list_contrib": ["kingzhuky", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess lofreq {\n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/vcf_lofreq\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n    input:\n    set val(sample_ID), file(sample_bam), file(sample_bai), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(dbsnp_ref_vcf) from samples_dd_ra_rc_bam_ref_dbsnp\n\n    output:\n    file(\"${sample_ID}.vcf\")\n    file(\"${sample_ID}.norm.vcf\")\n    file(\"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\") into lofreq_annotations\n    file(\"${sample_ID}.eval.grp\")\n    val(sample_ID) into sample_lofreq_done\n\n    script:\n    \"\"\"\n    \"${params.lofreq_bin}\" call-parallel \\\n    --call-indels \\\n    --pp-threads \\${NSLOTS:-1} \\\n    --ref \"${ref_fasta}\" \\\n    --bed \"${targets_bed_file}\" \\\n    --out \"${sample_ID}.vcf\" \\\n    \"${sample_bam}\"\n\n    bgzip -c \"${sample_ID}.vcf\" > \"${sample_ID}.vcf.bgz\"\n\n    bcftools index \"${sample_ID}.vcf.bgz\"\n\n    bcftools norm \\\n    --multiallelics \\\n    -both \\\n    --output-type v \\\n    \"${sample_ID}.vcf.bgz\" | \\\n    bcftools norm \\\n    --fasta-ref \"${ref_fasta}\" \\\n    --output-type v - | \\\n    bcftools view \\\n    --exclude 'DP<5' \\\n    --output-type v >  \"${sample_ID}.norm.vcf\"\n\n    # annotate the vcf\n    annotate_vcf.sh \"${sample_ID}.norm.vcf\" \"${sample_ID}.norm\"\n\n    # add a column with the sample ID\n    paste_col.py -i \"${sample_ID}.norm.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\" -o \"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\" --header \"Sample\" -v \"${sample_ID}\" -d \"\\t\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T VariantEval \\\n    -R \"${ref_fasta}\" \\\n    -o \"${sample_ID}.eval.grp\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --eval \"${sample_ID}.norm.vcf\"\n\n    \"\"\"\n}", "\nprocess BWAAlign{\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/bwa\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from bwa_reads\n    file index from bwa_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_bwa.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into bwa_bam\n    file \"*\" into bwa_result\n\n    when:\n    aligner == \"bwa\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.pac)?(\\.bwt)?(\\.ann)?(\\.amb)?(\\.sa)?(\\.fa)?$/\n    if (reads_single_end) {\n        \"\"\"\n        bwa aln -t ${task.cpus} \\\n                -f ${reads.baseName}.sai \\\n                $index_base \\\n                $reads\n        bwa samse \\\n                $index_base \\\n                ${reads.baseName}.sai \\\n                $reads  | \\\n            samtools view -@ ${task.cpus} -h -bS - > ${sample_name}_bwa.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        bwa aln -t ${task.cpus} \\\n                -f ${reads[0].baseName}.sai \\\n                $index_base \\\n                ${reads[0]}\n        bwa aln -t ${task.cpus} \\\n                -f ${reads[1].baseName}.sai \\\n                $index_base \\\n                ${reads[1]}\n        bwa sampe \\\n                $index_base \\\n                ${reads[0].baseName}.sai ${reads[1].baseName}.sai \\\n                ${reads[0]} ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -h -bS - > ${sample_name}_bwa.bam\n        \"\"\"\n    }\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/lofreq", "kingzhuky/meripseqpipe/BWAAlign"], "list_wf_names": ["stevekm/nextflow-pipeline-demo", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 3, "tools": ["STAR", "lumi", "MultiQC", "SraTailor"], "nb_own": 3, "list_own": ["kingzhuky", "lengfei5", "stevekm"], "nb_wf": 3, "list_wf": ["meripseqpipe", "nf_visium_kallisto", "nextflow-pipeline-demo"], "list_contrib": ["kingzhuky", "lengfei5", "stevekm", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess makeSeurat10x {\n\n    storeDir \"${params.outdir}/${params.samplename}\"\n\n    input:\n    file outbus from kallisto_count\n    file umic from kallisto_umic.collect()\n\n    output:\n    file \"${params.samplename}_UMIrank.pdf\"\n    file \"${params.samplename}_UMIduplication.pdf\"\n    file \"${params.samplename}_srat.RDS\"\n\n    when: params.protocol=='10xv3' || params.protocol=='10xv2'\n\n    script:\n    \"\"\"\n    #!/usr/local/bin/Rscript --vanilla\n\n    library(Seurat)\n    library(DropletUtils)\n\n    # read in data\n    topdir = \"${outbus}\" # source dir\n    exp = Matrix::readMM(paste0(topdir, \"/genecounts.mtx\")) #read matrix\n    bc = read.csv(paste0(topdir, \"/genecounts.barcodes.txt\"), header = F, stringsAsFactors = F)\n    g = read.csv(paste0(topdir, \"/genecounts.genes.txt\"), header = F, stringsAsFactors = F)\n    dimnames(exp) = list(paste0(bc\\$V1,\"-1\"),g\\$V1) # number added because of seurat format for barcodes\n    count.data = Matrix::t(exp)\n\n    # get emptyDrops and default cutoff cell estimates\n    iscell_dd = defaultDrops(count.data, expected = 5000)\n    eout = emptyDrops(count.data, lower = 200)\n    eout\\$FDR[is.na(eout\\$FDR)] = 1\n    iscell_ed = eout\\$FDR<=0.01\n    meta = data.frame(row.names = paste0(bc\\$V1,\"-1\"),\n                      iscell_dd = iscell_dd, iscell_ed = iscell_ed)\n\n    # plot rankings for number of UMI\n    br.out <- barcodeRanks(count.data)\n    pdf(\"${params.samplename}_UMIrank.pdf\", height = 5, width = 5, useDingbats = F)\n    plot(br.out\\$rank, br.out\\$total, log=\"xy\", xlab=\"Rank\", ylab=\"Total\")\n    o <- order(br.out\\$rank)\n    lines(br.out\\$rank[o], br.out\\$fitted[o], col=\"red\")\n    abline(h=metadata(br.out)\\$knee, col=\"dodgerblue\", lty=2)\n    abline(h=metadata(br.out)\\$inflection, col=\"forestgreen\", lty=2)\n    legend(\"bottomleft\", lty=2, col=c(\"dodgerblue\", \"forestgreen\"),\n        legend=c(\"knee\", \"inflection\"))\n    dev.off()\n\n    # UMI duplication\n    umi = read.table(\"${umic}\", sep = \"\\t\", header = F, stringsAsFactors = F)\n    sumUMI = c()\n    sumi = sum(umi\\$V4)\n    for(i in 0:250){ sumUMI = c(sumUMI, sum(umi\\$V4[umi\\$V4>i])/sumi) }\n    pdf(\"${params.samplename}_UMIduplication.pdf\", height = 3.5, width = 7, useDingbats = F)\n    par(mfrow = c(1,2))\n    plot(sumUMI, ylim = c(0,1), pch = 20, col = \"grey30\", ylab = \"% of total reads\",\n         xlab = \"More than xx UMI\", main = \"${params.samplename}\")\n    diffUMI = sumUMI[-length(sumUMI)] - sumUMI[-1]\n    plot(diffUMI, ylim = c(0,0.2), pch = 20, col = \"grey30\", ylab = \"Change in % of total reads\",\n         xlab = \"More than xx UMI\", main = \"${params.samplename}\")\n    dev.off()\n\n    # create Seurat object\n    ## we're only keeping what might potentially be a cell (by DD or ED)\n    srat = CreateSeuratObject(counts = count.data[,iscell_dd | iscell_ed],\n                              meta.data = meta[iscell_dd | iscell_ed,])\n    amb_prop = estimateAmbience(count.data)[rownames(srat@assays\\$RNA@meta.features)]\n    srat@assays\\$RNA@meta.features = data.frame(row.names = rownames(srat@assays\\$RNA@meta.features),\n                                                \"ambient_prop\" = amb_prop)\n\n    # get MT% (genes curated from NCBI chrMT genes)\n    mtgenes = c(\"COX1\", \"COX2\", \"COX3\", \"ATP6\", \"ND1\", \"ND5\", \"CYTB\", \"ND2\", \"ND4\",\n                \"ATP8\", \"MT-CO1\", \"COI\", \"LOC9829747\")\n    mtgenes = c(mtgenes, paste0(\"MT\", mtgenes), paste0(\"MT-\", mtgenes))\n    mtgenes = mtgenes[mtgenes %in% g[,1]]\n    srat = PercentageFeatureSet(srat, col.name = \"percent.mt\", assay = \"RNA\",\n                                features = mtgenes)\n\n    saveRDS(srat, file = \"${params.samplename}_srat.RDS\")\n\n    \"\"\"\n\n}", "\nprocess multiqc {\n    publishDir \"${params.output_dir}\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    executor \"local\"\n    module 'python/2.7.3'\n\n    input:\n    val(comparisonID) from mutect2_sampleIDs.mix(sample_gatk_hc_done)\n                                            .mix(sample_lofreq_done)\n                                            .collect()                                            \n    file(output_dir) from Channel.fromPath(\"${params.output_dir}\")\n\n    output:\n    file \"multiqc_report.html\" into email_files\n    file \"multiqc_data\"\n\n    script:\n    \"\"\"\n    export PS=\\${PS:-''} # needed for virtualenv bug\n    export PS1=\\${PS1:-''}\n    unset PYTHONPATH\n    source activate\n    multiqc \"${output_dir}\"\n    \"\"\"\n}", "\nprocess StarAlign {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/star\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from star_reads\n    file star_index from star_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_star.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into star_bam\n    file \"*.final.out\" into star_log\n\n    when:\n    aligner == \"star\"\n\n    script:\n    gzip_cmd = gzip ? \"--readFilesCommand zcat\" : \"\"\n    if (reads_single_end) {\n        \"\"\"\n        STAR --runThreadN ${task.cpus} $gzip_cmd \\\n            --twopassMode Basic \\\n            --genomeDir $star_index \\\n            --readFilesIn $reads  \\\n            --outSAMtype BAM Unsorted \\\n            --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 \\\n            --outFilterIntronMotifs RemoveNoncanonical \\\n            --outFilterMultimapNmax 20 \\\n            --alignIntronMin 20 \\\n            --alignIntronMax 100000 \\\n            --alignMatesGapMax 1000000 \\\n            --outFileNamePrefix ${sample_name}  > ${sample_name}_log.txt\n        mv ${sample_name}Aligned.out.bam ${sample_name}_star.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        STAR --runThreadN ${task.cpus} $gzip_cmd \\\n            --twopassMode Basic \\\n            --genomeDir $star_index \\\n            --readFilesIn ${reads[0]} ${reads[1]}  \\\n            --outSAMtype BAM Unsorted \\\n            --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 \\\n            --outFilterIntronMotifs RemoveNoncanonical \\\n            --outFilterMultimapNmax 20 \\\n            --alignIntronMin 20 \\\n            --alignIntronMax 1000000 \\\n            --alignMatesGapMax 1000000 \\\n            --outFileNamePrefix ${sample_name} > ${sample_name}_log.txt\n        mv ${sample_name}Aligned.out.bam ${sample_name}_star.bam\n        \"\"\"\n    }\n}"], "list_proc": ["lengfei5/nf_visium_kallisto/makeSeurat10x", "stevekm/nextflow-pipeline-demo/multiqc", "kingzhuky/meripseqpipe/StarAlign"], "list_wf_names": ["stevekm/nextflow-pipeline-demo", "kingzhuky/meripseqpipe", "lengfei5/nf_visium_kallisto"]}, {"nb_reuse": 2, "tools": ["SAMtools", "RTREE"], "nb_own": 2, "list_own": ["kingzhuky", "stevekm"], "nb_wf": 2, "list_wf": ["meripseqpipe", "nextflow-samplesheet-demo"], "list_contrib": ["kingzhuky", "stevekm", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess SortRename {\n    label 'sort'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/samtoolsSort/\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_id), file(bam_file), val(reads_single_end), val(gzip), val(input), val(group) from merge_bam_file\n\n    output:\n    set val(group), val(sample_id), file(\"*.bam\"), file(\"*.bai\") into sorted_bam\n    file \"*.{bam,bai}\" into rseqc_bam, bedgraph_bam, feacount_bam, cuffbam, peakquan_bam, diffpeak_bam, sng_bam\n    file \"*.bam\" into bam_results\n\n    script:\n    sample_name = sample_id + (input ? \".input_\" : \".ip_\") + group\n    output = sample_name + \".bam\"\n    mapq_cutoff = (params.mapq_cutoff).toInteger() \n    if (!params.skip_sort){\n        \"\"\"\n        if [ \"$mapq_cutoff\" -gt \"0\" ]; then\n            samtools view -hbq $mapq_cutoff $bam_file | samtools sort -@ ${task.cpus} -O BAM -o $output -\n        else\n            samtools sort -@ ${task.cpus} -O BAM -o $output $bam_file\n        fi\n        samtools index -@ ${task.cpus} $output\n        \"\"\"\n    } else {\n        \"\"\"\n        if [ \"$mapq_cutoff\" -gt \"0\" ]; then\n            samtools view -hbq $mapq_cutoff $bam_file > $output\n        else\n            mv $bam_file $output\n        fi\n        samtools index -@ ${task.cpus} $output\n        \"\"\"\n    }\n}", "\nprocess use_dir {\n    echo true\n\n    input:\n    file(dir) from samples_dir\n\n    script:\n    \"\"\"\n    echo \"[use_dir]\"\n    pwd\n    tree \"${dir}/\"\n    \"\"\"\n}"], "list_proc": ["kingzhuky/meripseqpipe/SortRename", "stevekm/nextflow-samplesheet-demo/use_dir"], "list_wf_names": ["kingzhuky/meripseqpipe", "stevekm/nextflow-samplesheet-demo"]}, {"nb_reuse": 3, "tools": ["IMPACT_S", "Gene", "SAMtools", "Distanced", "MultiQC", "Flagser", "GATK"], "nb_own": 3, "list_own": ["kingzhuky", "lengfei5", "stevekm"], "nb_wf": 3, "list_wf": ["meripseqpipe", "vep-annotation-nf", "smallRNA_nf"], "list_contrib": ["kingzhuky", "lengfei5", "stevekm", "juneb4869"], "nb_contrib": 4, "codes": ["\nprocess multiqc{\n    publishDir \"${params.outdir}/Report/QCReadsReport\" , mode: 'link', overwrite: true\n    \n    when:\n    !params.skip_qc\n\n    input:\n    file arranged_qc from arranged_qc.collect()\n\n    output:\n    file \"multiqc*\" into multiqc_results\n\n    script:\n    \"\"\"\n    multiqc -n multiqc_$aligner .\n    \"\"\"\n}", "\nprocess vcf_to_tsv {\n    tag \"${sampleID}\"\n    publishDir \"${params.outputDir}/VEP/vcf_tsv\", mode: 'copy'\n\n    input:\n    set val(sampleID), file(vcf), file(refFasta), file(refFai), file(refDict) from vcf_annotated.combine(ref_fa2)\n        .combine(ref_fai2)\n        .combine(ref_dict2)\n\n    output:\n    set val(sampleID), file(\"${output_file}\"), file(\"csq_key.txt\") into tsv_annotations\n\n    script:\n    prefix = \"${sampleID}\"\n    output_file = \"${prefix}.vep.tsv\"\n    \"\"\"\n    gatk VariantsToTable \\\n    -R \"${refFasta}\" \\\n    --variant \"${vcf}\" \\\n    -F CHROM \\\n    -F POS \\\n    -F ID \\\n    -F REF \\\n    -F ALT \\\n    -F QUAL \\\n    -F FILTER \\\n    -F CSQ \\\n    --output \"${output_file}\"\n\n    grep '##INFO=<ID=CSQ' \"${vcf}\" | sed -e 's|\\\\(^.*Format: \\\\)\\\\(.*\\\\)\">\\$|\\\\2|g' | tr '|' '\\\\n' > csq_key.txt\n\n    ##INFO=<ID=CSQ,Number=.,Type=String,Description=\"Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID\">\n    \"\"\"\n}", "\nprocess align {\n\n    tag \"Channel: ${name}\"\n\n    input:\n\t   file index from index_tailor\n     set name, file(fastq) from fastq_cleanReads\n\n    output:\n      set name, file(\"tailor.bam\") into bam_tailor, bam_tailor2\n\n    script:\n    \"\"\"\n    tailor_v1.1_linux_static map -i ${fastq} -p index.tailor -l ${params.minAlign} -n ${task.cpus} | perl $baseDir/scripts/bam.NH2fraction.pl -m ${params.maxMultialign} | samtools view -bS > tailor.bam\n    \"\"\"\n}"], "list_proc": ["kingzhuky/meripseqpipe/multiqc", "stevekm/vep-annotation-nf/vcf_to_tsv", "lengfei5/smallRNA_nf/align"], "list_wf_names": ["kingzhuky/meripseqpipe", "lengfei5/smallRNA_nf", "stevekm/vep-annotation-nf"]}, {"nb_reuse": 2, "tools": ["BEDTools", "VGE"], "nb_own": 2, "list_own": ["kingzhuky", "subwaystation"], "nb_wf": 2, "list_wf": ["meripseqpipe", "pangenome"], "list_contrib": ["nf-core-bot", "subwaystation", "kingzhuky", "Zethson", "AndreaGuarracino", "heuermh", "juneb4869"], "nb_contrib": 7, "codes": [" process MeyerPrepration{\n        label 'build_index'\n        tag \"onecore_peak\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/meyerPrepration\" : params.outdir },\n                saveAs: { params.saveReference ? it : null }, mode: 'copy'       \n\n        input:\n        file fasta\n        file chromsizesfile from chromsizesfile.collect()\n\n        output:\n        file \"chrName.txt\" into chrNamefile\n        file \"genome.bin25.bed\" into bin25file\n        file \"genomebin\" into genomebin\n\n        when:\n        !params.skip_meyer && !params.skip_peakCalling\n\n        shell:\n        '''\n        awk '{print $1}' !{chromsizesfile} > chrName.txt\n        mkdir genomebin\n        bedtools makewindows -g !{chromsizesfile} -w 25 > genome.bin25.bed\n        awk '{print \"cat genome.bin25.bed | grep \"$1\" > genomebin/\"$1\".bin25.bed\"}' chrName.txt | xargs -iCMD -P!{task.cpus} bash -c CMD\n        '''\n    }", "\nprocess vg_deconstruct {\n  publishDir \"${params.outdir}/vg_deconstruct\", mode: \"${params.publish_dir_mode}\"\n\n  input:\n  path(graph)\n\n  output:\n  path(\"${graph}.*.vcf\")\n\n  \"\"\"\n  for s in \\$(echo \"${params.vcf_spec}\" | tr ',' ' ');\n  do\n    ref=\\$(echo \"\\$s\" | cut -f 1 -d:)\n    delim=\\$(echo \"\\$s\" | cut -f 2 -d:)\n    vcf=\"${graph}\".\\$(echo \\$ref | tr '/|' '_').vcf\n    vg deconstruct -P \\$ref -H \\$delim -e -a -t \"${task.cpus}\" \"${graph}\" > \\$vcf\n  done\n  \"\"\"\n}"], "list_proc": ["kingzhuky/meripseqpipe/MeyerPrepration", "subwaystation/pangenome/vg_deconstruct"], "list_wf_names": ["subwaystation/pangenome", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 2, "tools": ["FastQC", "fastafrombed", "MultiQC"], "nb_own": 2, "list_own": ["kingzhuky", "subwaystation"], "nb_wf": 2, "list_wf": ["meripseqpipe", "pangenome"], "list_contrib": ["nf-core-bot", "subwaystation", "kingzhuky", "Zethson", "AndreaGuarracino", "heuermh", "juneb4869"], "nb_contrib": 7, "codes": ["\nprocess MotifSearching {\n    label 'onecore_peak'\n    tag \"${bed_file.baseName}\"\n    publishDir \"${params.outdir}/m6AAnalysis/motif\", mode: 'link', overwrite: true\n    \n    input:\n    file bed_file from motif_collection\n    file chromsizesfile from chromsizesfile.collect()\n    file bed12 from bed12file.collect()\n    file fasta\n    file gtf\n\n    output:\n    file \"*_{dreme,homer}\" into motif_results, motif_results_for_report\n\n    when:\n    !params.skip_motif\n\n    script:\n    motif_file_dir = baseDir + \"/bin\"\n    bed_prefix = bed_file.baseName\n    println LikeletUtils.print_purple(\"Motif analysis is going on by DREME and Homer\")\n    \"\"\"\n    cp ${motif_file_dir}/m6A_motif.meme ./\n    sort -k5,5 -g ${bed_file} | awk 'FNR <= 2000{ print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$3}' > ${bed_prefix}.location\n    intersectBed -wo -a ${bed_prefix}.location -b $gtf | awk -v OFS=\"\\\\t\" '{print \\$1,\\$2,\\$3,\"*\",\"*\",\\$10}' | sort -k1,2 | uniq > ${bed_prefix}_bestpeaks.bed\n    fastaFromBed -name+ -split -s -fi $fasta -bed ${bed_prefix}_bestpeaks.bed > ${bed_prefix}_bestpeaks.fa\n    # ame -oc ${bed_prefix}_ame ${bed_prefix}_bestpeaks.fa m6A_motif.meme\n    shuffleBed -incl ${bed12} -seed 12345 -noOverlapping -i ${bed_prefix}_bestpeaks.bed -g ${chromsizesfile} > ${bed_prefix}_random_peak.bed\n    fastaFromBed -name+ -split -s -fi $fasta -bed ${bed_prefix}_random_peak.bed > ${bed_prefix}_random_peak.fa\n    findMotifs.pl ${bed_prefix}_bestpeaks.fa fasta ${bed_prefix}_homer -fasta ${bed_prefix}_random_peak.fa -p ${task.cpus} \\\n        -len 5,6,7,8 -S 10 -rna -dumpFasta > ${bed_prefix}_homer_run.log 2>&1\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["kingzhuky/meripseqpipe/MotifSearching", "subwaystation/pangenome/get_software_versions"], "list_wf_names": ["subwaystation/pangenome", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 3, "tools": ["SAMtools", "BCFtools", "BWA"], "nb_own": 3, "list_own": ["lepsalex", "supark87", "kingzhuky"], "nb_wf": 3, "list_wf": ["meripseqpipe", "minion_hrp2", "nextflow-pcawg-bwa-mem-workflow"], "list_contrib": ["lepsalex", "supark87", "kingzhuky", "sjsabin", "juneb4869"], "nb_contrib": 5, "codes": ["\nprocess consensus{\n    container 'supark87/minion'\n    publishDir \"$params.output.folder/cns/\", pattern: \"*.fastq\", mode : \"copy\"\n    \n    input:\n    file(samfile) from sam\n    path(ref_dir) from ref_dir\n    path(pyscripts_path) from pyscripts\n\n\n    output:\n    file(\"cns.fastq\") into csn_seq\n    script:\n    \"\"\"\n     samtools view -S -b $samfile > bamfile1.bam\n     samtools sort bamfile1.bam -o sorted.bam\n     bcftools mpileup -Ou -f $ref_dir/XM_002808697.2.fasta sorted.bam | bcftools call -mv -Oz -o calls.vcf.gz\n     bcftools index calls.vcf.gz\n     bcftools norm -f $ref_dir/XM_002808697.2.fasta calls.vcf.gz -Ob -o calls.norm.bcf\n     bcftools filter --IndelGap 5 calls.norm.bcf -Ob -o calls.norm.flt-indels.bcf\n     cat $ref_dir/XM_002808697.2.fasta | bcftools consensus calls.vcf.gz > cns.fastq\n\n    \"\"\"\n}", "\nprocess align {\n\n    tag \"$bamName\"\n\n    cpus params.cpus_big\n    memory \"${params.mem_big} MB\"\n\n    input:\n    file reference_gz from reference_gz_ch\n                                                              \n                                                                                   \n    file reference_gz_fai from reference_gz_fai_ch\n    file reference_gz_amb from reference_gz_amb_ch\n    file reference_gz_ann from reference_gz_ann_ch\n    file reference_gz_bwt from reference_gz_bwt_ch\n    file reference_gz_pac from reference_gz_pac_ch\n    file reference_gz_sa from reference_gz_sa_ch\n                                                               \n    set val(bamName), file(bam), file(bamHeader), val(headerText), file(readCount) from headers.map {\n        [it[0], it[1], it[2], file(it[2]).text]\n    }.join(counts)\n\n    output:\n    set val(bamName), file(\"${bamName}_aligned.bam\"), file(bamHeader), file(readCount) into aligned_bams\n\n    \"\"\"\n    bamtofastq exclude=QCFAIL,SECONDARY,SUPPLEMENTARY T=${bamName}.t S=${bamName}.s O=${bamName}.o O2=${bamName}.o2 collate=1 tryoq=1 filename=${bam} | \\\\\n    bwa mem -p -t ${params.threads} -T 0 -R \"${headerText.trim()}\" ${reference_gz} - | \\\\\n    bamsort blockmb=${params.mem} inputformat=sam level=1 outputthreads=2 calmdnm=1 calmdnmrecompindetonly=1 calmdnmreference=${reference_gz} tmpfile=${bamName}.sorttmp O=${bamName}_aligned.bam\n    \"\"\"\n}", "\nprocess CreateIGVjs {\n    publishDir \"${params.outdir}/Report\" , mode: 'link', overwrite: true,\n        saveAs: {filename ->\n                 if (filename.indexOf(\".html\") > 0)  \"Igv_js/$filename\"\n                 else if (filename.indexOf(\".pdf\") > 0)  \"Igv_js/$filename\"\n                 else \"Igv_js/$filename\"\n        }        \n    input:\n    file m6APipe_result from m6APipe_result\n    file fasta \n    file gtf\n    file formatted_designfile from formatted_designfile.collect()\n    file group_bed from group_merged_bed.collect()\n    file all_bed from all_merged_bed.collect()\n    file bedgraph from bedgraph_for_igv.collect()\n    \n    output:\n    file \"*\" into igv_js\n\n    script:    \n    igv_fasta = fasta.baseName.toString() + \".igv.fa\"\n    igv_gtf = gtf.baseName.toString() + \".igv.gtf\"\n    merged_allpeaks_igvfile = all_bed.baseName.toString() + \".igv.bed\"\n    \"\"\"\n    ls -l $fasta | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_fasta'\"}' | bash\n    ls -l $gtf | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_gtf'\"}' | bash\n    ls -l $m6APipe_result | awk '{print \"ln -s \"\\$11\" initial.m6APipe\"}' | bash\n    ls -l $group_bed $all_bed | awk '{sub(\".bed\\$\",\".igv.bed\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    ls -l $bedgraph | awk '{sub(\".bedgraph\\$\",\".igv.bedgraph\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    samtools faidx $igv_fasta\n    bash $baseDir/bin/create_IGV_js.sh $igv_fasta $igv_gtf $merged_allpeaks_igvfile $formatted_designfile\n    \"\"\"\n}"], "list_proc": ["supark87/minion_hrp2/consensus", "lepsalex/nextflow-pcawg-bwa-mem-workflow/align", "kingzhuky/meripseqpipe/CreateIGVjs"], "list_wf_names": ["supark87/minion_hrp2", "lepsalex/nextflow-pcawg-bwa-mem-workflow", "kingzhuky/meripseqpipe"]}, {"nb_reuse": 1, "tools": ["genomes"], "nb_own": 1, "list_own": ["kkerns85"], "nb_wf": 1, "list_wf": ["midas_nextflow"], "list_contrib": ["kkerns85"], "nb_contrib": 1, "codes": ["\nprocess buildMIDASdb {\n    container \"quay.io/fhcrc-microbiome/midas:v1.3.2--6\"\n    label 'mem_medium'\n    publishDir \"${params.output_folder}\"\n    \n    input:\n    path \"GENOMES\" from file(params.genome_folder)\n    path \"mapfile\" from file(params.mapfile)\n    \n    output:\n    path \"*\"\n\n    \"\"\"\n#!/bin/bash\n\nset -e\n\nbuild_midas_db.py \\\n    GENOMES \\\n    mapfile \\\n    ./ \\\n    --threads ${task.cpus} \\\n    --compress\n\"\"\"\n}"], "list_proc": ["kkerns85/midas_nextflow/buildMIDASdb"], "list_wf_names": ["kkerns85/midas_nextflow"]}, {"nb_reuse": 2, "tools": ["QIIME", "JSpecies"], "nb_own": 2, "list_own": ["laclac102", "kkerns85"], "nb_wf": 2, "list_wf": ["ampliseq", "midas_nextflow"], "list_contrib": ["xingaulaglag", "kkerns85"], "nb_contrib": 2, "codes": ["process QIIME2_TRAIN {\n    tag \"${meta.FW_primer}-${meta.RV_primer}\"\n    label 'process_high'\n    label 'single_cpu'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple val(meta), path(qza)\n\n    output:\n    path(\"*-classifier.qza\"), emit: qza\n    path \"versions.yml\"    , emit: versions\n\n    script:\n    \"\"\"\n    export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n    #Train classifier\n    qiime feature-classifier fit-classifier-naive-bayes \\\n        --i-reference-reads ${meta.FW_primer}-${meta.RV_primer}-ref-seq.qza \\\n        --i-reference-taxonomy ref-taxonomy.qza \\\n        --o-classifier ${meta.FW_primer}-${meta.RV_primer}-classifier.qza \\\n        --quiet\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess midas {\n    container \"quay.io/fhcrc-microbiome/midas:v1.3.2--6\"\n    label \"mem_veryhigh\"\n    publishDir \"${params.output_folder}/${specimen}\"\n\n    input:\n    tuple val(specimen), file(\"${specimen}.R*.fastq.gz\") from trimmed_fastq_ch\n    file DB from file(params.db_midas)\n\n    output:\n    file \"${specimen}.species.tar.gz\" into species_ch\n    file \"${specimen}.genes.tar.gz\" into gene_ch\n    file \"${specimen}.snps.tar.gz\" into snps_ch\n\n\"\"\"\n#!/bin/bash\nset -e\necho \"Running species summary\"\n# If the input is single-end, change the filename to match the pattern used for paired-end\nif [[ ! -s ${specimen}.R2.fastq.gz ]]; then\n    mv ${specimen}.R.fastq.gz ${specimen}.R1.fastq.gz\nfi\n# Run the same command differently, depending on whether the input is single- or paired-end\nif [[ -s ${specimen}.R2.fastq.gz ]]; then\n    # Run the species abundance summary\n    run_midas.py \\\n        species \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -2 ${specimen}.R2.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB}\nelse\n    # Run the species abundance summary\n    run_midas.py \\\n        species \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB}\nfi\n# Run the gene abundance summary\nif [[ -s ${specimen}.R2.fastq.gz ]]; then\n    echo \"Running gene summary\"\n    run_midas.py \\\n        genes \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -2 ${specimen}.R2.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nelse\n    echo \"Running gene summary\"\n    run_midas.py \\\n        genes \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nfi\n# Run the SNP summary\necho \"Running SNP summary\"\nif [[ -s ${specimen}.R2.fastq.gz ]]; then\n    run_midas.py \\\n        snps \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -2 ${specimen}.R2.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nelse\n    run_midas.py \\\n        snps \\\n        ${specimen} \\\n        -1 ${specimen}.R1.fastq.gz \\\n        -t ${task.cpus} \\\n        -d ${DB} \\\n        --species_cov ${params.species_cov}\nfi\necho \"Gathering output files\"\n# Species-level results\necho \"Tarring up species results\"\ntar cvf ${specimen}.species.tar ${specimen}/species/*\ngzip ${specimen}.species.tar\n# Gene-level results\necho \"Tarring up gene results\"\ntar cvf ${specimen}.genes.tar ${specimen}/genes/*\ngzip ${specimen}.genes.tar\n# SNP-level results\necho \"Tarring up SNP results\"\ntar cvf ${specimen}.snps.tar ${specimen}/snps/*\ngzip ${specimen}.snps.tar\necho \"Done\"\n\"\"\"\n}"], "list_proc": ["laclac102/ampliseq/QIIME2_TRAIN", "kkerns85/midas_nextflow/midas"], "list_wf_names": ["kkerns85/midas_nextflow", "laclac102/ampliseq"]}, {"nb_reuse": 3, "tools": ["QIIME", "Taxa", "PCFamily", "SNPs"], "nb_own": 2, "list_own": ["laclac102", "kkerns85"], "nb_wf": 2, "list_wf": ["ampliseq", "midas_nextflow"], "list_contrib": ["xingaulaglag", "kkerns85"], "nb_contrib": 2, "codes": ["\nprocess midas_merge_snps {\n    container \"quay.io/fhcrc-microbiome/midas:v1.3.2--6\"\n    label \"mem_veryhigh\"\n    publishDir \"${params.output_folder}\"\n\n    input:\n    file snps_tar_list from snps_ch.toSortedList()\n    file DB from file(params.db_midas)\n\n    output:\n    file \"SNPS/*\"\n\n\"\"\"\n#!/bin/bash\nset -e\nls -lahtr\n# Keep track of the folders created while unpacking input files\ninput_string=\"\"\necho \"Unpacking all of the input files\"\nfor tarfile in ${snps_tar_list}; do\n    echo \"Making sure that \\$tarfile was downloaded correctly\"\n    [[ -s \\$tarfile ]]\n    echo \"Unpacking \\$tarfile\"\n    tar xzvf \\$tarfile\n    # Add this folder to the input string\n    input_string=\"\\$input_string,\\$( echo \\$tarfile | sed 's/.snps.tar.gz//' )\"\n    echo \"Updated input string: \\$input_string\"\ndone\n# Remove the leading comma from the input string\ninput_string=\\$( echo \\$input_string | sed 's/^,//' )\necho \"Merging snps results\"\nmerge_midas.py \\\n    snps \\\n    SNPS \\\n    -i \\$input_string \\\n    -t list \\\n    -d ${DB} \\\n    --sample_depth ${params.merge_sample_depth}\necho \"Done merging data\"\ntouch SNPS/DONE\nls -lahtr SNPS\necho \"Compressing output files\"\nfind SNPS -type f | xargs gzip\necho \"Done\"\n\"\"\"\n}", "process QIIME2_DIVERSITY_BETA {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core), val(category)\n\n    output:\n    path(\"beta_diversity/*\"), emit: beta\n    path \"versions.yml\"     , emit: versions\n\n    script:\n    if ( category.length() > 0 ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        IFS=',' read -r -a metacategory <<< \\\"$category\\\"\n        for j in \\\"\\${metacategory[@]}\\\"\n        do\n            qiime diversity beta-group-significance \\\n                --i-distance-matrix ${core} \\\n                --m-metadata-file ${metadata} \\\n                --m-metadata-column \\\"\\$j\\\" \\\n                --o-visualization ${core.baseName}-\\$j.qzv \\\n                --p-pairwise\n            qiime tools export --input-path ${core.baseName}-\\$j.qzv \\\n                --output-path beta_diversity/${core.baseName}-\\$j\n        done\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir beta_diversity\n        echo \"\" > \"beta_diversity/WARNING No column in ${metadata.baseName} seemed suitable.txt\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process DADA2_TAXONOMY {\n    tag \"${fasta},${database}\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    path(fasta)\n    path(database)\n    val(outfile)\n\n    output:\n    path(outfile), emit: tsv\n    path( \"ASV_tax.rds\" ), emit: rds\n    path \"versions.yml\"  , emit: versions\n    path \"*.args.txt\"    , emit: args\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    suppressPackageStartupMessages(library(dada2))\n    set.seed(100) # Initialize random number generator for reproducibility\n\n    seq <- getSequences(\\\"$fasta\\\", collapse = TRUE, silence = FALSE)\n    taxa <- assignTaxonomy(seq, \\\"$database\\\", taxLevels = c(\"Domain\", \"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\"), $args, multithread = $task.cpus, verbose=TRUE, outputBootstraps = TRUE)\n\n    # Make a data frame, add ASV_ID from seq, set confidence to the bootstrap for the most specific taxon and reorder columns before writing to file\n    tx <- data.frame(ASV_ID = names(seq), taxa, sequence = row.names(taxa\\$tax), row.names = names(seq))\n    tx\\$confidence <- with(tx,\n        ifelse(!is.na(tax.Genus), boot.Genus,\n            ifelse(!is.na(tax.Family), boot.Family,\n                ifelse(!is.na(tax.Order), boot.Order,\n                    ifelse(!is.na(tax.Class), boot.Class,\n                        ifelse(!is.na(tax.Phylum), boot.Phylum,\n                            ifelse(!is.na(tax.Kingdom), boot.Kingdom,\n                                ifelse(!is.na(tax.Domain), boot.Domain, 0)\n                            )\n                        )\n                    )\n                )\n            )\n        )\n    )/100\n    taxa_export <- data.frame(\n        ASV_ID = tx\\$ASV_ID,\n        Domain = tx\\$tax.Domain,\n        Kingdom = tx\\$tax.Kingdom,\n        Phylum = tx\\$tax.Phylum,\n        Class = tx\\$tax.Class,\n        Order = tx\\$tax.Order,\n        Family = tx\\$tax.Family,\n        Genus = tx\\$tax.Genus,\n        confidence = tx\\$confidence,\n        sequence = tx\\$sequence,\n        row.names = names(seq)\n    )\n\n    write.table(taxa_export, file = \\\"$outfile\\\", sep = \"\\\\t\", row.names = FALSE, col.names = TRUE, quote = FALSE, na = '')\n\n    # Save a version with rownames for addSpecies\n    taxa_export <- cbind( ASV_ID = tx\\$ASV_ID, taxa\\$tax, confidence = tx\\$confidence)\n    saveRDS(taxa_export, \"ASV_tax.rds\")\n\n    write.table('assignTaxonomy\\t$args', file = \"assignTaxonomy.args.txt\", row.names = FALSE, col.names = FALSE, quote = FALSE, na = '')\n    writeLines(c(\"\\\\\"${task.process}\\\\\":\", paste0(\"    R: \", paste0(R.Version()[c(\"major\",\"minor\")], collapse = \".\")),paste0(\"    dada2: \", packageVersion(\"dada2\")) ), \"versions.yml\")\n    \"\"\"\n}"], "list_proc": ["kkerns85/midas_nextflow/midas_merge_snps", "laclac102/ampliseq/QIIME2_DIVERSITY_BETA", "laclac102/ampliseq/DADA2_TAXONOMY"], "list_wf_names": ["kkerns85/midas_nextflow", "laclac102/ampliseq"]}, {"nb_reuse": 2, "tools": ["QIIME", "JSpecies"], "nb_own": 2, "list_own": ["laclac102", "kkerns85"], "nb_wf": 2, "list_wf": ["ampliseq", "midas_nf_tower"], "list_contrib": ["xingaulaglag", "kkerns85"], "nb_contrib": 2, "codes": ["process QIIME2_DIVERSITY_ADONIS {\n    tag \"${core.baseName}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.8\"\n\n    input:\n    tuple path(metadata), path(core), val(category)\n\n    output:\n    path(\"adonis/*\")     , emit: html\n    path \"versions.yml\"  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def formula = params.qiime_adonis_formula ?: ''\n    if ( category.length() > 0 || params.qiime_adonis_formula ) {\n        \"\"\"\n        export XDG_CONFIG_HOME=\"\\${PWD}/HOME\"\n\n        if [ \"${formula}\" == '' ]; then\n            adonisformula=\\$( echo $category | sed \"s/,/+/g\" )\n        else\n            adonisformula=${formula}\n        fi\n\n        qiime diversity adonis \\\\\n            --p-n-jobs $task.cpus \\\\\n            --i-distance-matrix ${core} \\\\\n            --m-metadata-file ${metadata} \\\\\n            --o-visualization ${core.baseName}_adonis.qzv \\\\\n            $args \\\\\n            --p-formula \\$adonisformula\n        qiime tools export --input-path ${core.baseName}_adonis.qzv \\\\\n            --output-path adonis/${core.baseName}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        mkdir adonis\n        echo \"\" > \"adonis/WARNING No formula was given with --qiime_adonis_formula and no column in ${metadata.baseName} seemed suitable.txt\"\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            qiime2: \\$( qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess midas_merge_species {\n    container \"quay.io/fhcrc-microbiome/midas:v1.3.2--6\"\n    label \"mem_veryhigh\"\n    publishDir \"${params.output_folder}\"\n\n    input:\n    file species_tar_list from species_ch.toSortedList()\n    file DB from file(params.db_midas)\n\n    output:\n    file \"SPECIES/*\"\n\n\"\"\"\n#!/bin/bash\nset -e\nls -lahtr\n# Keep track of the folders created while unpacking input files\ninput_string=\"\"\necho \"Unpacking all of the input files\"\nfor tarfile in ${species_tar_list}; do\n    echo \"Making sure that \\$tarfile was downloaded correctly\"\n    [[ -s \\$tarfile ]]\n    echo \"Unpacking \\$tarfile\"\n    tar xzvf \\$tarfile\n    # Add this folder to the input string\n    input_string=\"\\$input_string,\\$( echo \\$tarfile | sed 's/.species.tar.gz//' )\"\n    echo \"Updated input string: \\$input_string\"\ndone\n# Remove the leading comma from the input string\ninput_string=\\$( echo \\$input_string | sed 's/^,//' )\necho \"Merging species results\"\nmerge_midas.py \\\n    species \\\n    SPECIES \\\n    -i \\$input_string \\\n    -t list \\\n    -d ${DB} \\\n    --sample_depth ${params.merge_sample_depth}\necho \"Done merging data\"\nls -lahtr SPECIES\necho \"Compressing output files\"\nfind SPECIES -type f | xargs gzip\necho \"Done\"\n\"\"\"\n}"], "list_proc": ["laclac102/ampliseq/QIIME2_DIVERSITY_ADONIS", "kkerns85/midas_nf_tower/midas_merge_species"], "list_wf_names": ["kkerns85/midas_nf_tower", "laclac102/ampliseq"]}, {"nb_reuse": 1, "tools": ["VSEARCH"], "nb_own": 1, "list_own": ["klamkiew"], "nb_wf": 1, "list_wf": ["viralclust"], "list_contrib": ["klamkiew"], "nb_contrib": 1, "codes": ["\nprocess vclust {\n  label 'vclust'\n  publishDir \"${params.output}/${params.vclust_output}\", mode: 'copy', pattern: '*vclust*'\n  publishDir \"${params.output}/${params.vclust_output}\", mode: 'copy', pattern: '*UNCLUSTERED*'\n  publishDir \"${params.output}/${params.summary_output}/unclustered_sequences\", mode: 'copy', pattern: '*UNCLUSTERED.fasta'\n  publishDir \"${params.output}/${params.summary_output}/clustered_sequences\", mode: 'copy', pattern: '*_vclust.fasta'\n\n  input:\n    path(sequences)\n    val(addParams)\n    val(goi)\n\n  output:\n    tuple val(\"${params.output}/${params.vclust_output}\"), path(\"${sequences.baseName}_vclust.fasta\"), path(\"${sequences.baseName}_vclust_cluster.uc.clstr\")\n    path \"${sequences.baseName}_vclust_cluster.uc\"\n                                                                                 \n    path \"${sequences.baseName}_vclust_UNCLUSTERED.fasta\"\n\n  script:\n  def GOI = goi != 'NO FILE' ? \"${goi}\" : ''\n  \"\"\"\n    vsearch ${addParams} --threads ${task.cpus} --cluster_fast ${sequences} --centroids ${sequences.baseName}_vclust.fasta --uc ${sequences.baseName}_vclust_cluster.uc\n    if [ \"{$GOI}\" != 'NO FILE' ]; then\n      for ID in \\$(grep '>' ${GOI}); do\n        grep -m 1 \"\\$ID\" \"${sequences.baseName}_vclust.fasta\" || grep -A1 \"\\$ID\" ${GOI} >> \"${sequences.baseName}_vclust.fasta\"\n      done\n    fi\n\n\n    python3 ${projectDir}/bin/vclust2cdhit.py ${sequences.baseName}_vclust_cluster.uc ${GOI}\n    python3 ${projectDir}/bin/filter_unclustered.py \"${sequences.baseName}_vclust.fasta\" \"${sequences.baseName}_vclust_cluster.uc.clstr\"\n    mv \"${sequences.baseName}_vclust.fastaTEST\" \"${sequences.baseName}_vclust.fasta\"\n\n  \"\"\"\n\n\n}"], "list_proc": ["klamkiew/viralclust/vclust"], "list_wf_names": ["klamkiew/viralclust"]}, {"nb_reuse": 1, "tools": ["MMseqs"], "nb_own": 1, "list_own": ["klamkiew"], "nb_wf": 1, "list_wf": ["viralclust"], "list_contrib": ["klamkiew"], "nb_contrib": 1, "codes": ["\nprocess mmseqs{\n  label 'mmseqs'\n  publishDir \"${params.output}/${params.mmseqs_output}\", mode: 'copy', pattern: \"*_mmseqs*\"\n  publishDir \"${params.output}/${params.mmseqs_output}\", mode: 'copy', pattern: \"*UNCLUSTERED*\"\n  publishDir \"${params.output}/${params.summary_output}/unclustered_sequences\", mode: 'copy', pattern: '*UNCLUSTERED.fasta'\n  publishDir \"${params.output}/${params.summary_output}/clustered_sequences\", mode: 'copy', pattern: '*_mmseqs.fasta'\n\n  input:\n    path(sequences)\n    val(addParams)\n    val(goi)\n\n  output:\n    tuple val(\"${params.output}/${params.mmseqs_output}\"), path(\"${sequences.baseName}_mmseqs.fasta\"), path(\"${sequences.baseName}_mmseqs.fasta.clstr\")\n                                                                            \n    path \"${sequences.baseName}_mmseqs_UNCLUSTERED.fasta\"\n\n  script:\n  def GOI = goi != 'NO FILE' ? \"${goi}\" : ''\n  \"\"\"\n    mmseqs easy-linclust ${addParams} --threads \"${task.cpus}\" \"${sequences}\" \"${sequences.baseName}_mmseqs\" tmp\n    mv ${sequences.baseName}_mmseqs_rep_seq.fasta ${sequences.baseName}_mmseqs.fasta\n\n    if [ \"{$GOI}\" != 'NO FILE' ]; then\n      for ID in \\$(grep '>' ${GOI}); do\n        grep -m 1 \"\\$ID\" \"${sequences.baseName}_mmseqs.fasta\" || grep -A1 \"\\$ID\" ${GOI}  >> \"${sequences.baseName}_mmseqs.fasta\"\n      done\n    fi\n\n    python3 ${projectDir}/bin/mmseqs2cdhit.py ${sequences.baseName}_mmseqs_cluster.tsv \"${sequences}\" ${GOI}\n    mv ${sequences.baseName}_mmseqs_cluster.tsv.clstr \"${sequences.baseName}_mmseqs.fasta.clstr\"\n    python3 ${projectDir}/bin/filter_unclustered.py \"${sequences.baseName}_mmseqs.fasta\" \"${sequences.baseName}_mmseqs.fasta.clstr\"\n    mv \"${sequences.baseName}_mmseqs.fastaTEST\" \"${sequences.baseName}_mmseqs.fasta\"  \n\n  \"\"\"\n}"], "list_proc": ["klamkiew/viralclust/mmseqs"], "list_wf_names": ["klamkiew/viralclust"]}, {"nb_reuse": 2, "tools": ["FastQC", "MultiQC"], "nb_own": 1, "list_own": ["kmayerb"], "nb_wf": 1, "list_wf": ["knead-docker"], "list_contrib": ["kmayerb"], "nb_contrib": 1, "codes": ["\nprocess fastqc_on_raw_files {\n\ttag \"FASTQC ON RAW INPUT .fq FILES\"\n\n\tcontainer 'quay.io/kmayerb/mitochondria@sha256:d48892f367b217116874ca18e5f5fa602413d6a6030bccd02228f2a4153a3067'\n\n\tpublishDir params.output_folder\n\n\tinput:\n\tset sample_name, file(fastq1), file(fastq2) from raw_reads_to_fastqc_channel\n\n\toutput:\n    file(\"outputs/${fastq1.getBaseName()}_fastqc.{zip,html}\") into raw_fastqc_R1\n    file(\"outputs/${fastq2.getBaseName()}_fastqc.{zip,html}\") into raw_fastqc_R2\n\n\tscript:\n\t\"\"\"\n\tmkdir outputs\n\tfastqc -t $task.cpus -o outputs -f fastq -q ${fastq1}\n\tfastqc -t $task.cpus -o outputs -f fastq -q ${fastq2}\n\t\"\"\"\n}", "\nprocess multiqc {\n\n\tcontainer \"quay.io/kmayerb/nf-multiqc:v1.8nf\"\n\n\tpublishDir params.output_folder\n\n\ttag \"Pre-Trimming MULTIQC report generation\"\n\n\tinput:\n    file('fastqc/*') from raw_fastqc_R1.mix(raw_fastqc_R2).mix(postknead_fastqc_R1).mix(postknead_fastqc_R2).collect()\n\n    output:\n    file('pre_multiqc_report_raw.html')\n\n    script:\n    \"\"\"\n    multiqc . -o ./ -n pre_multiqc_report_raw.html -m fastqc\n    \"\"\"\n\n}"], "list_proc": ["kmayerb/knead-docker/fastqc_on_raw_files", "kmayerb/knead-docker/multiqc"], "list_wf_names": ["kmayerb/knead-docker"]}, {"nb_reuse": 1, "tools": ["QResearch"], "nb_own": 1, "list_own": ["ksumngs"], "nb_wf": 1, "list_wf": ["nf-modules"], "list_contrib": ["MillironX"], "nb_contrib": 1, "codes": ["process EDIRECT_ESEARCH {\n    tag \"${ (query.length() > 100) ? query.substring(0, 100) + '...' : query }\"\n    label 'run_local'\n    label 'process_low'\n    label 'error_backoff'\n\n    conda (params.enable_conda ? \"bioconda::entrez-direct=16.2\" : null)\n    container 'docker.io/ncbi/edirect:12.5'\n\n                                                                                  \n                                                                    \n    maxForks 1\n\n    input:\n    val(query)\n    val(db)\n\n    output:\n    path \"search.xml\"   , emit: xml\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    esearch \\\\\n            -db ${db} \\\\\n            -query \"${query}\" \\\\\n            ${args} \\\\\n        > search.xml\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        esearch: \\$(esearch -version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ksumngs/nf-modules/EDIRECT_ESEARCH"], "list_wf_names": ["ksumngs/nf-modules"]}, {"nb_reuse": 2, "tools": ["MultiQC", "Trimmomatic"], "nb_own": 2, "list_own": ["kviljoen", "ksumngs"], "nb_wf": 2, "list_wf": ["nf-modules", "fastq_QC"], "list_contrib": ["alesssia", "MillironX", "kviljoen"], "nb_contrib": 3, "codes": ["process TRIMMOMATIC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::trimmomatic=0.39\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/trimmomatic:0.39--hdfd78af_2':\n        'quay.io/biocontainers/trimmomatic:0.39--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def readtype = meta.single_end ? 'SE' : 'PE'\n    def trimmed = meta.single_end ?\n        \"${prefix}_trimmed.fastq.gz\" :\n        \"${prefix}_trimmed_R1.fastq.gz /dev/null ${prefix}_trimmed_R2.fastq.gz /dev/null\"\n    def jmemstring = task.memory.toMega() + 'M'\n    \"\"\"\n    trimmomatic \\\\\n            -Xmx${jmemstring} \\\\\n            ${readtype} \\\\\n            -threads ${task.cpus} \\\\\n            ${reads} \\\\\n            ${trimmed} \\\\\n            ${args} \\\\\n        2> >(tee ${prefix}.log >&2)\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        trimmomatic: \\$(trimmomatic -version)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess runMultiQC_postfilterandtrim {\n\tcache 'deep'\n    tag { \"rMQC_post_FT\" }\n\n    publishDir \"${params.outdir}/FastQC_post_filter_trim\", mode: 'copy'\n\n    input:\n        file('*') from fastqc_files_2.collect()\n\n    output:\n        file('multiqc_report.html')\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["ksumngs/nf-modules/TRIMMOMATIC", "kviljoen/fastq_QC/runMultiQC_postfilterandtrim"], "list_wf_names": ["ksumngs/nf-modules", "kviljoen/fastq_QC"]}, {"nb_reuse": 3, "tools": ["FastQC", "RAxML-NG", "MultiQC"], "nb_own": 2, "list_own": ["ksumngs", "kviljoen"], "nb_wf": 2, "list_wf": ["nf-modules", "fastq_QC"], "list_contrib": ["alesssia", "MillironX", "kviljoen"], "nb_contrib": 3, "codes": ["\nprocess runMultiQC{\n    cache 'deep'\n    tag { \"rMQC\" }\n\n    publishDir \"${params.outdir}/FilterAndTrim\", mode: 'copy'\n\n    input:\n        file('*') from fastqc_files.collect()\n\n    output:\n        file('multiqc_report.html')\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n}", "\nprocess runFastQC_postfilterandtrim {\n    cache 'deep'\n    tag { \"rFQC_post_FT.${pairId}\" }\n\n    publishDir \"${params.outdir}/FastQC_post_filter_trim\", mode: \"copy\"\n\n    input:\n    \tset val(pairId), file(\"${pairId}_trimmed_R1.fq\"), file(\"${pairId}_trimmed_R2.fq\") from filteredReadsforQC\n\n    output:\n        file(\"${pairId}_fastqc_postfiltertrim/*.zip\") into fastqc_files_2\n\n    \"\"\"\n    mkdir ${pairId}_fastqc_postfiltertrim\n    fastqc --outdir ${pairId}_fastqc_postfiltertrim \\\n    ${pairId}_trimmed_R1.fq \\\n    ${pairId}_trimmed_R2.fq\n    \"\"\"\n}", "process RAXMLNG_PARSE {\n    tag \"$msa\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::raxml-ng=1.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/raxml-ng:1.1.0--h32fcf60_0':\n        'quay.io/biocontainers/raxml-ng:1.1.0--h32fcf60_0' }\"\n\n    input:\n    path(msa)\n\n    output:\n    path \"*.raxml.rba\" , emit: rba\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    raxml-ng \\\\\n        --parse \\\\\n        --threads auto{${task.cpus}} \\\\\n        --msa ${msa} \\\\\n        ${args}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        raxmlng: \\$(echo \\$(raxml-ng --version 2>&1) | sed 's/^.*RAxML-NG v. //; s/released.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["kviljoen/fastq_QC/runMultiQC", "kviljoen/fastq_QC/runFastQC_postfilterandtrim", "ksumngs/nf-modules/RAXMLNG_PARSE"], "list_wf_names": ["ksumngs/nf-modules", "kviljoen/fastq_QC"]}, {"nb_reuse": 1, "tools": ["eFetch Pmc"], "nb_own": 1, "list_own": ["ksumngs"], "nb_wf": 1, "list_wf": ["nf-modules"], "list_contrib": ["MillironX"], "nb_contrib": 1, "codes": ["process EDIRECT_EFETCH {\n    tag \"$search\"\n    label 'run_local'\n    label 'process_low'\n    label 'error_backoff'\n\n    conda (params.enable_conda ? \"bioconda::entrez-direct=16.2\" : null)\n    container 'docker.io/ncbi/edirect:12.5'\n\n    input:\n    path(search)\n    val(format)\n    val(mode)\n\n    output:\n    path \"*.${mode ?: format ?: 'txt'}\", emit: txt\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def ext = mode ?: format ?: 'txt'\n    def format_flag = format ? \"-format ${format}\" : ''\n    def mode_flag = mode ? \"-mode ${mode}\" : ''\n    \"\"\"\n    efetch \\\\\n            < ${search} \\\\\n            ${format_flag} \\\\\n            ${mode_flag} \\\\\n            ${args} \\\\\n        > result.${ext}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        efetch: \\$(efetch -version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ksumngs/nf-modules/EDIRECT_EFETCH"], "list_wf_names": ["ksumngs/nf-modules"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["ksumngs"], "nb_wf": 1, "list_wf": ["v-met"], "list_contrib": ["MillironX"], "nb_contrib": 1, "codes": ["process TRIMMOMATIC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::trimmomatic=0.39\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/trimmomatic:0.39--hdfd78af_2':\n        'quay.io/biocontainers/trimmomatic:0.39--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def readtype = meta.single_end ? 'SE' : 'PE'\n    def trimmed = meta.single_end ?\n        \"${prefix}_trimmed.fastq.gz\" :\n        \"${prefix}_trimmed_R1.fastq.gz /dev/null ${prefix}_trimmed_R2.fastq.gz /dev/null\"\n    def jmemstring = task.memory.toMega() + 'M'\n    \"\"\"\n    trimmomatic \\\\\n            -Xmx${jmemstring} \\\\\n            ${readtype} \\\\\n            -threads ${task.cpus} \\\\\n            ${reads} \\\\\n            ${trimmed} \\\\\n            ${args} \\\\\n        2> >(tee ${prefix}.log >&2)\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        trimmomatic: \\$(trimmomatic -version)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ksumngs/v-met/TRIMMOMATIC"], "list_wf_names": ["ksumngs/v-met"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["kviljoen"], "nb_wf": 1, "list_wf": ["16S_quick_QC"], "list_contrib": ["kviljoen"], "nb_contrib": 1, "codes": ["\nprocess runMultiQC{\n    tag { \"${params.projectName}.rMQC\" }\n    publishDir \"${out_dir}/\", mode: 'copy', overwrite: true\n\n    input:\n        file('*') from fastqc_files.collect()\n\n    output:\n        file('multiqc_report.html')\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["kviljoen/16S_quick_QC/runMultiQC"], "list_wf_names": ["kviljoen/16S_quick_QC"]}, {"nb_reuse": 1, "tools": ["ITSxpress"], "nb_own": 1, "list_own": ["laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["xingaulaglag"], "nb_contrib": 1, "codes": ["process ITSX_CUTASV {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::itsx=1.1.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/itsx:1.1.3--hdfd78af_1' :\n        'quay.io/biocontainers/itsx:1.1.3--hdfd78af_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"ASV_ITS_seqs.full.fasta\", emit: fasta\n    path \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    ITSx \\\\\n        -i $fasta \\\\\n        $args \\\\\n        --cpu $task.cpus \\\\\n        -o ASV_ITS_seqs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ITSx: \\$( ITSx -h 2>&1 > /dev/null | tail -n 2 | head -n 1 | cut -f 2 -d ' ' )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/ITSX_CUTASV"], "list_wf_names": ["laclac102/ampliseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["laclac102"], "nb_wf": 1, "list_wf": ["ampliseq"], "list_contrib": ["xingaulaglag"], "nb_contrib": 1, "codes": ["process MULTIQC {\n    label 'process_medium'\n\n\n    input:\n    path multiqc_files\n    path \"multiqc_custom_plugins\"\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cd multiqc_custom_plugins/\n    python setup.py develop\n    cd ../\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["laclac102/ampliseq/MULTIQC"], "list_wf_names": ["laclac102/ampliseq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["lakinsm"], "nb_wf": 1, "list_wf": ["bear-wgs"], "list_contrib": ["lakinsm"], "nb_contrib": 1, "codes": ["\nprocess FastQC {\n    tag { dataset_id }\n    \n    publishDir \"${params.output}/QualityMetrics\", mode: \"symlink\"\n    \n    input:\n        set dataset_id, file(forward), file(reverse) from fastqc_pairs\n    \n    output:\n        set dataset_id, file(\"*_fastqc.zip\") into (fastqc_logs)\n    \n    \"\"\"\n    mkdir output   \n    fastqc -f fastq ${forward} ${reverse} -t ${threads} -o output\n    mv output/*.zip .\n    \"\"\"\n}"], "list_proc": ["lakinsm/bear-wgs/FastQC"], "list_wf_names": ["lakinsm/bear-wgs"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["lakinsm"], "nb_wf": 1, "list_wf": ["bear-wgs"], "list_contrib": ["lakinsm"], "nb_contrib": 1, "codes": ["\nprocess BuildReferenceIndex {\n    tag { reference.baseName }\n    \n    publishDir \"${params.output}/BuildReferenceIndex\", mode: \"symlink\"\n    \n    input:\n        file(reference)\n    \n    output:\n        file 'genome.index*' into index\n    \n    \"\"\"\n    bwa index -p genome.index ${reference}\n    \"\"\"\n}"], "list_proc": ["lakinsm/bear-wgs/BuildReferenceIndex"], "list_wf_names": ["lakinsm/bear-wgs"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["lakinsm"], "nb_wf": 1, "list_wf": ["bear-wgs"], "list_contrib": ["lakinsm"], "nb_contrib": 1, "codes": ["\nprocess AlignReadsToGenome {\n    tag { dataset_id }\n    \n    publishDir \"${params.output}/AlignedFiles\", mode: \"symlink\"\n    \n    input:\n        set dataset_id, file(forward), file(reverse) from paired_fastq_alignment\n        file ref_index from index.first()\n    \n    output:\n        set dataset_id, file(\"${dataset_id}.aligned.sam\") into prokka_sam\n        set dataset_id, file(\"${dataset_id}.aligned.sam\") into freebayes_sam\n    \n    \"\"\"\n    bwa mem genome.index ${forward} ${reverse} -t ${threads} > ${dataset_id}.aligned.sam\n    \"\"\"\n}"], "list_proc": ["lakinsm/bear-wgs/AlignReadsToGenome"], "list_wf_names": ["lakinsm/bear-wgs"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["lakinsm"], "nb_wf": 1, "list_wf": ["bear-wgs"], "list_contrib": ["lakinsm"], "nb_contrib": 1, "codes": ["\nprocess AnnotateGenomeAssemblies {\n    tag { dataset_id }\n    \n    publishDir \"${params.output}/AnnotatedGenomeAssemblies\", mode: \"symlink\"\n    \n    input:\n        set dataset_id, file(genome_contig_file) from genome_contigs\n    output:\n        set dataset_id, file(\"${dataset_id}.*\") into (annotated_genome_assemblies)\n    \n    \"\"\"\n    #!/bin/bash\n    prokka --outdir annotations --usegenus --genus $db --cpus $threads --prefix ${dataset_id}.genome --centre x --compliant $genome_contig_file\n    mv annotations/* .\n    \"\"\"\n}"], "list_proc": ["lakinsm/bear-wgs/AnnotateGenomeAssemblies"], "list_wf_names": ["lakinsm/bear-wgs"]}, {"nb_reuse": 1, "tools": ["GATK", "SortMeRna"], "nb_own": 2, "list_own": ["lauramble", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "rnaseq-vizfada"], "list_contrib": ["lauramble", "evanfloden", "vincenthhu", "pditommaso"], "nb_contrib": 4, "codes": ["process GATK4_GENOMICSDBIMPORT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(intervalfile), val(intervalval), path(wspace)\n    val run_intlist\n    val run_updatewspace\n    val input_map\n\n    output:\n    tuple val(meta), path(\"${prefix}\")      , optional:true, emit: genomicsdb\n    tuple val(meta), path(\"$updated_db\")    , optional:true, emit: updatedb\n    tuple val(meta), path(\"*.interval_list\"), optional:true, emit: intervallist\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n                                                     \n    inputs_command = input_map ? \"--sample-name-map ${vcf[0]}\" : \"${'-V ' + vcf.join(' -V ')}\"\n    dir_command = \"--genomicsdb-workspace-path ${prefix}\"\n    intervals_command = intervalfile ? \" -L ${intervalfile} \" : \" -L ${intervalval} \"\n\n                                                                                  \n    if (run_intlist) {\n        inputs_command = ''\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = \"--output-interval-list-to-file ${prefix}.interval_list\"\n    }\n\n                                                                                                                                        \n    if (run_updatewspace) {\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = ''\n        updated_db = wspace.toString()\n    }\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenomicsDBImport] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GenomicsDBImport \\\\\n        $inputs_command \\\\\n        $dir_command \\\\\n        $intervals_command \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/sortmerna:4.2.0--0\"\n    } else {\n        container \"quay.io/biocontainers/sortmerna:4.2.0--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def Refs = \"\"\n    for (i=0; i<fasta.size(); i++) { Refs+= \" --ref ${fasta[i]}\" }\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $options.args\n\n        gzip -f < non_rRNA_reads.fq > ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $options.args\n\n        gzip -f < non_rRNA_reads_fwd.fq > ${prefix}_1.fastq.gz\n        gzip -f < non_rRNA_reads_rev.fq > ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["lauramble/rnaseq-vizfada/SORTMERNA"], "list_wf_names": ["lauramble/rnaseq-vizfada"]}, {"nb_reuse": 4, "tools": ["MarkDuplicates (IP)", "SAMtools", "MultiQC", "Picard", "GATK"], "nb_own": 5, "list_own": ["raygozag", "nibscbioinformatics", "lauramble", "vincenthhu", "liameabbott"], "nb_wf": 4, "list_wf": ["nf-core-westest", "rnaseq-vizfada", "reference-database", "nf-core-viralevo", "viralevo", "rnaseq"], "list_contrib": ["kaurravneet4123", "lescai", "pditommaso", "raygozag", "lauramble", "evanfloden", "bleazard", "vincenthhu", "liameabbott"], "nb_contrib": 9, "codes": ["process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess buildsamtoolsindex {\n  label 'process_medium'\n  tag \"Samtools index of fasta reference\"\n\n  input:\n  file(fasta) from ch_fasta\n\n  output:\n  file(\"${fasta}.fai\") into ch_samtoolsIndex\n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "process GATK4_GENOTYPEGVCFS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(gvcf), path(gvcf_index), path(intervals)\n    path  fasta\n    path  fasta_index\n    path  fasta_dict\n    path  dbsnp\n    path  dbsnp_index\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path  \"versions.yml\"             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def dbsnp_options    = dbsnp ? \"-D ${dbsnp}\" : \"\"\n    def interval_options = intervals ? \"-L ${intervals}\" : \"\"\n    def gvcf_options     = gvcf.name.endsWith(\".vcf\") || gvcf.name.endsWith(\".vcf.gz\") ? \"$gvcf\" : \"gendb://$gvcf\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenotypeGVCFs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        GenotypeGVCFs \\\\\n        $args \\\\\n        $interval_options \\\\\n        $dbsnp_options \\\\\n        -R $fasta \\\\\n        -V $gvcf_options \\\\\n        -O ${prefix}.vcf.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::picard=2.23.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.23.9--0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.23.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"*.version.txt\"                 , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.bam \\\\\n        METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt\n\n    echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d: > ${software}.version.txt\n    \"\"\"\n}", "\nprocess normalize_fasta {\n    publishDir \"${params.genomes_directory}/fasta\", \\\n        pattern: \"reference.fa.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(fasta)\n\n    output:\n    path(\"normalized.fa\")\n    path(\"reference.fa.gz\")\n\n    \"\"\"\n    picard NormalizeFasta \\\n    --INPUT ${fasta} \\\n    --OUTPUT normalized.fa \\\n    --LINE_LENGTH 60 \\\n    --USE_JDK_DEFLATER true \\\n    --USE_JDK_INFLATER true\n    \n    bgzip -c normalized.fa > reference.fa.gz\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/PICARD_MARKDUPLICATES", "nibscbioinformatics/viralevo/buildsamtoolsindex", "lauramble/rnaseq-vizfada/PICARD_MARKDUPLICATES", "liameabbott/reference-database/normalize_fasta"], "list_wf_names": ["nibscbioinformatics/viralevo", "raygozag/rnaseq", "lauramble/rnaseq-vizfada", "liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 2, "list_own": ["lauramble", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "rnaseq-vizfada"], "list_contrib": ["lauramble", "evanfloden", "vincenthhu", "pditommaso"], "nb_contrib": 4, "codes": ["\nprocess STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n    }\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    def args     = options.args.tokenize()\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["lauramble/rnaseq-vizfada/STAR_GENOMEGENERATE"], "list_wf_names": ["lauramble/rnaseq-vizfada"]}, {"nb_reuse": 1, "tools": ["Salmon", "GATK"], "nb_own": 2, "list_own": ["lauramble", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "rnaseq-vizfada"], "list_contrib": ["lauramble", "evanfloden", "vincenthhu", "pditommaso"], "nb_contrib": 4, "codes": ["process GATK4_LEARNREADORIENTATIONMODEL {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(f1r2)\n\n    output:\n    tuple val(meta), path(\"*.tar.gz\"), emit: artifactprior\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def inputs_list = []\n    f1r2.each() { a -> inputs_list.add(\" -I \" + a) }\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK LearnReadOrientationModel] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        LearnReadOrientationModel \\\\\n        ${inputs_list.join(' ')} \\\\\n        -O ${prefix}.tar.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.4.0--hf69c8f4_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.4.0--hf69c8f4_0\"\n    }\n    \n    afterScript \"if [ ${params.keep_fastq} == 'false' ];\\\n                 then \\\n                    ls -l *.fastq.gz & \\\n                    ls -l *.fastq.gz | \\\n                    grep -- '->' | \\\n                    sed -e's/.*-> //' | \\\n                    xargs rm;\\\n                 fi;\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n        \n    script:\n    def software    = getSoftwareName(task.process)\n    prefix          = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $options.args \\\\\n        -o $prefix\n\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n    \n}"], "list_proc": ["lauramble/rnaseq-vizfada/SALMON_QUANT"], "list_wf_names": ["lauramble/rnaseq-vizfada"]}, {"nb_reuse": 2, "tools": ["STAR", "RTREE"], "nb_own": 2, "list_own": ["leipzig", "stevekm"], "nb_wf": 2, "list_wf": ["nextflow-samplesheet-demo", "m6a"], "list_contrib": ["leipzig", "stevekm"], "nb_contrib": 2, "codes": [" process MakeStarIndex {\n        label 'build_index'\n        tag \"star_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n        file gtf\n\n        output:\n        file \"StarIndex\" into star_index\n\n        when:\n        aligner == \"star\"\n\n        script:\n        readLength = 50\n        overhang = readLength - 1\n        \"\"\"\n        mkdir StarIndex\n        STAR --runThreadN ${task.cpus} \\\n        --runMode genomeGenerate \\\n        --genomeDir StarIndex \\\n        --genomeFastaFiles $fasta \\\n        --sjdbGTFfile $gtf \\\n        --sjdbOverhang $overhang \n        \"\"\"\n    }", "\nprocess make_dir {\n    echo true\n    stageInMode \"copy\"\n\n    input:\n    file(\"*\") from samples_files.collect()\n\n    output:\n    file(\"samples_dir\") into samples_dir\n\n    script:\n    \"\"\"\n    echo \"[make_dir]\"\n    pwd\n    for item in *; do\n        mkdir -p samples_dir\n        mv \"\\${item}\" samples_dir/\n    done\n    tree\n    \"\"\"\n}"], "list_proc": ["leipzig/m6a/MakeStarIndex", "stevekm/nextflow-samplesheet-demo/make_dir"], "list_wf_names": ["stevekm/nextflow-samplesheet-demo", "leipzig/m6a"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["leipzig"], "nb_wf": 1, "list_wf": ["m6a"], "list_contrib": ["leipzig"], "nb_contrib": 1, "codes": ["\nprocess Fastp{\n    tag \"$sample_name\"\n                            \n    publishDir path: { params.skip_fastp ? params.outdir : \"${params.outdir}/QC/fastp\" },\n             saveAs: { params.skip_fastp ? null : it }, mode: 'link'\n        \n    input:\n    set sample_name, file(reads) from raw_data\n\n    output:\n    val sample_name into pair_id_fastqc, pair_id_tophat2, pair_id_hisat2, pair_id_bwa, pair_id_star, pair_id_rRNA\n    file \"*_aligners.fastq\" into fastqc_reads ,tophat2_reads, hisat2_reads, bwa_reads, star_reads, rRNA_reads\n    file \"*.{html,json}\" into fastp_results\n\n    when:\n    aligner != \"none\"\n\n    shell:\n    skip_fastp = params.skip_fastp\n    gzip = params.gzip\n    if ( params.single_end ){\n        filename = reads.toString() - ~/(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners = sample_name + \"_aligners.fastq\"\n        \"\"\"\n        if [ \\$(ls ${sample_name}*.gz | wc -w) -gt 0 && $gzip == \"false\" ]; \n            then echo \"Please check whether your data is compressed and add '--gzip' for running pipeline\"; \n            exit 1;\n        fi \n        if [ $gzip == \"true\" ]; then\n            zcat ${reads} > ${sample_name}.fastq\n        fi\n        if [ $skip_fastp == \"false\" ]; then\n            fastp -i ${sample_name}.fastq -o ${add_aligners} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${sample_name}.fastq ${add_aligners}\n        fi\n        \"\"\"\n    } else {\n        filename = reads[0].toString() - ~/(_R[0-9])?(_[0-9])?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners_1 = sample_name + \"_1_aligners.fastq\"\n        add_aligners_2 = sample_name + \"_2_aligners.fastq\"\n        \"\"\"\n        if [ \\$(ls ${sample_name}*.gz | wc -w) -gt 0 && $gzip == \"false\" ];\n            then echo \"Please check whether your data is compressed and add '--gzip' for running pipeline\"; \n            exit 1;\n        fi \n        if [ $gzip == \"true\" ]; then\n            zcat ${reads[0]} > ${sample_name}_1.fastq\n            zcat ${reads[1]} > ${sample_name}_2.fastq\n        fi\n        if [ $skip_fastp == \"false\" ]; then  \n            fastp -i ${sample_name}*1.fastq -o ${add_aligners_1} -I ${sample_name}*2.fastq -O ${add_aligners_2} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${sample_name}*1.fastq ${add_aligners_1}\n            mv ${sample_name}*2.fastq ${add_aligners_2}\n        fi\n        \"\"\"\n    } \n}"], "list_proc": ["leipzig/m6a/Fastp"], "list_wf_names": ["leipzig/m6a"]}, {"nb_reuse": 2, "tools": ["FastQC", "totalVI"], "nb_own": 2, "list_own": ["leipzig", "stjacqrm"], "nb_wf": 2, "list_wf": ["berrywood", "m6a"], "list_contrib": ["leipzig", "stjacqrm"], "nb_contrib": 2, "codes": ["\nprocess build_report {\n  beforeScript 'ulimit -s unlimited'\n  tag \"$name\"\n  publishDir \"${params.outdir}\", mode: 'copy', pattern:\"*.csv\"\n\n  input:\n  file(nextclade_clades) from nextclade_results\n  file(pangolin_lineage) from lineage_results\n  file(nextclade_version) from nextclade_version\n  file(pangolin_version) from pangolin_version\n  file(vadr_results) from vadr_results\n\n  output:\n  file(\"*.csv\") into render_berrywood\n\n  script:\n\"\"\"\n#!/usr/bin/env python3\nimport pandas as pd\nimport numpy as np\nimport os, sys\nimport glob, csv\nimport xml.etree.ElementTree as ET\nfrom datetime import datetime\ntoday = datetime.today()\ntoday = today.strftime(\"%m%d%y\")\nnextclade_df = pd.read_csv('nextclade_results.csv',sep=';')\nnextclade_df = nextclade_df[['seqName','clade','insertions','deletions','substitutions','aaSubstitutions']]\nnextclade_df.columns = ['sample','clade','insertions','deletions','nucleotide_subs','amino_acid_subs']\nnextclade_df['sample'] = pd.Series(nextclade_df['sample'], dtype=\"string\")\npangolin_df = pd.read_csv('lineage_report.csv',sep=',')\npangolin_df = pangolin_df[['taxon','lineage']]\npangolin_df.columns = ['sample','pangolin_lineage']\npangolin_df['sample'] = pd.Series(pangolin_df['sample'], dtype=\"string\")\nvadr_df = pd.read_csv('vadr_results.csv',sep=',')\nvadr_df = vadr_df[['isolate','status']]\nvadr_df.columns = ['sample','vadr_status']\nvadr_df['sample'] = pd.Series(vadr_df['sample'], dtype=\"string\")\nnextclade_version_df = pd.read_csv('nextclade_complete_version.csv',sep=',')\nnextclade_version_df = nextclade_version_df[['version']]\nnextclade_version_df.columns = ['nextclade_version']\npangolin_version_df = pd.read_csv('complete_lineage_version.csv',sep=',')\npangolin_version_df = pangolin_version_df[['version']]\npangolin_version_df.columns = ['pangolin_version']\ndataframes = [nextclade_df,pangolin_df,vadr_df]\ntotal = pd.merge(nextclade_df,pangolin_df, on='sample')\ntotal = pd.merge(total,vadr_df,on='sample')\ntotal['pangolin_version'] = pangolin_version_df._get_value(0,'pangolin_version')\ntotal['nextclade_version'] = nextclade_version_df._get_value(0,'nextclade_version')\ntotal = total[['sample', 'vadr_status', 'pangolin_lineage','pangolin_version','clade','nextclade_version','insertions','deletions','nucleotide_subs','amino_acid_subs']]\ntitle = [\"berrywood_\", today,\".csv\"]\npd.DataFrame.to_csv(total, \"\".join(title), sep=';',index=False)\n\"\"\"\n}", "\nprocess Fastqc{\n    tag \"$sample_name\"\n    publishDir path: { params.skip_fastqc ? params.outdir : \"${params.outdir}/QC\" },\n             saveAs: { params.skip_fastqc ? null : it }, mode: 'link'\n\n    input:\n    val sample_name from pair_id_fastqc\n    file(reads) from fastqc_reads\n\n    output:\n    file \"fastqc/*\" into fastqc_results\n\n    when:\n    aligner != \"none\" && !params.skip_fastqc\n\n    shell:\n    skip_fastqc = params.skip_fastqc\n    if ( params.single_end ){\n        \"\"\"\n        mkdir fastqc\n        fastqc -o fastqc --noextract ${reads}\n        \"\"\"       \n    } else {\n        \"\"\"\n        mkdir fastqc   \n        fastqc -o fastqc --noextract ${reads[0]}\n        fastqc -o fastqc --noextract ${reads[1]}\n        \"\"\"      \n    }\n}"], "list_proc": ["stjacqrm/berrywood/build_report", "leipzig/m6a/Fastqc"], "list_wf_names": ["stjacqrm/berrywood", "leipzig/m6a"]}, {"nb_reuse": 2, "tools": ["FastQC", "TopHat", "MultiQC"], "nb_own": 2, "list_own": ["leipzig", "subwaystation"], "nb_wf": 2, "list_wf": ["pangenome", "m6a"], "list_contrib": ["nf-core-bot", "subwaystation", "Zethson", "leipzig", "AndreaGuarracino", "heuermh"], "nb_contrib": 6, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess Tophat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/tophat2\", mode: 'link', overwrite: true\n\n    input:\n    val sample_name from pair_id_tophat2\n    file(reads) from tophat2_reads\n    file index from tophat2_index.collect()\n    file gtf\n\n    output:\n    file \"*_tophat2.bam\" into tophat2_bam\n    file \"*_log.txt\" into tophat2_log\n    \n    when:\n    aligner == \"tophat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.rev)?(\\.\\d)?(\\.fa)?(\\.bt2)?$/\n    strand_info = params.stranded == \"no\" ? \"fr-unstranded\" : params.stranded == \"reverse\" ? \"fr-secondstrand\" : \"fr-firststrand\"\n    if (params.single_end) {\n        \"\"\"\n        tophat  -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                $reads > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        tophat -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                ${reads[0]} ${reads[1]} > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    }\n}"], "list_proc": ["subwaystation/pangenome/get_software_versions", "leipzig/m6a/Tophat2Align"], "list_wf_names": ["subwaystation/pangenome", "leipzig/m6a"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["leipzig"], "nb_wf": 1, "list_wf": ["m6a"], "list_contrib": ["leipzig"], "nb_contrib": 1, "codes": [" process MeyerPrepration{\n        label 'build_index'\n        tag \"MeyerPrepration\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/meyerPrepration\" : params.outdir },\n                saveAs: { params.saveReference ? it : null }, mode: 'copy'       \n\n        input:\n        file fasta\n\n        output:\n        file \"chromsizes.file\" into chromsizesfile\n        file \"chrName.txt\" into chrNamefile\n        file \"genome.bin25.bed\" into bin25file\n        file \"genomebin\" into genomebin\n\n        when:\n        !params.skip_meyer && !params.skip_peakCalling\n\n        shell:\n        '''\n        #cat !{fasta} | awk 'BEGIN{len=\"\"}{if($0~\">\"){split($0,ID,\"[> ]\");printf len\"ABC\"ID[2]\"\\\\t\";len=0}else{len=len+length($0)}}END{print len}' |sed 's/ABC/\\\\n/g' |awk NF > chromsizes.file\n        samtools faidx !{fasta}\n        cut -f1,2 !{fasta}.fai > chromsizes.file\n        awk '{print $1}' chromsizes.file > chrName.txt\n        mkdir genomebin\n        bedtools makewindows -g chromsizes.file -w 25 > genome.bin25.bed\n        awk '{print \"cat genome.bin25.bed | grep \"$1\" > genomebin/\"$1\".bin25.bed\"}' chrName.txt | xargs -iCMD -P!{task.cpus} bash -c CMD\n        '''\n    }"], "list_proc": ["leipzig/m6a/MeyerPrepration"], "list_wf_names": ["leipzig/m6a"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FastQC", "MultiQC"], "nb_own": 2, "list_own": ["leipzig", "steepale"], "nb_wf": 1, "list_wf": ["nf-core-mutenrich", "m6a"], "list_contrib": ["leipzig", "steepale"], "nb_contrib": 2, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess CreateIGVjs {\n    publishDir \"${params.outdir}/Report\" , mode: 'link', overwrite: true,\n        saveAs: {filename ->\n                 if (filename.indexOf(\".html\") > 0)  \"Igv_js/$filename\"\n                 else if (filename.indexOf(\".pdf\") > 0)  \"Igv_js/$filename\"\n                 else \"Igv_js/$filename\"\n        }        \n    input:\n    file m6APipe_result from m6APipe_result\n    file fasta \n    file gtf\n    file formatted_designfile from formatted_designfile.collect()\n    file group_bed from group_merged_bed.collect()\n    file all_bed from all_merged_bed.collect()\n    file bedgraph from bedgraph_for_genebody.collect()\n    \n    output:\n    file \"*\" into igv_js\n\n    script:    \n    igv_fasta = fasta.baseName.toString() + \".igv.fa\"\n    igv_gtf = gtf.baseName.toString() + \".igv.gtf\"\n    merged_allpeaks_igvfile = all_bed.baseName.toString() + \".igv.bed\"\n    \"\"\"\n    ls -l $fasta | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_fasta'\"}' | bash\n    ls -l $gtf | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_gtf'\"}' | bash\n    ls -l $m6APipe_result | awk '{print \"ln -s \"\\$11\" initial.m6APipe\"}' | bash\n    ls -l $group_bed $all_bed | awk '{sub(\".bed\\$\",\".igv.bed\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    ls -l $bedgraph | awk '{sub(\".bedgraph\\$\",\".igv.bedgraph\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    samtools faidx $igv_fasta\n    bash $baseDir/bin/create_IGV_js.sh $igv_fasta $igv_gtf $merged_allpeaks_igvfile $formatted_designfile\n    \"\"\"\n}"], "list_proc": ["leipzig/m6a/CreateIGVjs"], "list_wf_names": ["leipzig/m6a"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["atacseq_nf"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "ggabernet", "ewels", "apeltzer", "jinmingda", "mashehu", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess splitfastq {\n    tag \"Channel: ${name}\"\n\n    publishDir \"${params.outdir}/FASTQs\", mode: 'copy', pattern: '*.fastq'\n\n    input:\n        set val(name), file(bam) from read_files\n\n    output:\n        set name, file(\"*.fastq\") into fastq_split\n        set name, file(\"cntTotal.txt\") into cnt_total\n    script:\n    \"\"\"\n        module load bedtools/2.25.0-foss-2018b\n        ml load samtools/1.10-foss-2018b\n        samtools view -c ${bam} > cntTotal.txt\n        bamToFastq -i ${bam} -fq ${name}.fastq\n\n    \"\"\"\n}"], "list_proc": ["lengfei5/atacseq_nf/splitfastq"], "list_wf_names": ["lengfei5/atacseq_nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Cutadapt"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["atacseq_nf"], "list_contrib": ["lengfei5", "drewjbeh", "maxulysse", "ggabernet", "ewels", "apeltzer", "jinmingda", "mashehu", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess  cutadapt {\n    tag \"Channel: ${name}\"\n\n    publishDir \"${params.outdir}/cutadapt\", mode: 'copy', pattern: '*.err'\n\n    input:\n        set val(name), file(fastq) from fastq_split\n\n    output:\n        set name, file(\"cutadapt.fastq\") into fastq_cutadapt, fastq_cutadapt2\n        set name, file(\"cutadapt.${name}.err\") into stat_cutadapt\n        set name, file(\"cnt_cutadapt.txt\") into cnt_cutadapt\n\n    script:\n    \"\"\"\n        PYTHON_EGG_CACHE=`pwd` #cutadapt wants to write into home FIXME\n        export PYTHON_EGG_CACHE\n        perl ${baseDir}/scripts/extract.unaligned.pl -b ${bam} -f ${fastq} > cleanReads.fastq\n        samtools view -c ${bam} > cntTotal.txt\n        bamToFastq -i ${bam} -fq /dev/stdout |\\\n            cutadapt -e ${params.adapterER} -a ${params.adapter} -f fastq -o cutadapt.fastq -O ${params.adapterMin} --discard-untrimmed - > cutadapt.${name}.err\n\n        cat cutadapt.fastq | paste - - - - | wc -l > cnt_cutadapt.txt\n\n    \"\"\"\n}"], "list_proc": ["lengfei5/atacseq_nf/cutadapt"], "list_wf_names": ["lengfei5/atacseq_nf"]}, {"nb_reuse": 1, "tools": ["BUStools"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["nf_visium_kallisto"], "list_contrib": ["lengfei5"], "nb_contrib": 1, "codes": ["\nprocess umicounts {\n\n    storeDir params.outdir\n\n    input:\n    file outbus from kal_sort_to_umi\n\n    output:\n    file \"${outbus}/umicount.txt\" into kallisto_umic\n\n    when: params.protocol!='plate'\n\n    script:\n    \"\"\"\n    ml load bustools/0.40.0-foss-2018b\n    bustools text -o ${outbus}/umicount.txt ${outbus}/output.cor.sort.bus\n\n    \"\"\"\n}"], "list_proc": ["lengfei5/nf_visium_kallisto/umicounts"], "list_wf_names": ["lengfei5/nf_visium_kallisto"]}, {"nb_reuse": 1, "tools": ["SraTailor"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["nf_visium_kallisto"], "list_contrib": ["lengfei5"], "nb_contrib": 1, "codes": ["\nprocess makeSeuratPlate {\n\n    storeDir \"${params.outdir}/${params.samplename}\"\n\n    input:\n    file outps from kallisto_pseudo\n    file t2g from t2g_plate.collect()\n\n    output:\n    file \"${params.samplename}_srat.RDS\"\n\n    when: params.protocol=='plate'\n\n    script:\n    \"\"\"\n    #!/usr/local/bin/Rscript --vanilla\n\n    library(Seurat)\n\n    # read in data\n    topdir = \"${outps}\" # source dir\n    exp = Matrix::readMM(paste0(topdir, \"/matrix.abundance.mtx\")) #read matrix\n    bc = read.csv(paste0(topdir, \"/matrix.cells\"), header = F, stringsAsFactors = F)\n    g = read.csv(paste0(topdir, \"/transcripts.txt\"), header = F, stringsAsFactors = F)\n    dimnames(exp) = list(paste0(bc\\$V1,\"-1\"),g\\$V1) # number added because of seurat format for barcodes\n    count.data = Matrix::t(exp)\n\n    # summarise transcripts by gene name\n    t2g = read.table(\"$t2g\", header = F, stringsAsFactors = F, sep = \"\\t\")\n    exp_gene = rowsum(as.matrix(count.data), t2g\\$V2)\n\n    # create Seurat object\n    srat = CreateSeuratObject(counts = exp_gene)\n\n    # get MT% (genes curated from NCBI chrMT genes)\n    mtgenes = c(\"COX1\", \"COX2\", \"COX3\", \"ATP6\", \"ND1\", \"ND5\", \"CYTB\", \"ND2\", \"ND4\",\n                \"ATP8\", \"MT-CO1\", \"COI\", \"LOC9829747\")\n    mtgenes = c(mtgenes, paste0(\"MT\", mtgenes), paste0(\"MT-\", mtgenes))\n    mtgenes = mtgenes[mtgenes %in% rownames(exp_gene)]\n    srat = PercentageFeatureSet(srat, col.name = \"percent.mt\", assay = \"RNA\",\n                                features = mtgenes)\n\n    saveRDS(srat, file = \"${params.samplename}_srat.RDS\")\n    \"\"\"\n\n}"], "list_proc": ["lengfei5/nf_visium_kallisto/makeSeuratPlate"], "list_wf_names": ["lengfei5/nf_visium_kallisto"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["smallRNA_nf"], "list_contrib": ["lengfei5"], "nb_contrib": 1, "codes": ["\nprocess alignSpike {\n\n    tag \"Channel: ${name}\"\n    publishDir \"${params.outdir}/spikeIns\", mode: 'copy', pattern: '*.spike.bam'\n\n    when:\n    spikeIn_file.exists()\n\n    input:\n      file index from index_tailor_spikeIn\n      set name, file(fastq) from fastq_spike\n\n    output:\n      set name, file(\"spike.bam\") into bam_tailor_spike, bam_tailor_spike2\n      file \"${name}.spike.bam\"\n\n    script:\n    \"\"\"\n    if [ -e index.tailor.spike.t_bwt.bwt ]; then\n        tailor_v1.1_linux_static map -i ${fastq} -p index.tailor.spike -l 13 -n ${task.cpus} | samtools view -bS > spike.bam\n    else\n        touch spike.bam\n    fi\n    cp spike.bam ${name}.spike.bam\n\n    \"\"\"\n}"], "list_proc": ["lengfei5/smallRNA_nf/alignSpike"], "list_wf_names": ["lengfei5/smallRNA_nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["smallRNA_nf"], "list_contrib": ["lengfei5"], "nb_contrib": 1, "codes": ["\nprocess alignContamination {\n\n    tag \"Channel: ${name}\"\n\n    input:\n      file index from index_tailor_cont\n      set name, file(fastq) from fastq_trimmed\n\n    output:\n      set name, file(\"contamination.bam\") into bam_tailor_cont, bam_tailor_cont2\n\n    script:\n    \"\"\"\n    if [ -e index.tailor.cont.t_bwt.bwt ]; then\n        tailor_v1.1_linux_static map -i ${fastq} -p index.tailor.cont -l ${params.minAlign}  -n ${task.cpus} | samtools view -bS > contamination.bam\n    else\n        touch contamination.bam\n    fi\n    \"\"\"\n}"], "list_proc": ["lengfei5/smallRNA_nf/alignContamination"], "list_wf_names": ["lengfei5/smallRNA_nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "htseqcount"], "nb_own": 1, "list_own": ["lengfei5"], "nb_wf": 1, "list_wf": ["smallRNA_nf"], "list_contrib": ["lengfei5"], "nb_contrib": 1, "codes": ["\nprocess assignFeat {\n\n    tag \"Channel: ${name}\"\n\n    publishDir \"${params.outdir}/seqCnt\", mode: 'copy', pattern: '*.seqCnt.txt'\n\n    input:\n        set name, file(bam) from bam_tailor\n\t      file gtf from gtf_split\n    output:\n        set name, file(\"seqCnt.txt\") into seq_cnt\n        file \"${name}.seqCnt.txt\"\n\n    script:\n    \"\"\"\n    samtools view ${bam} |\\\n        htseq-count -s yes -m intersection-nonempty - ${gtf} -o assign.tmp\n\n    egrep -v no_feature assign.tmp > assign2feat.tmp\n    perl $baseDir/scripts/reduceBam.tailor.umi.pl -g ${gtf} -s assign2feat.tmp > seqCnt.txt\n    cp seqCnt.txt ${name}.seqCnt.txt\n\n    \"\"\"\n}"], "list_proc": ["lengfei5/smallRNA_nf/assignFeat"], "list_wf_names": ["lengfei5/smallRNA_nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lepsalex"], "nb_wf": 1, "list_wf": ["nextflow-pcawg-bwa-mem-workflow"], "list_contrib": ["lepsalex"], "nb_contrib": 1, "codes": ["\nprocess countReads {\n\n    tag \"$bam.baseName\"\n\n    cpus params.cpus\n    memory \"${params.mem} MB\"\n\n    input:\n    file bam from bams_cr.flatMap()\n\n    output:\n    set val(\"${bam.baseName}\"), file(\"${bam.baseName}_read_count.txt\") into counts\n\n    \"\"\"\n    samtools view ${bam} | \\\\\n    wc -l > ${bam.baseName}_read_count.txt\n    \"\"\"\n}"], "list_proc": ["lepsalex/nextflow-pcawg-bwa-mem-workflow/countReads"], "list_wf_names": ["lepsalex/nextflow-pcawg-bwa-mem-workflow"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["lescailab"], "nb_wf": 1, "list_wf": ["toolprofiler"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0=hf69c8f4_0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.4.0--hf69c8f4_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.4.0--hf69c8f4_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    prefix          = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness = meta.single_end ? 'U' : 'IU'\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? 'SF' : 'ISF'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? 'SR' : 'ISR'\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $options.args \\\\\n        -o $prefix\n\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["lescailab/toolprofiler/SALMON_QUANT"], "list_wf_names": ["lescailab/toolprofiler"]}, {"nb_reuse": 4, "tools": ["MarkDuplicates (IP)", "BCFtools", "SAMtools", "STAR", "Picard", "GATK"], "nb_own": 5, "list_own": ["noelnamai", "nibscbioinformatics", "razielar", "vincenthhu", "lescailab"], "nb_wf": 4, "list_wf": ["differential-expression-analysis", "toolprofiler", "nf-core-westest", "lncRNA_annotation_cufflinks_nf", "viralevo"], "list_contrib": ["lescai", "noelnamai", "razielar", "bleazard", "vincenthhu"], "nb_contrib": 5, "codes": ["\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::picard=2.23.9=0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.23.9--0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.23.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"*.version.txt\"                 , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.bam \\\\\n        METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt\n\n    echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d: > ${software}.version.txt\n    \"\"\"\n}", "\nprocess convert_sam_to_bam {\n\n    cpus = 2\n    tag \"$state_replicate\"\n    container \"noelnamai/asimov:1.0\"\n\n    input:\n    set state_replicate, file(sam_file) from aligned_sam_ch\n\n    output:\n    set state_replicate, file(\"${sam_file.baseName}.bam\") into aligned_bam_ch\n\n    script:\n    \"\"\"\n    samtools view -bS ${sam_file} > ${sam_file.baseName}.bam \n    \"\"\"\n}", "\nprocess GATK4_CALLCOPYRATIOSEGMENTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(cr_seg)\n\n    output:\n    tuple val(meta), path(\"*.called.seg\"), emit: call_seg\n\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK ModelSegments] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CallCopyRatioSegments \\\\\n            --input $cr_seg \\\\\n            --output ${prefix}.called.seg \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess buildconsensus {\n  publishDir \"$params.outdir/calling/${caller}/${sampleprefix}\", mode: \"copy\"\n  label 'process_medium'\n\n  input:\n  file(vcfin) from filteredvars.flatten()                                           \n  file(fastaref) from ch_fasta\n\n  output:\n  tuple val(sampleprefix), val(caller), file(\"${sampleprefix}_${caller}_consensus.fa\") into consensus_ch\n  tuple val(sampleprefix), val(caller), file(\"${vcfin}\") into annotated_vcf_ch\n\n  when: 'lofreq' in tools | 'ivar' in tools | 'all' in tools\n\n  script:\n  sampleprefix = ((vcfin.name).replace(\"_lofreq_filtered.vcf\",\"\")).replace(\"_ivar_filtered.vcf\",\"\")\n  caller = ((vcfin.name).replace(sampleprefix+\"_\",\"\")).replace(\"_filtered.vcf\",\"\")\n  \"\"\"\n  cut -f 1-8 $vcfin > ${sampleprefix}_${caller}.cutup.vcf\n  bcftools norm --rm-dup all ${sampleprefix}_${caller}.cutup.vcf > ${sampleprefix}_${caller}.nodups.vcf\n  bcftools view ${sampleprefix}_${caller}.nodups.vcf -Oz -o ${sampleprefix}_${caller}.vcf.gz\n  bcftools index ${sampleprefix}_${caller}.vcf.gz\n  cat $fastaref | bcftools consensus ${sampleprefix}_${caller}.vcf.gz > ${sampleprefix}_${caller}.consensus.fasta\n\n  perl $baseDir/scripts/change_fasta_name.pl \\\n  -fasta ${sampleprefix}_${caller}.consensus.fasta \\\n  -name ${sampleprefix}L \\\n  -out ${sampleprefix}_${caller}_consensus.fa\n  \"\"\"\n}", "\nprocess mapping {\n    tag \"reads: $name\"\n\n    input:\n    file STARgenome from STARgenomeIndex.first()\n    set val(name), file(reads:'*') from read_files\n\n    output:\n    set val(name), file(\"STAR_${name}\") into STARmappedReads \n\n    script:\n      \n                  \n      \n    \"\"\"\n        STAR --genomeDir ${STARgenome} \\\n             --readFilesIn ${reads} \\\n             --readFilesCommand zcat \\\n             --outFilterType BySJout \\\n             --outSAMunmapped Within \\\n             --outSAMtype BAM SortedByCoordinate \\\n             --outSAMattrIHstart 0 \\\n             --outFilterIntronMotifs RemoveNoncanonical \\\n             --runThreadN ${task.cpus} \\\n             --quantMode TranscriptomeSAM \\\n             --outWigType bedGraph \\\n             --outWigStrand Stranded \\\n             --outFileNamePrefix ${name}\n        \n        mkdir STAR_${name}\n        mv ${name}Aligned* STAR_${name}/.\n        mv ${name}Signal* STAR_${name}/.\n        mv ${name}SJ* STAR_${name}/.\n        mv ${name}Log* STAR_${name}/.\n    \"\"\"\n   \n}"], "list_proc": ["lescailab/toolprofiler/PICARD_MARKDUPLICATES", "noelnamai/differential-expression-analysis/convert_sam_to_bam", "nibscbioinformatics/viralevo/buildconsensus", "razielar/lncRNA_annotation_cufflinks_nf/mapping"], "list_wf_names": ["nibscbioinformatics/viralevo", "noelnamai/differential-expression-analysis", "razielar/lncRNA_annotation_cufflinks_nf", "lescailab/toolprofiler"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["letaylor"], "nb_wf": 1, "list_wf": ["qtlmap"], "list_contrib": ["kauralasoo", "kerimoff"], "nb_contrib": 2, "codes": ["\nprocess extract_all_variant_info {\n    tag \"${study_name}\"\n                                                                      \n\n    input:\n    set study_name, file(expression_matrix), file(phenotype_metadata), file(sample_metadata), file(vcf), file(tpm_file) from genotype_vcf_extract_variant_info\n    \n    output:\n    set study_name, file(expression_matrix), file(phenotype_metadata), file(sample_metadata), file(vcf), file(\"${vcf.simpleName}.variant_information.txt.gz\"), file(tpm_file) into variant_info_create_QTLTools_input\n\n    script:\n    if (params.is_imputed) {\n        \"\"\"\n        set +o pipefail; bcftools +fill-tags $vcf | bcftools query -f '%CHROM\\\\t%POS\\\\t%ID\\\\t%REF\\\\t%ALT\\\\t%TYPE\\\\t%AC\\\\t%AN\\\\t%MAF\\\\t%R2\\\\n' | gzip > ${vcf.simpleName}.variant_information.txt.gz\n        \"\"\"\n    } else {\n        \"\"\"\n        set +o pipefail; bcftools +fill-tags $vcf | bcftools query -f '%CHROM\\\\t%POS\\\\t%ID\\\\t%REF\\\\t%ALT\\\\t%TYPE\\\\t%AC\\\\t%AN\\\\t%MAF\\\\tNA\\\\n' | gzip > ${vcf.simpleName}.variant_information.txt.gz\n        \"\"\"\n    }\n}"], "list_proc": ["letaylor/qtlmap/extract_all_variant_info"], "list_wf_names": ["letaylor/qtlmap"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["letaylor"], "nb_wf": 1, "list_wf": ["qtlmap"], "list_contrib": ["kauralasoo", "kerimoff"], "nb_contrib": 2, "codes": ["\nprocess extract_samples {\n    tag \"${study_name}_${sample_names.simpleName}\"\n                                                      \n\n    input:\n    set study_name, file(genotype_vcf), file(sample_names) from qtl_group_samplenames.transpose()\n\n    output:\n    set val(\"${study_name}_${sample_names.simpleName}\"), file(\"${sample_names.simpleName}.vcf.gz\") into vcfs_extract_variant_info, vcfs, vcfs_perform_pca, vcf_temp \n    set val(\"${study_name}_${sample_names.simpleName}\"), file(\"${sample_names.simpleName}.vcf.gz.csi\") into vcf_indexes, vcf_index_temp\n\n    script:\n    \"\"\"\n    bcftools view -S $sample_names $genotype_vcf -Oz -o ${sample_names.simpleName}.vcf.gz\n    bcftools index ${sample_names.simpleName}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["letaylor/qtlmap/extract_samples"], "list_wf_names": ["letaylor/qtlmap"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["letaylor"], "nb_wf": 1, "list_wf": ["qtlmap"], "list_contrib": ["kauralasoo", "kerimoff"], "nb_contrib": 2, "codes": ["\nprocess extract_variant_info {\n    tag \"${study_qtl_group}\"\n    publishDir \"${params.outdir}/final/${study_qtl_group}\", mode: 'copy'\n\n    input:\n    set study_qtl_group, file(vcf) from vcfs_extract_variant_info\n    \n    output:\n    file \"${study_qtl_group}.variant_information.txt.gz\"\n\n    script:\n    if (params.is_imputed) {\n        \"\"\"\n        set +o pipefail; bcftools +fill-tags $vcf | bcftools query -f '%CHROM\\\\t%POS\\\\t%ID\\\\t%REF\\\\t%ALT\\\\t%TYPE\\\\t%AC\\\\t%AN\\\\t%MAF\\\\t%R2\\\\n' | gzip > ${study_qtl_group}.variant_information.txt.gz\n        \"\"\"\n    } else {\n        \"\"\"\n        set +o pipefail; bcftools +fill-tags $vcf | bcftools query -f '%CHROM\\\\t%POS\\\\t%ID\\\\t%REF\\\\t%ALT\\\\t%TYPE\\\\t%AC\\\\t%AN\\\\t%MAF\\\\tNA\\\\n' | gzip > ${study_qtl_group}.variant_information.txt.gz\n        \"\"\"\n    }\n}"], "list_proc": ["letaylor/qtlmap/extract_variant_info"], "list_wf_names": ["letaylor/qtlmap"]}, {"nb_reuse": 1, "tools": ["QTLtools"], "nb_own": 1, "list_own": ["letaylor"], "nb_wf": 1, "list_wf": ["qtlmap"], "list_contrib": ["kauralasoo", "kerimoff"], "nb_contrib": 2, "codes": ["\nprocess run_permutation {\n    tag \"${study_qtl_group} - ${batch_index}/${params.n_batches}\"\n                                                               \n    \n    when:\n    params.run_permutation\n\n    input:\n    each batch_index from 1..params.n_batches\n    set study_qtl_group, file(bed), file(bed_index), file(vcf), file(vcf_index), file(covariate) from tuple_run_permutation.join(covariates_run_permutation)\n\n    output:\n    set val(study_qtl_group), file(\"${study_qtl_group}.permutation.batch.${batch_index}.${params.n_batches}.txt\") into batch_files_merge_permutation_batches\n\n    script:\n    \"\"\"\n    QTLtools cis --vcf $vcf --bed $bed --cov $covariate --chunk $batch_index ${params.n_batches} --out ${study_qtl_group}.permutation.batch.${batch_index}.${params.n_batches}.txt --window ${params.cis_window} --permute 10000 --grp-best\n    \"\"\"\n}"], "list_proc": ["letaylor/qtlmap/run_permutation"], "list_wf_names": ["letaylor/qtlmap"]}, {"nb_reuse": 1, "tools": ["QTLtools"], "nb_own": 1, "list_own": ["letaylor"], "nb_wf": 1, "list_wf": ["qtlmap"], "list_contrib": ["kauralasoo", "kerimoff"], "nb_contrib": 2, "codes": ["\nprocess run_nominal {\n    tag \"${study_qtl_group} - ${batch_index}/${params.n_batches}\"\n\n    when:\n    params.run_nominal\n    \n    input:\n    each batch_index from 1..params.n_batches\n    set study_qtl_group, file(bed), file(bed_index), file(vcf), file(vcf_index), file(covariate) from tuple_run_nominal.join(covariates_run_nominal)\n\n    output:\n    set study_qtl_group, file(\"${study_qtl_group}.nominal.batch.${batch_index}.${params.n_batches}.txt\") into batch_files_merge_nominal_batches\n\n    script:\n    \"\"\"\n\tQTLtools cis --vcf $vcf --bed $bed --cov $covariate --chunk $batch_index ${params.n_batches} --out ${study_qtl_group}.nominal.batch.${batch_index}.${params.n_batches}.txt --window ${params.cis_window} --nominal 1\n    \"\"\"\n}"], "list_proc": ["letaylor/qtlmap/run_nominal"], "list_wf_names": ["letaylor/qtlmap"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess clean_single_reads_fastp {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}/qc\", \\\n        mode: \"copy\", overwrite: true, \\\n        pattern: \"*.fastp.{json,html}\"\n\n    input:\n    tuple val(id), path(read)\n\n    output:\n    tuple val(id), path(\"*fastp*\")\n\n    \"\"\"\n    fastp \\\n    --in1 ${read} \\\n    --out1 ${id}.fastp.fastq.gz \\\n    --json ${id}.fastp.json \\\n    --html ${id}.fastp.html \\\n    --thread ${task.cpus} \\\n    --qualified_quality_phred 20\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/clean_single_reads_fastp"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess clean_paired_reads_fastp {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}/qc\", \\\n        mode: \"copy\", overwrite: true, \\\n        pattern: \"*.fastp.{json,html}\"\n\n    input:\n    tuple val(id), path(reads)\n\n    output:\n    tuple val(id), path(\"*fastp*\")\n\n    \"\"\"\n    fastp \\\n    --in1 ${reads[0]} \\\n    --out1 ${id}_1.fastp.fastq.gz \\\n    --in2 ${reads[1]} \\\n    --out2 ${id}_2.fastp.fastq.gz \\\n    --detect_adapter_for_pe \\\n    --json ${id}.fastp.json \\\n    --html ${id}.fastp.html \\\n    --thread ${task.cpus} \\\n    --qualified_quality_phred 20\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/clean_paired_reads_fastp"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 2, "tools": ["Picard", "Salmon"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 2, "list_wf": ["reference-database", "bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess normalize_fasta {\n    publishDir \"${params.genomes_directory}/fasta\", \\\n        pattern: \"reference.fa.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(fasta)\n\n    output:\n    path(\"normalized.fa\")\n    path(\"reference.fa.gz\")\n\n    \"\"\"\n    picard NormalizeFasta \\\n    --INPUT ${fasta} \\\n    --OUTPUT normalized.fa \\\n    --LINE_LENGTH 60 \\\n    --USE_JDK_DEFLATER true \\\n    --USE_JDK_INFLATER true\n    \n    bgzip -c normalized.fa > reference.fa.gz\n    \"\"\"\n}", "\nprocess quantify_single_reads_salmon {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}\", \\\n        mode: \"copy\", overwrite: true, \\\n        saveAs: { filename -> \"salmon\" }\n\n    input:\n    tuple val(id), path(fastp_fastq)\n    path(salmon_index)\n    path(reference_gtf)\n\n    output:\n    tuple val(id), env(strand), path(\"${id}-salmon\", type: \"dir\")\n\n    \"\"\"\n    salmon quant \\\n    --index ${salmon_index} \\\n    --libType A \\\n    --output ${id}-salmon \\\n    --unmatedReads ${fastp_fastq} \\\n    --gcBias \\\n    --seqBias \\\n    --recoverOrphans \\\n    --threads ${task.cpus}\n\n    strand=\\$(\n        cat ${id}-salmon/lib_format_counts.json | \\\n        jq '.expected_format' | \\\n        sed 's/\"//g'\n    )\n    \n    mv ${id}-salmon/quant.sf ${id}-salmon/${id}.quant.sf\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/normalize_fasta", "liameabbott/bulk-rna-seq-pipeline/quantify_single_reads_salmon"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline", "liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess quantify_paired_reads_salmon {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}\", \\\n        mode: \"copy\", overwrite: true, \\\n        saveAs: { filename -> \"salmon\" }\n\n    input:\n    tuple val(id), path(fastp_fastqs)\n    path(salmon_index)\n    path(reference_gtf)\n\n    output:\n    tuple val(id), env(strand), path(\"${id}-salmon\", type: \"dir\")\n\n    \"\"\"\n    salmon quant \\\n    --index ${salmon_index} \\\n    --libType A \\\n    --output ${id}-salmon \\\n    --mates1 ${fastp_fastqs[0]} \\\n    --mates2 ${fastp_fastqs[1]} \\\n    --gcBias \\\n    --seqBias \\\n    --recoverOrphans \\\n    --threads ${task.cpus}\n\n    strand=\\$(\n        cat ${id}-salmon/lib_format_counts.json | \\\n        jq '.expected_format' | \\\n        sed 's/\"//g'\n    )\n    \n    mv ${id}-salmon/quant.sf ${id}-salmon/${id}.quant.sf\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/quantify_paired_reads_salmon"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 2, "tools": ["STAR", "BEDTools"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 2, "list_wf": ["reference-database", "bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess align_single_reads_star {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    tuple val(id), path(fastp_fastq)\n    path(star_index)\n\n    output:\n    tuple val(id), path(\"star\", type: \"dir\")\n\n    \"\"\"\n    STAR \\\n    --runThreadN ${task.cpus} \\\n    --genomeDir ${star_index} \\\n    --readFilesIn ${fastp_fastq} \\\n    --readFilesCommand gunzip -c \\\n    --outSAMtype BAM SortedByCoordinate \\\n    --outFileNamePrefix star/${id}. \\\n    --twopassMode Basic \\\n    --quantMode TranscriptomeSAM GeneCounts \\\n    --outSAMunmapped Within \\\n    --outFilterMultimapNmax 1\n    \"\"\"\n}", "\nprocess extract_genes {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.genic.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(bed)\n\n    output:\n    path(\"reference.genic.bed\")\n    path(\"reference.genic.bed.gz\")\n\n    \"\"\"\n    awk '\\$13 == \"gene\"' ${bed} | \\\n    cut -f1-6 | \\\n    bedtools merge -i stdin > reference.genic.bed\n    gzip -c reference.genic.bed > reference.genic.bed.gz\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/align_single_reads_star", "liameabbott/reference-database/extract_genes"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline", "liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess align_paired_reads_star {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    tuple val(id), path(fastp_fastqs)\n    path(star_index)\n\n    output:\n    tuple val(id), path(\"star\", type: \"dir\")\n\n    \"\"\"\n    STAR \\\n    --runThreadN ${task.cpus} \\\n    --genomeDir ${star_index} \\\n    --readFilesIn ${fastp_fastqs} \\\n    --readFilesCommand gunzip -c \\\n    --outSAMtype BAM SortedByCoordinate \\\n    --outFileNamePrefix star/${id}. \\\n    --twopassMode Basic \\\n    --quantMode TranscriptomeSAM GeneCounts \\\n    --outSAMunmapped Within \\\n    --outFilterMultimapNmax 1\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/align_paired_reads_star"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess compute_qc_metrics_picard {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/${id}/qc\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    tuple val(id), val(strand), path(star_genome_bam)\n    path(ref_flat)\n    path(rRNA_interval_list)\n\n    output:\n    tuple val(id), path(\"${id}.rna_metrics\")\n\n    \"\"\"\n    case ${strand} in\n        *F ) strand=FIRST_READ_TRANSCRIPTION_STRAND;;\n        *R ) strand=SECOND_READ_TRANSCRIPTION_STRAND;;\n        *  ) strand=NONE;;\n    esac\n\n    picard -Xmx16g CollectRnaSeqMetrics \\\n    --INPUT ${star_genome_bam} \\\n    --OUTPUT ${id}.rna_metrics \\\n    --REF_FLAT ${ref_flat} \\\n    --RIBOSOMAL_INTERVALS ${rRNA_interval_list} \\\n    --STRAND_SPECIFICITY \\$strand \\\n    --USE_JDK_DEFLATER true \\\n    --USE_JDK_INFLATER true\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/compute_qc_metrics_picard"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 2, "tools": ["Salmon", "BEDTools"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 2, "list_wf": ["reference-database", "bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess extract_intronic_regions {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.intronic.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(genes_bed)\n    path(exons_bed)\n\n    output:\n    path(\"reference.intronic.bed\")\n    path(\"reference.intronic.bed.gz\")\n\n    \"\"\"\n    bedtools subtract -a ${genes_bed} -b ${exons_bed} | \\\n    bedtools merge -i stdin > reference.intronic.bed\n    gzip -c reference.intronic.bed > reference.intronic.bed.gz\n    \"\"\"\n}", "\nprocess collect_salmon_quants {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/summary\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(gtf)\n    path(salmon_quants)\n\n    output:\n    path(\"${params.dataset_name}.salmon.*.*.tximport.tsv\")\n\n    \"\"\"\n    collect_quants.R \\\n    ${params.dataset_name} \\\n    \"salmon\" \\\n    ${gtf} \\\n    ${salmon_quants} \n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/extract_intronic_regions", "liameabbott/bulk-rna-seq-pipeline/collect_salmon_quants"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline", "liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["RSEM"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess collect_rsem_quants {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/summary\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(gtf)\n    path(rsem_quants)\n\n    output:\n    path(\"${params.dataset_name}.rsem.*.*.tximport.tsv\")\n\n    \"\"\"\n    collect_quants.R \\\n    ${params.dataset_name} \\\n    \"rsem\" \\\n    ${gtf} \\\n    ${rsem_quants}\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/collect_rsem_quants"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["bulk-rna-seq-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess generate_multiqc_report {\n    label \"lrg\"\n    publishDir \"${params.output_directory}/summary\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(multiqc_config)\n    path(pipeline_logs)\n\n    output:\n    path(\"${params.dataset_name}.multiqc_report.html\")\n\n    \"\"\"\n    multiqc \\\n    --config ${multiqc_config} \\\n    --filename ${params.dataset_name}.multiqc_report.html \\\n    ${pipeline_logs}\n    \"\"\"\n}"], "list_proc": ["liameabbott/bulk-rna-seq-pipeline/generate_multiqc_report"], "list_wf_names": ["liameabbott/bulk-rna-seq-pipeline"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["drop-seq-tools-pipeline"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess align_annotate_reads {\n\tpublishDir \"${output_directory}/${sample_name}/bams\", \\\n\t\tpattern: \"${sample_name}.annotated.bam\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tpath(picard_jar)\n\ttuple val(sample_name), path(prepared_bam)\n\tpath(star_index)\n\tpath(normalized_fasta)\n\tpath(reference_dict)\n\tpath(reference_refFlat)\n\n\toutput:\n\ttuple val(sample_name), path(\"${sample_name}.annotated.bam\")\n\n\t\"\"\"\n\tjava -jar ${picard_jar} SamToFastq \\\n\tINPUT=${prepared_bam} \\\n\tFASTQ=/dev/stdout \\\n\tQUIET=true \\\n\tUSE_JDK_DEFLATER=true \\\n\tUSE_JDK_INFLATER=true \\\n\tTMP_DIR=./.tmp | \\\n\n\tSTAR \\\n\t--genomeDir ${star_index} \\\n\t--runThreadN ${params.star_n_cores} \\\n\t--readFilesIn /dev/stdin \\\n\t--outFileNamePrefix ${sample_name}-star-alignment/ \\\n\t--limitOutSJcollapsed ${params.star_limit_out_sj_collapsed}\n\n\tjava -jar ${picard_jar} SortSam \\\n\tINPUT=${sample_name}-star-alignment/Aligned.out.sam \\\n\tOUTPUT=${sample_name}.sorted.bam \\\n\tSORT_ORDER=queryname \\\n\tUSE_JDK_DEFLATER=true \\\n\tUSE_JDK_INFLATER=true \\\n\tTMP_DIR=./.tmp \n\n\tjava -jar ${picard_jar} MergeBamAlignment \\\n\tREFERENCE_SEQUENCE=${normalized_fasta} \\\n\tUNMAPPED_BAM=${prepared_bam} \\\n\tALIGNED_BAM=${sample_name}.sorted.bam \\\n\tINCLUDE_SECONDARY_ALIGNMENTS=false \\\n\tCLIP_ADAPTERS=false \\\n\tOUTPUT=${sample_name}.merged.bam \\\n\tUSE_JDK_DEFLATER=true \\\n\tUSE_JDK_INFLATER=true \\\n\tTMP_DIR=./.tmp \n\n\tTagReadWithGeneFunction \\\n\tINPUT=${sample_name}.merged.bam \\\n\tOUTPUT=${sample_name}.annotated.bam \\\n\tANNOTATIONS_FILE=${reference_refFlat} \\\n\tUSE_JDK_DEFLATER=true \\\n\tUSE_JDK_INFLATER=true \\\n\tTMP_DIR=./.tmp \n\t\"\"\"\n}"], "list_proc": ["liameabbott/drop-seq-tools-pipeline/align_annotate_reads"], "list_wf_names": ["liameabbott/drop-seq-tools-pipeline"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess create_sequence_dictionary {\n    publishDir \"${params.genomes_directory}/fasta\", \\\n        pattern: \"reference.dict\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(fasta)\n\n    output:\n    path(\"reference.dict\")\n\n    \"\"\"\n    picard CreateSequenceDictionary \\\n    --REFERENCE ${fasta} \\\n    --OUTPUT reference.dict \\\n    --SPECIES ${params.species} \\\n    --GENOME_ASSEMBLY ${params.assembly} \\\n    --USE_JDK_DEFLATER true \\\n    --USE_JDK_INFLATER true\n     \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/create_sequence_dictionary"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["Gene"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess get_reference_gtf {\n    publishDir \"${params.genomes_directory}/gtf\", \\\n        pattern: \"reference.gtf.gz{.url,}\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    val(gtf_url)\n    path(fasta_fai)\n\n    output:\n    path(\"reference.gtf\")\n    path(\"reference.gtf.gz\")\n    path(\"reference.gtf.gz.url\")\n\n    \"\"\"\n    wget -O - \"${gtf_url}\" | \\\n    gunzip -c | \\\n    awk -v FS=\\$'\\t' -v OFS=\\$'\\t' '\n        (NR==FNR) { a[\\$1]++; next; }\n        (\\$0 ~ /^#/) { print \\$0; next; }\n        (\\$1 in a) { \n            gsub(/%/, \"%%\");\n            gsub(/gene \\\\\"/, \"gene_name \\\\\"\");\n            if (\\$3==\"gene\") { \n                gsub(/transcript_id \\\\\"\\\\\"/, \"\");\n            }\n            printf \\$0;\n            if (\\$0 !~ /gene_name/) { \n                printf \" gene_name \\\\\"NA\\\\\";\"; \n            }\n            if ((\\$3!=\"gene\") && (\\$0 !~ /transcript_name/)) {\n                printf \" transcript_name \\\\\"NA\\\\\";\";\n            }\n            print \"\";\n        }\n    ' ${fasta_fai} - > reference.gtf\n    gzip -c reference.gtf > reference.gtf.gz\n    printf \"${gtf_url}\" > reference.gtf.gz.url\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/get_reference_gtf"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 2, "tools": ["FastQC", "BEDTools"], "nb_own": 2, "list_own": ["lifebit-ai", "liameabbott"], "nb_wf": 2, "list_wf": ["GenomeChronicler-Sarek-nf", "reference-database"], "list_contrib": ["liameabbott", "cgpu"], "nb_contrib": 2, "codes": ["\nprocess extract_exons {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.exonic.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(bed)\n\n    output:\n    path(\"reference.exonic.bed\")\n    path(\"reference.exonic.bed.gz\")\n\n    \"\"\"\n    awk '\\$13 == \"exon\"' ${bed} | \\\n    cut -f1-6 | \\\n    bedtools merge -i stdin > reference.exonic.bed\n    gzip -c reference.exonic.bed > reference.exonic.bed.gz\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/extract_exons", "lifebit-ai/GenomeChronicler-Sarek-nf/FastQCBAM"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf", "liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess extract_CDS {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.CDS.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(bed)\n\n    output:\n    path(\"reference.CDS.bed\")\n    path(\"reference.CDS.bed.gz\")\n\n    \"\"\"\n    awk '\\$13 == \"CDS\"' ${bed} | \\\n    cut -f1-6 | \\\n    bedtools merge -i stdin > reference.CDS.bed\n    gzip -c reference.CDS.bed > reference.CDS.bed.gz\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/extract_CDS"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess extract_UTR {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.UTR.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(exons_bed)\n    path(CDS_bed)\n\n    output:\n    path(\"reference.UTR.bed\")\n    path(\"reference.UTR.bed.gz\")\n\n    \"\"\"\n    bedtools subtract -a ${exons_bed} -b ${CDS_bed} | \\\n    bedtools merge -i stdin > reference.UTR.bed\n    gzip -c reference.UTR.bed > reference.UTR.bed.gz\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/extract_UTR"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess extract_MT_genes {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.MT.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n    \n    input:\n    path(bed)\n\n    output:\n    path(\"reference.MT.bed\")\n    path(\"reference.MT.bed.gz\")\n\n    \"\"\"\n    sed 's/^chr//' ${bed} | \\\n    grep -E '^(M|MT)\\t' | \\\n    awk '\\$13 == \"gene\"' | \\\n    cut -f1-6 | \\\n    bedtools merge -i stdin > reference.MT.bed\n    gzip -c reference.MT.bed > reference.MT.bed.gz\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/extract_MT_genes"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess extract_intergenic_regions {\n    publishDir \"${params.genomes_directory}/bed\", \\\n        pattern: \"reference.intergenic.bed.gz\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(genes_bed)\n    path(fasta_fai)\n\n    output:\n    path(\"reference.intergenic.bed\")\n    path(\"reference.intergenic.bed.gz\")\n\n    \"\"\"\n    awk -v OFS=\\$'\\t' '{print \\$1,\\$2}' ${fasta_fai} | \\\n    sort -k1V > chr_sizes.tsv\n\n    bedtools complement -i ${genes_bed} -g chr_sizes.tsv | \\\n    bedtools merge -i stdin > reference.intergenic.bed\n    gzip -c reference.intergenic.bed > reference.intergenic.bed.gz\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/extract_intergenic_regions"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess bed_to_interval_list {\n    publishDir \"${params.genomes_directory}/interval-list\", \\\n        pattern: \"*.interval_list\", \\\n        mode: \"copy\", overwrite: true\n\n    input:\n    path(dict)\n    path(genes_bed)\n    path(exons_bed)\n    path(CDS_bed)\n    path(UTR_bed)\n    path(rRNA_bed)\n    path(MT_bed)\n    path(intronic_bed)\n    path(intergenic_bed)\n    \n    output:\n    path(\"reference.genes.interval_list\")\n    path(\"reference.exons.interval_list\")\n    path(\"reference.CDS.interval_list\")\n    path(\"reference.UTR.interval_list\")\n    path(\"reference.genes.rRNA.interval_list\")\n    path(\"reference.genes.MT.interval_list\")\n    path(\"reference.intronic.interval_list\")\n    path(\"reference.intergenic.interval_list\")\n\n    \"\"\"\n    declare -A files=(\n        [genes]=${genes_bed}\n        [exons]=${exons_bed}\n        [CDS]=${CDS_bed}\n        [UTR]=${UTR_bed}\n        [genes.rRNA]=${rRNA_bed}\n        [genes.MT]=${MT_bed}\n        [intronic]=${intronic_bed}\n        [intergenic]=${intergenic_bed}\n    )\n    \n    for name in \"\\${!files[@]}\"; do\n        picard BedToIntervalList \\\n        --INPUT \\${files[\\$name]} \\\n        --OUTPUT reference.\\${name}.interval_list \\\n        --SEQUENCE_DICTIONARY ${dict} \\\n        --USE_JDK_DEFLATER true \\\n        --USE_JDK_INFLATER true\n    done\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/bed_to_interval_list"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["liameabbott"], "nb_wf": 1, "list_wf": ["reference-database"], "list_contrib": ["liameabbott"], "nb_contrib": 1, "codes": ["\nprocess star_create_index {\n    publishDir \"${params.genomes_directory}/star-index\", \\\n        mode: \"move\", overwrite: true\n\n    input:\n    path(fasta)\n    path(gtf)\n\n    output:\n    path(\"Genome\")\n    path(\"Log.out\")\n    path(\"SA\")\n    path(\"SAindex\")\n    path(\"chrLength.txt\")\n    path(\"chrName.txt\")\n    path(\"chrNameLength.txt\")\n    path(\"chrStart.txt\")\n    path(\"exonGeTrInfo.tab\")\n    path(\"exonInfo.tab\")\n    path(\"geneInfo.tab\")\n    path(\"genomeParameters.txt\")\n    path(\"sjdbInfo.txt\")\n    path(\"sjdbList.fromGTF.out.tab\")\n    path(\"sjdbList.out.tab\")\n    path(\"transcriptInfo.tab\")\n\n    \"\"\"\n    gunzip -c ${fasta} > reference.fa\n    gunzip -c ${gtf} > reference.gtf\n    STAR \\\n    --runThreadN ${task.cpus} \\\n    --runMode genomeGenerate \\\n    --genomeDir . \\\n    --genomeFastaFiles reference.fa \\\n    --sjdbGTFfile reference.gtf \\\n    --sjdbOverhang ${params.star_sjdbOverhang} \\\n    --genomeSAindexNbases ${params.star_genomeSAindexNbases} \\\n    --limitGenomeGenerateRAM 200000000000\n    \"\"\"\n}"], "list_proc": ["liameabbott/reference-database/star_create_index"], "list_wf_names": ["liameabbott/reference-database"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["GenomeChronicler-Sarek-nf", "DeepVariant"], "list_contrib": ["cgpu", "Vlad-Dembrovskyi", "luisas", "PhilPalmer", "mariach", "pprieto"], "nb_contrib": 6, "codes": ["\nprocess CalculateContamination {\n\n    label 'cpus_1'\n\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamCalculateContamination \n        file(\"${idSampleTumor}_pileupsummaries.table\") from mergedPileupFile\n  \n    output:\n        file(\"${idSampleTumor}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools && params.pon\n\n    script:     \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSampleTumor}_pileupsummaries.table \\\n        -O ${idSampleTumor}_contamination.table\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n\n    label 'cpus_1'\n\n    tag {idSampleTN}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTN}/${\"$variantCaller\"}\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idPatient, idSampleTN, file(unfiltered), file(unfilteredIndex) from vcfConcatenatedForFilter\n        file(\"${idSampleTN}.vcf.gz.stats\") from mergedStatsFile\n        file(\"${idSampleTN}_contamination.table\") from contaminationTable\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        \n    output:\n        set val(\"Mutect2\"), idPatient, idSampleTN,\n            file(\"filtered_${variantCaller}_${idSampleTN}.vcf.gz\"),\n            file(\"filtered_${variantCaller}_${idSampleTN}.vcf.gz.tbi\"),\n            file(\"filtered_${variantCaller}_${idSampleTN}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${idSampleTN}_contamination.table \\\n        --stats ${idSampleTN}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O filtered_${variantCaller}_${idSampleTN}.vcf.gz\n    \"\"\"\n}", "\nprocess preprocessBAM{\n\n\n  tag \"${bam[0]}\"\n  container 'lifebitai/samtools'\n  publishDir \"$baseDir/sampleDerivatives\"\n\n  input:\n  set val(prefix), file(bam) from bamChannel\n  output:\n  set file(\"ready/${bam[0]}\"), file(\"ready/${bam[0]}.bai\") into completeChannel, completeStats\n  script:\n  \"\"\"\n\t  mkdir ready\n  [[ `samtools view -H ${bam[0]} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready;}|| { picard AddOrReplaceReadGroups \\\n    I=${bam[0]} \\\n    O=ready/${bam[0]} \\\n    RGID=${params.rgid} \\\n    RGLB=${params.rglb} \\\n    RGPL=${params.rgpl} \\\n    RGPU=${params.rgpu} \\\n    RGSM=${params.rgsm};}\n    cd ready ;samtools index ${bam[0]};\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/CalculateContamination", "lifebit-ai/GenomeChronicler-Sarek-nf/FilterMutect2Calls", "lifebit-ai/DeepVariant/preprocessBAM"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf", "lifebit-ai/DeepVariant"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["DeepVariant"], "list_contrib": ["Vlad-Dembrovskyi", "luisas", "PhilPalmer", "mariach", "pprieto"], "nb_contrib": 5, "codes": ["\nprocess BAMstats{\n\n  tag \"${bam[0]}\"\n  container 'lifebitai/samtools'\n\n  input:\n  set file(bam), file(bai) from completeStats\n  output:\n  file(\"*\") into bam_multiqc\n  script:\n  \"\"\"\n  samtools stats $bam > stats.txt\n  samtools flagstat $bam > flagstat.txt\n  samtools idxstats $bam > idxstats.txt\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/DeepVariant/BAMstats"], "list_wf_names": ["lifebit-ai/DeepVariant"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["DeepVariant"], "list_contrib": ["Vlad-Dembrovskyi", "luisas", "PhilPalmer", "mariach", "pprieto"], "nb_contrib": 5, "codes": ["\nprocess multiqc{\n  tag \"multiqc_report.html\"\n\n  publishDir \"${params.resultdir}/MultiQC\", mode: 'copy'\n  container 'lifebitai/multiqc:v1.7'\n\n  input:\n  file(vcfout) from vcfout\n  file(bamout) from bam_multiqc\n  output:\n  file(\"*\") into multiqc\n\n  script:\n  \"\"\"\n  multiqc . -m vcftools -m samtools\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/DeepVariant/multiqc"], "list_wf_names": ["lifebit-ai/DeepVariant"]}, {"nb_reuse": 2, "tools": ["SAMtools", "VCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["GenomeChronicler-Sarek-nf", "DeepVariantTrain"], "list_contrib": ["cgpu", "sivakhno", "luisas", "mariach", "pprieto"], "nb_contrib": 5, "codes": ["\nprocess Vcftools {\n\n    label 'cpus_1'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publishDirMode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess preprocessBAM{\n\n\n  tag \"${bam[0]}\"\n  container 'lifebitai/samtools'\n  publishDir \"$baseDir/sampleDerivatives\"\n\n  input:\n  set val(prefix), file(bam) from bamChannel\n  output:\n  set file(\"ready/${bam[0]}\"), file(\"ready/${bam[0]}.bai\") into completeChannel\n  script:\n  \"\"\"\n\t  mkdir ready\n  [[ `samtools view -H ${bam[0]} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready;}|| { java -jar /picard.jar AddOrReplaceReadGroups \\\n    I=${bam[0]} \\\n    O=ready/${bam[0]} \\\n    RGID=${params.rgid} \\\n    RGLB=${params.rglb} \\\n    RGPL=${params.rgpl} \\\n    RGPU=${params.rgpu} \\\n    RGSM=${params.rgsm};}\n    cd ready ;samtools index ${bam[0]};\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/Vcftools", "lifebit-ai/DeepVariantTrain/preprocessBAM"], "list_wf_names": ["lifebit-ai/DeepVariantTrain", "lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["DeepVariantWGS"], "list_contrib": ["mariach", "sivakhno", "luisas", "pprieto"], "nb_contrib": 4, "codes": ["\nprocess preprocessBAM{\n\n\n  tag \"${bam[0]}\"\n  container 'lifebitai/samtools'\n  publishDir \"$baseDir/sampleDerivatives\"\n\n  input:\n  set val(prefix), file(bam) from bamChannel\n  output:\n  set file(\"ready/${bam[0]}\"), file(\"ready/${bam[0]}.bai\") into completeChannel\n  script:\n  \"\"\"\n\t  mkdir ready\n  [[ `samtools view -H ${bam[0]} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready;}|| { java -jar /picard.jar AddOrReplaceReadGroups \\\n    I=${bam[0]} \\\n    O=ready/${bam[0]} \\\n    RGID=${params.rgid} \\\n    RGLB=${params.rglb} \\\n    RGPL=${params.rgpl} \\\n    RGPU=${params.rgpu} \\\n    RGSM=${params.rgsm};}\n    cd ready ;samtools index ${bam[0]};\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/DeepVariantWGS/preprocessBAM"], "list_wf_names": ["lifebit-ai/DeepVariantWGS"]}, {"nb_reuse": 4, "tools": ["SAMtools", "kallisto", "QualiMap", "fastPHASE"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 4, "list_wf": ["long-reads", "ReleaseTheKraken", "ExpansionHunter", "kallisto-sra"], "list_contrib": ["cgpu", "luisas", "pditommaso", "PhilPalmer", "imendes93", "evanfloden", "pprieto"], "nb_contrib": 7, "codes": ["\nprocess mapping {\n    tag \"reads: $name\"\n    cpus threads\n    input:\n    file index from transcriptome_index\n    set val(name), file(reads) from read_files\n\n    output:\n    file \"kallisto_${name}\" into kallisto_out_dirs \n\n    script:\n      \n                            \n      \n    def single = reads instanceof Path\n    if( !single ) {\n        \"\"\"\n        mkdir kallisto_${name}\n        kallisto quant -b ${params.bootstrap} -i ${index} -t ${task.cpus} -o kallisto_${name} ${reads}\n        \"\"\"\n    }  \n    else {\n        \"\"\"\n        mkdir kallisto_${name}\n        kallisto quant --single -l ${params.fragment_len} -s ${params.fragment_sd} -b ${params.bootstrap} -i ${index} -t ${task.cpus} -o kallisto_${name} ${reads}\n        \"\"\"\n    }\n\n}", "\nprocess preprocess_bam{\n\n  tag \"${bam}\"\n\tcontainer 'lifebitai/samtools'\n\n  input:\n  file bam from bam\n\n  output:\n  set file(\"ready/${bam}\"), file(\"ready/${bam}.bai\") into completeChannel\n\n  script:\n  \"\"\"\n  mkdir ready\n  [[ `samtools view -H ${bam} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready;}|| { picard AddOrReplaceReadGroups \\\n  I=${bam} \\\n  O=ready/${bam} \\\n  RGID=${params.rgid} \\\n  RGLB=${params.rglb} \\\n  RGPL=${params.rgpl} \\\n  RGPU=${params.rgpu} \\\n  RGSM=${params.rgsm};}\n  cd ready ;samtools index ${bam};\n  \"\"\"\n}", "\nprocess fastp {\n\n    tag { sample_id }\n\n    input:\n    set sample_id, file(fastq_pair) from IN_fastq_raw\n\n    output:\n    set sample_id, file(\"*trim_*.fq.gz\") into OUT_fastp\n\n    script:\n    \"\"\"\n    a=(${fastq_pair})\n\n    if ((\\${#a[@]} > 1));\n    then\n        fastp -i ${fastq_pair[0]} -o ${sample_id}_trim_1.fq.gz -I ${fastq_pair[1]} -O ${sample_id}_trim_2.fq.gz \n    else\n        fastp -i ${fastq_pair} -o ${sample_id}_trim_1.fq.gz \n    fi\n    \"\"\"\n}", "\nprocess bam_qc {\n    tag \"$bam\"\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(name), file(bam), file(bai) from marked_bam_qc\n\n    output:\n    file(\"${name}\") into bam_qc_report\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/kallisto-sra/mapping", "lifebit-ai/ExpansionHunter/preprocess_bam", "lifebit-ai/ReleaseTheKraken/fastp", "lifebit-ai/long-reads/bam_qc"], "list_wf_names": ["lifebit-ai/kallisto-sra", "lifebit-ai/ReleaseTheKraken", "lifebit-ai/ExpansionHunter", "lifebit-ai/long-reads"]}, {"nb_reuse": 1, "tools": ["ExpansionHunter"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["ExpansionHunter"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess expansion_hunter {\n\tpublishDir \"${params.outdir}\", mode: 'copy'\n\tcontainer 'lifebitai/expansionhunter'\n\n\tinput:\n\tset file(bam), file(bai) from completeChannel\n\tfile fasta from fasta\n\tfile repeat_specs from repeat_specs\n\n\toutput:\n\tfile('output.*') into results\n\n\tscript:\n\t\"\"\"\n\tExpansionHunter \\\n\t--bam ${bam} \\\n\t--ref-fasta ${fasta}\\\n\t--repeat-specs ${repeat_specs} \\\n\t--vcf output.vcf \\\n\t--json output.json \\\n\t--log output.log $extraflags\n\t\"\"\"\n}"], "list_proc": ["lifebit-ai/ExpansionHunter/expansion_hunter"], "list_wf_names": ["lifebit-ai/ExpansionHunter"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Minimap2", "Sniffles"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["GangSTR", "long-reads"], "list_contrib": ["PhilPalmer", "pprieto", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess sniffles {\n    tag \"$bam\"\n    publishDir \"${params.outdir}/sniffles\", mode: 'copy'\n    container 'lifebitai/sniffles:latest'\n\n    cpus threads\n\n    input:\n    set val(name), file(bam), file(bai) from bam_md_sniffles\n\n    output:\n    file(\"sniffles_${name}.vcf\") into sniffles_vcf\n\n    \"\"\"\n    sniffles --mapped_reads $bam --vcf sniffles_${name}.vcf -s ${params.min_support} --threads ${task.cpus}\n    \"\"\"\n}", "\nprocess minimap2 {\n    tag \"$reads\"\n    container 'evolbioinfo/minimap2:v2.14'\n\n    cpus threads\n\n    input:\n    set val(name), file(reads), file(fasta) from minimap2\n\n    output:\n    set val(name), file(\"${name}.sam\") into mapped_reads\n\n    script:\n    \"\"\"\n    minimap2 -ax map-ont -t ${task.cpus} $fasta $reads > ${name}.sam\n    \"\"\"\n}", "\nprocess preprocess_bam{\n\n  tag \"${bam}\"\n\tcontainer 'lifebitai/samtools'\n\n  input:\n  file bam from bam\n\n  output:\n  set file(\"ready/${bam}\"), file(\"ready/${bam}.bai\") into completeChannel\n\n  script:\n  \"\"\"\n  mkdir ready\n  [[ `samtools view -H ${bam} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready;}|| { picard AddOrReplaceReadGroups \\\n  I=${bam} \\\n  O=ready/${bam} \\\n  RGID=${params.rgid} \\\n  RGLB=${params.rglb} \\\n  RGPL=${params.rgpl} \\\n  RGPU=${params.rgpu} \\\n  RGSM=${params.rgsm};}\n  cd ready ;samtools index ${bam};\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/long-reads/sniffles", "lifebit-ai/long-reads/minimap2", "lifebit-ai/GangSTR/preprocess_bam"], "list_wf_names": ["lifebit-ai/GangSTR", "lifebit-ai/long-reads"]}, {"nb_reuse": 2, "tools": ["SAMtools", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["GangSTR", "long-reads"], "list_contrib": ["PhilPalmer", "pprieto", "cgpu"], "nb_contrib": 3, "codes": [" process preprocess_genome {\n\n\t\t\ttag \"${fasta}\"\n\t\t\tcontainer 'lifebitai/preprocessingvctools'\n\n      input:\n      file fasta from fastaToFai\n\n      output:\n      file(\"${fasta}.fai\") into fai\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      \"\"\"\n  }", "\nprocess mark_duplicates {\n    tag \"$bam\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(name), file(bam) from sorted_bam\n\n    output:\n    set val(name), file(\"${name}-marked_dup.bam\"), file(\"${name}-marked_dup.bai\") into marked_bam_qc, marked_bam_clairvoyante, marked_bam_sniffles, marked_bam_svim\n    file \"${name}.bam.metrics\" into mark_dup_report\n\n    \"\"\"\n    gatk MarkDuplicates \\\n    -I ${bam} \\\n    --CREATE_INDEX true \\\n    -M ${name}.bam.metrics \\\n    -O ${name}-marked_dup.bam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GangSTR/preprocess_genome", "lifebit-ai/long-reads/mark_duplicates"], "list_wf_names": ["lifebit-ai/GangSTR", "lifebit-ai/long-reads"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess BuildCache_snpEff {\n  tag {snpeffDb}\n\n  publishDir params.snpEff_cache, mode: params.publishDirMode\n\n  input:\n    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)\n\n  output:\n    file(\"*\")\n\n  when: params.snpEff_cache && params.download_cache && !params.offline\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeffDb} -dataDir \\${PWD}\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/BuildCache_snpEff"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["BCFtools", "BWA", "SAMtools", "FreeBayes", "MultiQC", "TIDDIT", "FastQC", "QualiMap", "GATK", "VCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess GetSoftwareVersions {\n    publishDir path:\"${params.outdir}/pipeline_info\", mode: params.publishDirMode\n\n    output:\n        file 'software_versions_mqc.yaml' into yamlSoftwareVersion\n\n    when: !('versions' in skipQC)\n\n    script:\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt  || true\n    bcftools version > v_bcftools.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    configManta.py --version > v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version > v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    echo \"SNPEFF version\"\\$(snpEff -h 2>&1) > v_snpeff.txt\n    fastqc --version > v_fastqc.txt 2>&1 || true\n    freebayes --version > v_freebayes.txt 2>&1 || true\n    gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt  || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/GetSoftwareVersions"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BWA", "GATK"], "nb_own": 3, "list_own": ["nibscbioinformatics", "melnel000", "lifebit-ai"], "nb_wf": 2, "list_wf": ["nf-core-conva", "GenomeChronicler-Sarek-nf", "Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "kaurravneet4123", "cgpu", "alneberg", "ewels", "malinlarsson", "waffle-iron", "marcelm", "szilvajuhos", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 14, "codes": ["\nprocess BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 mem \\\\\n        $options.args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $options.args2 -@ $task.cpus -bhS -o ${prefix}.bam -\n\n    echo \\$(bwa-mem2 version 2>&1) > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.*\") into bwaIndexes\n\n    when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess RunMutect2 {\n  tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFMT2\n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex), file(cosmic), file(cosmicIndex) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.dbsnp,\n      referenceMap.dbsnpIndex,\n      referenceMap.cosmic,\n      referenceMap.cosmicIndex\n    ])\n\n  output:\n    set val(\"mutect2\"), idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n\n  when: 'mutect2' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n\tgatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n\t\tMutect2 \\\n\t\t-R ${genomeFile}\\\n\t\t-I ${bamTumor}  -tumor ${idSampleTumor} \\\n\t\t-I ${bamNormal} -normal ${idSampleNormal} \\\n\t\t-L ${intervalBed} \\\n\t\t-O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/BuildBWAindexes", "melnel000/Sarek_CBIO/RunMutect2"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf", "melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess MapReads {\n    label 'cpus_max'\n    label 'memory_max'\n    echo true\n\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwaIndex\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n                                                                                              \n                                                                         \n    bwa_cpus  = !params.bwa_cpus_fraction ? task.cpus : Math.floor ( params.bwa_cpus_fraction * task.cpus) as Integer\n    sort_cpus = !params.bwa_cpus_fraction ? task.cpus : task.cpus - bwa_cpus\n    \"\"\"\n        ${convertToFastq}\n        bwa mem -k 23 -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${bwa_cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${sort_cpus}  - > ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/MapReads"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess IndexBamMergedForSentieon {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, file(bam) from mergedBamForSentieon\n\n    output:\n        set idPatient, idSample, file(bam), file(\"${idSample}.bam.bai\") into bamForSentieonDedup\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/IndexBamMergedForSentieon"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess IndexBamFile {\n    label 'cpus_4'\n\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        set idPatient, idSample, file(bam) from mergedBamToIndex\n\n    output:\n        set idPatient, idSample, file(bam), file(\"*.bai\") into indexedBam\n\n    when: !params.knownIndels\n\n    script:\n    \"\"\"\n    samtools index ${bam}\n    mv ${bam}.bai ${bam.baseName}.bai\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/IndexBamFile"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["MarkDuplicates (IP)", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess MarkDuplicates {\n    label 'cpus_max'\n    label 'memory_max'\n\n    tag {idPatient + \"-\" + idSample}\n\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\" && 'markduplicates' in skipQC) null\n            else if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicateMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from mergedBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bai\") into duplicateMarkedBams\n        file (\"${idSample}.bam.metrics\") into markDuplicatesReport\n\n    when: params.knownIndels\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --MAX_RECORDS_IN_RAM 50000 \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n        \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/MarkDuplicates"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess GatherBQSRReports {\n\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publishDirMode, overwrite: false\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/GatherBQSRReports"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess MergeBamRecal {\n    label 'cpus_max'\n    label 'memory_max'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamMergeBamRecal\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bamRecal\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bamRecalQC\n        set idPatient, idSample into bamRecalTSV\n        file(\"${idSample}.recal.bam\") into (bamGenomeChronicler, bamGenomeChroniclerToPrint)\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/MergeBamRecal"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess HaplotypeCaller {\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        each file(dbsnp) from ch_dbsnp\n        each file(dbsnpIndex) from ch_dbsnpIndex\n        each file(dict) from ch_dict\n        each file(fasta) from ch_fasta\n        each file(fastaFai) from ch_fastaFai\n\n    output:\n                                 \n        set val(\"HaplotypeCaller${gvcf_tag}\"), idPatient, idSample, file(outputVcf), file(\"${outputVcf}.idx\") into vcfHaplotypeCallerVEP\n        set val(\"HaplotypeCaller${gvcf_tag}\"), idPatient, idSample, file(outputVcf) into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(outputVcf) into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    name = \"${intervalBed.baseName}_${idSample}\"\n    output_suffix = params.noGVCF ? '.vcf' : '.g.vcf'\n    outputVcf = name + output_suffix\n    gvcf_arg = params.noGVCF ? '' : '-ERC GVCF'\n    gvcf_tag = params.noGVCF ? '' : 'GVCF'\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${outputVcf} \\\n        ${gvcf_arg}\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/HaplotypeCaller"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess GenotypeGVCFs {\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: !(params.noGVCF) && ('haplotypecaller' in tools)\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -F ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/GenotypeGVCFs"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess Mutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n    label 'cpus_1'\n\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_ponIndex\n\n    output:\n        set val(\"Mutect2\"), \n            idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/Mutect2"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess MergeMutect2Stats {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publishDirMode\n\n    input:\n        set caller, idPatient, idSampleTumor_vs_idSampleNormal, file(vcfFiles) from mutect2OutForStats                                  \n        set idPatient, idSampleTumor, idSampleNormal, file(statsFiles) from mutect2Stats                                        \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n        file(intervals) from ch_intervals\n\n    output:\n        file(\"${idSampleTumor_vs_idSampleNormal}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/MergeMutect2Stats"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess PileupSummariesForMutect2 {\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"_\" + intervalBed.baseName }\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamPileupSummaries \n        set idPatient, idSampleNormal, idSampleTumor, file(statsFile) from intervalStatsFiles\n        file(germlineResource) from ch_germlineResource\n        file(germlineResourceIndex) from ch_germlineResourceIndex\n\n    output:\n        set idPatient,\n            idSampleTumor,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools && params.pon\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/PileupSummariesForMutect2"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess Mpileup {\n    label 'cpus_2'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup.gz\") into mpileupMerge\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n    \"\"\"\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} \\\n    | bgzip --threads ${task.cpus} -c > ${prefix}${idSample}.pileup.gz\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/Mpileup"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["GenomeChronicler-Sarek-nf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess MultiQC {\n\n    label 'cpus_2'\n\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publishDirMode\n\n    input:\n        file (multiqcConfig) from Channel.value(params.multiqc_config ? file(params.multiqc_config) : \"\")\n        file (versions) from yamlSoftwareVersion\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from markDuplicatesReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        set file(\"*multiqc_report.html\"), file(\"*multiqc_data\") into multiQCOut\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    \"\"\"\n    multiqc -f -v .\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/GenomeChronicler-Sarek-nf/MultiQC"], "list_wf_names": ["lifebit-ai/GenomeChronicler-Sarek-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["HipSTR"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": [" process preprocess_genome {\n\n\t\t\ttag \"${fasta}\"\n\t\t\tcontainer 'lifebitai/preprocessingvctools'\n\n      input:\n      file fasta from fastaToFai\n\n      output:\n      file(\"${fasta}.fai\") into fai\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      \"\"\"\n  }"], "list_proc": ["lifebit-ai/HipSTR/preprocess_genome"], "list_wf_names": ["lifebit-ai/HipSTR"]}, {"nb_reuse": 1, "tools": ["hipSTR"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["HipSTR"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess hipstr {\n\tcontainer 'lifebitai/hipstr'\n\n\tpublishDir \"${params.outdir}\", mode: 'copy'\n\n\tinput:\n\tset file(bam), file(bai) from completeChannel\n\tfile fasta from fastaToHipSTR\n\tfile fai from fai\n\tfile bed from bed\n\n\toutput:\n\tfile('output.*') into results\n\n\tscript:\n\t\"\"\"\n\tHipSTR \\\n\t--bams ${bam} \\\n\t--fasta ${fasta} \\\n\t--regions ${bed} \\\n\t--min-reads ${minreads} \\\n\t--str-vcf output.vcf.gz \\\n\t--log output.log \\\n\t--viz-out output.viz.gz \\\n\t\"\"\"\n}"], "list_proc": ["lifebit-ai/HipSTR/hipstr"], "list_wf_names": ["lifebit-ai/HipSTR"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["ReleaseTheKraken"], "list_contrib": ["imendes93"], "nb_contrib": 1, "codes": ["\nprocess kraken2 {\n\n    tag { sample_id }\n\n    publishDir \"results/kraken/\"\n\n    input:\n    set sample_id, file(fastq_pair) from OUT_fastp\n\n    output:\n    set sample_id, file(\"*_kraken_report.txt\") into OUT_KRAKEN\n\n    script:\n    \"\"\"\n    a=(${fastq_pair})\n\n    if ((\\${#a[@]} > 1));\n    then\n        kraken2 --memory-mapping --threads $task.cpus --report ${sample_id}_kraken_report.txt --db minikraken2_v1_8GB --paired --gzip-compressed ${fastq_pair[0]} ${fastq_pair[1]}\n    else\n        kraken2 --memory-mapping --threads $task.cpus --report ${sample_id}_kraken_report.txt --db minikraken2_v1_8GB --single --gzip-compressed ${fastq_pair}\n    fi\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/ReleaseTheKraken/kraken2"], "list_wf_names": ["lifebit-ai/ReleaseTheKraken"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["Tardis"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": [" process preprocess_bam{\n\n  tag \"${bam}\"\n\tcontainer 'lifebitai/samtools'\n\n  input:\n  set val(name), file(bam) from bam\n\n  output:\n  set val(name), file(\"ready/${bam}\"), file(\"ready/${bam}.bai\") into completeChannel\n\n  script:\n  \"\"\"\n  mkdir ready\n  [[ `samtools view -H ${bam} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready;}|| { picard AddOrReplaceReadGroups \\\n  I=${bam} \\\n  O=ready/${bam} \\\n  RGID=${params.rgid} \\\n  RGLB=${params.rglb} \\\n  RGPL=${params.rgpl} \\\n  RGPU=${params.rgpu} \\\n  RGSM=${params.rgsm};}\n  cd ready ;samtools index ${bam};\n  \"\"\"\n  }"], "list_proc": ["lifebit-ai/Tardis/preprocess_bam"], "list_wf_names": ["lifebit-ai/Tardis"]}, {"nb_reuse": 6, "tools": ["SAMtools", "QualiMap", "Minimap2", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["bam2cram", "long-reads"], "list_contrib": ["dependabot[bot]", "cgpu", "PhilPalmer", "imendes93", "pprieto"], "nb_contrib": 5, "codes": ["\nprocess samtools_default_30 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_0\n    each file(reference) from ch_reference_0\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.0 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", " process preprocess_fai {\n      tag \"${fasta}\"\n      container 'lifebitai/samtools:latest'\n\n      input:\n      file(fasta) from fasta_to_index\n\n      output:\n      file(\"${fasta}.fai\") into (fai_clairvoyante, fai_sniffles)\n\n      script:\n      \"\"\"\n      samtools faidx $fasta\n      \"\"\"\n  }", "\nprocess minimap2 {\n    tag \"$reads\"\n    container 'evolbioinfo/minimap2:v2.14'\n\n    cpus threads\n\n    input:\n    set val(name), file(reads), file(fasta) from minimap2\n\n    output:\n    set val(name), file(\"${name}.sam\") into mapped_reads\n\n    script:\n    \"\"\"\n    minimap2 -ax map-ont -t ${task.cpus} $fasta $reads > ${name}.sam\n    \"\"\"\n}", "\nprocess bwa_sort {\n    tag \"$sam\"\n    container 'lifebitai/samtools:latest'\n\n    input:\n    set val(name), file(sam) from mapped_reads\n\n    output:\n    set val(name), file(\"${name}-sorted.bam\") into sorted_bam\n\n    \"\"\"\n    samtools sort -o ${name}-sorted.bam -O BAM $sam\n    \"\"\"\n}", "\nprocess mark_duplicates {\n    tag \"$bam\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(name), file(bam) from sorted_bam\n\n    output:\n    set val(name), file(\"${name}-marked_dup.bam\"), file(\"${name}-marked_dup.bai\") into marked_bam_qc, marked_bam_clairvoyante, marked_bam_sniffles, marked_bam_svim\n    file \"${name}.bam.metrics\" into mark_dup_report\n\n    \"\"\"\n    gatk MarkDuplicates \\\n    -I ${bam} \\\n    --CREATE_INDEX true \\\n    -M ${name}.bam.metrics \\\n    -O ${name}-marked_dup.bam\n    \"\"\"\n}", "\nprocess bam_qc {\n    tag \"$bam\"\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(name), file(bam), file(bai) from marked_bam_qc\n\n    output:\n    file(\"${name}\") into bam_qc_report\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/bam2cram/samtools_default_30", "lifebit-ai/long-reads/preprocess_fai", "lifebit-ai/long-reads/minimap2", "lifebit-ai/long-reads/bwa_sort", "lifebit-ai/long-reads/mark_duplicates", "lifebit-ai/long-reads/bam_qc"], "list_wf_names": ["lifebit-ai/long-reads", "lifebit-ai/bam2cram"]}, {"nb_reuse": 9, "tools": ["effectR", "SAMtools", "MultiQC", "Sniffles", "Metal", "ECMarker", "QualiMap", "KAnalyze", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 4, "list_wf": ["panelR", "long-reads", "bam2cram", "metagwas"], "list_contrib": ["dependabot[bot]", "cgpu", "mmeier93", "evagradovich", "PhilPalmer", "imendes93", "pprieto"], "nb_contrib": 7, "codes": ["\nprocess sniffles_preprocessing {\n    tag \"$bam\"\n    publishDir \"${params.outdir}/bam\", mode: 'copy'\n    container 'lifebitai/samtools:latest'\n\n    input:\n    set val(name), file(bam), file(bai), file(fasta), file(fai) from sniffles_preprocessing\n\n    output:\n    set val(name), file(\"${name}.bam\"), file(\"${name}.bam.bai\") into bam_md_sniffles \n\n    \"\"\"\n    samtools calmd $bam $fasta | samtools view -S -b - > ${name}.bam\n    samtools index ${name}.bam\n    \"\"\"\n}", "\nprocess sniffles {\n    tag \"$bam\"\n    publishDir \"${params.outdir}/sniffles\", mode: 'copy'\n    container 'lifebitai/sniffles:latest'\n\n    cpus threads\n\n    input:\n    set val(name), file(bam), file(bai) from bam_md_sniffles\n\n    output:\n    file(\"sniffles_${name}.vcf\") into sniffles_vcf\n\n    \"\"\"\n    sniffles --mapped_reads $bam --vcf sniffles_${name}.vcf -s ${params.min_support} --threads ${task.cpus}\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"multiqc_report.html\"\n\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n    container 'ewels/multiqc:v1.7'\n\n    input:\n    file bam_metrics from mark_dup_report\n    file bam_qc_report from bam_qc_report\n\n    when: \n    !params.skip_multiqc\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc . -m qualimap -m picard -m gatk -m bcftools\n    \"\"\"\n}", "\nprocess run_metal {\npublishDir \"${params.outdir}\", mode: \"copy\"\n\ninput:\nfile(study) from all_input_studies_ch.collect()\n\noutput:\nfile(\"METAANALYSIS*\") into results_ch\n\nshell:\n'''\n# 1 - Dynamically obtain files to process\ntouch process_commands.txt\n\nfor csv in $(ls *.csv)\ndo \necho \"PROCESS $csv\" >> process_commands.txt\ndone\n\nprocess_commands=$(cat process_commands.txt)\n\n# 2 - Make METAL script \n\ncat > metal_command.txt <<EOF\nMARKER SNPID\nALLELE Allele1 Allele2\nEFFECT BETA\nPVALUE p.value \nSEPARATOR COMMA\n!{extra_flags}\n$process_commands\n\n\nANALYZE \nQUIT\nEOF\n\n# 3 - Run METAL\n\nmetal metal_command.txt\n'''\n}", "\nprocess GatherVcfs {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/subsampled_multisample_vcf/\", mode: 'copy'\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(pop_name), file (vcf_bundle) from ch_grouped_pop_vcfs\n    each file(fasta) from ch_fasta_gather\n    each file(fai) from ch_fai_gather\n    each file(dict) from ch_dict_gather\n\n    output:\n    file(\"*\") into ch_complete_chr_vcf\n    set val(\"${pop_name}\"), file(\"${pop_name}.vcf.gz\") into (ch_plink_count_freqs, ch_plink_count_freqs_to_inspect)\n\n\n    script:\n    \"\"\"\n    ls *.vcf.gz | while read vcf; do tabix -fp vcf \\$vcf; done\n\n    ## make list of input variant files\n    for vcf in \\$(ls *vcf.gz); do\n    echo \\$vcf >> ${pop_name}.vcf.list\n    done\n\n    gatk GatherVcfs \\\n    --INPUT  ${pop_name}.vcf.list \\\n    --OUTPUT ${pop_name}.vcf.gz\n    \"\"\"\n    }", "\nprocess samtools_default_31 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_1\n    each file(reference) from ch_reference_1\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.1 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", "\nprocess bwa_sort {\n    tag \"$sam\"\n    container 'lifebitai/samtools:latest'\n\n    input:\n    set val(name), file(sam) from mapped_reads\n\n    output:\n    set val(name), file(\"${name}-sorted.bam\") into sorted_bam\n\n    \"\"\"\n    samtools sort -o ${name}-sorted.bam -O BAM $sam\n    \"\"\"\n}", "\nprocess mark_duplicates {\n    tag \"$bam\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(name), file(bam) from sorted_bam\n\n    output:\n    set val(name), file(\"${name}-marked_dup.bam\"), file(\"${name}-marked_dup.bai\") into marked_bam_qc, marked_bam_clairvoyante, marked_bam_sniffles, marked_bam_svim\n    file \"${name}.bam.metrics\" into mark_dup_report\n\n    \"\"\"\n    gatk MarkDuplicates \\\n    -I ${bam} \\\n    --CREATE_INDEX true \\\n    -M ${name}.bam.metrics \\\n    -O ${name}-marked_dup.bam\n    \"\"\"\n}", "\nprocess bam_qc {\n    tag \"$bam\"\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(name), file(bam), file(bai) from marked_bam_qc\n\n    output:\n    file(\"${name}\") into bam_qc_report\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/long-reads/sniffles_preprocessing", "lifebit-ai/long-reads/sniffles", "lifebit-ai/long-reads/multiqc", "lifebit-ai/metagwas/run_metal", "lifebit-ai/panelR/GatherVcfs", "lifebit-ai/bam2cram/samtools_default_31", "lifebit-ai/long-reads/bwa_sort", "lifebit-ai/long-reads/mark_duplicates", "lifebit-ai/long-reads/bam_qc"], "list_wf_names": ["lifebit-ai/bam2cram", "lifebit-ai/panelR", "lifebit-ai/long-reads", "lifebit-ai/metagwas"]}, {"nb_reuse": 10, "tools": ["PLINK", "effectR", "SAMtools", "QualiMap", "MultiQC", "Sniffles", "Metal", "ECMarker", "VCFFilterJS", "KAnalyze", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 5, "list_wf": ["panelR", "bam2cram", "pcgr-nf", "long-reads", "metagwas"], "list_contrib": ["dependabot[bot]", "cgpu", "Vlad-Dembrovskyi", "mmeier93", "evagradovich", "PhilPalmer", "imendes93", "pprieto"], "nb_contrib": 8, "codes": ["\nprocess sniffles_preprocessing {\n    tag \"$bam\"\n    publishDir \"${params.outdir}/bam\", mode: 'copy'\n    container 'lifebitai/samtools:latest'\n\n    input:\n    set val(name), file(bam), file(bai), file(fasta), file(fai) from sniffles_preprocessing\n\n    output:\n    set val(name), file(\"${name}.bam\"), file(\"${name}.bam.bai\") into bam_md_sniffles \n\n    \"\"\"\n    samtools calmd $bam $fasta | samtools view -S -b - > ${name}.bam\n    samtools index ${name}.bam\n    \"\"\"\n}", "\nprocess sniffles {\n    tag \"$bam\"\n    publishDir \"${params.outdir}/sniffles\", mode: 'copy'\n    container 'lifebitai/sniffles:latest'\n\n    cpus threads\n\n    input:\n    set val(name), file(bam), file(bai) from bam_md_sniffles\n\n    output:\n    file(\"sniffles_${name}.vcf\") into sniffles_vcf\n\n    \"\"\"\n    sniffles --mapped_reads $bam --vcf sniffles_${name}.vcf -s ${params.min_support} --threads ${task.cpus}\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"multiqc_report.html\"\n\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n    container 'ewels/multiqc:v1.7'\n\n    input:\n    file bam_metrics from mark_dup_report\n    file bam_qc_report from bam_qc_report\n\n    when: \n    !params.skip_multiqc\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc . -m qualimap -m picard -m gatk -m bcftools\n    \"\"\"\n}", "\nprocess run_metal {\npublishDir \"${params.outdir}\", mode: \"copy\"\n\ninput:\nfile(study) from all_input_studies_ch.collect()\n\noutput:\nfile(\"METAANALYSIS*\") into results_ch\n\nshell:\n'''\n# 1 - Dynamically obtain files to process\ntouch process_commands.txt\n\nfor csv in $(ls *.csv)\ndo \necho \"PROCESS $csv\" >> process_commands.txt\ndone\n\nprocess_commands=$(cat process_commands.txt)\n\n# 2 - Make METAL script \n\ncat > metal_command.txt <<EOF\nMARKER SNPID\nALLELE Allele1 Allele2\nEFFECT BETA\nPVALUE p.value \nSEPARATOR COMMA\n!{extra_flags}\n$process_commands\n\n\nANALYZE \nQUIT\nEOF\n\n# 3 - Run METAL\n\nmetal metal_command.txt\n'''\n}", "\nprocess SubsetMultiVCF {\n\n    tag {\"${sample_list.simpleName}-${vcf.baseName}\"}\n    container 'broadinstitute/gatk:latest'\n    publishDir \"${params.outdir}/${sample_list.simpleName}/individual_chr_vcfs/\", mode: 'copy'\n\n    input:\n    set file(sample_list), file(vcf), file(vcf_index) from ch_multiVCF\n    each file(fasta) from ch_fasta\n    each file(fai) from ch_fai\n    each file(dict) from ch_dict\n\n    output:\n    set val(\"${sample_list.simpleName}\"), file(\"${vcf.baseName}.${sample_list.simpleName}.vcf.gz\") into (ch_pops_vcfs, ch_pops_vcfs_to_inspect)\n\n    script:\n    \"\"\"\n    gatk SelectVariants \\\n    -R ${fasta} \\\n    -V $vcf \\\n    -O ${vcf.baseName}.${sample_list.simpleName}.vcf \\\n    --sample-name ${sample_list}  \\\n    --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true'\n\n    bgzip -c ${vcf.baseName}.${sample_list.simpleName}.vcf > ${vcf.baseName}.${sample_list.simpleName}.vcf.gz\n   \"\"\"\n}", "\nprocess GatherVcfs {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/subsampled_multisample_vcf/\", mode: 'copy'\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(pop_name), file (vcf_bundle) from ch_grouped_pop_vcfs\n    each file(fasta) from ch_fasta_gather\n    each file(fai) from ch_fai_gather\n    each file(dict) from ch_dict_gather\n\n    output:\n    file(\"*\") into ch_complete_chr_vcf\n    set val(\"${pop_name}\"), file(\"${pop_name}.vcf.gz\") into (ch_plink_count_freqs, ch_plink_count_freqs_to_inspect)\n\n\n    script:\n    \"\"\"\n    ls *.vcf.gz | while read vcf; do tabix -fp vcf \\$vcf; done\n\n    ## make list of input variant files\n    for vcf in \\$(ls *vcf.gz); do\n    echo \\$vcf >> ${pop_name}.vcf.list\n    done\n\n    gatk GatherVcfs \\\n    --INPUT  ${pop_name}.vcf.list \\\n    --OUTPUT ${pop_name}.vcf.gz\n    \"\"\"\n    }", "\nprocess PlinkFilterAndFreqCount {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/plink_metrics/\", mode: 'copy'\n    container 'alliecreason/plink:1.90'\n\n    input:\n    set val(pop_name), file(all_chr_vcf) from ch_plink_count_freqs\n\n    output:\n    file(\"*\") into ch_plink_results\n    file(\"${pop_name}.frq.counts\") into ch_plink_frq_counts\n\n\n    script:\n    \"\"\"\n    plink \\\n    --vcf $all_chr_vcf \\\n    --snps-only \\\n    --biallelic-only strict list \\\n    --geno 0.05 \\\n    --maf  0.05 \\\n    --freq counts \\\n    --out $pop_name > ${pop_name}_plink.stdout.log\n\n    rm *.vcf.gz\n    \"\"\"\n    }", " process vcffilter {\n        tag \"$input_file\"\n        label 'process_low'\n\n        publishDir \"${params.outdir}/process-logs/${task.process}/${input_file}/\", pattern: \"command-logs-*\", mode: 'copy'\n\n        input:\n        file input_file from ch_input_2\n        file filter from filterstr\n\n        output:\n        file \"*filtered.vcf\" into ch_vcf_for_pcgr\n        file(\"command-logs-*\") optional true\n\n        script:\n        \"\"\"\n        if [ -s $filter ]; then \n            echo \"No tags present in VCF for filtering\"\n            cp $input_file ${input_file.baseName}_filtered.vcf\n        else\n            vcffilter -s -f \\$(cat $filter) $input_file > ${input_file.baseName}_filtered.vcf\n        fi\n\n        # save .command.* logs\n        ${params.savescript}\n        \"\"\"\n    }", "\nprocess samtools_normal_30 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_2\n    each file(reference) from ch_reference_2\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.0 --output-fmt-option seqs_per_slice=10000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", "\nprocess bam_qc {\n    tag \"$bam\"\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(name), file(bam), file(bai) from marked_bam_qc\n\n    output:\n    file(\"${name}\") into bam_qc_report\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/long-reads/sniffles_preprocessing", "lifebit-ai/long-reads/sniffles", "lifebit-ai/long-reads/multiqc", "lifebit-ai/metagwas/run_metal", "lifebit-ai/panelR/SubsetMultiVCF", "lifebit-ai/panelR/GatherVcfs", "lifebit-ai/panelR/PlinkFilterAndFreqCount", "lifebit-ai/pcgr-nf/vcffilter", "lifebit-ai/bam2cram/samtools_normal_30", "lifebit-ai/long-reads/bam_qc"], "list_wf_names": ["lifebit-ai/metagwas", "lifebit-ai/pcgr-nf", "lifebit-ai/bam2cram", "lifebit-ai/long-reads", "lifebit-ai/panelR"]}, {"nb_reuse": 9, "tools": ["BCFtools", "PLINK", "effectR", "SAMtools", "MultiQC", "Sniffles", "Metal", "ECMarker", "VCFFilterJS", "KAnalyze", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 6, "list_wf": ["panelR", "phewas", "bam2cram", "pcgr-nf", "long-reads", "metagwas"], "list_contrib": ["dependabot[bot]", "cgpu", "Vlad-Dembrovskyi", "mmeier93", "evagradovich", "PhilPalmer", "imendes93", "mcamarad", "pprieto"], "nb_contrib": 9, "codes": ["\nprocess sniffles {\n    tag \"$bam\"\n    publishDir \"${params.outdir}/sniffles\", mode: 'copy'\n    container 'lifebitai/sniffles:latest'\n\n    cpus threads\n\n    input:\n    set val(name), file(bam), file(bai) from bam_md_sniffles\n\n    output:\n    file(\"sniffles_${name}.vcf\") into sniffles_vcf\n\n    \"\"\"\n    sniffles --mapped_reads $bam --vcf sniffles_${name}.vcf -s ${params.min_support} --threads ${task.cpus}\n    \"\"\"\n}", "\nprocess multiqc {\n    tag \"multiqc_report.html\"\n\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n    container 'ewels/multiqc:v1.7'\n\n    input:\n    file bam_metrics from mark_dup_report\n    file bam_qc_report from bam_qc_report\n\n    when: \n    !params.skip_multiqc\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc . -m qualimap -m picard -m gatk -m bcftools\n    \"\"\"\n}", "\nprocess run_metal {\npublishDir \"${params.outdir}\", mode: \"copy\"\n\ninput:\nfile(study) from all_input_studies_ch.collect()\n\noutput:\nfile(\"METAANALYSIS*\") into results_ch\n\nshell:\n'''\n# 1 - Dynamically obtain files to process\ntouch process_commands.txt\n\nfor csv in $(ls *.csv)\ndo \necho \"PROCESS $csv\" >> process_commands.txt\ndone\n\nprocess_commands=$(cat process_commands.txt)\n\n# 2 - Make METAL script \n\ncat > metal_command.txt <<EOF\nMARKER SNPID\nALLELE Allele1 Allele2\nEFFECT BETA\nPVALUE p.value \nSEPARATOR COMMA\n!{extra_flags}\n$process_commands\n\n\nANALYZE \nQUIT\nEOF\n\n# 3 - Run METAL\n\nmetal metal_command.txt\n'''\n}", "\nprocess SubsetMultiVCF {\n\n    tag {\"${sample_list.simpleName}-${vcf.baseName}\"}\n    container 'broadinstitute/gatk:latest'\n    publishDir \"${params.outdir}/${sample_list.simpleName}/individual_chr_vcfs/\", mode: 'copy'\n\n    input:\n    set file(sample_list), file(vcf), file(vcf_index) from ch_multiVCF\n    each file(fasta) from ch_fasta\n    each file(fai) from ch_fai\n    each file(dict) from ch_dict\n\n    output:\n    set val(\"${sample_list.simpleName}\"), file(\"${vcf.baseName}.${sample_list.simpleName}.vcf.gz\") into (ch_pops_vcfs, ch_pops_vcfs_to_inspect)\n\n    script:\n    \"\"\"\n    gatk SelectVariants \\\n    -R ${fasta} \\\n    -V $vcf \\\n    -O ${vcf.baseName}.${sample_list.simpleName}.vcf \\\n    --sample-name ${sample_list}  \\\n    --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true'\n\n    bgzip -c ${vcf.baseName}.${sample_list.simpleName}.vcf > ${vcf.baseName}.${sample_list.simpleName}.vcf.gz\n   \"\"\"\n}", "\nprocess GatherVcfs {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/subsampled_multisample_vcf/\", mode: 'copy'\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(pop_name), file (vcf_bundle) from ch_grouped_pop_vcfs\n    each file(fasta) from ch_fasta_gather\n    each file(fai) from ch_fai_gather\n    each file(dict) from ch_dict_gather\n\n    output:\n    file(\"*\") into ch_complete_chr_vcf\n    set val(\"${pop_name}\"), file(\"${pop_name}.vcf.gz\") into (ch_plink_count_freqs, ch_plink_count_freqs_to_inspect)\n\n\n    script:\n    \"\"\"\n    ls *.vcf.gz | while read vcf; do tabix -fp vcf \\$vcf; done\n\n    ## make list of input variant files\n    for vcf in \\$(ls *vcf.gz); do\n    echo \\$vcf >> ${pop_name}.vcf.list\n    done\n\n    gatk GatherVcfs \\\n    --INPUT  ${pop_name}.vcf.list \\\n    --OUTPUT ${pop_name}.vcf.gz\n    \"\"\"\n    }", "\nprocess PlinkFilterAndFreqCount {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/plink_metrics/\", mode: 'copy'\n    container 'alliecreason/plink:1.90'\n\n    input:\n    set val(pop_name), file(all_chr_vcf) from ch_plink_count_freqs\n\n    output:\n    file(\"*\") into ch_plink_results\n    file(\"${pop_name}.frq.counts\") into ch_plink_frq_counts\n\n\n    script:\n    \"\"\"\n    plink \\\n    --vcf $all_chr_vcf \\\n    --snps-only \\\n    --biallelic-only strict list \\\n    --geno 0.05 \\\n    --maf  0.05 \\\n    --freq counts \\\n    --out $pop_name > ${pop_name}_plink.stdout.log\n\n    rm *.vcf.gz\n    \"\"\"\n    }", " process vcffilter {\n        tag \"$input_file\"\n        label 'process_low'\n\n        publishDir \"${params.outdir}/process-logs/${task.process}/${input_file}/\", pattern: \"command-logs-*\", mode: 'copy'\n\n        input:\n        file input_file from ch_input_2\n        file filter from filterstr\n\n        output:\n        file \"*filtered.vcf\" into ch_vcf_for_pcgr\n        file(\"command-logs-*\") optional true\n\n        script:\n        \"\"\"\n        if [ -s $filter ]; then \n            echo \"No tags present in VCF for filtering\"\n            cp $input_file ${input_file.baseName}_filtered.vcf\n        else\n            vcffilter -s -f \\$(cat $filter) $input_file > ${input_file.baseName}_filtered.vcf\n        fi\n\n        # save .command.* logs\n        ${params.savescript}\n        \"\"\"\n    }", " process merge_ind_vcfs {\n\n        label 'file_preprocessing'\n\n        input:\n        file vcf_file from ch_vcf_file\n        file vcfs from ch_vcf_ind.collect()\n\n        output:\n        file 'vcf_files.txt' into ch_updated_vcf_list\n\n        script:\n        \"\"\"\n        # iterate through urls in csv replacing s3 path with the local one\n        urls=\"\\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}')\"\n        for url in \\$(echo \\$urls); do\n            vcf=\"\\${url##*/}\"\n            sed -i -e \"s~\\$url~\\$vcf~g\" $vcf_file\n        done\n        # bgzip uncompressed vcfs\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            if [ \\${vcf: -4} == \".vcf\" ]; then\n                    sed -i \"s/\\$vcf/\\${vcf}.gz/g\" $vcf_file \n            fi\n        done\n        # remove any prexisting columns for sex \n        if grep -Fq \"SEX\" $vcf_file; then\n            awk -F, -v OFS=, 'NR==1{for (i=1;i<=NF;i++)if (\\$i==\"SEX\"){n=i-1;m=NF-(i==NF)}} {for(i=1;i<=NF;i+=1+(i==n))printf \"%s%s\",\\$i,i==m?ORS:OFS}' $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        fi\n        # determine sex of each individual from VCF file & add to csv file\n        echo 'SEX' > sex.txt\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            bcftools index -f \\$vcf\n            SEX=\"\\$(bcftools plugin vcf2sex \\$vcf)\"\n            if [[ \\$SEX == *M ]]; then\n                    echo \"1\" >> sex.txt\n            elif [ \\$SEX == *F ]]; then\n                    echo \"2\" >> sex.txt\n            fi\n        done\n        paste -d, sex.txt $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        tail -n+2 $vcf_file | awk -F',' '{print \\$3}' > vcf_files.txt\n        \"\"\"\n    }", "\nprocess samtools_normal_31 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_3\n    each file(reference) from ch_reference_3\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.1 --output-fmt-option seqs_per_slice=10000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }"], "list_proc": ["lifebit-ai/long-reads/sniffles", "lifebit-ai/long-reads/multiqc", "lifebit-ai/metagwas/run_metal", "lifebit-ai/panelR/SubsetMultiVCF", "lifebit-ai/panelR/GatherVcfs", "lifebit-ai/panelR/PlinkFilterAndFreqCount", "lifebit-ai/pcgr-nf/vcffilter", "lifebit-ai/phewas/merge_ind_vcfs", "lifebit-ai/bam2cram/samtools_normal_31"], "list_wf_names": ["lifebit-ai/metagwas", "lifebit-ai/pcgr-nf", "lifebit-ai/phewas", "lifebit-ai/long-reads", "lifebit-ai/bam2cram", "lifebit-ai/panelR"]}, {"nb_reuse": 8, "tools": ["BCFtools", "PLINK", "effectR", "SAMtools", "Metal", "ECMarker", "VCFFilterJS", "KAnalyze", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 5, "list_wf": ["panelR", "phewas", "bam2cram", "pcgr-nf", "metagwas"], "list_contrib": ["dependabot[bot]", "cgpu", "Vlad-Dembrovskyi", "evagradovich", "mmeier93", "PhilPalmer", "imendes93", "mcamarad"], "nb_contrib": 8, "codes": ["\nprocess run_metal {\npublishDir \"${params.outdir}\", mode: \"copy\"\n\ninput:\nfile(study) from all_input_studies_ch.collect()\n\noutput:\nfile(\"METAANALYSIS*\") into results_ch\n\nshell:\n'''\n# 1 - Dynamically obtain files to process\ntouch process_commands.txt\n\nfor csv in $(ls *.csv)\ndo \necho \"PROCESS $csv\" >> process_commands.txt\ndone\n\nprocess_commands=$(cat process_commands.txt)\n\n# 2 - Make METAL script \n\ncat > metal_command.txt <<EOF\nMARKER SNPID\nALLELE Allele1 Allele2\nEFFECT BETA\nPVALUE p.value \nSEPARATOR COMMA\n!{extra_flags}\n$process_commands\n\n\nANALYZE \nQUIT\nEOF\n\n# 3 - Run METAL\n\nmetal metal_command.txt\n'''\n}", "\nprocess SubsetMultiVCF {\n\n    tag {\"${sample_list.simpleName}-${vcf.baseName}\"}\n    container 'broadinstitute/gatk:latest'\n    publishDir \"${params.outdir}/${sample_list.simpleName}/individual_chr_vcfs/\", mode: 'copy'\n\n    input:\n    set file(sample_list), file(vcf), file(vcf_index) from ch_multiVCF\n    each file(fasta) from ch_fasta\n    each file(fai) from ch_fai\n    each file(dict) from ch_dict\n\n    output:\n    set val(\"${sample_list.simpleName}\"), file(\"${vcf.baseName}.${sample_list.simpleName}.vcf.gz\") into (ch_pops_vcfs, ch_pops_vcfs_to_inspect)\n\n    script:\n    \"\"\"\n    gatk SelectVariants \\\n    -R ${fasta} \\\n    -V $vcf \\\n    -O ${vcf.baseName}.${sample_list.simpleName}.vcf \\\n    --sample-name ${sample_list}  \\\n    --java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true'\n\n    bgzip -c ${vcf.baseName}.${sample_list.simpleName}.vcf > ${vcf.baseName}.${sample_list.simpleName}.vcf.gz\n   \"\"\"\n}", "\nprocess GatherVcfs {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/subsampled_multisample_vcf/\", mode: 'copy'\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(pop_name), file (vcf_bundle) from ch_grouped_pop_vcfs\n    each file(fasta) from ch_fasta_gather\n    each file(fai) from ch_fai_gather\n    each file(dict) from ch_dict_gather\n\n    output:\n    file(\"*\") into ch_complete_chr_vcf\n    set val(\"${pop_name}\"), file(\"${pop_name}.vcf.gz\") into (ch_plink_count_freqs, ch_plink_count_freqs_to_inspect)\n\n\n    script:\n    \"\"\"\n    ls *.vcf.gz | while read vcf; do tabix -fp vcf \\$vcf; done\n\n    ## make list of input variant files\n    for vcf in \\$(ls *vcf.gz); do\n    echo \\$vcf >> ${pop_name}.vcf.list\n    done\n\n    gatk GatherVcfs \\\n    --INPUT  ${pop_name}.vcf.list \\\n    --OUTPUT ${pop_name}.vcf.gz\n    \"\"\"\n    }", "\nprocess PlinkFilterAndFreqCount {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/plink_metrics/\", mode: 'copy'\n    container 'alliecreason/plink:1.90'\n\n    input:\n    set val(pop_name), file(all_chr_vcf) from ch_plink_count_freqs\n\n    output:\n    file(\"*\") into ch_plink_results\n    file(\"${pop_name}.frq.counts\") into ch_plink_frq_counts\n\n\n    script:\n    \"\"\"\n    plink \\\n    --vcf $all_chr_vcf \\\n    --snps-only \\\n    --biallelic-only strict list \\\n    --geno 0.05 \\\n    --maf  0.05 \\\n    --freq counts \\\n    --out $pop_name > ${pop_name}_plink.stdout.log\n\n    rm *.vcf.gz\n    \"\"\"\n    }", " process vcffilter {\n        tag \"$input_file\"\n        label 'process_low'\n\n        publishDir \"${params.outdir}/process-logs/${task.process}/${input_file}/\", pattern: \"command-logs-*\", mode: 'copy'\n\n        input:\n        file input_file from ch_input_2\n        file filter from filterstr\n\n        output:\n        file \"*filtered.vcf\" into ch_vcf_for_pcgr\n        file(\"command-logs-*\") optional true\n\n        script:\n        \"\"\"\n        if [ -s $filter ]; then \n            echo \"No tags present in VCF for filtering\"\n            cp $input_file ${input_file.baseName}_filtered.vcf\n        else\n            vcffilter -s -f \\$(cat $filter) $input_file > ${input_file.baseName}_filtered.vcf\n        fi\n\n        # save .command.* logs\n        ${params.savescript}\n        \"\"\"\n    }", " process merge_ind_vcfs {\n\n        label 'file_preprocessing'\n\n        input:\n        file vcf_file from ch_vcf_file\n        file vcfs from ch_vcf_ind.collect()\n\n        output:\n        file 'vcf_files.txt' into ch_updated_vcf_list\n\n        script:\n        \"\"\"\n        # iterate through urls in csv replacing s3 path with the local one\n        urls=\"\\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}')\"\n        for url in \\$(echo \\$urls); do\n            vcf=\"\\${url##*/}\"\n            sed -i -e \"s~\\$url~\\$vcf~g\" $vcf_file\n        done\n        # bgzip uncompressed vcfs\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            if [ \\${vcf: -4} == \".vcf\" ]; then\n                    sed -i \"s/\\$vcf/\\${vcf}.gz/g\" $vcf_file \n            fi\n        done\n        # remove any prexisting columns for sex \n        if grep -Fq \"SEX\" $vcf_file; then\n            awk -F, -v OFS=, 'NR==1{for (i=1;i<=NF;i++)if (\\$i==\"SEX\"){n=i-1;m=NF-(i==NF)}} {for(i=1;i<=NF;i+=1+(i==n))printf \"%s%s\",\\$i,i==m?ORS:OFS}' $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        fi\n        # determine sex of each individual from VCF file & add to csv file\n        echo 'SEX' > sex.txt\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            bcftools index -f \\$vcf\n            SEX=\"\\$(bcftools plugin vcf2sex \\$vcf)\"\n            if [[ \\$SEX == *M ]]; then\n                    echo \"1\" >> sex.txt\n            elif [ \\$SEX == *F ]]; then\n                    echo \"2\" >> sex.txt\n            fi\n        done\n        paste -d, sex.txt $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        tail -n+2 $vcf_file | awk -F',' '{print \\$3}' > vcf_files.txt\n        \"\"\"\n    }", " process combine_vcfs {\n        publishDir \"${params.outdir}/vcf\", mode: 'copy'\n\n        input:\n        file(vcfs) from ch_vcfs.collect()\n        file vcf_list from ch_updated_vcf_list\n        file sample_file from ch_samples_to_combine_vcfs\n\n        output:\n        file 'filtered_by_sample.vcf.gz' into ch_vcf_plink\n\n        script:\n        if ( params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools concat \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n        else if ( !params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            bcftools merge --force-samples \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n    }", "\nprocess samtools_fast_30 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_4\n    each file(reference) from ch_reference_4\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.0,level=1 --output-fmt-option seqs_per_slice=1000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }"], "list_proc": ["lifebit-ai/metagwas/run_metal", "lifebit-ai/panelR/SubsetMultiVCF", "lifebit-ai/panelR/GatherVcfs", "lifebit-ai/panelR/PlinkFilterAndFreqCount", "lifebit-ai/pcgr-nf/vcffilter", "lifebit-ai/phewas/merge_ind_vcfs", "lifebit-ai/phewas/combine_vcfs", "lifebit-ai/bam2cram/samtools_fast_30"], "list_wf_names": ["lifebit-ai/metagwas", "lifebit-ai/pcgr-nf", "lifebit-ai/phewas", "lifebit-ai/bam2cram", "lifebit-ai/panelR"]}, {"nb_reuse": 7, "tools": ["BCFtools", "PLINK", "SAMtools", "VCFFilterJS", "GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 4, "list_wf": ["panelR", "pcgr-nf", "bam2cram", "phewas"], "list_contrib": ["dependabot[bot]", "cgpu", "Vlad-Dembrovskyi", "evagradovich", "mmeier93", "PhilPalmer", "imendes93", "mcamarad"], "nb_contrib": 8, "codes": ["\nprocess GatherVcfs {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/subsampled_multisample_vcf/\", mode: 'copy'\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(pop_name), file (vcf_bundle) from ch_grouped_pop_vcfs\n    each file(fasta) from ch_fasta_gather\n    each file(fai) from ch_fai_gather\n    each file(dict) from ch_dict_gather\n\n    output:\n    file(\"*\") into ch_complete_chr_vcf\n    set val(\"${pop_name}\"), file(\"${pop_name}.vcf.gz\") into (ch_plink_count_freqs, ch_plink_count_freqs_to_inspect)\n\n\n    script:\n    \"\"\"\n    ls *.vcf.gz | while read vcf; do tabix -fp vcf \\$vcf; done\n\n    ## make list of input variant files\n    for vcf in \\$(ls *vcf.gz); do\n    echo \\$vcf >> ${pop_name}.vcf.list\n    done\n\n    gatk GatherVcfs \\\n    --INPUT  ${pop_name}.vcf.list \\\n    --OUTPUT ${pop_name}.vcf.gz\n    \"\"\"\n    }", "\nprocess PlinkFilterAndFreqCount {\n\n    tag \"${pop_name}\"\n    publishDir \"${params.outdir}/${pop_name}/plink_metrics/\", mode: 'copy'\n    container 'alliecreason/plink:1.90'\n\n    input:\n    set val(pop_name), file(all_chr_vcf) from ch_plink_count_freqs\n\n    output:\n    file(\"*\") into ch_plink_results\n    file(\"${pop_name}.frq.counts\") into ch_plink_frq_counts\n\n\n    script:\n    \"\"\"\n    plink \\\n    --vcf $all_chr_vcf \\\n    --snps-only \\\n    --biallelic-only strict list \\\n    --geno 0.05 \\\n    --maf  0.05 \\\n    --freq counts \\\n    --out $pop_name > ${pop_name}_plink.stdout.log\n\n    rm *.vcf.gz\n    \"\"\"\n    }", " process vcffilter {\n        tag \"$input_file\"\n        label 'process_low'\n\n        publishDir \"${params.outdir}/process-logs/${task.process}/${input_file}/\", pattern: \"command-logs-*\", mode: 'copy'\n\n        input:\n        file input_file from ch_input_2\n        file filter from filterstr\n\n        output:\n        file \"*filtered.vcf\" into ch_vcf_for_pcgr\n        file(\"command-logs-*\") optional true\n\n        script:\n        \"\"\"\n        if [ -s $filter ]; then \n            echo \"No tags present in VCF for filtering\"\n            cp $input_file ${input_file.baseName}_filtered.vcf\n        else\n            vcffilter -s -f \\$(cat $filter) $input_file > ${input_file.baseName}_filtered.vcf\n        fi\n\n        # save .command.* logs\n        ${params.savescript}\n        \"\"\"\n    }", " process merge_ind_vcfs {\n\n        label 'file_preprocessing'\n\n        input:\n        file vcf_file from ch_vcf_file\n        file vcfs from ch_vcf_ind.collect()\n\n        output:\n        file 'vcf_files.txt' into ch_updated_vcf_list\n\n        script:\n        \"\"\"\n        # iterate through urls in csv replacing s3 path with the local one\n        urls=\"\\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}')\"\n        for url in \\$(echo \\$urls); do\n            vcf=\"\\${url##*/}\"\n            sed -i -e \"s~\\$url~\\$vcf~g\" $vcf_file\n        done\n        # bgzip uncompressed vcfs\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            if [ \\${vcf: -4} == \".vcf\" ]; then\n                    sed -i \"s/\\$vcf/\\${vcf}.gz/g\" $vcf_file \n            fi\n        done\n        # remove any prexisting columns for sex \n        if grep -Fq \"SEX\" $vcf_file; then\n            awk -F, -v OFS=, 'NR==1{for (i=1;i<=NF;i++)if (\\$i==\"SEX\"){n=i-1;m=NF-(i==NF)}} {for(i=1;i<=NF;i+=1+(i==n))printf \"%s%s\",\\$i,i==m?ORS:OFS}' $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        fi\n        # determine sex of each individual from VCF file & add to csv file\n        echo 'SEX' > sex.txt\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            bcftools index -f \\$vcf\n            SEX=\"\\$(bcftools plugin vcf2sex \\$vcf)\"\n            if [[ \\$SEX == *M ]]; then\n                    echo \"1\" >> sex.txt\n            elif [ \\$SEX == *F ]]; then\n                    echo \"2\" >> sex.txt\n            fi\n        done\n        paste -d, sex.txt $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        tail -n+2 $vcf_file | awk -F',' '{print \\$3}' > vcf_files.txt\n        \"\"\"\n    }", " process combine_vcfs {\n        publishDir \"${params.outdir}/vcf\", mode: 'copy'\n\n        input:\n        file(vcfs) from ch_vcfs.collect()\n        file vcf_list from ch_updated_vcf_list\n        file sample_file from ch_samples_to_combine_vcfs\n\n        output:\n        file 'filtered_by_sample.vcf.gz' into ch_vcf_plink\n\n        script:\n        if ( params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools concat \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n        else if ( !params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            bcftools merge --force-samples \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n    }", " process vcf_2_plink {\n        tag \"plink\"\n        \n\n        input:\n        file vcf from ch_vcf_plink\n\n        output:\n        set file('*.bed'), file('*.bim'), file('*.fam') into ch_plink, ch_plink2\n\n        script:\n        \"\"\"\n        # remove contigs eg GL000229.1 to prevent errors\n        gunzip $vcf -c > temp.vcf\n        sed -i '/^GL/ d' temp.vcf\n        plink --keep-allele-order \\\n        --vcf temp.vcf \\\n        --make-bed \\\n        --double-id \\\n        --vcf-half-call m\n        \"\"\"\n    }", "\nprocess samtools_fast_31 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_5\n    each file(reference) from ch_reference_5\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.1,level=1 --output-fmt-option seqs_per_slice=1000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }"], "list_proc": ["lifebit-ai/panelR/GatherVcfs", "lifebit-ai/panelR/PlinkFilterAndFreqCount", "lifebit-ai/pcgr-nf/vcffilter", "lifebit-ai/phewas/merge_ind_vcfs", "lifebit-ai/phewas/combine_vcfs", "lifebit-ai/phewas/vcf_2_plink", "lifebit-ai/bam2cram/samtools_fast_31"], "list_wf_names": ["lifebit-ai/pcgr-nf", "lifebit-ai/panelR", "lifebit-ai/bam2cram", "lifebit-ai/phewas"]}, {"nb_reuse": 6, "tools": ["SAMtools", "VCFFilterJS", "PLINK", "BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 3, "list_wf": ["phewas", "pcgr-nf", "bam2cram"], "list_contrib": ["dependabot[bot]", "cgpu", "Vlad-Dembrovskyi", "evagradovich", "mmeier93", "PhilPalmer", "imendes93", "mcamarad"], "nb_contrib": 8, "codes": [" process vcffilter {\n        tag \"$input_file\"\n        label 'process_low'\n\n        publishDir \"${params.outdir}/process-logs/${task.process}/${input_file}/\", pattern: \"command-logs-*\", mode: 'copy'\n\n        input:\n        file input_file from ch_input_2\n        file filter from filterstr\n\n        output:\n        file \"*filtered.vcf\" into ch_vcf_for_pcgr\n        file(\"command-logs-*\") optional true\n\n        script:\n        \"\"\"\n        if [ -s $filter ]; then \n            echo \"No tags present in VCF for filtering\"\n            cp $input_file ${input_file.baseName}_filtered.vcf\n        else\n            vcffilter -s -f \\$(cat $filter) $input_file > ${input_file.baseName}_filtered.vcf\n        fi\n\n        # save .command.* logs\n        ${params.savescript}\n        \"\"\"\n    }", " process merge_ind_vcfs {\n\n        label 'file_preprocessing'\n\n        input:\n        file vcf_file from ch_vcf_file\n        file vcfs from ch_vcf_ind.collect()\n\n        output:\n        file 'vcf_files.txt' into ch_updated_vcf_list\n\n        script:\n        \"\"\"\n        # iterate through urls in csv replacing s3 path with the local one\n        urls=\"\\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}')\"\n        for url in \\$(echo \\$urls); do\n            vcf=\"\\${url##*/}\"\n            sed -i -e \"s~\\$url~\\$vcf~g\" $vcf_file\n        done\n        # bgzip uncompressed vcfs\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            if [ \\${vcf: -4} == \".vcf\" ]; then\n                    sed -i \"s/\\$vcf/\\${vcf}.gz/g\" $vcf_file \n            fi\n        done\n        # remove any prexisting columns for sex \n        if grep -Fq \"SEX\" $vcf_file; then\n            awk -F, -v OFS=, 'NR==1{for (i=1;i<=NF;i++)if (\\$i==\"SEX\"){n=i-1;m=NF-(i==NF)}} {for(i=1;i<=NF;i+=1+(i==n))printf \"%s%s\",\\$i,i==m?ORS:OFS}' $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        fi\n        # determine sex of each individual from VCF file & add to csv file\n        echo 'SEX' > sex.txt\n        for vcf in \\$(tail -n+2 $vcf_file | awk -F',' '{print \\$2}'); do\n            bcftools index -f \\$vcf\n            SEX=\"\\$(bcftools plugin vcf2sex \\$vcf)\"\n            if [[ \\$SEX == *M ]]; then\n                    echo \"1\" >> sex.txt\n            elif [ \\$SEX == *F ]]; then\n                    echo \"2\" >> sex.txt\n            fi\n        done\n        paste -d, sex.txt $vcf_file > tmp.csv && mv tmp.csv $vcf_file\n        tail -n+2 $vcf_file | awk -F',' '{print \\$3}' > vcf_files.txt\n        \"\"\"\n    }", " process combine_vcfs {\n        publishDir \"${params.outdir}/vcf\", mode: 'copy'\n\n        input:\n        file(vcfs) from ch_vcfs.collect()\n        file vcf_list from ch_updated_vcf_list\n        file sample_file from ch_samples_to_combine_vcfs\n\n        output:\n        file 'filtered_by_sample.vcf.gz' into ch_vcf_plink\n\n        script:\n        if ( params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools concat \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n        else if ( !params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            bcftools merge --force-samples \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n    }", " process vcf_2_plink {\n        tag \"plink\"\n        \n\n        input:\n        file vcf from ch_vcf_plink\n\n        output:\n        set file('*.bed'), file('*.bim'), file('*.fam') into ch_plink, ch_plink2\n\n        script:\n        \"\"\"\n        # remove contigs eg GL000229.1 to prevent errors\n        gunzip $vcf -c > temp.vcf\n        sed -i '/^GL/ d' temp.vcf\n        plink --keep-allele-order \\\n        --vcf temp.vcf \\\n        --make-bed \\\n        --double-id \\\n        --vcf-half-call m\n        \"\"\"\n    }", " process get_snps {\n        tag \"plink\"\n\n        input:\n        set file(bed), file(bim), file(fam) from ch_plink\n        file pheno_file from ch_pheno_for_assoc_test\n\n        output:\n        file(\"snps.txt\") into ch_snps\n\n        script:\n        \"\"\"\n        plink --keep-allele-order \\\n        --bed $bed \\\n        --bim $bim \\\n        --fam $fam \\\n        --allow-no-sex \\\n        --pheno ${pheno_file} \\\n        --pheno-name PHE \\\n        --threads ${task.cpus} \\\n        --assoc \\\n        --out out \n        awk -F' ' '{if(\\$9<${params.snp_threshold}) print \\$2}' out.assoc > snps.txt\n        \"\"\"\n    }", "\nprocess samtools_small_30 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_6\n    each file(reference) from ch_reference_6\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.0,level=6,use_bzip2=1 --output-fmt-option seqs_per_slice=25000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }"], "list_proc": ["lifebit-ai/pcgr-nf/vcffilter", "lifebit-ai/phewas/merge_ind_vcfs", "lifebit-ai/phewas/combine_vcfs", "lifebit-ai/phewas/vcf_2_plink", "lifebit-ai/phewas/get_snps", "lifebit-ai/bam2cram/samtools_small_30"], "list_wf_names": ["lifebit-ai/pcgr-nf", "lifebit-ai/bam2cram", "lifebit-ai/phewas"]}, {"nb_reuse": 5, "tools": ["SAMtools", "BCFtools", "PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["phewas", "bam2cram"], "list_contrib": ["dependabot[bot]", "cgpu", "mmeier93", "evagradovich", "PhilPalmer", "imendes93", "mcamarad"], "nb_contrib": 7, "codes": ["\nprocess samtools_small_31 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_7\n    each file(reference) from ch_reference_7\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.1,level=6,use_bzip2=1,use_fqz=1 --output-fmt-option seqs_per_slice=25000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", " process combine_vcfs {\n        publishDir \"${params.outdir}/vcf\", mode: 'copy'\n\n        input:\n        file(vcfs) from ch_vcfs.collect()\n        file vcf_list from ch_updated_vcf_list\n        file sample_file from ch_samples_to_combine_vcfs\n\n        output:\n        file 'filtered_by_sample.vcf.gz' into ch_vcf_plink\n\n        script:\n        if ( params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools concat \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n        else if ( !params.concat_vcfs )\n            \"\"\"\n            for vcf in ${vcfs}; do\n                if [ \\${vcf} == *vcf ]; then\n                    bgzip -c \\$vcf > \\${vcf}.gz\n                fi\n            done\n            for i in *.vcf.gz; do bcftools index \\${i}; done\n            vcfs_to_combine=\\$(find . -name '*.vcf.gz'| paste -sd \" \")\n            bcftools merge --force-samples \\${vcfs_to_combine} -Oz -o merged.vcf.gz\n            sed '1d' $sample_file | awk -F' ' '{print \\$1}' > sample_file.txt\n            bcftools view -S sample_file.txt merged.vcf.gz --force-samples -Oz -o filtered_by_sample.vcf.gz\n            \"\"\"\n    }", " process vcf_2_plink {\n        tag \"plink\"\n        \n\n        input:\n        file vcf from ch_vcf_plink\n\n        output:\n        set file('*.bed'), file('*.bim'), file('*.fam') into ch_plink, ch_plink2\n\n        script:\n        \"\"\"\n        # remove contigs eg GL000229.1 to prevent errors\n        gunzip $vcf -c > temp.vcf\n        sed -i '/^GL/ d' temp.vcf\n        plink --keep-allele-order \\\n        --vcf temp.vcf \\\n        --make-bed \\\n        --double-id \\\n        --vcf-half-call m\n        \"\"\"\n    }", " process get_snps {\n        tag \"plink\"\n\n        input:\n        set file(bed), file(bim), file(fam) from ch_plink\n        file pheno_file from ch_pheno_for_assoc_test\n\n        output:\n        file(\"snps.txt\") into ch_snps\n\n        script:\n        \"\"\"\n        plink --keep-allele-order \\\n        --bed $bed \\\n        --bim $bim \\\n        --fam $fam \\\n        --allow-no-sex \\\n        --pheno ${pheno_file} \\\n        --pheno-name PHE \\\n        --threads ${task.cpus} \\\n        --assoc \\\n        --out out \n        awk -F' ' '{if(\\$9<${params.snp_threshold}) print \\$2}' out.assoc > snps.txt\n        \"\"\"\n    }", "\nprocess recode {\ntag \"plink\"\n\ninput:\nset file(bed), file(bim), file(fam) from ch_plink2\nfile snps from ch_snps\n\noutput:\nfile('*.raw') into phewas\n\nscript:\n\"\"\"\nplink --keep-allele-order \\\n--recodeA \\\n--bfile ${bed.baseName} \\\n--out r_genotypes \\\n--extract $snps\n\"\"\"\n}"], "list_proc": ["lifebit-ai/bam2cram/samtools_small_31", "lifebit-ai/phewas/combine_vcfs", "lifebit-ai/phewas/vcf_2_plink", "lifebit-ai/phewas/get_snps", "lifebit-ai/phewas/recode"], "list_wf_names": ["lifebit-ai/bam2cram", "lifebit-ai/phewas"]}, {"nb_reuse": 4, "tools": ["SAMtools", "PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 3, "list_wf": ["phewas", "bam2cram", "prs"], "list_contrib": ["dependabot[bot]", "cgpu", "mmeier93", "evagradovich", "PhilPalmer", "imendes93", "sk-sahu", "mcamarad"], "nb_contrib": 8, "codes": ["\nprocess samtools_archive_30 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_8\n    each file(reference) from ch_reference_8\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.0,level=7,use_bzip2=1 --output-fmt-option seqs_per_slice=100000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", " process get_snps {\n        tag \"plink\"\n\n        input:\n        set file(bed), file(bim), file(fam) from ch_plink\n        file pheno_file from ch_pheno_for_assoc_test\n\n        output:\n        file(\"snps.txt\") into ch_snps\n\n        script:\n        \"\"\"\n        plink --keep-allele-order \\\n        --bed $bed \\\n        --bim $bim \\\n        --fam $fam \\\n        --allow-no-sex \\\n        --pheno ${pheno_file} \\\n        --pheno-name PHE \\\n        --threads ${task.cpus} \\\n        --assoc \\\n        --out out \n        awk -F' ' '{if(\\$9<${params.snp_threshold}) print \\$2}' out.assoc > snps.txt\n        \"\"\"\n    }", "\nprocess recode {\ntag \"plink\"\n\ninput:\nset file(bed), file(bim), file(fam) from ch_plink2\nfile snps from ch_snps\n\noutput:\nfile('*.raw') into phewas\n\nscript:\n\"\"\"\nplink --keep-allele-order \\\n--recodeA \\\n--bfile ${bed.baseName} \\\n--out r_genotypes \\\n--extract $snps\n\"\"\"\n}", " process merge_plink {\n      publishDir \"${params.outdir}/LDpred\", mode: \"copy\"\n        input:\n        tuple val(name), file(\"*\") from target_plink_dir_to_merge_ch\n\n        output:\n        file(\"merged.*\") into (merged_plink_ch, merged_plink_ldpred_gibbs_ch, merged_plink_ldpred_score_ch)\n\n        script:\n        \"\"\" \n        ls *.bed > bed.txt\n        ls *.bim > bim.txt\n        ls *.fam > fam.txt\n\n        paste bed.txt bim.txt fam.txt > merge.temp.list\n\n        grep -v \"_chr1_\" merge.temp.list > merge.list\n\n        plink --keep-allele-order --bfile ${name}_chr1_filtered --merge-list merge.list --make-bed --out merged\n        \"\"\"\n    }"], "list_proc": ["lifebit-ai/bam2cram/samtools_archive_30", "lifebit-ai/phewas/get_snps", "lifebit-ai/phewas/recode", "lifebit-ai/prs/merge_plink"], "list_wf_names": ["lifebit-ai/prs", "lifebit-ai/bam2cram", "lifebit-ai/phewas"]}, {"nb_reuse": 3, "tools": ["SAMtools", "PLINK", "MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 3, "list_wf": ["qc-dsl2", "bam2cram", "prs"], "list_contrib": ["dependabot[bot]", "cgpu", "mmeier93", "evagradovich", "PhilPalmer", "imendes93", "sk-sahu"], "nb_contrib": 7, "codes": ["\nprocess samtools_archive_31 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_9\n    each file(reference) from ch_reference_9\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.1,level=7,use_bzip2=1,use_fqz=1,use_arith=1 --output-fmt-option seqs_per_slice=100000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", " process merge_plink {\n      publishDir \"${params.outdir}/LDpred\", mode: \"copy\"\n        input:\n        tuple val(name), file(\"*\") from target_plink_dir_to_merge_ch\n\n        output:\n        file(\"merged.*\") into (merged_plink_ch, merged_plink_ldpred_gibbs_ch, merged_plink_ldpred_score_ch)\n\n        script:\n        \"\"\" \n        ls *.bed > bed.txt\n        ls *.bim > bim.txt\n        ls *.fam > fam.txt\n\n        paste bed.txt bim.txt fam.txt > merge.temp.list\n\n        grep -v \"_chr1_\" merge.temp.list > merge.list\n\n        plink --keep-allele-order --bfile ${name}_chr1_filtered --merge-list merge.list --make-bed --out merged\n        \"\"\"\n    }", "\nprocess multiqc {\n\n    input:\n    file ('fastqc/*')\n\n    output:\n    file \"*multiqc_report.html\"\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc . -m fastqc\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/bam2cram/samtools_archive_31", "lifebit-ai/prs/merge_plink", "lifebit-ai/qc-dsl2/multiqc"], "list_wf_names": ["lifebit-ai/qc-dsl2", "lifebit-ai/bam2cram", "lifebit-ai/prs"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["qc-dsl2", "bam2cram"], "list_contrib": ["dependabot[bot]", "imendes93", "PhilPalmer", "cgpu"], "nb_contrib": 4, "codes": ["\nprocess samtools_archive_lzma_30 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_10\n    each file(reference) from ch_reference_10\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.0,level=7,use_bzip2=1,use_lzma=1 --output-fmt-option seqs_per_slice=100000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }", "\nprocess fastqc {\n  tag \"$name\"\n  \n  input:\n  set val(name), file(reads)\n\n  output:\n  file \"*_fastqc.{zip,html}\"\n\n  script:\n  \"\"\"\n  fastqc $reads\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/bam2cram/samtools_archive_lzma_30", "lifebit-ai/qc-dsl2/fastqc"], "list_wf_names": ["lifebit-ai/qc-dsl2", "lifebit-ai/bam2cram"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["bam2cram"], "list_contrib": ["dependabot[bot]", "imendes93", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess samtools_archive_lzma_31 {\n    tag \"$bam_file\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${task.process}/\", mode: 'copy'\n\n    input:\n    file(bam_file) from ch_input_11\n    each file(reference) from ch_reference_11\n\n    output:\n    file \"*.cra*\"\n\n    script:\n    \"\"\"\n    ${params.pre_script}\n    samtools view --threads $task.cpus -T $reference -o ${bam_file.simpleName}.cram -O cram,version=3.1,level=7,use_bzip2=1,use_fqz=1,use_arith=1,use_lzma=1 --output-fmt-option seqs_per_slice=100000 $bam_file\n    samtools index ${bam_file.simpleName}.cram\n    ${params.post_script}\n    \"\"\"\n  }"], "list_proc": ["lifebit-ai/bam2cram/samtools_archive_lzma_31"], "list_wf_names": ["lifebit-ai/bam2cram"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n    container 'flowcraft/fastqc:0.11.7-1'\n\n    input:\n    set val(name), val(bed_name), file(reads) from reads_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/fastqc"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess bwaMem {\n  tag \"${name}\"\n\n  container 'broadinstitute/genomes-in-the-cloud:2.3.1-1512499786'\n  memory \"14.GB\"\n  cpus 4\n\n  input:\n  set val(name), val(bed_name), file(reads), file(fasta), file(amb), file(ann), file(bwt), file(pac), file(sa) from bwa\n\n  output:\n  set val(name), file(\"${name}.aligned.bam\") into aligned_bam\n\n  script:\n  \"\"\"\n  /usr/gitc/bwa mem \\\n      -p -v 3 -t 16 -M \\\n      ${fasta} ${reads} | samtools view -1bS > ${name}.aligned.bam\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/bwaMem"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess mergeBamAlignment {\n  tag \"${name}\"\n\n  container 'broadinstitute/gatk:4.0.4.0'\n  memory \"8.GB\"\n\n  input:\n  set val(name), file(unmapped_bam), file(aligned_bam), file(fasta), file(fai), file(dict), file(intervals) from merge_bams\n\n  output:\n  set val(name), file(\"${name}.merged.bam\"), file(\"${name}.merged.bai\"), file(intervals) into merged_bam, merged_bam_applybqsr, merged_bam_qc\n\n  script:\n  \"\"\"\n  /gatk/gatk --java-options \"-Dsamjdk.compression_level=5 -Xms4g\" \\\n      MergeBamAlignment \\\n     --ALIGNED_BAM ${aligned_bam} \\\n     --UNMAPPED_BAM ${unmapped_bam} \\\n     --OUTPUT ${name}.merged.bam \\\n     --REFERENCE_SEQUENCE ${fasta} \\\n     --PAIRED_RUN true \\\n     --SORT_ORDER coordinate \\\n     --CREATE_INDEX true \\\n     --CLIP_ADAPTERS true \\\n     --MAX_RECORDS_IN_RAM 2000000 \\\n     --MAX_INSERTIONS_OR_DELETIONS -1 \\\n     --PRIMARY_ALIGNMENT_STRATEGY MostDistant \\\n     --UNMAPPED_READ_STRATEGY COPY_TO_TAG \\\n     --ALIGNER_PROPER_PAIR_FLAGS true\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/mergeBamAlignment"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 2, "tools": ["Roary", "QualiMap"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["consensus-variant-caller", "roary"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess runBamQCmapped {\n    tag \"$bam\"\n    container 'maxulysse/sarek:2.3.FIX1'\n\n    input:\n    set val(name), file(bam), file(index), file(intervals) from merged_bam_qc\n\n    output:\n    file(\"${name}_mapped\") into bamqc_mapped_report\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    qualimap \\\n      bamqc \\\n      -bam ${bam} \\\n      --paint-chromosome-limits \\\n      --genome-gc-distr HUMAN \\\n      -nt ${task.cpus} \\\n      -skip-duplicated \\\n      --skip-dup-mode 0 \\\n      -outdir ${name}_mapped \\\n      -outformat HTML\n    \"\"\"\n}", "\nprocess roary {\n    publishDir \"${params.outdir}/roary\", mode: 'copy'\n\n    input:\n    file gff from gff.collect()\n\n    output:\n    file(\"*\") into roary\n    file(\"pan_genome_reference.fa\") into pan_genome\n\n    script:\n    \"\"\"\n    roary -e -n -v -r $gff\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/runBamQCmapped", "lifebit-ai/roary/roary"], "list_wf_names": ["lifebit-ai/roary", "lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess baseRecalibrator {\n  tag \"${name}\"\n\n  container 'broadinstitute/gatk:4.0.4.0'\n  memory \"8G\"\n\n  input:\n  set val(name), file(bam), file(bai), file(intervals), file(fasta), file(fai), file(dict), file(dbsnp), file(dbsnp_idx), file(golden_indel), file(golden_indel_idx) from baserecalibrator\n\n  output:\n  set val(name), file(\"${name}.recal_data.table\") into baserecalibrator_table\n  \n  script:\n                        \n  \"\"\"\n  /gatk/gatk --java-options \"-Xms4g\" \\\n      BaseRecalibrator \\\n      -R ${fasta} \\\n      -I ${bam} \\\n      --use-original-qualities \\\n      -O ${name}.recal_data.table \\\n      --known-sites ${dbsnp} \\\n      --known-sites ${golden_indel} \\\n      --intervals ${intervals} \\\n      --interval-padding 100\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/baseRecalibrator"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess applyBQSR {\n  tag \"${name}\"\n  publishDir \"${params.outdir}/applyBQSR\", mode: 'copy'\n\n  container 'broadinstitute/gatk:4.0.4.0'\n  memory \"8G\"\n\n  input:\n  set val(name), file(bam), file(bai), file(intervals), file(recalibration_table), file(fasta), file(fai), file(dict) from applybqsr\n\n  output:\n  set val(name), file(\"${name}.recal.bam\"), file(\"${name}.recal.bai\"), file(intervals) into recalibrated_bams, recalibrated_bams_bcftools, recalibrated_bams_qc\n  \n  script:\n                        \n  \"\"\"\n  /gatk/gatk --java-options \"-Xms3000m\" \\\n      ApplyBQSR \\\n      -bqsr ${recalibration_table} \\\n      -I ${bam} \\\n      -O ${name}.recal.bam \\\n      -R ${fasta} \\\n      --intervals ${intervals} \\\n      --interval-padding 100  \n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/applyBQSR"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess runBamQCrecalibrated {\n    tag \"$bam\"\n    container 'maxulysse/sarek:2.3.FIX1'\n\n    input:\n    set val(name), file(bam), file(index), file(intervals) from recalibrated_bams_qc\n\n    output:\n    file(\"${name}_recalibrated\") into bamqc_recalibrated_report\n\n    when: !params.skip_multiqc\n\n    script:\n    \"\"\"\n    qualimap \\\n      bamqc \\\n      -bam ${bam} \\\n      --paint-chromosome-limits \\\n      --genome-gc-distr HUMAN \\\n      -nt ${task.cpus} \\\n      -skip-duplicated \\\n      --skip-dup-mode 0 \\\n      -outdir ${name}_recalibrated \\\n      -outformat HTML\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/runBamQCrecalibrated"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess haplotypeCaller {\n  tag \"${name}\"\n  publishDir \"${params.outdir}/haplotypeCaller\", mode: 'copy'\n  \n  container 'broadinstitute/gatk:4.0.4.0'\n  memory \"14.GB\"\n  cpus 4\n\n  input:\n  set val(name), file(bam), file(bai), file(intervals), file(fasta), file(fai), file(dict) from haplotypecaller\n\n  output:\n  set val(name), file(\"${name}.GATK.vcf\"), file(\"${name}.GATK.vcf.idx\") into haplotypecaller_vcf\n\n  script:\n                                      \n  \"\"\"\n  /gatk/gatk --java-options \"-Xmx4g\" \\\n      HaplotypeCaller \\\n      -R ${fasta} \\\n      -I ${bam} \\\n      -O ${name}.GATK.vcf \\\n      --intervals ${intervals} \\\n      --interval-padding 100\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/haplotypeCaller"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess bcftoolsMpileup {\n  tag \"${name}\"\n  publishDir \"${params.outdir}/bcftoolsMpileup\", mode: 'copy'\n  \n  container \"quay.io/biocontainers/bcftools:1.9--h4da6232_0\"\n  memory \"14.GB\"\n  cpus 4\n\n  input:\n  set val(name), file(bam), file(bai), file(intervals), file(fasta), file(fai), file(dict), file(bed) from bcftools\n\n  output:\n  set val(name), file(\"${name}.SAM.vcf\") into bcftools_vcf\n\n  script:\n  \"\"\"\n  bcftools mpileup \\\n      --max-depth 10000 \\\n      --max-idepth 10000 \\\n      --annotate \"FORMAT/AD,FORMAT/DP\" \\\n      --fasta-ref ${fasta} \\\n      --regions-file ${bed} \\\n      --ignore-RG \\\n      --no-BAQ \\\n      ${bam} | bcftools call -Ov -mv \\\n          -o ${name}.SAM.vcf\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/bcftoolsMpileup"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess bcftoolsStats {\n  tag \"${name}\"\n  \n  container \"quay.io/biocontainers/bcftools:1.9--h4da6232_0\"\n\n  input:\n  set file(gatk_vcf), file(gatk), file(sam_vcf), file(sam) from annotated_results\n\n  output:\n  set file(\"bcfstats_${gatk_vcf}.txt\"), file(\"bcfstats_${sam_vcf}.txt\") into bcftools_stats\n\n  when: !params.skip_multiqc\n\n  script:\n  \"\"\"\n  bcftools stats $gatk_vcf > bcfstats_${gatk_vcf}.txt\n  bcftools stats $sam_vcf > bcfstats_${sam_vcf}.txt\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/bcftoolsStats"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["consensus-variant-caller"], "list_contrib": ["PhilPalmer", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n  tag \"multiqc_report.html\"\n\n  publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n  container 'ewels/multiqc:v1.7'\n\n  input:\n  file fastqc from fastqc_results.collect()\n  file bamqc_mapped from bamqc_mapped_report.collect()\n  file bamqc_recalibrated from bamqc_recalibrated_report.collect()\n  file bcftools_stats from bcftools_stats.collect()\n\n  output:\n  file(\"*\") into viz\n\n  when: !params.skip_multiqc\n\n  script:\n  \"\"\"\n  multiqc . -m fastqc -m qualimap -m bcftools \n  \"\"\"\n}"], "list_proc": ["lifebit-ai/consensus-variant-caller/multiqc"], "list_wf_names": ["lifebit-ai/consensus-variant-caller"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["drs-nf"], "list_contrib": ["sk-sahu"], "nb_contrib": 1, "codes": ["\nprocess gen3_drs_fasp {\n    tag \"${file_name}\"\n    label 'low_memory'\n    \n    input:\n    set val(subj_id), val(file_name), val(md5sum), val(obj_id), val(file_size) from ch_gtex_gen3_ids\n    each file(key_file) from ch_key_file\n    each file(genome_fasta) from ch_genome_fasta\n    \n    output:\n    set env(sample_name), file(\"*.bam\"), val(false) into bamtofastq\n    \n    script:\n    \"\"\"\n    sample_name=\\$(echo ${file_name} | cut -f1 -d\".\")\n    \n    drs_url=\\$(python /fasp-scripts/fasp/scripts/get_drs_url.py ${obj_id} ${params.gcp_id} ${key_file})\n    signed_url=\\$(echo \\$drs_url | awk '\\$1=\"\";1')\n    \n    if [[ \\$signed_url == *\".bam\"* ]]; then\n        wget -O \\${sample_name}.bam \\$(echo \\$signed_url)\n        file_md5sum=\\$(md5sum \\${sample_name}.bam)\n        if [[ ! \"\\$file_md5sum\" =~ ${md5sum} ]]; then exit 1; else echo \"file is good\"; fi\n    fi\n    \n    if [[ \\$signed_url == *\".cram\"* ]]; then\n        wget -O \\${sample_name}.cram \\$(echo \\$signed_url)\n        file_md5sum=\\$(md5sum \\${sample_name}.cram)\n        if [[ ! \"\\$file_md5sum\" =~ ${md5sum} ]]; then exit 1; else echo \"file is good\"; fi\n        samtools view -b -T ${genome_fasta} -o \\${sample_name}.bam \\${sample_name}.cram\n    fi\n    \"\"\"\n  }"], "list_proc": ["lifebit-ai/drs-nf/gen3_drs_fasp"], "list_wf_names": ["lifebit-ai/drs-nf"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["fam-gwas"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess vcf2plink {\n    publishDir \"${params.outdir}/vcf2plink\", mode: 'copy'\n\n    input:\n    file vcf from vcf\n    file fam from data\n\n    output:\n    set file('*.bed'), file('*.bim'), file('*.fam') into plink\n\n    script:\n    \"\"\"\n    sed '1d' $fam > tmpfile; mv tmpfile $fam\n    plink --vcf $vcf\n    rm plink.fam\n    mv $fam plink.fam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/fam-gwas/vcf2plink"], "list_wf_names": ["lifebit-ai/fam-gwas"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["fam-gwas"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess association {\n    publishDir \"${params.outdir}/associations\", mode: 'copy'\n\n    input:\n    set file(bed), file(bim), file(fam) from plink\n\n    output:\n    file('*') into asssociation\n\n    script:\n    \"\"\"\n    plink --bfile plink --mind $params.mind --geno $params.geno --maf $params.maf --hwe $params.hwe --me $params.me 0.1 --ci $params.ci --tdt\n    plink --bfile plink --mind $params.mind --geno $params.geno --maf $params.maf --hwe $params.hwe --me $params.me 0.1 --ci $params.ci --tdt poo\n    plink --bfile plink --mind $params.mind --geno $params.geno --maf $params.maf --hwe $params.hwe --me $params.me 0.1 --ci $params.ci --dfam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/fam-gwas/association"], "list_wf_names": ["lifebit-ai/fam-gwas"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["freebayes"], "list_contrib": ["luisas", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess preprocess_genome{\n\n  container 'lifebitai/samtools'\n\n\n  input:\n  file fasta from fasta\n  file fai from fai\n  file fastagz from fastagz\n  file gzfai from gzfai\n  file gzi from gzi\n  output:\n  set file(fasta),file(\"${fasta}.fai\"),file(\"${fasta}.gz\"),file(\"${fasta}.gz.fai\"), file(\"${fasta}.gz.gzi\"),file(\"${fasta.baseName}.dict\") into fastaChannel\n  script:\n  \"\"\"\n  [[ ${params.fai} == \"nofai\" ]] &&  samtools faidx $fasta || echo \" fai file of user is used, not created\"\n  [[ ${params.fastagz} == \"nofastagz\" ]]  && bgzip -c ${fasta} > ${fasta}.gz || echo \"fasta.gz file of user is used, not created \"\n  [[ ${params.gzfai} == \"nogzi\" ]] && bgzip -c -i ${fasta} > ${fasta}.gz || echo \"gzi file of user is used, not created\"\n  [[ ${params.gzi} == \"nogzfai\" ]] && samtools faidx \"${fasta}.gz\" || echo \"gz.fai file of user is used, not created\"\n  picard CreateSequenceDictionary R= $fasta O= ${fasta.baseName}.dict\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/freebayes/preprocess_genome"], "list_wf_names": ["lifebit-ai/freebayes"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["freebayes"], "list_contrib": ["luisas", "pprieto"], "nb_contrib": 2, "codes": ["\nprocess preprocess_bam {\n\n  tag \"${bam[0]}\"\n  container 'lifebitai/samtools'\n\n\n  input:\n  set val(prefix), file(bam) from bamChannel\n  set file(genome),file(genomefai),file(genomegz),file(genomegzfai),file(genomegzgzi),file(genomedict) from fastaChannel\n\n  output:\n  set file(\"ready/${bam[0]}\"), file(\"ready/${bam[0]}.bai\") into completeChannel\n\n  script:\n  \"\"\"\n  ## Add RG line in case it is missing\n    mkdir ready\n    [[ `samtools view -H ${bam[0]} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready; }|| { picard  AddOrReplaceReadGroups \\\n    I=${bam[0]} O=ready/${bam[0]} RGID=${params.rgid} RGLB=${params.rglb} RGPL=${params.rgpl} RGPU=${params.rgpu} RGSM=${params.rgsm}; }\n  ## Index Bam file\n    cd ready; samtools index ${bam[0]};\n  \"\"\"\n\n}"], "list_proc": ["lifebit-ai/freebayes/preprocess_bam"], "list_wf_names": ["lifebit-ai/freebayes"]}, {"nb_reuse": 1, "tools": ["HISAT2"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["hisat2"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess hisat2 {\n  tag \"${name}\"\n  container 'makaho/hisat2-zstd'\n  publishDir \"${params.outdir}/HISAT2\", mode: 'copy'\n\n  input:\n  set val(name), file(fastq) from reads_ch\n  val(hisat2_index_name) from hisat2_index_name\n  file(hisat2_index) from hs2_indices\n\n  output:\n  file(\"${name}.sam\") into hs2_sam\n\n  script:\n  \"\"\"\n  hisat2 \\\n  -p ${task.cpus} \\\n  -x $hisat2_index_name \\\n  -1 ${fastq[0]} \\\n  -2 ${fastq[1]} \\\n  -S ${name}.sam\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/hisat2/hisat2"], "list_wf_names": ["lifebit-ai/hisat2"]}, {"nb_reuse": 2, "tools": ["effectR", "SAMtools", "Metal", "ECMarker", "KAnalyze"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["metagwas", "htslib-s3-nf"], "list_contrib": ["evagradovich", "mmeier93", "sk-sahu"], "nb_contrib": 3, "codes": ["\nprocess run_metal {\npublishDir \"${params.outdir}\", mode: \"copy\"\n\ninput:\nfile(study) from all_input_studies_ch.collect()\n\noutput:\nfile(\"METAANALYSIS*\") into results_ch\n\nshell:\n'''\n# 1 - Dynamically obtain files to process\ntouch process_commands.txt\n\nfor csv in $(ls *.csv)\ndo \necho \"PROCESS $csv\" >> process_commands.txt\ndone\n\nprocess_commands=$(cat process_commands.txt)\n\n# 2 - Make METAL script \n\ncat > metal_command.txt <<EOF\nMARKER SNPID\nALLELE Allele1 Allele2\nEFFECT BETA\nPVALUE p.value \nSEPARATOR COMMA\n!{extra_flags}\n$process_commands\n\n\nANALYZE \nQUIT\nEOF\n\n# 3 - Run METAL\n\nmetal metal_command.txt\n'''\n}", " process samtools_view {\n        echo true\n        \n        input:\n        set val(name), val(bam_file) from ch_input\n        \n        script:\n        \"\"\"\n        samtools view ${bam_file} | head -n 3\n        \"\"\"\n    }"], "list_proc": ["lifebit-ai/metagwas/run_metal", "lifebit-ai/htslib-s3-nf/samtools_view"], "list_wf_names": ["lifebit-ai/htslib-s3-nf", "lifebit-ai/metagwas"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["htslib-s3-nf"], "list_contrib": ["sk-sahu"], "nb_contrib": 1, "codes": [" process bcftools_view {\n        echo true\n        \n        input:\n        set val(name), val(vcf_file) from ch_input\n        \n        script:\n        \"\"\"\n        bcftools view ${vcf_file}\n        \"\"\"\n    }"], "list_proc": ["lifebit-ai/htslib-s3-nf/bcftools_view"], "list_wf_names": ["lifebit-ai/htslib-s3-nf"]}, {"nb_reuse": 1, "tools": ["eFetch Pmc", "QResearch"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["kallisto-sra"], "list_contrib": ["pditommaso", "evanfloden", "luisas", "pprieto"], "nb_contrib": 4, "codes": ["\nprocess getSRAIDs {\n    \n    cpus 1\n\n    input:\n    val id from accessionID\n    \n    output:\n    file 'sra.txt' into sraIDs\n    \n    script:\n    \"\"\"\n    esearch -db sra -query $id  | efetch --format runinfo | grep SRR | cut -d ',' -f 1 > sra.txt\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/kallisto-sra/getSRAIDs"], "list_wf_names": ["lifebit-ai/kallisto-sra"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["kallisto-sra"], "list_contrib": ["pditommaso", "evanfloden", "luisas", "pprieto"], "nb_contrib": 4, "codes": ["\nprocess index {\n    input:\n    file transcriptome_file\n    \n    output:\n    file \"transcriptome.index\" into transcriptome_index\n      \n    script:\n      \n                                  \n      \n    \"\"\"\n    kallisto index -i transcriptome.index ${transcriptome_file}\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/kallisto-sra/index"], "list_wf_names": ["lifebit-ai/kallisto-sra"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["qc-nf"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n  tag \"$name\"\n  publishDir params.outdir, mode: 'copy'\n\n  input:\n  set val(name), file(reads) from raw_reads_fastqc\n\n  output:\n  file \"*_fastqc.{zip,html}\" into fastqc_results\n\n  script:\n  \"\"\"\n  fastqc $reads\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/qc-nf/fastqc"], "list_wf_names": ["lifebit-ai/qc-nf"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["qc-nf"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode: 'copy'\n\n    input:\n    file ('fastqc/*') from fastqc_results.collect()\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc . -m fastqc\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/qc-nf/multiqc"], "list_wf_names": ["lifebit-ai/qc-nf"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess filter_regions {\n    publishDir \"${params.outdir}/regionsFiltered/\", mode: params.publish_dir_mode\n\n    input:\n    set val(region), file(bcf), file(index) from ch_bcfs\n    set val(region2),file (\"BCFtools_site_metrics_SUBCOLS${region}_sorted.txt.gz\"), file(\"BCFtools_site_metrics_SUBCOLS${region}_sorted.txt.gz.tbi\") from ch_sort_compress\n\n\n    output:\n    set val(region), file (\"${region}_regionsFiltered.bcf\") into ch_regions_filtered\n\n    script:\n    \"\"\"\n    bcftools view ${bcf} \\\n    -T BCFtools_site_metrics_SUBCOLS${region}_sorted.txt.gz  \\\n    -Ob \\\n    -o ${region}_regionsFiltered.bcf\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/filter_regions"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess further_filtering {\n    publishDir \"${params.outdir}/further_filtering/\", mode: params.publish_dir_mode\n\n    input:\n    set val(region), file(bcf_filtered) from ch_regions_filtered\n\n    output:\n    set val(region), file(\"MichiganLD_regionsFiltered_${region}.bcf\"), file(\"MAF_filtered_1kp3intersect_${region}.txt\") into ch_further_filtering\n\n    script:\n    \"\"\"\n    bcftools view ${bcf_filtered} \\\n    -i 'INFO/OLD_MULTIALLELIC=\".\" & INFO/OLD_CLUMPED=\".\"' \\\n    -v snps  | \\\n    bcftools annotate \\\n    --set-id '%CHROM:%POS-%REF/%ALT-%INFO/OLD_CLUMPED-%INFO/OLD_MULTIALLELIC' | \\\n    bcftools +fill-tags -Ob \\\n    -o MichiganLD_regionsFiltered_${region}.bcf \\\n    -- -t MAF\n    #Produce filtered txt file\n    bcftools query MichiganLD_regionsFiltered_${region}.bcf \\\n    -i 'MAF[0]>0.01' -f '%CHROM\\\\t%POS\\\\t%REF\\\\t%ALT\\\\t%MAF\\\\n' | \\\n    awk -F \"\\t\" '{ if((\\$5 == \"G\" && \\$6 == \"C\") || (\\$6 == \"G\" && \\$5 == \"C\") || (\\$5 == \"A\" && \\$6 == \"T\") || (\\$6 == \"A\" && \\$5 == \"T\")) {next} { print \\$0} }' \\\n    > MAF_filtered_1kp3intersect_${region}.txt\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/further_filtering"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess create_final_king_vcf {\n    publishDir \"${params.outdir}/create_final_king_vcf/\", mode: params.publish_dir_mode\n\n    input:\n    set val(region), file(\"MichiganLD_regionsFiltered_${region}.bcf\"), file(\"MAF_filtered_1kp3intersect_${region}.txt\") from ch_further_filtering\n    file agg_samples_txt from ch_inputFinalPlatekeys\n\n    output:\n    set val(region), file(\"${region}_filtered.vcf.gz\"), file(\"${region}_filtered.vcf.gz.tbi\") into ch_create_final_king_vcf\n    file \"${region}_filtered.vcf.gz\" into ch_vcfs_create_final_king_vcf\n    file \"${region}_filtered.vcf.gz.tbi\" into ch_tbi_create_final_king_vcf\n    script:\n    \"\"\"\n    #Now filter down our file to just samples we want in our GRM. This removes any withdrawals that we learned of during the process of aggregation\n    #Store the header\n    bcftools view \\\n    -S ${agg_samples_txt} \\\n    --force-samples \\\n    -h MichiganLD_regionsFiltered_${region}.bcf \\\n    > ${region}_filtered.vcf\n\n    #Then match against all variant cols in our subsetted bcf to our maf filtered, intersected sites and only print those that are in the variant file.\n    #Then append this to the stored header, SNPRelate needs vcfs so leave as is\n    bcftools view \\\n    -H MichiganLD_regionsFiltered_${region}.bcf \\\n    -S ${agg_samples_txt} \\\n    --force-samples \\\n    | awk -F '\\t' '${awk_expr_create_final_king_vcf_1}' MAF_filtered_1kp3intersect_${region}.txt - >> ${region}_filtered.vcf\n    bgzip ${region}_filtered.vcf\n    tabix ${region}_filtered.vcf.gz\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/create_final_king_vcf"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess concat_king_vcf {\n    publishDir \"${params.outdir}/concat_king_vcf/\", mode: params.publish_dir_mode\n\n    input:\n    set val(region), file(\"${region}_filtered.vcf.gz\"), file(\"${region}_filtered.vcf.gz.tbi\") from ch_create_final_king_vcf\n    file \"*.vcf.gz\" from ch_vcfs_create_final_king_vcf.collect()\n    file \"*.tbi\" from ch_tbi_create_final_king_vcf.collect()\n    each chr from chrs\n    output:\n    set val(chr),file(\"chrom${chr}_merged_filtered.vcf.gz\"),file(\"chrom${chr}_merged_filtered.vcf.gz.tbi\") into ch_vcfs_per_chromosome\n\n    script:\n    \"\"\"\n    find -L . -type f -name chr${chr}_*.vcf.gz > tmp.files_chrom${chr}.txt\n    bcftools concat \\\n    -f tmp.files_chrom${chr}.txt \\\n    -Oz \\\n    -o chrom${chr}_merged_filtered.vcf.gz && \\\n    tabix chrom${chr}_merged_filtered.vcf.gz && \\\n    rm tmp.files_chrom${chr}.txt\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/concat_king_vcf"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": [" process ld_bed {\n    publishDir \"${params.outdir}/ld_bed/\", mode: params.publish_dir_mode\n\n    input:\n    set val(chr),file(\"BED_${chr}.bed\"),file(\"BED_${chr}.bim\"),file(\"BED_${chr}.fam\") from ch_make_bed_all\n\n    output:\n    file \"BED_LDpruned_${chr}*\" into ch_ld_bed\n\n    script:\n    \"\"\"\n    #Not considering founders in this as all of our SNPs are common\n    plink  \\\n    --keep-allele-order \\\n    --bfile BED_${chr} \\\n    --indep-pairwise 500kb 1 0.1 \\\n    --threads 30 \\\n    --out BED_LD_${chr}\n\n    #Now that we have our correct list of SNPs (prune.in), filter the original\n    #bed file to just these sites\n    plink \\\n    --make-bed \\\n    --bfile BED_${chr} \\\n    --keep-allele-order \\\n    --extract BED_LD_${chr}.prune.in \\\n    --double-id \\\n    --allow-extra-chr \\\n    --threads 30 \\\n    --out BED_LDpruned_${chr}\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/ld_bed"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess merge_autosomes {\n    publishDir \"${params.outdir}/merge_autosomes/\", mode: params.publish_dir_mode\n\n    input:\n    file chr_ld_pruned_bed from ch_ld_bed.collect()\n\n    output:\n    set file(\"autosomes_LD_pruned_1kgp3Intersect.bed\"), file(\"autosomes_LD_pruned_1kgp3Intersect.bim\"), file(\"autosomes_LD_pruned_1kgp3Intersect.fam\"),file(\"autosomes_LD_pruned_1kgp3Intersect.nosex\") into (ch_merge_autosomes , ch_merge_autosomes2, ch_merge_autosomes3, ch_merge_autosomes4)\n\n    script:\n    \"\"\"\n    for i in {1..22}; do if [ -f \"BED_LDpruned_\\$i.bed\" ]; then echo BED_LDpruned_\\$i >> mergelist.txt; fi ;done\n    plink --merge-list mergelist.txt \\\n    --make-bed \\\n    --out autosomes_LD_pruned_1kgp3Intersect\n    rm mergelist.txt\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/merge_autosomes"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["relate"], "list_contrib": ["aldembert", "Vlad-Dembrovskyi", "cgpu"], "nb_contrib": 3, "codes": ["\nprocess hwe_pruning_30k_snps {\n    publishDir \"${params.outdir}/hwe_pruning_30k_snps/\", mode: params.publish_dir_mode\n    input:\n    set file(\"autosomes_LD_pruned_1kgp3Intersect.bed\"), file(\"autosomes_LD_pruned_1kgp3Intersect.bim\"), file(\"autosomes_LD_pruned_1kgp3Intersect.fam\"),file(\"autosomes_LD_pruned_1kgp3Intersect.nosex\") from ch_merge_autosomes\n    file (ancestry_assignment_probs) from ch_inputAncestryAssignmentProbs\n    file (pc_sancestry_related) from ch_inputPCsancestryrelated\n\n    output:\n    file \"hwe1e-5_superpops_195ksnps\" into hwe_pruning_30k_snps\n\n    script:\n    \"\"\"\n    R -e 'library(data.table);\n    library(dplyr);\n    dat <- fread(\"${ancestry_assignment_probs}\") %>% as_tibble();\n    unrels <- fread(\"${pc_sancestry_related}\") %>% as_tibble() %>% filter(unrelated_set == 1);\n    dat <- dat %>% filter(plate_key %in% unrels\\$plate_key);\n    for(col in c(\"AFR\",\"EUR\",\"SAS\",\"EAS\")){dat[dat[col]>0.8,c(\"plate_key\",col)] %>% write.table(paste0(col,\"pop.txt\"), quote = F, row.names=F)}\n    '\n\n    bedmain=\"autosomes_LD_pruned_1kgp3Intersect\"\n    for pop in AFR EUR SAS EAS; do\n        echo \\${pop}\n        awk '{print \\$1\"\\t\"\\$1}' \\${pop}pop.txt > \\${pop}keep\n        plink \\\n        --keep-allele-order \\\n        --make-bed \\\n        --bfile \\${bedmain} \\\n        --out \\${pop}\n\n        plink --bfile \\${pop} --hardy midp --out \\${pop} --nonfounders\n    done\n\n    #Combine the HWE and produce a list of pass\n    R -e 'library(data.table);\n    library(dplyr);\n    dat <- lapply(c(\"EUR.hwe\",\"AFR.hwe\", \"SAS.hwe\", \"EAS.hwe\"),fread);\n    names(dat) <- c(\"EUR.hwe\",\"AFR.hwe\", \"SAS.hwe\", \"EAS.hwe\");\n    dat <- dat %>% bind_rows(.id=\"id\");\n    write.table(dat, \"combinedHWE.txt\", row.names = F, quote = F)\n    #Create set that is just SNPS that are >1e-5 in all pops\n    dat %>% filter(P >1e-5) %>% group_by(SNP) %>% count() %>% filter(n==4) %>% select(SNP) %>% distinct() %>%\n    write.table(\"hwe1e-5_superpops_195ksnps\", row.names = F, quote = F)\n    '\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/relate/hwe_pruning_30k_snps"], "list_wf_names": ["lifebit-ai/relate"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["roary"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess prokka {\n    publishDir \"${params.outdir}/prokka\", mode: 'copy'\n\n    input:\n    set fasta_prefix, file(fasta_file) from fasta_dataset\n\n    output:\n    file(\"${fasta_prefix}/${fasta_prefix}.gff\") into gff\n    set file(\"${fasta_prefix}/${fasta_prefix}.err\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.ffn\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.fsa\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.log\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.tsv\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.faa\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.fna\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.tbl\"),\n        file(\"${fasta_prefix}/${fasta_prefix}.txt\") into prokka\n\n    script:\n    \"\"\"\n    prokka --kingdom $params.kingdom --outdir ${fasta_prefix} --prefix ${fasta_prefix} ${fasta_file}\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/roary/prokka"], "list_wf_names": ["lifebit-ai/roary"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FeatureCounts"], "nb_own": 2, "list_own": ["wtsi-hgi", "lifebit-ai"], "nb_wf": 2, "list_wf": ["nextflow_ci", "sanger-demo-rnaseq"], "list_contrib": ["PhilPalmer", "gn5", "wtsi-mercury", "cgpu"], "nb_contrib": 4, "codes": ["process crams_to_fastq {\n    tag \"${sample}-${study_id}\"\n    publishDir \"${params.outdir}/crams_to_fastq/fastq/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"*.fastq.gz\"\n    publishDir \"${params.outdir}/crams_to_fastq/merged_crams/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"${study_id}.${sample}_merged.cram\"\n    \n    when: \n    params.run_crams_to_fastq\n    \n    input: \n    tuple val(study_id), val(sample), path(crams) \n\n    output: \n    tuple val(study_id), val(sample), path(\"*.fastq.gz\"), emit: study_sample_fastqs\n    tuple val(study_id), val(sample), path(\"${study_id}.${sample}_merged.cram\"), emit: study_sample_mergedcram\n    path('*.lostcause.tsv'), emit: lostcause optional true \n    path('*.numreads.tsv'), emit: numreads optional true \n    env(study_id), emit: study_id\n\n    script:\n    def cramfile = \"${study_id}.${sample}_merged.cram\"\n    \"\"\"\n    export REF_PATH=/lustre/scratch117/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch118/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    \n    samtools merge -@ ${task.cpus} -f $cramfile ${crams}\n    f1=${study_id}.${sample}_1.fastq.gz\n    f2=${study_id}.${sample}_2.fastq.gz\n    f0=${study_id}.${sample}.fastq.gz\n    numreads=\\$(samtools view -c -F 0x900 $cramfile)\n    if (( \\$numreads >= ${params.crams_to_fastq_min_reads} )); then\n                              # -O {stdout} -u {no compression}\n                              # -N {always append /1 and /2 to the read name}\n                              # -F 0x900 (bit 1, 8, filter secondary and supplementary reads)\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.numreads.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.numreads.tsv\n      samtools collate    \\\\\n          -O -u           \\\\\n          -@ ${task.cpus} \\\\\n          $cramfile pfx-${sample} | \\\\\n      samtools fastq      \\\\\n          -N              \\\\\n          -F 0x900        \\\\\n          -@ ${task.cpus} \\\\\n          -1 \\$f1 -2 \\$f2 -0 \\$f0 \\\\\n          -\n      sleep 2\n      find . -name \\\"*.fastq.gz\\\" -type 'f' -size -160k -delete\n    else\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.lostcause.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.lostcause.tsv\n    fi\n\n    study_id=gsheet\n    \"\"\"\n}", "\nprocess featureCounts {\n    tag \"${samplename}\"\n    container \"lifebitai/nfcore-rnaseq:latest\"\n    memory = '5G'\n    time '300m'\n    cpus 1\n    errorStrategy { task.attempt <= 5 ? 'retry' : 'ignore' }\n    maxRetries 5\n    publishDir \"${params.outdir}/featureCounts/\", mode: 'symlink',\n        saveAs: {filename ->\n            if (filename.indexOf(\".biotype_counts_mqc.txt\") > 0) \"biotype_counts_mqc/$filename\"\n            else if (filename.indexOf(\".biotype.fc.txt\") > 0) \"biotype_counts/$filename\"\n            else if (filename.indexOf(\".biotype.fc.txt.summary\") > 0) \"biotype_counts_summaries/$filename\"\n            else if (filename.indexOf(\".gene.fc.txt.summary\") > 0) \"gene_count_summaries/$filename\"\n            else if (filename.indexOf(\".gene.fc.txt\") > 0) \"gene_counts/$filename\"\n            else \"$filename\"\n        }\n\n    when:\n    params.run\n    \n    input:\n    set val(aligner), val(samplename), file(thebam)                        \n    file gtf                                          \n    file biotypes_header\n\n    output:\n    set val(aligner), file(\"*.gene.fc.txt\")                   \n    set val(aligner), file(\"*.gene.fc.txt.summary\")                     \n    set val(aligner), file(\"*.biotype_counts_mqc.txt\")                            \n\n    script:\n    def extraparams = params.fcextra.toString() - ~/^dummy/\n    def fc_direction = 0\n    def tag = \"${samplename}.${aligner}\"\n\n    def pairedend = params.singleend ? \"\" : \"-p\"\n    if (params.forward_stranded && !params.unstranded) {\n        fc_direction = 1\n    } else if (params.reverse_stranded && !params.unstranded){\n        fc_direction = 2\n    }\n    outfile = \"${tag}.gene.fc.txt\"\n    \"\"\"\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:\\$PATH\n\n    featureCounts -T ${task.cpus} -a $gtf -g gene_id          \\\\\n      -o ${outfile} $pairedend                                \\\\\n      -s $fc_direction ${extraparams} $thebam\n    cut -f 1,7 ${outfile} > reduced.${outfile}   #  This\n    mv reduced.${outfile} ${outfile}             #  reduces the file size from ~ 30M to ~1M\n    featureCounts -T ${task.cpus} -a $gtf -g gene_id  \\\\\n      -o ${tag}.biotype.fc.txt $pairedend                     \\\\\n      -s $fc_direction ${extraparams} $thebam\n    cut -f 1,7 ${tag}.biotype.fc.txt |                        \\\\\n        tail -n +3 | cat $biotypes_header - >> ${tag}.biotype_counts_mqc.txt\n    \"\"\"\n}"], "list_proc": ["wtsi-hgi/nextflow_ci/crams_to_fastq", "lifebit-ai/sanger-demo-rnaseq/featureCounts"], "list_wf_names": ["wtsi-hgi/nextflow_ci", "lifebit-ai/sanger-demo-rnaseq"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools"], "nb_own": 2, "list_own": ["wtsi-hgi", "lifebit-ai"], "nb_wf": 2, "list_wf": ["nextflow-pipelines", "sanger-demo-rnaseq"], "list_contrib": ["PhilPalmer", "gn5", "wtsi-mercury", "cgpu"], "nb_contrib": 4, "codes": ["\nprocess concat_vcfs {\n    memory '60G'\n    tag \"$vcfs_location $name\"\n    cpus 2\n                   \n    time '700m'\n    queue 'normal'\n    container \"graphtyper\"\n    containerOptions = \"--bind /lustre\"\n                                \n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    publishDir \"${params.outdir}/vcfs_concat/\", mode: 'symlink', overwrite: true, pattern: \"${name}.vcf.gz\"\n    publishDir \"${params.outdir}/vcfs_concat/\", mode: 'symlink', overwrite: true, pattern: \"${name}.vcf.gz.csi\"\n    maxRetries 3\n\n    when:\n    params.run\n     \n    input:\n    val(vcfs_location)\n    val(name)\n    \n    output: \n    tuple file(\"${name}.vcf.gz\"), file(\"${name}.vcf.gz.csi\"), emit: vcf_gz\n    tuple file(\"to_concat.list\"), file(\"to_concat_non_empty.list\"), emit: concat_list\n\n    script:\n\"\"\" \nfind $vcfs_location -maxdepth 1 -name 'chr1:*.vcf.gz' -exec sh -c \\\"echo {} \\\\\\$(zcat {} | grep -v '^#' | wc -l)\\\" \\\\; >> to_concat.list\n\ncat to_concat.list | grep -v 'gz 0\\$' | cut -f1 -d ' ' | sort > to_concat_non_empty.list\n\nbcftools concat -f to_concat_non_empty.list --allow-overlaps | bcftools sort -o ${name}.vcf.gz -O z\nbcftools index ${name}.vcf.gz\n\"\"\"\n}", "\nprocess crams_to_fastq_gz {\n    tag \"crams to fastq_gz ${samplename}\"\n\n                                \n                                                   \n    \n    container \"lifebitai/samtools:latest\"                                      \n    containerOptions = \"--bind /lustre/scratch117/core/sciops_repository/cram_cache --bind /lustre/scratch118/core/sciops_repository/cram_cache\"\n                                \n    errorStrategy 'retry'\n    maxRetries 6\n    time '400m'\n    cpus 1\n    memory '4G'\n\n                                                                                                            \n                                                                                                                            \n                                                                                \n    publishDir \"${params.outdir}/fastq12/\", mode: 'symlink'\n    \n    when:\n    params.run\n    \n    input: \n        set val(samplename), file(crams) \n    output: \n        set val(samplename), file(\"${samplename}_1.fastq.gz\"), file(\"${samplename}_2.fastq.gz\") optional true\n        file('*.lostcause.txt') optional true \n        file('numreads.txt') optional true \n    script:\n\n                                                                                \n                                                   \n                                                                                                                                    \n    def cramfile = \"${samplename}_merged.cram\"\n    \"\"\"\n    export REF_PATH=/lustre/scratch117/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch118/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s\n\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    \n    samtools merge -@ ${task.cpus} -f $cramfile ${crams}\n\n    f1=${samplename}_1.fastq.gz\n    f2=${samplename}_2.fastq.gz\n\n    numreads=\\$(samtools view -c -F 0x900 $cramfile)\n    if (( numreads >= ${params.min_reads} )); then\n                              # -O {stdout} -u {no compression}\n                              # -N {always append /1 and /2 to the read name}\n                              # -F 0x900 (bit 1, 8, filter secondary and supplementary reads)\n      echo -n \\$numreads > numreads.txt\n      samtools collate    \\\\\n          -O -u           \\\\\n          -@ ${task.cpus} \\\\\n          $cramfile pfx-${samplename} | \\\\\n      samtools fastq      \\\\\n          -N              \\\\\n          -F 0x900        \\\\\n          -@ ${task.cpus} \\\\\n          -1 \\$f1 -2 \\$f2 \\\\\n          -\n      sync \\$f1 \\$f2          # this line and next to tackle k8s weirdness (see k8s)\n      sleep 1\n    else\n      echo -e \"${samplename}\\\\tcram\\\\tlowreads\" > ${samplename}.lostcause.txt\n    fi\n    \"\"\"\n}"], "list_proc": ["wtsi-hgi/nextflow-pipelines/concat_vcfs", "lifebit-ai/sanger-demo-rnaseq/crams_to_fastq_gz"], "list_wf_names": ["wtsi-hgi/nextflow-pipelines", "lifebit-ai/sanger-demo-rnaseq"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Maligner"], "nb_own": 2, "list_own": ["wtsi-hgi", "lifebit-ai"], "nb_wf": 2, "list_wf": ["nextflow-pipelines", "sanger-demo-rnaseq"], "list_contrib": ["PhilPalmer", "gn5", "wtsi-mercury", "cgpu"], "nb_contrib": 4, "codes": ["\nprocess index_cram {\n    memory '3G'\n    tag \"$cram_file\"\n    cpus 1\n                   \n    time '100m'\n    queue 'normal'\n    container \"graphtyper\"\n    containerOptions = \"--bind /lustre\"\n                                \n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    publishDir \"${params.outdir}/cram_index/\", mode: 'symlink', overwrite: true, pattern: \"${cram_file}.crai\"\n    maxRetries 3\n\n    when:\n    params.run\n     \n    input:\n    file(cram_file)\n\n    output: \n    tuple file(\"${cram_file}.crai\"), emit: indexes\n\n    script:\n\"\"\" \nsamtools index $cram_file\n\"\"\"\n}", "\nprocess merge_featureCounts {\n    tag \"$aligner\"\n    scratch '/tmp'\n    stageInMode 'copy'\n    stageOutMode 'rsync'\n    container \"lifebitai/nfcore-rnaseq:1.0\"\n    publishDir \"${params.outdir}/combined\", mode: 'symlink'\n    label 'merge_feature'\n    memory = '100G'\n    cpus 2\n    time '600m'\n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    when:\n    params.run\n\n    input:\n    file(collected_fc_gene_txt)\n\n    output:\n    file '*-fc-genecounts.txt'\n    file(\"fofn_gene_featurecount.txt\")\n\n    shell:\n    suffix=['star':'.star.gene.fc.txt', 'hisat2':'.hisat2.gene.fc.txt']\n    aligner = \"star\"                           \n    outputname = \"${params.runtag}-${aligner}-fc-genecounts.txt\"\n    thesuffix  = suffix[aligner] ?: '.txt'\n    '''\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:$PATH\n\n    ls . | grep gene.fc.txt\\$ > fofn_gene_featurecount.txt\n\n    merge_featurecounts.py        \\\\\n      --rm-suffix !{thesuffix}                                       \\\\\n      -c 1 --skip-comments --header                                  \\\\\n      -o !{outputname} -I fofn_gene_featurecount.txt\n    '''\n}"], "list_proc": ["wtsi-hgi/nextflow-pipelines/index_cram", "lifebit-ai/sanger-demo-rnaseq/merge_featureCounts"], "list_wf_names": ["wtsi-hgi/nextflow-pipelines", "lifebit-ai/sanger-demo-rnaseq"]}, {"nb_reuse": 2, "tools": ["Salmon", "Vireo"], "nb_own": 2, "list_own": ["wtsi-hgi", "lifebit-ai"], "nb_wf": 2, "list_wf": ["nf_scrna_deconvolution", "sanger-demo-rnaseq"], "list_contrib": ["PhilPalmer", "cgpu", "wtsi-mercury", "gn5"], "nb_contrib": 4, "codes": ["\nprocess salmon {\n    tag \"salmon $samplename\"\n                    \n    memory = {  10.GB + 20.GB * (task.attempt-1) }\n    container \"lifebitai/salmon:latest\"\n    time '700m'\n    errorStrategy { task.attempt <= 6 ? 'retry' : 'ignore' }\n    maxRetries 6\n    \n    publishDir \"${params.outdir}/salmon\", mode: 'symlink'\n\n    when:\n    params.run\n\n    input:\n    set val(samplename), file(reads)                  \n    file salmon_index_dir                                  \n    file salmon_trans_gene_txt                                       \n\n    output:\n    file \"${samplename}.quant.sf\"                        \n    file \"${samplename}.quant.genes.sf\"                       \n    file \"my_outs/${samplename}\"                                 \n\n    script:\n    \"\"\"\n    salmon quant \\\\\n        -i ${salmon_index_dir} \\\\\n        -l ISR \\\\\n        -p ${task.cpus} \\\\\n        --seqBias \\\\\n        --gcBias \\\\\n        --posBias \\\\\n        --no-version-check \\\\\n        -q \\\\\n        -o . \\\\\n        -1 ${reads[0]} \\\\\n        -2 ${reads[1]} \\\\\n        -g ${salmon_trans_gene_txt} \\\\\n        --useVBOpt \\\\\n        --numBootstraps 100\n    mv quant.sf ${samplename}.quant.sf\n    mv quant.genes.sf ${samplename}.quant.genes.sf\n    mkdir -p my_outs/${samplename}/libParams\n    mkdir -p my_outs/${samplename}/aux_info\n    ln -f aux_info/meta_info.json my_outs/${samplename}/aux_info/meta_info.json\n    ln -f libParams/flenDist.txt  my_outs/${samplename}/libParams/flenDist.txt\n    \"\"\"\n\n                                                                                          \n                                                          \n                                                                                        \n}", "process vireo_with_genotype {\n    tag \"${samplename}.${donors_gt_vcf}\"\n\n    publishDir \"${params.outdir}/vireo_gt/${samplename}/\", mode: \"${params.vireo.copy_mode}\", overwrite: true,\n\t  saveAs: {filename -> filename.replaceFirst(\"vireo_${samplename}/\",\"\") }\n    \n    when: \n      params.run_with_genotype_input\n    \n    input:\n      tuple val(samplename), path(cell_data), path(donors_gt_vcf)\n    \n    output:\n      tuple val(samplename), path(\"vireo_${samplename}/*\"), emit: output_dir\n      tuple val(samplename), path(\"vireo_${samplename}/donor_ids.tsv\"), emit: sample_donor_ids \n      path(\"vireo_${samplename}/${samplename}.sample_summary.txt\"), emit: sample_summary_tsv\n      path(\"vireo_${samplename}/${samplename}__exp.sample_summary.txt\"), emit: sample__exp_summary_tsv\n\n    script:\n    \"\"\"\n\n      umask 2 # make files group_writable\n\n      vireo -c $cell_data -o vireo_${samplename} -d ${donors_gt_vcf} -t GT\n\n      # add samplename to summary.tsv,\n      # to then have Nextflow concat summary.tsv of all samples into a single file:\n\n      cat vireo_${samplename}/summary.tsv | \\\\\n        tail -n +2 | \\\\\n        sed s\\\"/^/${samplename}\\\\t/\\\"g > vireo_${samplename}/${samplename}.sample_summary.txt\n\n      cat vireo_${samplename}/summary.tsv | \\\\\n        tail -n +2 | \\\\\n        sed s\\\"/^/${samplename}__/\\\"g > vireo_${samplename}/${samplename}__exp.sample_summary.txt\n\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/sanger-demo-rnaseq/salmon", "wtsi-hgi/nf_scrna_deconvolution/vireo_with_genotype"], "list_wf_names": ["wtsi-hgi/nf_scrna_deconvolution", "lifebit-ai/sanger-demo-rnaseq"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["wtsi-hgi", "lifebit-ai"], "nb_wf": 2, "list_wf": ["sanger-demo-rnaseq", "nf_irods_to_lustre"], "list_contrib": ["PhilPalmer", "cgpu", "wtsi-mercury", "gn5"], "nb_contrib": 4, "codes": ["\nprocess samtools_index_idxstats {\n    tag \"${samplename}\"\n    container \"lifebitai/nfcore-rnaseq:latest\"\n    memory = '8G'\n    cpus 1\n    time '300m'\n    errorStrategy { task.attempt <= 5 ? 'retry' : 'ignore' }\n    maxRetries 5\n    publishDir \"${params.outdir}/idxstats/\", mode: 'symlink', pattern: \"*.idxstats\"\n\n    when:\n    params.run\n\n    input:\n    set val(aligner), val(samplename), file(thebam)                   \n\n    output:\n    set val(samplename), file(\"*.idxstats\")                     \n\n    script:\n    \"\"\"\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:\\$PATH\n\n    samtools index $thebam\n    samtools idxstats $thebam > ${samplename}.idxstats\n    rm ${thebam}.bai\n    \"\"\"\n}", "process crams_to_fastq {\n    tag \"${sample}-${study_id}\"\n    publishDir \"${params.outdir}/crams_to_fastq/fastq/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"*.fastq.gz\"\n    publishDir \"${params.outdir}/crams_to_fastq/merged_crams/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"${study_id}.${sample}_merged.cram\"\n    \n    when: \n    params.run_crams_to_fastq\n    \n    input: \n    tuple val(study_id), val(sample), path(crams) \n\n    output: \n    tuple val(study_id), val(sample), path(\"*.fastq.gz\"), emit: study_sample_fastqs\n    tuple val(study_id), val(sample), path(\"${study_id}.${sample}_merged.cram\"), emit: study_sample_mergedcram\n    path('*.lostcause.tsv'), emit: lostcause optional true \n    path('*.numreads.tsv'), emit: numreads optional true \n    env(study_id), emit: study_id\n\n    script:\n    def cramfile = \"${study_id}.${sample}_merged.cram\"\n    \"\"\"\n    export REF_PATH=/lustre/scratch117/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch118/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    \n    samtools merge -@ ${task.cpus} -f $cramfile ${crams}\n    f1=${study_id}.${sample}_1.fastq.gz\n    f2=${study_id}.${sample}_2.fastq.gz\n    f0=${study_id}.${sample}.fastq.gz\n    numreads=\\$(samtools view -c -F 0x900 $cramfile)\n    if (( \\$numreads >= ${params.crams_to_fastq_min_reads} )); then\n                              # -O {stdout} -u {no compression}\n                              # -N {always append /1 and /2 to the read name}\n                              # -F 0x900 (bit 1, 8, filter secondary and supplementary reads)\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.numreads.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.numreads.tsv\n      samtools collate    \\\\\n          -O -u           \\\\\n          -@ ${task.cpus} \\\\\n          $cramfile pfx-${sample} | \\\\\n      samtools fastq      \\\\\n          -N              \\\\\n          -F 0x900        \\\\\n          -@ ${task.cpus} \\\\\n          -1 \\$f1 -2 \\$f2 -0 \\$f0 \\\\\n          -\n      sleep 2\n      find . -name \\\"*.fastq.gz\\\" -type 'f' -size -160k -delete\n    else\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.lostcause.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.lostcause.tsv\n    fi\n\n    study_id=gsheet\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/sanger-demo-rnaseq/samtools_index_idxstats", "wtsi-hgi/nf_irods_to_lustre/crams_to_fastq"], "list_wf_names": ["wtsi-hgi/nf_irods_to_lustre", "lifebit-ai/sanger-demo-rnaseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["sanger-demo-rnaseq"], "list_contrib": ["PhilPalmer", "cgpu"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    scratch '/tmp'\n    stageInMode 'copy'\n    stageOutMode 'rsync'\n    container \"lifebitai/nfcore-rnaseq:latest\"\n    errorStrategy = { task.attempt <= 5 ? 'retry' : 'ignore' }\n    cpus =   {  2 * 2 * Math.min(2, task.attempt) }\n    memory = {  40.GB + 20.GB * (task.attempt-1) }\n    maxRetries 5\n    cpus 2\n    queue 'long'\n    time '900m'\n\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy',\n\n    when:\n    params.run\n\n    input:\n    file (fastqc:'fastqc/*')                                               \n    file ('featureCounts/*')                                                   \n    file ('featureCounts_biotype/*')                                                          \n    file ('star/*')                                                    \n    file ('salmon/*')                                                      \n\n    output:\n    file \"*multiqc_report.html\"\n    file \"*_data\"\n\n    script:\n    def reporttitle = \"${params.runtag}\"\n    \"\"\"\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:\\$PATH\n    export LC_ALL=C.UTF-8\n    export LANG=C.UTF-8\n\n    multiqc . -f --filename multiqc_report.html --title \"$reporttitle\" -m featureCounts -m star -m fastqc -m salmon\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/sanger-demo-rnaseq/multiqc"], "list_wf_names": ["lifebit-ai/sanger-demo-rnaseq"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["simulate"], "list_contrib": ["mmeier93", "cgpu"], "nb_contrib": 2, "codes": [" process compress_index_reheader_vcf {\n        publishDir \"${params.outdir}/simulated_vcf/compressed_and_indexed\", mode: \"copy\"\n\n        input:\n        file(vcf) from not_compressed_and_indexed_simulated_vcf_ch\n        file(sample_ids) from ch_sample_ids\n\n        output:\n        file(\"*\") into compressed_and_indexed_simulated_vcf_ch\n\n        script:\n        if( params.sample_ids )\n        \"\"\"\n        # Compress the VCF file\n        bcftools view -I ${vcf} -Oz -o ${vcf}.gz\n\n        # Temp move to different name to reheader sample ids\n        mv ${vcf}.gz temp_${vcf}.gz\n\n        # Reheader\n        bcftools reheader --samples ${sample_ids} --output ${vcf}.gz temp_${vcf}.gz\n        rm temp_${vcf}.gz\n\n        # Index the compressed VCFc file\n        bcftools index ${vcf}.gz\n        \"\"\"\n      }"], "list_proc": ["lifebit-ai/simulate/compress_index_reheader_vcf"], "list_wf_names": ["lifebit-ai/simulate"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["simulate"], "list_contrib": ["mmeier93", "cgpu"], "nb_contrib": 2, "codes": [" process compress_and_index_vcf {\n        publishDir \"${params.outdir}/simulated_vcf/compressed_and_indexed\", mode: \"copy\"\n\n        input:\n        file(vcf) from not_compressed_and_indexed_simulated_vcf_ch\n\n        output:\n        file(\"*\") into compressed_and_indexed_simulated_vcf_ch\n\n        script:\n\n        \"\"\"\n        # Compress the VCF file\n        bcftools view -I ${vcf} -Oz -o ${vcf}.gz\n\n        # Index the compressed VCFc file\n        bcftools index ${vcf}.gz\n        \"\"\"\n      }"], "list_proc": ["lifebit-ai/simulate/compress_and_index_vcf"], "list_wf_names": ["lifebit-ai/simulate"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess BedToIntervalList {\n    tag \"$bed\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(name), file(bed) from bed_interval\n    file dict from dict_interval\n\n    output:\n    file(\"${name}.interval_list\") into interval_list\n\n    when: params.bed\n\n    script:\n    \"\"\"\n    gatk BedToIntervalList \\\n    -I ${bed} \\\n    -O ${name}.interval_list \\\n    -SD ${dict}\n\n    # remove header, columns 3 onwards & reformat\n    sed -i '/^@/d' ${name}.interval_list\n    cut -f 1-3 ${name}.interval_list > tmp.interval_list\n    awk 'BEGIN { OFS = \"\" }{ print \\$1,\":\",\\$2,\"-\",\\$3 }' tmp.interval_list > ${name}.interval_list\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/BedToIntervalList"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n    container 'flowcraft/fastqc:0.11.7-1'\n\n    when:\n    !params.skip_fastqc\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(reads) from reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/fastqc"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess BWA {\n    tag \"$name\"\n    container 'kathrinklee/bwa:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(reads),\n    file(fasta), file(amb), file(ann), file(bwt), file(pac), file(sa) from bwa\n\n    output:\n    set val(patientId), val(sampleId), val(status), val(name), file(\"${name}.sam\") into sam\n\n    \"\"\"\n    bwa mem -M -R '@RG\\\\tID:${name}\\\\tSM:${name}\\\\tPL:Illumina' $fasta $reads > ${name}.sam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/BWA"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess BWA_sort {\n    tag \"$sam\"\n    container 'lifebitai/samtools:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(sam) from sam\n\n    output:\n    set val(patientId), val(sampleId), val(status), val(name), file(\"${name}-sorted.bam\") into bam_sort, bam_sort_qc\n\n    \"\"\"\n    samtools sort -o ${name}-sorted.bam -O BAM $sam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/BWA_sort"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 2, "tools": ["QualiMap", "BCFtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 2, "list_wf": ["vcf-annotator", "somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess RunBamQCmapped {\n    tag \"$bam\"\n\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(bam) from bam_sort_qc\n\n    output:\n    file(\"${name}\") into bamQCmappedReport\n\n    when: !params.skip_multiqc\n\n    script:\n                                                         \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name} \\\n    -outformat HTML\n    \"\"\"\n}", "\nprocess annotate_vcf {\n  tag \"$name\"\n  publishDir params.outdir, mode: 'copy'\n\n  input:\n  set val(name), file(vcf) from vcf\n  each file(dbsnp) from dbsnp\n  each file(dbsnp_index) from dbsnp_index\n\n  output:\n  file(\"${name}.vcf.gz\") into annotated_vcf\n\n  script:\n  \"\"\"\n  vcf=$vcf\n\n  # uncompress bgzipped or gzipped input\n  if [[ $vcf == *.gz ]]; then\n    compression=\\$(htsfile $vcf)\n    if [[ \\$compression == *\"BGZF\"* ]]; then\n      bgzip -cdf $vcf > tmp.vcf && vcf=tmp.vcf\n    elif [[ \\$compression == *\"gzip\"* ]]; then\n      gzip -cdf $vcf > tmp.vcf && vcf=tmp.vcf\n    fi\n  fi\n\n  vcf_remapper.py --input_file \\$vcf --output_file ${name}\n  mv output/${name}.vcf ${name}.tmp.vcf\n  bgzip ${name}.tmp.vcf\n  tabix -p vcf ${name}.tmp.vcf.gz\n\n  bcftools annotate -c CHROM,FROM,TO,ID -a ${dbsnp} -Oz -o ${name}.vcf.gz ${name}.tmp.vcf.gz\n  \"\"\" \n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/RunBamQCmapped", "lifebit-ai/vcf-annotator/annotate_vcf"], "list_wf_names": ["lifebit-ai/somatic-variant-caller", "lifebit-ai/vcf-annotator"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess MarkDuplicates {\n    tag \"$bam_sort\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(bam_sort) from bam_sort\n\n    output:\n    set val(name), file(\"${name}.bam\"), file(\"${name}.bai\"), val(patientId), val(sampleId), val(status) into bam_markdup_baserecalibrator, bam_markdup_applybqsr\n    file (\"${name}.bam.metrics\") into markDuplicatesReport\n\n    \"\"\"\n    gatk MarkDuplicates \\\n    -I ${bam_sort} \\\n    --CREATE_INDEX true \\\n    -M ${name}.bam.metrics \\\n    -O ${name}.bam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/MarkDuplicates"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess BaseRecalibrator {\n    tag \"$bam_markdup\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(name), file(bam_markdup), file(bai), val(patientId), val(sampleId), val(status), \n    file(fasta), file(fai), file(dict), file(dbsnp), file(dbsnp_idx), file(golden_indel), file(golden_indel_idx) from baserecalibrator\n\n    output:\n    set val(name), file(\"${name}_recal_data.table\") into baserecalibrator_table\n    file(\"*data.table\") into baseRecalibratorReport\n\n    \"\"\"\n    gatk BaseRecalibrator \\\n    -I $bam_markdup \\\n    --known-sites $dbsnp \\\n    --known-sites $golden_indel \\\n    -O ${name}_recal_data.table \\\n    -R $fasta\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/BaseRecalibrator"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess ApplyBQSR {\n    tag \"$baserecalibrator_table\"\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    set val(name), file(baserecalibrator_table), file(bam), file(bai), val(patientId), val(sampleId), val(status) from applybqsr\n\n    output:\n    set val(patientId), val(sampleId), val(status), val(name), file(\"${name}_bqsr.bam\"), file(\"${name}_bqsr.bai\") into bam_for_qc, bam_haplotypecaller, bam_mutect\n\n    script:\n    \"\"\"\n    gatk ApplyBQSR -I $bam -bqsr $baserecalibrator_table -OBI -O ${name}_bqsr.bam\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/ApplyBQSR"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess RunBamQCrecalibrated {\n    tag \"$bam\"\n\n    container 'maxulysse/sarek:latest'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(bam), file(bai) from bam_for_qc\n\n    output:\n    file(\"${name}_recalibrated\") into bamQCrecalibratedReport\n\n    when: !params.skip_multiqc\n\n    script:\n                                                           \n    \"\"\"\n    qualimap \\\n    bamqc \\\n    -bam ${bam} \\\n    --paint-chromosome-limits \\\n    --genome-gc-distr HUMAN \\\n    -nt ${task.cpus} \\\n    -skip-duplicated \\\n    --skip-dup-mode 0 \\\n    -outdir ${name}_recalibrated \\\n    -outformat HTML\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/RunBamQCrecalibrated"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess HaplotypeCaller {\n    tag \"$intervals\"\n    container 'broadinstitute/gatk:latest'\n\n    memory threadmem\n\n    input:\n    set val(intervals), file(fasta), file(fai), file(dict),\n    val(patientId), val(sampleId), val(status), val(name), file(bam), file(bai) from haplotypecaller\n\n    output:\n    file(\"${name}.g.vcf\") into haplotypecaller_gvcf\n    file(\"${name}.g.vcf.idx\") into index\n    val(name) into name_mergevcfs\n\n    when: !params.skip_haplotypecaller\n\n    script:\n    \"\"\"\n    gatk HaplotypeCaller \\\n    --java-options -Xmx${task.memory.toMega()}M \\\n    -R $fasta \\\n    -O ${name}.g.vcf \\\n    -I $bam \\\n    -ERC GVCF \\\n    -L $intervals\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/HaplotypeCaller"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess MergeVCFs {\n    tag \"${name[0]}.g.vcf\"\n    publishDir \"${params.outdir}/GermlineVariantCalling\", mode: 'copy'\n    container 'broadinstitute/gatk:latest'\n\n    input:\n    file ('*.g.vcf') from haplotypecaller_gvcf.collect()\n    file ('*.g.vcf.idx') from index.collect()\n    val name from name_mergevcfs.collect()\n\n    output:\n    set file(\"${name[0]}.g.vcf\"), file(\"${name[0]}.g.vcf.idx\") into mergevcfs\n\n    script:\n    \"\"\"\n    ## make list of input variant files\n    for vcf in \\$(ls *vcf); do\n    echo \\$vcf >> input_variant_files.list\n    done\n    gatk MergeVcfs \\\n    --INPUT= input_variant_files.list \\\n    --OUTPUT= ${name[0]}.g.vcf\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/MergeVCFs"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess Mutect2 {\n    tag \"$bam\"\n    container 'broadinstitute/gatk:latest'\n    publishDir \"${params.outdir}/Somatic\", mode: 'copy'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(bam), file(bai),\n    val(tumourSampleId), val(tumourStatus), val(tumourName), file(tumourBam), file(tumourBai),\n    file(fasta), file(fai), file(dict) from mutect\n\n    output:\n    set val(\"${tumourSampleId}_vs_${sampleId}\"), file(\"${tumourSampleId}_vs_${sampleId}.vcf\") into vcf_variant_eval\n\n    script:\n    \"\"\"\n    gatk Mutect2 \\\n    -R ${fasta}\\\n    -I ${tumourBam}  -tumor ${tumourName} \\\n    -I ${bam} -normal ${name} \\\n    -O ${tumourSampleId}_vs_${sampleId}.vcf\n\n    #gatk --java-options \"-Xmx\\${task.memory.toGiga()}g\" \\\n    #-L \\${intervalBed} \\\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/Mutect2"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess kraken2 {\n    tag \"$name\"\n    publishDir \"${params.outdir}/kraken/${patientId}\", mode: 'copy'\n    container 'flowcraft/kraken2:2.0.7-1'\n\n    input:\n    set val(patientId), val(sampleId), val(status), val(name), file(reads) from reads_kraken\n\n    output:\n    set file(\"${sampleId}_kraken.out\"), file(\"${sampleId}_kraken.report\") into kraken_results\n\n    script:\n    \"\"\"\n    kraken2 \\\n    --threads ${task.cpus} \\\n    --paired \\\n    --db ${kraken_db} \\\n    --fastq-input $reads \\\n    --output ${sampleId}_kraken.out \\\n    --report ${sampleId}_kraken.report\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/kraken2"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["somatic-variant-caller"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n    container 'ewels/multiqc:v1.7'\n\n    when:\n    !params.skip_multiqc\n\n    input:\n    file (fastqc:'fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file (bam_metrics) from markDuplicatesReport.collect().ifEmpty([])\n    file (bamQC) from bamQCmappedReport.collect().ifEmpty([])\n    file (bamQCrecalibrated) from bamQCrecalibratedReport.collect().ifEmpty([])\n    file (baseRecalibrator) from baseRecalibratorReport.collect().ifEmpty([])\n    \n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    \"\"\"\n    multiqc . -m fastqc -m picard -m qualimap -m gatk\n    \"\"\"\n}"], "list_proc": ["lifebit-ai/somatic-variant-caller/multiqc"], "list_wf_names": ["lifebit-ai/somatic-variant-caller"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["strelka2"], "list_contrib": ["PhilPalmer", "luisas"], "nb_contrib": 2, "codes": ["\nprocess preprocess_bam{\n\n  tag \"${bam[0]}\"\n  container 'lifebitai/samtools'\n\n\n  input:\n  set val(prefix), file(bam) from bamChannel\n  set file(genome),file(genomefai),file(genomegz),file(genomegzfai),file(genomegzgzi),file(genomedict) from fastaChannel\n\n  output:\n  set file(\"ready/ordered/${bam[0]}\"), file(\"ready/ordered/${bam[0]}.bai\") into completeChannel\n\n  script:\n  \"\"\"\n  PICARD=`which picard.jar`\n  ## Add RG line in case it is missing\n    mkdir ready\n    [[ `samtools view -H ${bam[0]} | grep '@RG' | wc -l`   > 0 ]] && { mv $bam ready; }|| { java -jar  \\$PICARD  AddOrReplaceReadGroups \\\n    I=${bam[0]} O=ready/${bam[0]} RGID=${params.rgid} RGLB=${params.rglb} RGPL=${params.rgpl} RGPU=${params.rgpu} RGSM=${params.rgsm}; }\n  ## Reorder Bam file\n    cd ready; mkdir ordered;  java -jar \\$PICARD  ReorderSam I=${bam[0]} O=ordered/${bam[0]} ALLOW_INCOMPLETE_DICT_CONCORDANCE=true R=../$genome ;\n  ## Index Bam file\n    cd ordered ; samtools index ${bam[0]};\n  \"\"\"\n\n}"], "list_proc": ["lifebit-ai/strelka2/preprocess_bam"], "list_wf_names": ["lifebit-ai/strelka2"]}, {"nb_reuse": 2, "tools": ["SAMtools", "RTREE", "HISAT2"], "nb_own": 2, "list_own": ["likelet", "lifebit-ai"], "nb_wf": 2, "list_wf": ["circPipe", "tinynf"], "list_contrib": ["cgpu", "dengshuang0116", "bioinformatist", "weiqijin", "likelet"], "nb_contrib": 5, "codes": ["\nprocess with_scratch {\n  tag \"${fake_param}-${with_scratch_process_file}\"\n  echo true\n\n  input: \n  file(with_scratch_process_file) from ch_with_scratch_process_file\n  val(fake_param) from ch_fake_param\n\n  script:\n  \"\"\"\n  pwd\n  echo $fake_param\n  ls -l $with_scratch_process_file\n  df -h ~\n  tree -af /tmp\n  \"\"\"\n}", " process RECOUNT_generate_BSJ_Bamfile {\n      tag \"$sampleID\"\n      input:\n            file index from Candidate_circRNA_index.collect()\n            tuple val(sampleID),  file(query_file) from Fastpfiles_recount\n      output:\n            tuple val(sampleID),file(\"${sampleID}_denovo.bam\") into BSJ_mapping_bamfile\n            file \"fileforwaiting.txt\" into Wait_for_hisat2\n      when:\n            run_multi_tools\n      script:\n       if(params.singleEnd){\n            \"\"\"\n             hisat2 -p ${task.cpus} -t -k 1 -x candidate_circRNA_BSJ_flank -U ${query_file} | samtools view -bS  -q 10 -  > ${sampleID}_denovo.bam \n             touch fileforwaiting.txt\n            \"\"\"\n        }else{\n            \"\"\"\n            hisat2 -p ${task.cpus} -t -k 1 -x candidate_circRNA_BSJ_flank -1 ${query_file[0]}  -2 ${query_file[1]} | samtools view -bS -q 10 - > ${sampleID}_denovo.bam \n            touch fileforwaiting.txt\n            \"\"\"\n        }\n    }"], "list_proc": ["lifebit-ai/tinynf/with_scratch", "likelet/circPipe/RECOUNT_generate_BSJ_Bamfile"], "list_wf_names": ["lifebit-ai/tinynf", "likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["RTREE"], "nb_own": 1, "list_own": ["lifebit-ai"], "nb_wf": 1, "list_wf": ["tinynf"], "list_contrib": ["cgpu"], "nb_contrib": 1, "codes": ["\nprocess no_scratch {\n  tag \"${fake_param}-${with_scratch_process_file}\"\n  echo true\n\n  input: \n  file(no_scratch_process_file) from ch_no_scratch_process_file\n  val(fake_param) from ch_very_fake_param\n \n  script:\n  \"\"\"\n  pwd\n  echo $fake_param\n  ls -l $no_scratch_process_file\n  df -h ~\n  tree -af /tmp\n  \"\"\"\n}"], "list_proc": ["lifebit-ai/tinynf/no_scratch"], "list_wf_names": ["lifebit-ai/tinynf"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FastQC", "MultiQC", "ARTIC"], "nb_own": 3, "list_own": ["mpozuelo", "replikation", "likelet"], "nb_wf": 3, "list_wf": ["poreCov", "demultiplexing", "MesKit"], "list_contrib": ["Niinleslie", "likelet", "replikation", "DataSpott", "mpozuelo", "angelovangel", "bwlang", "mpozuelo-flomics", "chenjy327", "Ninomoriaty", "MarieLataretu", "lizheng141026", "LiuJie1117", "RaverJay", "hoelzer", "Wangxin555", "ZIWEI-WONG"], "nb_contrib": 17, "codes": ["\nprocess artic_medaka_custom_bed {\n        label 'artic'\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"\n        publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n\n    input:\n        tuple val(name), path(reads), path(external_scheme), path(primerBed)\n    output:\n        tuple val(name), path(\"*.consensus.fasta\"), emit: fasta\n        tuple val(name), path(\"${name}_mapped_*.primertrimmed.sorted.bam\"), path(\"${name}_mapped_*.primertrimmed.sorted.bam.bai\"), emit: reference_bam\n        tuple val(name), path(\"SNP_${name}.pass.vcf\"), emit: vcf\n        tuple val(name), path(\"${name}.pass.vcf.gz\"), path(\"${name}.coverage_mask.txt.*1.depths\"), path(\"${name}.coverage_mask.txt.*2.depths\"), emit: covarplot\n        tuple val(name), path(\"${name}.trimmed.rg.sorted.bam\"), emit: fullbam\n    script:   \n        \"\"\"\n        # create a new primer dir as input for artic\n        mkdir -p primer_scheme/nCoV-2019\n        cp -r ${external_scheme}/nCoV-2019/V_custom primer_scheme/nCoV-2019/\n\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerBed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > primer_scheme/nCoV-2019/V_custom/nCoV-2019.scheme.bed\n\n        # start artic\n        artic minion    --medaka \\\n                        --medaka-model ${params.medaka_model} \\\n                        --min-depth ${params.min_depth} \\\n                        --normalise 500 \\\n                        --threads ${task.cpus} \\\n                        --scheme-directory primer_scheme \\\n                        --read-file ${reads} \\\n                        nCoV-2019/V_custom ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths primer_scheme/nCoV-2019/V_custom/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"\n        stub:\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\"\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}", " process fastqc {\n     tag \"$sample\"\n     label 'process_medium'\n     publishDir \"${params.outdir}/${run_id}/${lane}/4-fastqc/${sample}\", mode: 'copy',\n     saveAs: { filename ->\n       filename.endsWith(\".zip\") ? \"zips/$filename\" : filename\n     }\n\n     input:\n     set val(sample), path(reads), val(run_id), val(lane) from ch_fastqc\n\n     output:\n     set path(\"*_fastqc.{zip,html}\"), val(run_id), val(lane) into fastqc_results\n\n     script:\n     \"\"\"\n     fastqc --quiet --threads $task.cpus $reads\n     \"\"\"\n   }"], "list_proc": ["replikation/poreCov/artic_medaka_custom_bed", "likelet/MesKit/get_software_versions", "mpozuelo/demultiplexing/fastqc"], "list_wf_names": ["mpozuelo/demultiplexing", "replikation/poreCov", "likelet/MesKit"]}, {"nb_reuse": 11, "tools": ["MED", "SAMtools", "FastTree", "MMseqs", "MultiQC", "FeatureCounts", "Bowtie", "SIDR", "fastPHASE", "G-BLASTN", "GATK"], "nb_own": 9, "list_own": ["sripaladugu", "replikation", "markgene", "mpozuelo", "supark87", "wslh-bio", "zamanianlab", "ralsallaq", "likelet"], "nb_wf": 10, "list_wf": ["cutnrun", "prac_nextflow", "metaGx_nf", "RNAseq-VC-nf", "poreCov", "dryad", "germline_somatic", "minion_hrp2", "exome_mosdepth", "MesKit"], "list_contrib": ["chelauk", "Niinleslie", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "AbigailShockey", "supark87", "sjsabin", "tiagochst", "davidmasp", "drpatelh", "Rotholandus", "replikation", "k-florek", "markgene", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "LiuJie1117", "hoelzer", "sofiahaglund", "DataSpott", "lescai", "ewels", "szilvajuhos", "FriederikeHanssen", "Ninomoriaty", "chuan-wang", "RaverJay", "likelet", "nf-core-bot", "maxulysse", "ggabernet", "mpozuelo", "apeltzer", "winni2k", "angelovangel", "wheelern", "bwlang", "chenjy327", "MarieLataretu", "adrlar", "lizheng141026", "ralsallaq", "Wangxin555", "ZIWEI-WONG"], "nb_contrib": 48, "codes": ["\nprocess assembly_coverage_stats {\n  publishDir \"${params.outdir}/mapping\", mode: 'copy'\n\n  input:\n  file(depth) from assembly_depth_results.collect()\n  file(mapped) from assembly_mapped_results.collect()\n\n  output:\n  file('coverage_stats.tsv') into assembly_mapping_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import glob\n  import os\n  from numpy import median\n  from numpy import average\n  \n  # function for summarizing samtools depth files\n  def summarize_depth(file):\n      # get sample id from file name and set up data list\n      sid = os.path.basename(file).split('.')[0]\n      data = []\n      # open samtools depth file and get depth\n      with open(file,'r') as inFile:\n          for line in inFile:\n              data.append(int(line.strip().split()[2]))\n      # get median and average depth\n      med = int(median(data))\n      avg = int(average(data))\n      # return sample id, median and average depth\n      result = f\"{sid}\\\\t{med}\\\\t{avg}\\\\n\"\n      return result\n  \n  # get all samtools depth files\n  files = glob.glob(\"*.assembly.depth.tsv*\")\n  \n  # summarize samtools depth files\n  results = map(summarize_depth,files)\n  \n  # write results to file\n  with open('coverage_stats.tsv', 'w') as outFile:\n      outFile.write(\"Sample\\\\tMedian Coverage (Mapped to Assembly)\\\\tMean Coverage (Mapped to Assembly)\\\\n\")\n      for result in results:\n          outFile.write(result)\n  \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess maptoreference {\n    container 'supark87/minion'\n\n    publishDir \"$params.output.folder/samfiles/\", pattern: \"*.sam\", mode : \"copy\"\n\n      input:\n  \n        path(ref_dir) from ref_dir\n        file(\"*\") from trim_out1.collect()\n\n\n       \n    output:\n       \n       file(\"bowtie.sam\") into sam\n\n    script:\n        \"\"\"\n     cat *.fq >> all.fq\n     bowtie2-build-s $ref_dir/XM_002808697.2.fasta myIndex\n     bowtie2-align-s -I 0 -X 800 -p 16 --fast --dovetail --met-file bmet.txt -x myIndex -U all.fq -S bowtie.sam\n\n\n        \"\"\"\n\n}", "\nprocess samtools {\n  tag \"$sample\"\n  label 'process_low'\n  publishDir \"${cluster_path}/data/05_QC/${project}/samtools/${sample}\", mode: params.publish_dir_mode\n\n\n  input:\n  set val(sample), val(experiment), path(bam), path(bed), path(interval), val(percentage) from ch_samtools\n\n  output:\n  set val(sample), path(bam), path(\"${bam.baseName}.bam.bai\"), path(\"${bam.baseName}_${percentage}.bam\"), path(\"${bam.baseName}_${percentage}.bam.bai\"), val(experiment), path(bed), path(interval) into ch_mosdepth\n  set val(sample), path(bam), path(\"${bam.baseName}.bam.bai\"), path(\"${bam.baseName}_${percentage}.bam\"), path(\"${bam.baseName}_${percentage}.bam.bai\"), path(interval) into ch_picard_hsmetrics,\n                                                                                                                                                                             ch_picard_alignmentmetrics\n\n  script:\n  subset = \"${bam.baseName}_${percentage}.bam\"\n  \"\"\"\n  samtools index $bam\n  samtools view -s $percentage -b $bam > $subset\n  samtools index $subset\n  \"\"\"\n}", "\nprocess blastrun{\n    container 'my-image-spades'\n\n    tag \"$sample_id\"\n    publishDir \"${params.outdir}/blast_out_files\",mode:'copy'\n    \n    input:\n    path cpmp from params.references1\n    val sample_id_db from blastdb_name \n    path dbdir from blastdb_dir\n\n    output:\n    file (\"${sample_id_db}.blast\") into blastout_ch\n    file(\"${sample_id_db}.fasta\") into sequences\n    val \"${sample_id_db}\" into seq_name\n    file(\"${sample_id_db}.fasta\") into cpmp_all\n    \n    script:\n\n\n    \"\"\"\n    blastn -query $cpmp -db $dbdir/$sample_id_db  -outfmt \"6 qseqid sseqid pident qlen qstart qend sstart send sseq\" -out ${sample_id_db}.blast\n    cat ${sample_id_db}.blast | awk '\\$8-\\$7 > 250 {print \">\" \\$2 \"_${sample_id_db}_\" \"\\\\n\" \\$9}' > ${sample_id_db}.fasta\n    \n    \"\"\"\n}", "\nprocess SortBAM {\n    tag \"$name\"\n    label 'process_medium'\n    if (params.save_align_intermeds) {\n        publishDir path: \"${params.outdir}/bwa/library\", mode: 'copy',\n            saveAs: { filename ->\n                          if (filename.endsWith(\".flagstat\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".idxstats\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".stats\")) \"samtools_stats/$filename\"\n                          else filename\n                    }\n    }\n\n    input:\n    set val(name), file(bam) from ch_bwa_bam\n\n    output:\n    set val(name), file(\"*.sorted.{bam,bam.bai}\") into ch_sort_bam_merge\n    file \"*.{flagstat,idxstats,stats}\" into ch_sort_bam_flagstat_mqc\n\n    script:\n    prefix = \"${name}.Lb\"\n    \"\"\"\n    samtools sort -@ $task.cpus -o ${prefix}.sorted.bam -T $name $bam\n    samtools index ${prefix}.sorted.bam\n    samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n    samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n    samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n    \"\"\"\n}", "\nprocess trim_reads {\n\n    tag \"$id\"\n    publishDir \"${output}/trim_stats/\", mode: 'copy', pattern: '*.html'\n    publishDir \"${output}/trim_stats/\", mode: 'copy', pattern: '*.json'\n\n    input:\n        tuple val(id), file(forward), file(reverse) from input_fq\n\n    output:\n        tuple id, file(\"${id}_R1.fq.gz\"), file(\"${id}_R2.fq.gz\") into trimmed_reads_star1, trimmed_reads_star2, trimmed_reads_picard\n        tuple file(\"*.html\"), file(\"*.json\") into trim_log\n\n    when:\n        params.fq || params.bam\n\n   \"\"\"\n       fastp -i ${forward} -I ${reverse} -o ${id}_R1.fq.gz -O ${id}_R2.fq.gz -y -l 50 -h ${id}.html -j ${id}.json\n   \"\"\"\n}", " process getRepFAA {\n        container \"${container_mmseqs2}\"\n        label 'multithread'\n        publishDir \"${params.outD}/gene_discover/${geneName}/\", mode: 'copy'\n        \n        input:\n        set geneName, file(extFAA) from geneExtFAA_ch\n        \n        output:\n        set geneName, file(\"${geneName}_clstr_rep_seq.fasta\"), file(\"${geneName}_clstr_all_seqs.fasta\"), file(\"${geneName}_clstr_cluster.tsv\") into clustGene_ch\n        \n        \"\"\"\n        mkdir tmp\n        #search the geneExDBFAA (database) for hits for the seedF at probability of substitution of 62\n        #cluster to 80% of sequence identity, and asking for the representative sequence that cover at least 90% of members \n        #and the members cover at least 90% of it, this seems comparable if not better than CD-HIT\n        \n        mmseqs easy-cluster ${extFAA} ${geneName}_clstr tmp --min-seq-id 0.8 -c 0.90 --cov-mode 0 --remove-tmp-files true\n\n        \"\"\"\n    }", "\nprocess ConsensusPeakSetDESeq {\n    tag \"${antibody}\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/bwa/mergedLibrary/macs/${PEAK_TYPE}/consensus/${antibody}/deseq2\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.endsWith(\".igv.txt\")) null\n                      else filename\n                }\n\n    when:\n    params.macs_gsize && replicatesExist && multipleGroups && !params.skip_diff_analysis\n\n    input:\n    set val(antibody), val(replicatesExist), val(multipleGroups), file(bams) ,file(saf) from ch_group_bam_deseq\n    file deseq2_pca_header from ch_deseq2_pca_header\n    file deseq2_clustering_header from ch_deseq2_clustering_header\n\n    output:\n    file \"*featureCounts.txt\" into ch_macs_consensus_counts\n    file \"*featureCounts.txt.summary\" into ch_macs_consensus_counts_mqc\n    file \"*.{RData,results.txt,pdf,log}\" into ch_macs_consensus_deseq_results\n    file \"sizeFactors\" into ch_macs_consensus_deseq_factors\n    file \"*vs*/*.{pdf,txt}\" into ch_macs_consensus_deseq_comp_results\n    file \"*vs*/*.bed\" into ch_macs_consensus_deseq_comp_bed\n    file \"*igv.txt\" into ch_macs_consensus_deseq_comp_igv\n    file \"*.tsv\" into ch_macs_consensus_deseq_mqc\n\n    script:\n    prefix = \"${antibody}.consensus_peaks\"\n    bam_files = bams.findAll { it.toString().endsWith('.bam') }.sort()\n    bam_ext = params.single_end ? \".mLb.clN.sorted.bam\" : \".mLb.clN.bam\"\n    pe_params = params.single_end ? '' : \"-p --donotsort\"\n    \"\"\"\n    featureCounts \\\\\n        -F SAF \\\\\n        -O \\\\\n        --fracOverlap 0.2 \\\\\n        -T $task.cpus \\\\\n        $pe_params \\\\\n        -a $saf \\\\\n        -o ${prefix}.featureCounts.txt \\\\\n        ${bam_files.join(' ')}\n\n    featurecounts_deseq2.r -i ${prefix}.featureCounts.txt -b '$bam_ext' -o ./ -p $prefix -s .mLb\n\n    sed 's/deseq2_pca/deseq2_pca_${task.index}/g' <$deseq2_pca_header >tmp.txt\n    sed -i -e 's/DESeq2:/${antibody} DESeq2:/g' tmp.txt\n    cat tmp.txt ${prefix}.pca.vals.txt > ${prefix}.pca.vals_mqc.tsv\n\n    sed 's/deseq2_clustering/deseq2_clustering_${task.index}/g' <$deseq2_clustering_header >tmp.txt\n    sed -i -e 's/DESeq2:/${antibody} DESeq2:/g' tmp.txt\n    cat tmp.txt ${prefix}.sample.dists.txt > ${prefix}.sample.dists_mqc.tsv\n\n    find * -type f -name \"*.FDR0.05.results.bed\" -exec echo -e \"bwa/mergedLibrary/macs/${PEAK_TYPE}/consensus/${antibody}/deseq2/\"{}\"\\\\t255,0,0\" \\\\; > ${prefix}.igv.txt\n    \"\"\"\n}", "process fasttree {\n    label \"fasttree\"\n    publishDir \"${params.output}/tree/\", mode: 'copy', pattern: \"clean.core.tree.nwk\"\n    input:\n        path(clean_core_alignment)\n    output:\n        path(\"clean.core.tree.nwk\")\n    script:\n        \"\"\"\n        FastTree -gtr -nt ${clean_core_alignment} > clean.core.tree.nwk\n        \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}"], "list_proc": ["wslh-bio/dryad/assembly_coverage_stats", "sripaladugu/germline_somatic/PileupSummariesForMutect2", "supark87/minion_hrp2/maptoreference", "mpozuelo/exome_mosdepth/samtools", "supark87/prac_nextflow/blastrun", "markgene/cutnrun/SortBAM", "zamanianlab/RNAseq-VC-nf/trim_reads", "ralsallaq/metaGx_nf/getRepFAA", "markgene/cutnrun/ConsensusPeakSetDESeq", "replikation/poreCov/fasttree", "likelet/MesKit/multiqc"], "list_wf_names": ["sripaladugu/germline_somatic", "markgene/cutnrun", "mpozuelo/exome_mosdepth", "ralsallaq/metaGx_nf", "supark87/minion_hrp2", "zamanianlab/RNAseq-VC-nf", "replikation/poreCov", "supark87/prac_nextflow", "wslh-bio/dryad", "likelet/MesKit"]}, {"nb_reuse": 2, "tools": ["SAMtools", "HISAT2", "MultiQC"], "nb_own": 2, "list_own": ["loipf", "likelet"], "nb_wf": 2, "list_wf": ["circPipe", "DNAseq-pipeline"], "list_contrib": ["loipf", "dengshuang0116", "bioinformatist", "weiqijin", "likelet"], "nb_contrib": 5, "codes": ["\nprocess MULTIQC_PREPRO { \n\tpublishDir \"$params.data_dir/quality_reports\", mode: \"copy\"\n\n\tinput:\n\t\tpath stat_files\n\n\toutput:\n\t\tpath \"*\"\n\n\tshell:\n\t'''\n\tmultiqc -f -o reads_prepro .\n\t'''\n}", " process Recount_generate_BSJ_Bamfile {\n      tag \"$sampleID\"\n      input:\n            file index from Candidate_circRNA_index.collect()\n            tuple val(sampleID),  file(query_file) from Fastpfiles_recount\n      output:\n            tuple val(sampleID),file(\"${sampleID}_denovo.bam\") into BSJ_mapping_bamfile\n            file \"fileforwaiting.txt\" into Wait_for_hisat2\n      when:\n            run_multi_tools\n      script:\n       if(params.singleEnd){\n            \"\"\"\n             hisat2 -p ${task.cpus} -t -k 1 -x candidate_circRNA_BSJ_flank -U ${query_file} | samtools view -bS  -q 10 -  > ${sampleID}_denovo.bam \n             touch fileforwaiting.txt\n            \"\"\"\n        }else{\n            \"\"\"\n            hisat2 -p ${task.cpus} -t -k 1 -x candidate_circRNA_BSJ_flank -1 ${query_file[0]}  -2 ${query_file[1]} | samtools view -bS -q 10 - > ${sampleID}_denovo.bam \n            touch fileforwaiting.txt\n            \"\"\"\n        }\n    }"], "list_proc": ["loipf/DNAseq-pipeline/MULTIQC_PREPRO", "likelet/circPipe/Recount_generate_BSJ_Bamfile"], "list_wf_names": ["likelet/circPipe", "loipf/DNAseq-pipeline"]}, {"nb_reuse": 2, "tools": ["SAMtools", "HISAT2"], "nb_own": 2, "list_own": ["likelet", "loipf"], "nb_wf": 2, "list_wf": ["circPipe", "DNAsmash-pipeline"], "list_contrib": ["loipf", "dengshuang0116", "bioinformatist", "weiqijin", "likelet"], "nb_contrib": 5, "codes": [" process Recount_generate_genome_Bamfile {\n      tag \"$sampleID\"\n      input:\n            file index from hisat2_index.collect()\n            tuple val(sampleID),  file(query_file) from Fastpfiles_hisat\n            file filewait from Wait_for_hisat2\n      output:\n            tuple val(sampleID),file(\"${sampleID}.bam\") into Genome_remapping_bamfile\n      when:\n            run_multi_tools\n      script:\n      index_base = index[0].toString() - ~/.\\d.ht2/\n       if(params.singleEnd){\n            \"\"\"\n             hisat2 -p ${task.cpus} -t -k 1 -x ${index_base} -U ${query_file} | samtools view -bS  -q 10 -  > ${sampleID}.bam \n            \"\"\"\n        }else{\n            \"\"\"\n            hisat2 -p ${task.cpus} -t -k 1 -x ${index_base} -1 ${query_file[0]}  -2 ${query_file[1]} | samtools view -bS -q 10 - > ${sampleID}.bam \n            \"\"\"\n        }\n    }", "\nprocess MAPPING_URMAP { \n\ttag \"$sample_folder\"\n\n\tinput:\n\t\tpath sample_folder\n\t\tval num_threads\n\t\tpath urmap_index\n\n\toutput:\n\t\ttuple path(\"*.bam\"), path(\"*.bam.bai\"), emit: reads_mapped\n\n\tshell:\n\t'''\n\t\n\t### theoretically sorting not needed but maybe speeds up things\n\treads_sorted=$(ls -d !{sample_folder}/* | xargs -n1 | sort | xargs)\n\t\n\t### combine multiple seq files in the same sample directory with same direction together\n\treads_sorted_1=$(find $reads_sorted -name \"*_1.fq.gz\" -o -name \"*_1.fastq.gz\")\n\treads_sorted_2=$(find $reads_sorted -name \"*_2.fq.gz\" -o -name \"*_2.fastq.gz\")\n\t\n\t# TODO: make cat copy operation optional, reads < urmap index size lead to error\n\tcat $reads_sorted_1 | seqkit seq --min-len 10 - -j !{num_threads} -o raw_reads_connected_1.fastq.gz\n\tcat $reads_sorted_2 | seqkit seq --min-len 10 - -j !{num_threads} -o raw_reads_connected_2.fastq.gz\n\n\t#/usr/src/urmap -veryfast -threads !{num_threads} -ufi !{urmap_index} -map2 raw_reads_connected_1.fastq.gz -reverse raw_reads_connected_2.fastq.gz -samout - \\\n\t#| samtools view -b -@ !{num_threads} - \\\n\t#| samtools sort -@ !{num_threads} > !{sample_folder}.bam\n\t\n\t/usr/src/urmap -veryfast -threads !{num_threads} -ufi !{urmap_index} -map2 raw_reads_connected_1.fastq.gz -reverse raw_reads_connected_2.fastq.gz -samout !{sample_folder}.sam\n\t\n\tsamtools view -b -@ !{num_threads} !{sample_folder}.sam | samtools sort -@ !{num_threads} > !{sample_folder}.bam\n\tsamtools index -b -@ !{num_threads} !{sample_folder}.bam\n\t\n\n\t'''\n}"], "list_proc": ["likelet/circPipe/Recount_generate_genome_Bamfile", "loipf/DNAsmash-pipeline/MAPPING_URMAP"], "list_wf_names": ["likelet/circPipe", "loipf/DNAsmash-pipeline"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process makeSTARindex {\n            storeDir \"${params.outdir}/reference_genome\"\n\n            input:\n            file genomefile\n            file gtffile\n\n            output:\n            file \"starindex\" into starindex\n\n            script:\n            \"\"\"\n            mkdir starindex\n            STAR \\\n                --runMode genomeGenerate \\\n                --runThreadN ${task.cpus} \\\n                --sjdbGTFfile ${gtffile} \\\n                --genomeDir starindex/ \\\n                --genomeFastaFiles ${genomefile} \\\n                --sjdbOverhang 149\n            \"\"\"\n        }"], "list_proc": ["likelet/circPipe/makeSTARindex"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 2, "tools": ["FastQC", "Bowtie"], "nb_own": 2, "list_own": ["loipf", "likelet"], "nb_wf": 2, "list_wf": ["circPipe", "DNAseq-pipeline"], "list_contrib": ["loipf", "dengshuang0116", "bioinformatist", "weiqijin", "likelet"], "nb_contrib": 5, "codes": ["\nprocess FASTQC_READS_RAW { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/reads_raw\", mode: \"copy\", overwrite: false, saveAs: { filename -> \"${sample_id}/$filename\" }\n\t                                                          \n\tcache false\n\t                        \n\n\tinput:\n\t\ttuple val(sample_id), path(reads) \n\t\tval num_threads\n\t\tpath adapter_3_seq_file\n\t\tpath adapter_5_seq_file\n\n\toutput:\n\t\tpath \"*.zip\", emit: reports\n\t\tpath \"*.html\"\n\n\tshell:\n\t'''\n\t#if [[ !{adapter_5_seq_file} != \"NO_FILE\" ]]\n\t#then\n\t#  cat !{adapter_3_seq_file} !{adapter_5_seq_file} > adapter_both.fasta\n\t#else\n\t#  cat !{adapter_3_seq_file} > adapter_both.fasta \n\t#fi\n\t\n\t#TODO make nicer way of handling this instead of ignoring error\n\t#cat !{adapter_3_seq_file} !{adapter_5_seq_file} 1>adapter_both.fasta 2>error_if_one_missing\n\t\n\tcat !{adapter_3_seq_file} !{adapter_5_seq_file} > adapter_both.fasta\n\tawk 'BEGIN{RS=\">\";OFS=\"\\t\"}NR>1{print $1,$2}' adapter_both.fasta > adapter_both.txt\n\n\tfastqc -a adapter_both.txt -t !{num_threads} --noextract !{reads}\n\t'''\n}", " process makeBowtie2index {\n             storeDir \"${params.outdir}/reference_genome\"\n\n            input:\n            file genomefile\n\n\n            output:\n            file \"*.bt2\" into Bowtie2index, Bowtie2index_fc\n\n            script:\n            \"\"\"\n            bowtie2-build -f ${genomefile} genome\n            \"\"\"\n            \n        }"], "list_proc": ["loipf/DNAseq-pipeline/FASTQC_READS_RAW", "likelet/circPipe/makeBowtie2index"], "list_wf_names": ["likelet/circPipe", "loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process makeBowtieindex {\n            storeDir \"${params.outdir}/reference_genome\"\n\n            input:\n            file genomefile\n\n            output:\n            file \"*.ebwt\" into Bowtieindex\n\n            script:\n            \"\"\"\n            bowtie-build ${genomefile} genome\n            \"\"\"\n        }"], "list_proc": ["likelet/circPipe/makeBowtieindex"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process makeBWAindex {\n            storeDir \"${params.outdir}/reference_genome\"\n\n            input:\n            file genomefile\n\n\n            output:\n            file \"*.{ann,amb,pac,bwt,sa}\" into bwaindex\n\n            script:\n            \"\"\"\n             bwa index -p genome ${genomefile} \n            \"\"\"\n            bowtie_run_index=\"genome\"\n        }"], "list_proc": ["likelet/circPipe/makeBWAindex"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process RUN_FASTP{\n        tag \"$sampleID\"\n        publishDir \"${params.outdir}/QC\", mode: 'copy', pattern: \"*_fastpreport.html\", overwrite: true\n\n        input:\n        tuple val(sampleID),  file(query_file) from Read_pairs_fastp\n\n        output:\n        tuple val(sampleID),  file ('unzip_fastp_*') into (Fastqfor_swhich,Fastpfiles_mapsplice,Fastpfiles_bwa,Fastpfiles_star,Fastpfiles_segemehl,Fastpfiles_bowtie2,Fastpfiles_recount,Fastpfiles_for_sailfish,Fastpfiles_hisat)\n        file ('*.html') into fastp_for_waiting\n        file ('*_fastp.json') into Fastp_for_multiqc\n\n       \n        \n        script:\n        if(params.singleEnd){\n            \"\"\"\n            fastp \\\n            -i ${query_file} \\\n            -o unzip_fastp_${sampleID}.fq \\\n            -h ${sampleID}_fastpreport.html \\\n            -j ${sampleID}_fastp.json\n            \"\"\"\n        }else{\n            \"\"\"\n            fastp \\\n            -i ${query_file[0]} \\\n            -I ${query_file[1]} \\\n            -o unzip_fastp_${sampleID}_1.fq \\\n            -O unzip_fastp_${sampleID}_2.fq \\\n            -h ${sampleID}_fastpreport.html \\\n            -j ${sampleID}_fastp.json \n            \"\"\"\n        }\n\n\n    }"], "list_proc": ["likelet/circPipe/RUN_FASTP"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["annotate", "CIRCexplorer2"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process RUN_Circexplorer2{\n        tag \"$sampleID\"\n        publishDir \"${params.outdir}/circRNA_Identification/CIRCexplorer2\", mode: 'copy', overwrite: true\n\n        input:\n        tuple val(sampleID),  file (query_file) from starfiles\n        file annotationfile\n        file genomefile\n\n        output:\n        tuple val(sampleID),  file ('*known.txt') into circexplorer2files\n\n\n\n        script:\n        \"\"\"\n            CIRCexplorer2 \\\n            parse -t STAR ${query_file} \\\n            > CIRCexplorer2_parse_${sampleID}.log\n\n            CIRCexplorer2 \\\n            annotate -r ${annotationfile} \\\n            -g ${genomefile} \\\n            -b back_spliced_junction.bed \\\n            -o CIRCexplorer2_${sampleID}_circularRNA_known.txt \\\n            > CIRCexplorer2_annotate_${sampleID}.log\n            \"\"\"\n    }"], "list_proc": ["likelet/circPipe/RUN_Circexplorer2"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process BWA_and_CIRI{\n        tag \"$sampleID\"\n        publishDir \"${params.outdir}/circRNA_Identification/CIRI\", pattern: \"*.txt\",mode: 'copy', overwrite: true\n\n        input:\n        tuple val(sampleID),  file (query_file) from Fastpfiles_bwa\n        file index from bwaindex.collect()\n        file genomefile\n        file gtffile\n\n\n        output:\n        tuple val(sampleID),  file ('*.txt') into cirifiles\n\n        shell:\n        \n        if(params.singleEnd){\n            \"\"\"\n           bwa mem -t ${task.cpus} -T 19 -M -R \"@RG\\\\tID:fastp_${sampleID}\\\\tPL:PGM\\\\tLB:noLB\\\\tSM:fastp_${sampleID}\" ${bowtie_run_index} ${query_file[0]} > ${sampleID}.sam \n\n\n            CIRI2.pl \\\n            -T 10 \\\n            -F ${genomefile} \\\n            -A ${gtffile} \\\n            -G CIRI_${sampleID}.log \\\n            -I ${sampleID}.sam \\\n            -O CIRI_${sampleID}.txt \\\n            > CIRI_${sampleID}_detail.log\n\n           \n\n\n             rm ${sampleID}.sam\n\n            \"\"\"\n        }else{\n            \"\"\"\n    \n            bwa mem -t ${task.cpus} -T 19 -M -R \"@RG\\\\tID:fastp_${sampleID}\\\\tPL:PGM\\\\tLB:noLB\\\\tSM:fastp_${sampleID}\" ${bowtie_run_index} ${query_file[0]} ${query_file[1]} > ${sampleID}.sam \n            CIRI2.pl \\\n            -T 10 \\\n            -F ${genomefile}  \\\n            -A ${gtffile} \\\n            -G CIRI_${sampleID}.log \\\n            -I ${sampleID}.sam \\\n            -O CIRI_${sampleID}.txt \\\n            > CIRI_${sampleID}_detail.log\n\n             # bwa sam to bam file \n           \n            rm ${sampleID}.sam\n            \"\"\"\n        }\n\n    }"], "list_proc": ["likelet/circPipe/BWA_and_CIRI"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process RUN_BOWTIE2{\n        tag \"$sampleID\"\n        publishDir \"${params.outdir}/Alignment/Bowtie2\", pattern: \"*.log\", mode: 'link', overwrite: true\n\n        input:\n        tuple val(sampleID),file (query_file) from Fastpfiles_bowtie2\n        file index from Bowtie2index.collect()\n\n        output:\n        tuple val(sampleID), file ('bowtie2_unmapped_*') into Bowtie2files\n        tuple val(sampleID), file ('bowtie2_unmapped_*') into Bowtie2files_for_autocirc\n        file ('*.log') into Bowtie2_multiqc\n\n\n        shell:\n        if(params.singleEnd){\n            \"\"\"\n            bowtie2 \\\n            -p ${task.cpus} \\\n            --very-sensitive \\\n            --score-min=C,-15,0 \\\n            --mm \\\n            -x ${bowtie2_run_index} \\\n            -q \\\n            -U ${query_file} 2> bowtie2_${sampleID}.log \\\n            | samtools view -hbuS - \\\n            | samtools sort - > bowtie2_output_${sampleID}.bam\n\n            samtools \\\n            view -hf 4 bowtie2_output_${sampleID}.bam \\\n            | samtools view -Sb - \\\n            > bowtie2_unmapped_${sampleID}.bam\n            rm  bowtie2_output_${sampleID}.bam\n            \"\"\"\n        }else{\n            \"\"\"\n            bowtie2 \\\n            -p ${task.cpus} \\\n            --very-sensitive \\\n            --score-min=C,-15,0 \\\n            --mm \\\n            -x ${bowtie2_run_index} \\\n            -q \\\n            -1 ${query_file[0]} \\\n            -2 ${query_file[1]} 2> bowtie2_${sampleID}.log \\\n            | samtools view -hbuS - \\\n            | samtools sort - > bowtie2_output_${sampleID}.bam\n\n            samtools \\\n            view -hf 4 bowtie2_output_${sampleID}.bam \\\n            | samtools view -Sb - \\\n            > bowtie2_unmapped_${sampleID}.bam\n\n            rm  bowtie2_output_${sampleID}.bam\n            \"\"\"\n        }\n\n    }"], "list_proc": ["likelet/circPipe/RUN_BOWTIE2"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": ["\nprocess RUN_MULTIQC{\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy', pattern: \"*.html\", overwrite: true\n\n    input:\n    file (query_file) from Fastp_for_multiqc.concat( Star_multiqc, Bowtie2_multiqc ).collect()\n\n    output:\n    file ('*.html') into Multiqc_results\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["likelet/circPipe/RUN_MULTIQC"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process getPsudoCircSequence_And_BuildHisatIndex {\n      input:\n           file (bed_file) from Bed_for_recount\n           file genomefile\n           file faifile \n      output:\n           file \"*.ht2\" into Candidate_circRNA_index\n      script:\n      \"\"\"\n      # extract bed file for obtaining seqeuence\n      sh ${baseDir}/bin/ProcessBedforGettingSequence.sh ${bed_file} temp.sort.bed temp.start.bed temp.end.bed\n\n      bedtools getfasta -name -fi ${genomefile} -s -bed temp.start.bed > temp.start.fa\n      bedtools getfasta -name -fi ${genomefile} -s -bed temp.end.bed > temp.end.fa\n      # circRNA <= 400 bp\n      bedtools getfasta -name -fi ${genomefile} -s -bed temp.sort.bed > temp.sort.fa \n\n      # merge and get combined fasta formatted psudoCirc sequences\n      sh ${baseDir}/bin/MergeBSJsequence.sh temp.sort.fa temp.start.fa temp.end.fa tmp_candidate.circular_BSJ_flank.fa\n\n      hisat2-build -p ${task.cpus}  tmp_candidate.circular_BSJ_flank.fa candidate_circRNA_BSJ_flank \n      \n      \"\"\"\n    }"], "list_proc": ["likelet/circPipe/getPsudoCircSequence_And_BuildHisatIndex"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process RECOUNT_generate_genome_Bamfile {\n      tag \"$sampleID\"\n      input:\n            file index from hisat2_index.collect()\n            tuple val(sampleID),  file(query_file) from Fastpfiles_hisat\n            file filewait from Wait_for_hisat2\n      output:\n            tuple val(sampleID),file(\"${sampleID}.bam\") into Genome_remapping_bamfile, Genome_remapping_bamfile_for_mRNAcounting\n      when:\n            run_multi_tools\n      script:\n      index_base = index[0].toString() - ~/.\\d.ht2/\n       if(params.singleEnd){\n            \"\"\"\n             hisat2 -p ${task.cpus} -t -k 1 -x ${index_base} -U ${query_file} --dta | samtools view -bS  -q 10 -  > ${sampleID}.bam \n            \"\"\"\n        }else{\n            \"\"\"\n            hisat2 -p ${task.cpus} -t -k 1 -x ${index_base} -1 ${query_file[0]}  -2 ${query_file[1]} --dta | samtools view -bS -q 10 - > ${sampleID}.bam \n            \"\"\"\n        }\n    }"], "list_proc": ["likelet/circPipe/RECOUNT_generate_genome_Bamfile"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["likelet"], "nb_wf": 1, "list_wf": ["circPipe"], "list_contrib": ["likelet", "dengshuang0116", "bioinformatist", "weiqijin"], "nb_contrib": 4, "codes": [" process mRNAmeasurementByFeatureCount {\n      input:\n        file gtffile\n        tuple val(sampleID),file(bamfile)  from  Genome_remapping_bamfile_for_mRNAcounting\n      output:\n        tuple val(sampleID),file(\"${sampleID}.gene.count\") into Feature_count_out\n      script:\n      \"\"\"\n      \n        featureCounts -a $gtf -T ${task.cpus} -p -t gene -g gene_id -o ${sampleID}.gene.count ${bamfile}\n\n      \"\"\"\n    }"], "list_proc": ["likelet/circPipe/mRNAmeasurementByFeatureCount"], "list_wf_names": ["likelet/circPipe"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["lisemangiante"], "nb_wf": 1, "list_wf": ["test_nextflow"], "list_contrib": ["mfoll", "lisemangiante"], "nb_contrib": 2, "codes": ["\nprocess coverage {\n\n    input:\n    file bam\n    file bed  \n      \n    output:\n    file 'coverage.txt' into coverage\n        \n    shell:\n    '''\n    bedtools coverage -d -a !{bed} -b !{bam} > coverage.txt\n    '''\n}"], "list_proc": ["lisemangiante/test_nextflow/coverage"], "list_wf_names": ["lisemangiante/test_nextflow"]}, {"nb_reuse": 1, "tools": ["Picard", "GATK"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess SELECT_MITO_READS {\n    label \"human_mito\"\n\n    input:\n        tuple val(sample_id), path(whole_bam), path(whole_bai)\n        path fasta\n        path dict\n        path index\n\n    output:\n        tuple val(sample_id), path(\"${sample_id}.mito.unaligned.bam\")\n\n    script:\n    \"\"\"\n    gatk PrintReads \\\n        -R $fasta \\\n        -L \"chrM\" \\\n        --read-filter MateOnSameContigOrNoMappedMateReadFilter \\\n        --read-filter MateUnmappedAndUnmappedReadFilter \\\n        -I $whole_bam \\\n        -O mito.bam\n\n    picard RevertSam \\\n        INPUT=mito.bam \\\n        OUTPUT_BY_READGROUP=false \\\n        OUTPUT=${sample_id}.mito.unaligned.bam \\\n        VALIDATION_STRINGENCY=LENIENT \\\n        ATTRIBUTE_TO_CLEAR=FT \\\n        ATTRIBUTE_TO_CLEAR=CO \\\n        SORT_ORDER=queryname \\\n        RESTORE_ORIGINAL_QUALITIES=false\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/SELECT_MITO_READS"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["Picard", "BWA"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess BWA_ALIGN_FROM_UBAM {\n    label \"human_mito\"\n\n    input:\n        tuple val(sample_id), path(ubam)\n        path fasta\n        path dict\n        path index\n        path amb\n        path ann\n        path bwt\n        path pac\n        path sa\n        path ref_alt\n    output:\n        tuple val(sample_id), path(\"${sample_id}.sorted.bam\"), path(\"${sample_id}.sorted.bai\")\n    \n    shell:\n    \"\"\"\n    picard SamToFastq \\\n            INPUT=!{ubam} \\\n            FASTQ=/dev/stdout \\\n            INTERLEAVE=true \\\n            NON_PF=true | \\\n        bwa mem -K 100000000 -p -v 3 -t 5 -Y !{fasta} /dev/stdin - 2> >(tee !{sample_id}.bwa.stderr.log >&2) | \\\n        picard MergeBamAlignment \\\n            VALIDATION_STRINGENCY=SILENT \\\n            EXPECTED_ORIENTATIONS=FR \\\n            ATTRIBUTES_TO_RETAIN=X0 \\\n            ATTRIBUTES_TO_REMOVE=NM \\\n            ATTRIBUTES_TO_REMOVE=MD \\\n            ALIGNED_BAM=/dev/stdin \\\n            UNMAPPED_BAM=!{ubam} \\\n            OUTPUT=!{sample_id}.temp.bam \\\n            REFERENCE_SEQUENCE=!{fasta} \\\n            SORT_ORDER=\"unsorted\" \\\n            IS_BISULFITE_SEQUENCE=false \\\n            ALIGNED_READS_ONLY=false \\\n            CLIP_ADAPTERS=false \\\n            MAX_RECORDS_IN_RAM=2000000 \\\n            ADD_MATE_CIGAR=true \\\n            MAX_INSERTIONS_OR_DELETIONS=-1 \\\n            PRIMARY_ALIGNMENT_STRATEGY=MostDistant \\\n            UNMAPPED_READ_STRATEGY=COPY_TO_TAG \\\n            ALIGNER_PROPER_PAIR_FLAGS=true \\\n            UNMAP_CONTAMINANT_READS=true \\\n            ADD_PG_TAG_TO_READS=false\n\n    picard SortSam \\\n        INPUT=\"!{sample_id}.temp.bam\" \\\n        OUTPUT=\"!{sample_id}.sorted.bam\" \\\n        SORT_ORDER=\"coordinate\" \\\n        CREATE_INDEX=true \\\n        MAX_RECORDS_IN_RAM=300000\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/BWA_ALIGN_FROM_UBAM"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["Picard", "BWA"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess BWA_ALIGN {\n    label \"human_mito\"\n\n    input:\n        tuple val(sampleId), path(input_bam)\n        path mito_fasta\n        path mito_dict\n        path mito_index\n        path mito_amb\n        path mito_ann\n        path mito_bwt\n        path mito_pac\n        path mito_sa\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.bam\"), \\\n            path(\"${sampleId}.bai\"), \\\n            path(\"${sampleId}.dup.metrics\")\n\n    shell:\n    \"\"\"\n    picard SamToFastq \\\n      INPUT=!{input_bam} \\\n      FASTQ=/dev/stdout \\\n      INTERLEAVE=true \\\n      NON_PF=true | \\\n    bwa mem -K 100000000 -p -v 3 -t 2 -Y !{mito_fasta} /dev/stdin - 2> >(tee !{sampleId}.bwa.stderr.log >&2) | \\\n    picard MergeBamAlignment \\\n      VALIDATION_STRINGENCY=SILENT \\\n      EXPECTED_ORIENTATIONS=FR \\\n      ATTRIBUTES_TO_RETAIN=X0 \\\n      ATTRIBUTES_TO_REMOVE=NM \\\n      ATTRIBUTES_TO_REMOVE=MD \\\n      ALIGNED_BAM=/dev/stdin \\\n      UNMAPPED_BAM=!{input_bam} \\\n      OUTPUT=mba.bam \\\n      REFERENCE_SEQUENCE=!{mito_fasta} \\\n      PAIRED_RUN=true \\\n      SORT_ORDER=\"unsorted\" \\\n      IS_BISULFITE_SEQUENCE=false \\\n      ALIGNED_READS_ONLY=false \\\n      CLIP_ADAPTERS=false \\\n      MAX_RECORDS_IN_RAM=2000000 \\\n      ADD_MATE_CIGAR=true \\\n      MAX_INSERTIONS_OR_DELETIONS=-1 \\\n      PRIMARY_ALIGNMENT_STRATEGY=MostDistant \\\n      UNMAPPED_READ_STRATEGY=COPY_TO_TAG \\\n      ALIGNER_PROPER_PAIR_FLAGS=true \\\n      UNMAP_CONTAMINANT_READS=true \\\n      ADD_PG_TAG_TO_READS=false\n\n    picard MarkDuplicates \\\n      INPUT=mba.bam \\\n      OUTPUT=md.bam \\\n      METRICS_FILE=!{sampleId}.dup.metrics \\\n      VALIDATION_STRINGENCY=SILENT \\\n      OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500 \\\n      ASSUME_SORT_ORDER=\"queryname\" \\\n      CLEAR_DT=\"false\" \\\n      ADD_PG_TAG_TO_READS=false\n\n    picard SortSam \\\n      INPUT=md.bam \\\n      OUTPUT=!{sampleId}.bam \\\n      SORT_ORDER=\"coordinate\" \\\n      CREATE_INDEX=true \\\n      MAX_RECORDS_IN_RAM=300000\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/BWA_ALIGN"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess COLLECT_ALIGNMENT_METRICS {\n    label \"human_mito\"\n\n    input:\n        tuple val(sample_id), path(bam), path(bai), path(dup_stats)\n        path reference\n        path reference_dict\n        path reference_index\n    \n    output:\n        tuple val(sample_id), path(\"${sample_id}.algn_metrics.txt\")\n\n    script:\n    \"\"\"\n    picard CollectAlignmentSummaryMetrics \\\n          R=$reference \\\n          I=$bam \\\n          O=${sample_id}.algn_metrics.txt\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/COLLECT_ALIGNMENT_METRICS"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess COLLECT_WGS_METRICS {\n    label \"human_mito\"\n\n    input:\n        tuple val(sample_id), path(bam), path(bai), path(dup_stats)\n        path reference\n        val readLen\n    output:\n        tuple val(sample_id), \\\n            path(\"${sample_id}.theoretical_sensitivity.txt\"), \\\n            path(\"${sample_id}.metrics.txt\")\n\n    script:\n    \"\"\"\n    picard CollectWgsMetrics \\\n        INPUT=${bam} \\\n        VALIDATION_STRINGENCY=SILENT \\\n        REFERENCE_SEQUENCE=${reference} \\\n        OUTPUT=${sample_id}.metrics.txt \\\n        USE_FAST_ALGORITHM=true \\\n        READ_LENGTH=${readLen} \\\n        INCLUDE_BQ_HISTOGRAM=true \\\n        COVERAGE_CAP=100000 \\\n        THEORETICAL_SENSITIVITY_OUTPUT=${sample_id}.theoretical_sensitivity.txt\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/COLLECT_WGS_METRICS"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess CALL_MUTECT {\n    label \"human_mito_mutect\"\n    clusterOptions \"-C avx2\"\n\n    input:\n        tuple val(sample_id), path(bam), path(bai), path(dup_stats)\n        path mito_fasta\n        path mito_dict\n        path mito_index\n        val prefix\n        val mutect_extra_args\n    output:\n        tuple val(sample_id), \\\n            path(\"${prefix}.${sample_id}.vcf.gz\"), \\\n            path(\"${prefix}.${sample_id}.vcf.gz.tbi\"), \\\n            path(\"${prefix}.${sample_id}.vcf.gz.stats\")\n\n    script:\n    \"\"\"\n    gatk Mutect2 \\\n        -R ${mito_fasta} \\\n        -I ${bam} \\\n        --read-filter MateOnSameContigOrNoMappedMateReadFilter \\\n        --read-filter MateUnmappedAndUnmappedReadFilter \\\n        -O \"${prefix}.${sample_id}.vcf.gz\" \\\n        ${mutect_extra_args} \\\n        --annotation StrandBiasBySample \\\n        --mitochondria-mode \\\n        --max-reads-per-alignment-start 75 \\\n        --max-mnp-distance 0\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/CALL_MUTECT"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess MERGE_STATS {\n    label \"human_mito\"\n\n    input:\n        tuple \\\n            val(sample_id), \\\n            path(standard_vcf), \\\n            path(standard_tbi), \\\n            path(stantard_stats), \\\n            path(shifted_vcf), \\\n            path(shifted_tbi), \\\n            path(shifted_stats)\n\n    output:\n        tuple val(sample_id), path(\"combined.stats\")\n    \n    script:\n    \"\"\"\n    gatk MergeMutectStats \\\n        --stats ${shifted_stats} \\\n        --stats ${stantard_stats} \\\n        -O combined.stats\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/MERGE_STATS"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess LIFTOVER_AND_COMBINE_VCFS {\n    label \"human_mito\"\n    input:\n        tuple \\\n            val(sample_id), \\\n            path(standard_vcf), \\\n            path(standard_tbi), \\\n            path(stantard_stats), \\\n            path(shifted_vcf), \\\n            path(shifted_tbi), \\\n            path(shifted_stats)\n        path mito_fasta\n        path mito_fasta_index\n        path mito_dict\n        path shift_back_chain\n\n    output:\n        tuple val(sample_id), \\\n            path(\"${sample_id}.rejected.vcf\"), \\\n            path(\"${sample_id}.merged.vcf\"), \\\n            path(\"${sample_id}.merged.vcf.idx\")\n\n    script:\n    \"\"\"\n    picard LiftoverVcf \\\n      I=${shifted_vcf} \\\n      O=${sample_id}.shifted_back.vcf \\\n      R=${mito_fasta} \\\n      CHAIN=${shift_back_chain} \\\n      REJECT=${sample_id}.rejected.vcf\n\n    picard MergeVcfs \\\n      I=${sample_id}.shifted_back.vcf \\\n      I=${standard_vcf} \\\n      O=${sample_id}.merged.vcf\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/LIFTOVER_AND_COMBINE_VCFS"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess FILTER {\n    label \"human_mito\"\n    input:\n        tuple val(sample_id), \\\n            path(rejected_vcf), \\\n            path(merged_vcf), \\\n            path(merged_vcf_index), \\\n            path(combined_stats)\n        path mito_fasta\n        path mito_index\n        path mito_dict\n        path blacklisted_sites\n        path blacklisted_sites_index\n\n    output:\n        tuple val(sample_id), \\\n            path(\"${sample_id}.vcf.gz\"), \\\n            path(\"${sample_id}.vcf.gz.tbi\")\n\n    \"\"\"\n    gatk FilterMutectCalls \\\n        -V ${merged_vcf} \\\n        -R ${mito_fasta} \\\n        -O filtered.vcf \\\n        --stats ${combined_stats} \\\n        --max-alt-allele-count 4 \\\n        --mitochondria-mode \\\n        --min-allele-fraction 0\n\n    gatk VariantFiltration -V filtered.vcf \\\n        -O ${sample_id}.vcf.gz \\\n        --apply-allele-specific-filters \\\n        --mask ${blacklisted_sites} \\\n        --mask-name \"blacklisted_site\"\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/FILTER"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lmtani"], "nb_wf": 1, "list_wf": ["wf-human-mito"], "list_contrib": ["lmtani"], "nb_contrib": 1, "codes": ["\nprocess SPLIT_MULTIALLELICS_AND_REMOVE_NON_PASS_SITES {\n    label \"human_mito\"\n    input:\n        path mito_fasta\n        path mito_index\n        path mito_dict\n        tuple val(sample_id), path(filtered_vcf), path(filtered_vcf_index)\n\n    output:\n        tuple val(sample_id), \\\n            path(\"${sample_id}.pass.vcf.gz\"), \\\n            path(\"${sample_id}.pass.vcf.gz.tbi\")\n\n    script:\n    \"\"\"\n    gatk LeftAlignAndTrimVariants \\\n      -R ${mito_fasta} \\\n      -V ${filtered_vcf} \\\n      -O split.vcf \\\n      --split-multi-allelics \\\n      --dont-trim-alleles \\\n      --keep-original-ac\n\n      gatk SelectVariants \\\n        -V split.vcf \\\n        -O ${sample_id}.pass.vcf.gz \\\n        --exclude-filtered\n    \"\"\"\n}"], "list_proc": ["lmtani/wf-human-mito/SPLIT_MULTIALLELICS_AND_REMOVE_NON_PASS_SITES"], "list_wf_names": ["lmtani/wf-human-mito"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess trim {\n\tpublishDir \"${params.outdir}/trimmedRead\"\n        container  'kathrinklee/cutadapt:latest'\n\n        when:  defined(fastq1) && defined(fastq2)\n\n        input:\n        file fastq1\n        file fastq2\n\n        output:\n        file \"\" into trim1\n        file \"\" into trim2 \n\n        \"\"\"\n        cutadapt $fastq1 $fastq2 $illumina $trim1 $trim2\n        \"\"\"\n\n}"], "list_proc": ["lobleya/GATK4_WGS/trim"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess BWA {\n\tpublishDir \"${params.outdir}/MappedRead\"\n\tcontainer 'kathrinklee/bwa:latest'\n\n        when:  defined(fastq1) && defined(fastq2)\n\n\tinput:\n\tfile reference\n\tfile bwa_index\n\tfile trim1\n\tfile trim2\n\n\toutput:\n\tfile 'aln-pe.sam' into samfile\n\t\n\t\"\"\"\n\tbwa mem -M -R '@RG\\\\tID:${params.rg}\\\\tSM:${params.samplename}\\\\tPL:Illumina' $reference $trim1 $trim2 > aln-pe.sam\n\t\"\"\"\n\t\t\n}"], "list_proc": ["lobleya/GATK4_WGS/BWA"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess BWA_sort {\n\tpublishDir \"${params.outdir}/MappedRead\"\n\tcontainer 'comics/samtools:latest'\n\t\n\tinput:\n\tfile samfile\n\n\toutput:\n\tfile 'aln-pe-sorted.bam' into bam_sort\n\n\t\"\"\"\n\tsamtools sort -o aln-pe-sorted.bam -O BAM $samfile\n\t\"\"\"\n\n}"], "list_proc": ["lobleya/GATK4_WGS/BWA_sort"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess MarkDuplicates {\n\tpublishDir \"${params.outdir}/MappedRead\"\n\tcontainer 'broadinstitute/gatk'\n\t\n\tinput:\n\tfile bam_sort\n\n\toutput:\n\tfile 'aln-pe_MarkDup.bam' into bam_markdup\n\n\t\"\"\"\n\tgatk MarkDuplicates -I $bam_sort -M metrics.txt -O aln-pe_MarkDup.bam\t\n\t\"\"\"\n\n}"], "list_proc": ["lobleya/GATK4_WGS/MarkDuplicates"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess BaseRecalibrator {\n\tpublishDir \"${params.outdir}/BaseRecalibrator\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile reference\n\tfile reference_fai\n\tfile reference_dict\n\tfile bam_markdup\n\tfile dbsnp\n\tfile dbsnp_idx\n\tfile golden_indel\n\tfile golden_indel_idx\n\n\toutput:\n\tfile 'recal_data.table' into BaseRecalibrator_table\n\n\t\"\"\"\n\tgatk BaseRecalibrator \\\n\t-I $bam_markdup \\\n\t--known-sites $dbsnp \\\n\t--known-sites $golden_indel \\\n\t-O recal_data.table \\\n\t-R $reference\n\t\"\"\"\n}"], "list_proc": ["lobleya/GATK4_WGS/BaseRecalibrator"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess ApplyBQSR {\n\tpublishDir \"${params.outdir}/BaseRecalibrator\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile BaseRecalibrator_table\n\tfile bam_markdup\n\n\toutput:\n\tfile 'aln-pe_bqsr.bam' into bam_bqsr\n\t\n\tscript:\n\t\"\"\"\n\tgatk ApplyBQSR -I $bam_markdup -bqsr $BaseRecalibrator_table -O aln-pe_bqsr.bam\n\t\"\"\"\n}"], "list_proc": ["lobleya/GATK4_WGS/ApplyBQSR"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess HaplotypeCaller {\n\tpublishDir \"${params.outdir}/HaplotypeCaller\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile reference\n\tfile reference_fai\n\tfile reference_dict\n\tfile bam_bqsr\n\n\toutput:\n\tfile 'haplotypecaller.g.vcf' into haplotypecaller_gvcf\n\t\n\tscript:\n\t\"\"\"\n\tgatk HaplotypeCaller -I $bam_bqsr -O haplotypecaller.g.vcf --emit-ref-confidence GVCF -R $reference\n\t\"\"\"\n}"], "list_proc": ["lobleya/GATK4_WGS/HaplotypeCaller"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess GenotypeGVCFs {\n\tpublishDir \"${params.outdir}/HaplotypeCaller\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile reference\n\tfile reference_fai\n\tfile reference_dict\n\tfile haplotypecaller_gvcf\n\n\toutput:\n\tfile 'haplotypecaller.vcf' into haplotypecaller_vcf\n\t\n\tscript:\n\t\"\"\"\n\tgatk GenotypeGVCFs --variant haplotypecaller.g.vcf -R $reference -O haplotypecaller.vcf\n\t\"\"\"\n}"], "list_proc": ["lobleya/GATK4_WGS/GenotypeGVCFs"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess ApplyVQSR_SNPs {\n\tpublishDir \"${params.outdir}/VariantRecalibrator\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile haplotypecaller_vcf\n\tfile variantrecalibrator_recal\n\tfile variantrecalibrator_recal_idx\n\tfile variantrecalibrator_tranches\n\n\toutput:\n\tfile 'recalibrated_snps_raw_indels.vcf' into recalibrated_snps_raw_indels\n\t\n\tscript:\n\t\"\"\"\n\tgatk ApplyVQSR \\\n\t-V $haplotypecaller_vcf \\\n\t--recal-file $variantrecalibrator_recal \\\n\t--tranches-file $variantrecalibrator_tranches \\\n\t-mode SNP \\\n\t-ts-filter-level 99.0 \\\n\t-O recalibrated_snps_raw_indels.vcf \n\t\"\"\"\n}"], "list_proc": ["lobleya/GATK4_WGS/ApplyVQSR_SNPs"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["lobleya"], "nb_wf": 1, "list_wf": ["GATK4_WGS"], "list_contrib": ["lobleya"], "nb_contrib": 1, "codes": ["\nprocess ApplyVQSR_INDELs {\n\tpublishDir \"${params.outdir}/VariantRecalibrator\"\n\tcontainer 'broadinstitute/gatk:latest'\n\t\n\tinput:\n\tfile recalibrated_snps_raw_indels\n\tfile variantrecalibrator_indel_recal\n\tfile variantrecalibrator_indel_recal_idx\n\tfile variantrecalibrator_indel_tranches\n\n\toutput:\n\tfile 'recalibrated_variants.vcf' into recalibrated_variants_vcf\n\t\n\tscript:\n\t\"\"\"\n\tgatk ApplyVQSR \\\n\t-V $recalibrated_snps_raw_indels \\\n\t--recal-file $variantrecalibrator_indel_recal \\\n\t--tranches-file $variantrecalibrator_indel_tranches \\\n\t-mode INDEL \\\n\t-ts-filter-level 99.0 \\\n\t-O recalibrated_variants.vcf\n\t\"\"\"\n}"], "list_proc": ["lobleya/GATK4_WGS/ApplyVQSR_INDELs"], "list_wf_names": ["lobleya/GATK4_WGS"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["DNAseq-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess PREPROCESS_READS_SINGLE { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/reads_prepro\", pattern:\"*cutadapt_output.txt\", mode: \"copy\", saveAs: { filename -> \"${sample_id}/$filename\" }\n\tstageInMode = 'copy'                                    \n\tcache false\n\n\tinput:\n\t\ttuple val(sample_id), path(reads) \n\t\tval num_threads\n\t\tpath adapter_3_seq_file\n\n\toutput:\n\t\ttuple val(sample_id), path(\"${sample_id}_prepro.fastq.gz\"), emit: reads_prepro\n\t\tpath \"${sample_id}_cutadapt_output.txt\", emit: cutadapt\n\n\tshell:\n\t'''\n\n\t### adapter 3' input as string or file\n\tif [[ !{adapter_3_seq_file} == *\".fasta\"* ]]\n\tthen\n  \t\tADAPTER_3=file:!{adapter_3_seq_file}\n\telse\n\t\tADAPTER_3=\"!{adapter_3_seq_file}\"\n\tfi\n\n\t\n\t### theoretically sorting not needed but maybe speeds up things\n\treads_sorted=$(echo !{reads} | xargs -n1 | sort | xargs)\n\n\t### combine multiple seq files in the same sample directory with same direction together\n\treads_sorted=$(find $reads_sorted -name \"*.fq.gz\" -o -name \"*.fastq.gz\")\n\tcat $reads_sorted > !{sample_id}_raw_reads_connected.fastq.gz\n\n\tcutadapt --cores=!{num_threads} --max-n 0.1 --discard-trimmed --minimum-length 10 -a $ADAPTER_3 -o !{sample_id}_prepro.fastq.gz !{sample_id}_raw_reads_connected.fastq.gz > !{sample_id}_cutadapt_output.txt\n\t'''\n}"], "list_proc": ["loipf/DNAseq-pipeline/PREPROCESS_READS_SINGLE"], "list_wf_names": ["loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["DNAseq-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess FASTQC_READS_PREPRO { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/reads_prepro\", mode: \"copy\", overwrite: false, saveAs: { filename -> \"${sample_id}/$filename\" }\n\t                        \n\n\tinput:\n\t\ttuple val(sample_id), path(reads_prepro) \n\t\tval num_threads\n\t\tpath adapter_3_seq_file\n\t\tpath adapter_5_seq_file\n\n\toutput:\n\t\tpath \"*.zip\", emit: reports\n\t\tpath \"*.html\"\n\n\tshell:\n\t'''\n\t#if [[ !{adapter_5_seq_file} != \"NO_FILE\" ]]\n\t#then\n\t#  cat !{adapter_3_seq_file} !{adapter_5_seq_file} > adapter_both.fasta\n\t#else\n\t#  cat !{adapter_3_seq_file} > adapter_both.fasta \n\t#fi\n\t\n\t#cat !{adapter_3_seq_file} !{adapter_5_seq_file} 1>adapter_both.fasta 2>error_if_one_missing\n\t\n\tcat !{adapter_3_seq_file} !{adapter_5_seq_file} > adapter_both.fasta\n\tawk 'BEGIN{RS=\">\";OFS=\"\\t\"}NR>1{print $1,$2}' adapter_both.fasta > adapter_both.txt\n\n\tfastqc -a adapter_both.txt -t !{num_threads} --noextract !{reads_prepro}\n\t'''\n}"], "list_proc": ["loipf/DNAseq-pipeline/FASTQC_READS_PREPRO"], "list_wf_names": ["loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["DNAseq-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess CREATE_BWA_INDEX { \n\tpublishDir \"$params.data_dir/bwa_index\", mode: \"copy\"\n\n\tinput:\n\t\tpath reference_genome\n\n\toutput:\n\t\tpath \"*.{amb,ann,bwt,pac,sa}\", emit: bwa_index\n\n\tshell:\n\t'''\n\tbwa index !{reference_genome} \n\t'''\n}"], "list_proc": ["loipf/DNAseq-pipeline/CREATE_BWA_INDEX"], "list_wf_names": ["loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["DNAseq-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess MAPPING_BWA_SINGLE { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/reads_mapped\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\tcache false\n\n\tinput:\n\t\ttuple val(sample_id), path(reads_prepro) \n\t\tval num_threads\n\t\tpath reference_genome\n\t\tpath bwa_index                               \n\n\toutput:\n\t\tpath \"${sample_id}.bam\", emit: reads_mapped\n\t\tpath \"${sample_id}.bam.bai\", emit: reads_mapped_index\n\t\tpath \"*\", emit: all\n\n\n\tshell:\n\t'''\n\tbwa aln -t !{num_threads} -n 2 !{reference_genome} !{reads_prepro} \\\n\t| bwa samse -r \"@RG\\\\tID:!{sample_id}\\\\tSM:!{sample_id}\" !{reference_genome} - !{reads_prepro} \\\n\t| samtools view -@ !{num_threads} -h -b -u - \\\n\t| samtools sort -n -@ !{num_threads} - \\\n\t| samtools fixmate -m -@ !{num_threads} - - \\\n\t| samtools sort -@ !{num_threads} - \\\n\t| samtools markdup -@ !{num_threads} -f !{sample_id}_markdup_stats.txt - !{sample_id}.bam\n\t\n\tsamtools index -b -@ !{num_threads} !{sample_id}.bam\n\tsamtools stats -@ !{num_threads} !{sample_id}.bam > !{sample_id}_stats.txt\n\t\n\t'''\n}"], "list_proc": ["loipf/DNAseq-pipeline/MAPPING_BWA_SINGLE"], "list_wf_names": ["loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["plotcoverage"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["DNAseq-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess DEEPTOOLS_ANALYSIS { \n\tpublishDir \"$params.data_dir/reads_mapped/_deepTools\", mode: 'copy'\n\n\tinput:\n\t\tpath reads_mapped\n\t\tpath reads_mapped_index\n\t\tval num_threads\n\n\toutput:\n\t\tpath \"*\", emit:all\n\n\n\tshell:\n\t'''\n\tmultiBamSummary bins -p !{num_threads} --smartLabels --bamfiles !{reads_mapped} -o multiBamSummary.npz\n\n\tplotCorrelation --corData multiBamSummary.npz --corMethod spearman --whatToPlot heatmap --outFileCorMatrix plotCorrelation_matrix.tsv\n\n\tplotPCA --corData multiBamSummary.npz --outFileNameData plotPCA_matrix.tsv\n\n\tplotCoverage -p !{num_threads} --ignoreDuplicates --smartLabels --bamfiles !{reads_mapped} --outRawCounts plotCoverage_rawCounts_woDuplicates.tsv > plotCoverage_output.tsv\n\n\tbamPEFragmentSize -p !{num_threads} --bamfiles !{reads_mapped} --table bamPEFragment_table.tsv --outRawFragmentLengths bamPEFragment_rawLength.tsv\n\n\testimateReadFiltering -p !{num_threads} --smartLabels --bamfiles !{reads_mapped} > estimateReadFiltering_output.tsv\n\n\t'''\n}"], "list_proc": ["loipf/DNAseq-pipeline/DEEPTOOLS_ANALYSIS"], "list_wf_names": ["loipf/DNAseq-pipeline"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Minimap2", "MultiQC", "G-BLASTN"], "nb_own": 2, "list_own": ["louperelo", "loipf"], "nb_wf": 2, "list_wf": ["DNAseq-pipeline", "longmetarg"], "list_contrib": ["louperelo", "loipf"], "nb_contrib": 2, "codes": ["\nprocess mapFlye_CARD {\n    label 'mapcard' \n\n    input:\n    file out_flye\n\t\n    publishDir \"$params.outdir\", mode: 'copy'\n\n    output:  \n    file '*' into flyecard_aln \n\n\twhen:\n\tparams.flye\t== true\n        \n    script:\n\t\n    \"\"\"\n    minimap2 -ax $params.readtype $params.argDb $out_flye -t $params.threads | samtools view -S -b | samtools view -b -F 4 > aln_F4.bam\n\n    \"\"\" \t\n}", "\nprocess taxonomy {\n    if( params.taxon == \"Blast\")\n        label 'blast' \n    if( params.taxon == \"Diamond\")\n        label 'diamond'\n \n    input:\n\tfile argHits from in_tax.mix(inflye_tax)\n\n    publishDir \"$params.outdir\", mode:'copy'\n\n    output:   \n    file 'result_tax.tsv' into (in_ssciname, out_taxonomy)\n        \n    script:\n    if( params.taxon == \"Blast\")\n    \"\"\"\n    blastn -task blastn -max_target_seqs 1 -num_threads $params.threads -outfmt \"6 delim=    qseqid staxid pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -db $params.blastDbDir/$params.blastDbName -query $argHits -out result_tax.tsv \n    \n    \"\"\"\n    else if( params.taxon == \"Diamond\")\n    \"\"\"\n    diamond blastx -d $params.diamondDbDir/$params.diamondDbName -q $argHits -p $params.threads -F 15 -f 6 qseqid staxids pident length mismatch gapopen qstart qend sstart send evalue bitscore --max-target-seqs 1 -o result_tax.tsv\n    \n    \"\"\"\n    else\n    throw new IllegalArgumentException(\"Unknown taxonomy aligner $params.taxon\")   \n}", "\nprocess MULTIQC_RAW { \n\tpublishDir \"$params.data_dir/quality_reports\", mode: \"copy\"\n\n\tinput:\n\t\tpath stat_files\n\n\toutput:\n\t\tpath \"*\"\n\n\tshell:\n\t'''\n\tmultiqc -f -o reads_raw .\n\t'''\n}"], "list_proc": ["louperelo/longmetarg/mapFlye_CARD", "louperelo/longmetarg/taxonomy", "loipf/DNAseq-pipeline/MULTIQC_RAW"], "list_wf_names": ["louperelo/longmetarg", "loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["DNAseq-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess MULTIQC_MAPPED { \n\tpublishDir \"$params.data_dir/quality_reports\", mode: \"copy\"\n\n\tinput:\n\t\tpath stat_files\n\n\toutput:\n\t\tpath \"*\"\n\n\tshell:\n\t'''\n\tmultiqc -f -o reads_mapped .\n\t'''\n}"], "list_proc": ["loipf/DNAseq-pipeline/MULTIQC_MAPPED"], "list_wf_names": ["loipf/DNAseq-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess INDEX_REFERENCE { \n\n\tinput:\n\t\tpath reference_genome\n\t\tpath bed_file\n\n\toutput:\n\t\ttuple path(\"*.fa\"), path(\"*.fa.fai\"), path(\"*.dict\"), emit: reference_genome\n\t\ttuple path(\"*.bed_sorted.gz\"), path(\"*.bed_sorted.gz.tbi\"), emit: bed_file\n\n\tshell:\n\t'''\n\treference_name=!{reference_genome}\n\treference_name=${reference_name%.*}  # removes last file extension .gz\n\n\tgunzip -c !{reference_genome} > $reference_name \n\tsamtools faidx $reference_name -o $reference_name.fai\n\tsamtools dict $reference_name -o ${reference_name%.*}.dict  # remove .fa so name is only .dict\n\n\tbedtools sort -i !{bed_file} | bgzip -c > !{bed_file}_sorted.gz\n\ttabix --zero-based -b 2 -e 3 !{bed_file}_sorted.gz\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/INDEX_REFERENCE"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "vt"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess SOMVC_LOFREQ { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/vc_caller/lofreq\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\n\tinput:\n\t\ttuple val(sample_id), path(normal_file), path(tumor_file), path(normal_file_index), path(tumor_file_index) \n\t\tpath reference_genome\n\t\tpath bed_file\n\t\tval num_threads\n\n\toutput:\n\t\ttuple val(\"$sample_id\"), path(\"lofreq_somatic_final.snvs.vcf.gz\"), path(\"lofreq_somatic_final.snvs.vcf.gz.tbi\"), path(\"lofreq_somatic_final.indels_vt.vcf.gz\"), path(\"lofreq_somatic_final.indels_vt.vcf.gz.tbi\"), emit: lofreq_output\n\t\ttuple path(\"lofreq_somatic_raw.snvs.vcf.gz\"), path(\"lofreq_somatic_raw.snvs.vcf.gz.tbi\")\n\t\ttuple path(\"lofreq_somatic_raw.indels.vcf.gz\"), path(\"lofreq_somatic_raw.indels.vcf.gz.tbi\")\n\n\tshell:\n\t'''\n\t### https://csb5.github.io/lofreq/commands/#somatic\n\tgunzip -c !{bed_file[0]} > bed_file_unzipped.bed   # vardict need unzipped\n\n\tlofreq viterbi -f !{reference_genome[0]} !{normal_file} | samtools sort -@ !{num_threads} -o normal_file_viterbi.bam -\n\tlofreq indelqual --dindel -f !{reference_genome[0]} -o normal_file_indelqual.bam normal_file_viterbi.bam\n\tsamtools index -b -@ !{num_threads} normal_file_viterbi.bam\n\n\tlofreq viterbi -f !{reference_genome[0]} !{tumor_file} | samtools sort -@ !{num_threads} -o tumor_file_viterbi.bam -\n\tlofreq indelqual --dindel -f !{reference_genome[0]} -o tumor_file_indelqual.bam tumor_file_viterbi.bam\n\tsamtools index -b -@ !{num_threads} tumor_file_viterbi.bam\n\n\tlofreq somatic -n normal_file_viterbi.bam -t tumor_file_viterbi.bam -f !{reference_genome[0]} --threads !{num_threads} -o lofreq_ -l bed_file_unzipped.bed --call-indels\n\t\n\t### vt normalization for indels\n\tvt normalize lofreq_somatic_final.indels.vcf.gz -r !{reference_genome[0]} -o lofreq_somatic_final.indels_vt.vcf.gz\n\ttabix -p vcf lofreq_somatic_final.indels_vt.vcf.gz\n\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/SOMVC_LOFREQ"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["vt", "BCFtools", "GATK"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess SOMVC_MUTECT2 { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/vc_caller/mutect2\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\n\tinput:\n\t\ttuple val(sample_id), path(normal_file), path(tumor_file), path(normal_file_index), path(tumor_file_index) \n\t\tpath reference_genome\n\t\tpath bed_file\n\t\tval num_threads\n\n\toutput:\n\t\ttuple val(\"$sample_id\"), path(\"mutect2_filtered_vt.vcf.gz\"), path(\"mutect2_filtered_vt.vcf.gz.tbi\"), emit: mutect2_output\n\t\tpath \"mutect2_filtered.vcf.filteringStats.tsv\"\n\n\tshell:\n\t'''\n\tgunzip -c !{bed_file[0]} > bed_file_unzipped.bed   # mutect2 need unzipped\n\t\n\tgatk GetSampleName -I !{normal_file} -O sample_normal.txt\n\tnormal_sample_name=$(cat sample_normal.txt)\n\tgatk GetSampleName -I !{tumor_file} -O sample_tumor.txt\n\ttumor_sample_name=$(cat sample_tumor.txt)\n\t\n\tgatk Mutect2 -R !{reference_genome[0]} -I !{normal_file} -normal $normal_sample_name -I !{tumor_file} -tumor $tumor_sample_name --native-pair-hmm-threads !{num_threads} --germline-resource /usr/src/mutect2_genome/af-only-gnomad_ensembl.hg38.vcf.gz --panel-of-normals /usr/src/mutect2_genome/1000g_pon_ensembl.hg38.vcf.gz --intervals bed_file_unzipped.bed -O mutect2_unfiltered.vcf\n\tgatk FilterMutectCalls -R !{reference_genome[0]} -V mutect2_unfiltered.vcf -O mutect2_filtered.vcf\n\n\t### rename for somatic-combiner\n\tprintf '%s\\n' !{sample_id}_normal !{sample_id}_tumor > sample_names.txt  \n\tbcftools reheader --samples sample_names.txt -o mutect2_filtered_name.vcf mutect2_filtered.vcf\n\n\tbgzip -c mutect2_filtered_name.vcf > mutect2_filtered.vcf.gz\n\tvt normalize mutect2_filtered.vcf.gz -r !{reference_genome[0]} -o mutect2_filtered_vt.vcf.gz\n\ttabix -p vcf mutect2_filtered_vt.vcf.gz\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/SOMVC_MUTECT2"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["vt", "BCFtools"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess SOMVC_STRELKA { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/vc_caller/strelka\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\n\tinput:\n\t\ttuple val(sample_id), path(normal_file), path(tumor_file), path(normal_file_index), path(tumor_file_index) \n\t\tpath reference_genome\n\t\tpath bed_file\n\t\tval num_threads\n\n\toutput:\n\t\ttuple val(\"$sample_id\"), path(\"somatic.snvs.vcf.gz\"), path(\"somatic.snvs.vcf.gz.tbi\"), path(\"somatic.indels_vt.vcf.gz\"), path(\"somatic.indels_vt.vcf.gz.tbi\"), emit: strelka_output\n\t\tpath manta_sv\n\n\tshell:\n\t'''\n\tconfigManta.py --tumorBam !{tumor_file} --normalBam !{normal_file} --referenceFasta !{reference_genome[0]} --runDir manta_dir\n\tmanta_dir/runWorkflow.py -m local -j !{num_threads}\t\n\n\tconfigureStrelkaSomaticWorkflow.py --tumorBam !{tumor_file} --normalBam !{normal_file} --referenceFasta !{reference_genome[0]} --exome --runDir strelka_dir --indelCandidates manta_dir/results/variants/candidateSmallIndels.vcf.gz --callRegions !{bed_file[0]}\n\tstrelka_dir/runWorkflow.py -m local -j !{num_threads}\n\n\tprintf '%s\\n' !{sample_id}_normal !{sample_id}_tumor > sample_names.txt \n\tbcftools reheader --samples sample_names.txt -o somatic.snvs.vcf.gz strelka_dir/results/variants/somatic.snvs.vcf.gz\n\tbcftools reheader --samples sample_names.txt -o somatic.indels.vcf.gz strelka_dir/results/variants/somatic.indels.vcf.gz\n\ttabix -p vcf somatic.snvs.vcf.gz\n\n\t### vt normalization for indels\n\tvt normalize somatic.indels.vcf.gz -r !{reference_genome[0]} -o somatic.indels_vt.vcf.gz\n\ttabix -p vcf somatic.indels_vt.vcf.gz\n\t\n\tmv manta_dir/results/variants manta_sv\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/SOMVC_STRELKA"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess SOMVC_VARDICT { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/vc_caller/vardict\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\n\tinput:\n\t\ttuple val(sample_id), path(normal_file), path(tumor_file), path(normal_file_index), path(tumor_file_index) \n\t\tpath reference_genome\n\t\tpath bed_file\n\t\tval num_threads\n\n\toutput:\n\t\ttuple val(\"$sample_id\"), path(\"vardict_output_filtered.vcf.gz\"), path(\"vardict_output_filtered.vcf.gz.tbi\"), emit: vardict_output\n\n\tshell: \n\t'''\n\tgunzip -c !{bed_file[0]} > bed_file_unzipped.bed   # vardict need unzipped\n\n\t/usr/src/VarDict-1.8.2/bin/VarDict -G !{reference_genome[0]} -k 1 -b \"!{tumor_file}|!{normal_file}\" -Q 5 -z 1 -c 1 -S 2 -E 3 -g 4 -th !{num_threads} bed_file_unzipped.bed | /usr/src/VarDict-1.8.2/bin/testsomatic.R | /usr/src/VarDict-1.8.2/bin/var2vcf_paired.pl -P 0.9 -m 4.25 -f 0.01 -M -N \"!{sample_id}_tumor|!{sample_id}_normal\" > vardict_output.vcf\n\n\t### https://github.com/bcbio/bcbio-nextgen/blob/5cfc02b5974d19908702fa21e6d2f7a50455b44c/bcbio/variation/vardict.py#L248\n\tgatk VariantFiltration -R !{reference_genome[0]} -V vardict_output.vcf -O vardict_output_filtered.vcf --filter-name bcbio_advised --filter-expression \"((AF*DP<6)&&((MQ<55.0&&NM>1.0)||(MQ<60.0&&NM>2.0)||(DP<10)||(QUAL<45)))\" \n\n\tbgzip -c vardict_output_filtered.vcf > vardict_output_filtered.vcf.gz\n\ttabix -p vcf vardict_output_filtered.vcf.gz\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/SOMVC_VARDICT"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess SOMATIC_COMBINER { \n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/vc_caller/somatic_combiner\", mode: 'copy', saveAs: { filename -> \"${sample_id}/$filename\" }\n\n\tinput:\n\t\ttuple val(sample_id), path(lofreq_snv_vcf), path(lofreq_snv_vcf_index), path(lofreq_indel_vcf), path(lofreq_indel_vcf_index), path(mutect2_vcf), path(mutect2_vcf_index), path(strelka_snv_vcf), path(strelka_snv_vcf_index), path(strelka_indel_vcf), path(strelka_indel_vcf_index), path(vardict_vcf), path(vardict_vcf_index)\n\n\toutput:\n\t\ttuple val(\"${sample_id}\"), path(\"*_somatic_combiner_all.vcf.gz\"), path(\"*_somatic_combiner_all.vcf.gz.tbi\"), emit: somatic_combiner_vcf\n\n\n\tshell:\n\t'''\n\tjava -jar /usr/src/somaticCombiner.jar -L !{lofreq_indel_vcf} -l !{lofreq_snv_vcf} -M !{mutect2_vcf} -s !{strelka_snv_vcf} -S !{strelka_indel_vcf} -D !{vardict_vcf} -o somatic_combiner_raw.vcf\n\n\tprintf '%s\\n' !{sample_id}_tumor !{sample_id}_normal > sample_names.txt \n\tbcftools reheader --samples sample_names.txt -o somatic_combiner_sample.vcf somatic_combiner_raw.vcf\n\t\n\t### somatic combiner defined but not added to vcf\n\tsed -i '7 i ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\"Allelic depths for the ref and alt alleles in the order listed, according to somatic-combiner\">' somatic_combiner_sample.vcf\n\t\n\tbgzip -c somatic_combiner_sample.vcf > !{sample_id}_somatic_combiner_all.vcf.gz\n\ttabix -p vcf !{sample_id}_somatic_combiner_all.vcf.gz\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/SOMATIC_COMBINER"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess VARIANT_CALLING_STATS { \n\tcontainer \"dnavc-pipeline:latest\"\n\ttag \"$sample_id\"\n\tpublishDir \"$params.data_dir/variants_vcf\", mode: \"copy\", overwrite: false, saveAs: { filename -> \"${sample_id}/$filename\" }\n\n\tinput:\n\t\ttuple val(sample_id), path(vcf_file), path(vcf_file_index)\n\t\tval num_threads\n\n\toutput:\n\t\tpath \"*_ADJ_PASS_vcfstats.txt\", emit: vcf_stats\n\n\tshell:\n\t'''\n\tbcftools stats -f ADJ_PASS -s !{sample_id}_normal --threads !{num_threads} !{vcf_file} > !{sample_id}_normal_ADJ_PASS_vcfstats.txt\n\tbcftools stats -f ADJ_PASS -s !{sample_id}_tumor --threads !{num_threads} !{vcf_file} > !{sample_id}_tumor_ADJ_PASS_vcfstats.txt\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/VARIANT_CALLING_STATS"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess MERGE_VCF { \n\tcontainer \"dnavc-pipeline:latest\"\n\tpublishDir \"$params.data_dir/variants_vcf/_all\", mode: \"copy\", overwrite: false\n\n\tinput:\n\t\tpath vcf_files\n\t\tval num_threads\n\n\toutput:\n\t\tpath \"all_samples_vcf_merged.vcf.gz\", emit: vcf_all\n\n\tshell:\n\t'''\n\tbcftools merge -Oz -o all_samples_vcf_merged.vcf.gz --threads !{num_threads} *_somatic_combiner_all.vcf.gz\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/MERGE_VCF"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["OCG"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess VARIANT_ANNOTATION { \n\tcontainer \"dnavc-pipeline:latest\"\n\tpublishDir \"$params.data_dir/variants_vcf/_all\", mode: \"copy\", overwrite: false\n\n\tinput:\n\t\tpath vcf_all\n\t\tval num_threads\n\n\toutput:\n\t\tpath \"*\"\n\n\tshell:\n\t'''\n\toc module ls -t annotator > oc_databases_version.txt  ### output OpenCRAVAT database versions\n\t\n\toc run -l hg38 -t csv -x --mp !{num_threads} !{vcf_all}\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/VARIANT_ANNOTATION"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["loipf"], "nb_wf": 1, "list_wf": ["SOMvc-pipeline"], "list_contrib": ["loipf"], "nb_contrib": 1, "codes": ["\nprocess MULTIQC_VCF { \n\tcontainer \"dnavc-pipeline:latest\"\n\tpublishDir \"$params.data_dir/quality_reports\", mode: \"copy\"\n\n\tinput:\n\t\tpath stat_files\n\t\tpath conpair_files\n\n\toutput:\n\t\tpath \"*\"\n\n\tshell:\n\t'''\n\tmultiqc -f -o variants_vcf .\n\t'''\n}"], "list_proc": ["loipf/SOMvc-pipeline/MULTIQC_VCF"], "list_wf_names": ["loipf/SOMvc-pipeline"]}, {"nb_reuse": 1, "tools": ["DEFormats"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess check_single_paired { \n    \n    input: \n    file manifest from ch_single_pair\n\n    output: \n    file 'manifest_format.txt' into manifest_type\n    file 'data_type.txt' into dataType\n    \n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n\n    import pandas as pd\n    import os \n\n    read_manifest = pd.read_table('${manifest}', index_col=0, sep='\\t+', engine='python')\n\n    if read_manifest.columns[0] == 'absolute-filepath':\n        print(\"single end analysis\")\n        format = \"SingleEndFastqManifestPhred33V2\"\n        data = \"SampleData[SequencesWithQuality]\"\n\n        with open(\"manifest_format.txt\", \"w\") as file:\n            file.write(format)\n\n        with open(\"data_type.txt\", \"w\") as d_file:\n            d_file.write(data)\n        print(format + \" \" + data)\n\n        \n    elif read_manifest.columns[0] == 'forward-absolute-filepath':\n        print(\"paired end analysis\")\n        format = \"PairedEndFastqManifestPhred33V2\"\n        data = \"SampleData[PairedEndSequencesWithQuality]\"\n        with open(\"manifest_format.txt\", \"w\") as file:\n            file.write(format)\n\n        with open(\"data_type.txt\", \"w\") as d_file:\n            d_file.write(data)\n        print(format + \" \" + data)\n    else:\n        print(\n            \"cannot determine if paired or single end, check manifest file\")\n        exit(1)\n\n    \n    \"\"\"\n    \n}"], "list_proc": ["lorentzben/automate_16_nf/check_single_paired"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess generate_seq_object{\n\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n    input: \n    file manifest from ch_make_qiime\n    file manifest_format from manifest_type\n    file data_type from dataType\n    path seqs from ch_make_qiime_seq\n\n    output: \n    file 'demux.qza' into qiime_obj\n\n    shell:\n    '''\n    DAT=$(head !{data_type})\n    MANI=$(head !{manifest_format})\n    module load  QIIME2/2020.11\n    qiime tools import \\\n    --type $DAT\\\n    --input-path !{manifest} \\\n    --output-path demux.qza \\\n    --input-format $MANI\n    '''\n}"], "list_proc": ["lorentzben/automate_16_nf/generate_seq_object"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["Foundry", "BIOISIS"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess VerifyManifest{\n    publishDir \"${params.outdir}\", mode: 'copy'\n    input:\n    file manifest from ch_mani_veri\n    path seqs_dir from ch_seqs_veri\n    file metadata from ch_meta_veri\n    val ioi from ch_ioi_veri\n\n    output:\n\n    file \"order_item_of_interest.csv\" into ch_format_ioi_order\n\n                                                                                                           \n                                                                                \n      \n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n    import os \n    from pathlib import Path\n    from pathlib import PurePath\n    import pandas as pd \n    import csv \n\n    try:\n        read_metadata = pd.read_table('${metadata}', index_col=0, sep='\\t')\n    except FileNotFoundError:\n        exit(1)\n\n    try:\n        read_order = pd.read_table('${baseDir}/order_item_of_interest.csv', index_col=0, sep=',')\n    except FileNotFoundError:\n        iois = list(pd.Series.unique(read_metadata['${ioi}']))\n        ioisdf = pd.DataFrame(iois[1:])\n        ioisdf.columns = ['${ioi}']\n        pd.DataFrame.to_csv(ioisdf, 'order_item_of_interest.csv', index=False)\n\n    seq_dir = '${seqs_dir}'\n    try:\n        read_manifest = pd.read_table('${manifest}', index_col=0, sep='\\t')\n    except FileNotFoundError:\n        exit(1)\n\n    # sets current dir and finds the fastq and fastq.gz files in the current directory\n    p = Path.cwd()\n    list_of_fastq = list(p.glob(seq_dir + '/*.fastq'))\n    list_of_gz = list(p.glob(seq_dir+'/*.fastq.gz'))\n\n    fastq_files = []\n    gz_files = []\n    found = []\n    missing = []\n\n    # pulls only the filename and saves to a list\n    for item in list_of_fastq:\n        filename = os.path.split(item)[1]\n        fastq_files.append(filename)\n\n    for item in list_of_gz:\n        filename = os.path.split(item)[1]\n        gz_files.append(filename)\n    if read_manifest.columns[0] == 'forward-absolute-filepath':\n        # iterates over the forward reads and then the reverse reads to check to make sure they are all accounted for\n        try:\n            for item in read_manifest['forward-absolute-filepath']:\n                filename = os.path.split(item)[1]\n                if filename in fastq_files:\n                    \n                    found.append(filename)\n                else:\n                    if filename in gz_files:\n                        \n                        found.append(filename)\n                    else:\n                        \n                        missing.append(filename)\n        except KeyError:\n            print('single read project')\n\n        # try except in the case that the user only has single end reads.\n        try:\n            for item in read_manifest['reverse-absolute-filepath']:\n                filename = os.path.split(item)[1]\n                if filename in fastq_files:\n                    \n                    found.append(filename)\n                else:\n                    if filename in gz_files:\n                        \n                        found.append(filename)\n                    else:\n                        \n                        missing.append(filename)\n\n        except KeyError:\n            print(\"looking for forward only reads\")\n    else:\n        # this case is if there are only single reads and after which we can figure that the manifest file is wrong\n        try:\n            for item in read_manifest['absolute-filepath']:\n                filename = os.path.split(item)[1]\n                if filename in fastq_files:\n                    \n                    found.append(filename)\n                else:\n                    if filename in gz_files:\n                        \n                        found.append(filename)\n                    else:\n                        \n                        missing.append(filename)\n        except KeyError:\n            print(\"headings in the manifest appear to be incorrect\")\n            exit(1)\n\n    # if missing is not an empty list, i.e. a file listed in the manifest is not detected, it raises an error and\n    # creates a list for the user\n    if missing != []:\n\n        print(\"files are missing, please see missing.csv to correct\")\n\n        with open('missing.csv', 'w', newline='') as csvfile:\n            fieldnames = ['filename']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n\n            for filename in missing:\n                writer.writerow({'filename': filename})\n\n        exit(0)\n\n\n    print(\"the manifest called: \" + '${manifest}' +\n                 \" is valid and ready to go\")\n    \"\"\"\n\n}"], "list_proc": ["lorentzben/automate_16_nf/VerifyManifest"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["DEFormats"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess CheckSinglePaired { \n\n    publishDir \"${params.outdir}\", mode: 'copy'\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n    \n    input: \n    file manifest from ch_single_pair\n    val ioi from ch_ioi_denoise_to_file\n\n    output: \n    file 'manifest_format.txt' into manifest_type\n    file 'data_type.txt' into dataType\n    file \"item_of_interest.csv\" into ch_ioi_file_out\n\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n\n    import pandas as pd\n    import os \n    import csv\n\n    \n    with open('item_of_interest.csv', 'w', newline='') as csvfile:\n        fieldnames = ['item name']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        writer.writerow({'item name': '${ioi}'})\n\n    read_manifest = pd.read_table('${manifest}', index_col=0, sep='\\t+', engine='python')\n\n    if read_manifest.columns[0] == 'absolute-filepath':\n        print(\"single end analysis\")\n        format = \"SingleEndFastqManifestPhred33V2\"\n        data = \"SampleData[SequencesWithQuality]\"\n\n        with open(\"manifest_format.txt\", \"w\") as file:\n            file.write(format)\n\n        with open(\"data_type.txt\", \"w\") as d_file:\n            d_file.write(data)\n        print(format + \" \" + data)\n\n        \n    elif read_manifest.columns[0] == 'forward-absolute-filepath':\n        print(\"paired end analysis\")\n        format = \"PairedEndFastqManifestPhred33V2\"\n        data = \"SampleData[PairedEndSequencesWithQuality]\"\n        with open(\"manifest_format.txt\", \"w\") as file:\n            file.write(format)\n\n        with open(\"data_type.txt\", \"w\") as d_file:\n            d_file.write(data)\n        print(format + \" \" + data)\n    else:\n        print(\n            \"cannot determine if paired or single end, check manifest file\")\n        exit(1)\n\n    \n    \"\"\"\n    \n}"], "list_proc": ["lorentzben/automate_16_nf/CheckSinglePaired"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess GenerateSeqObject{\n\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input: \n    file manifest from ch_make_qiime\n    file manifest_format from manifest_type\n    file data_type from dataType\n    path seqs from ch_make_qiime_seq\n\n    output: \n    file 'demux.qza' into ch_qiime_obj\n    file manifest_format into ch_manifest_type\n    \n\n    shell:\n    '''\n    DAT=$(head !{data_type})\n    MANI=$(head !{manifest_format})\n    #module load  QIIME2/2020.11\n    qiime tools import \\\n    --type $DAT\\\n    --input-path !{manifest} \\\n    --output-path demux.qza \\\n    --input-format $MANI\n    '''\n}"], "list_proc": ["lorentzben/automate_16_nf/GenerateSeqObject"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess QualControl{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n    input:\n    file seq_obj from ch_qiime_obj\n    \n    output: \n    file('demux_summary/*') into ch_qiime_qual\n    file seq_obj into ch_qiime_denoise\n    file 'demux_summary.qzv' into ch_demux_export\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    qiime demux summarize \\\n    --i-data ${seq_obj} \\\n    --o-visualization demux_summary.qzv\n\n    qiime tools export \\\n    --input-path demux_summary.qzv \\\n    --output-path demux_summary/\n    \"\"\"\n\n}"], "list_proc": ["lorentzben/automate_16_nf/QualControl"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["WDAC", "SummaryAUC"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess FindCutoffs{\n                                           \n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n    input:\n    file 'manifest_format.txt' from ch_manifest_type\n    file('demux_summary/*') from ch_qiime_qual\n    \n    output: \n    file(\"cutoffs.csv\") into ch_cutoff_vals\n    file(\"manifest_format.txt\") into ch_manifest_type_denoise\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n    import pandas as pd \n    from pathlib import Path\n    import numpy as np \n    import csv \n\n    wd = Path.cwd()\n\n    seq_file = pd.read_table(\"manifest_format.txt\")\n    if seq_file.columns[0] == \"SingleEndFastqManifestPhred33V2\":\n        seq_format = \"single\"\n    else:\n        seq_format = \"paired\"\n\n    def find_cutoffs(dataframe):\n        mean_qual = dataframe[4:5]\n        mean_count = dataframe[0:1]\n\n        average_qual = np.round(mean_qual.mean(axis=1), 0)\n        average_count = np.round(mean_count.mean(axis=1), 0)\n\n        mean_qual_vals = np.array(mean_qual)[0]\n        mean_count_vals = np.array(mean_count)[0]\n\n        if int(average_qual) < 30:\n            print(\n                \"The Average Quality of these sequences may be a concern would you like to continue?\")\n            exit(0)\n\n        for i in range(0, len(mean_qual_vals)):\n            if mean_qual_vals[i] >= int(average_qual):\n                if mean_count_vals[i] >= int(average_count):\n                    left_cutoff = i+1\n                    break\n        for i in range(0, len(mean_qual_vals)):\n            if mean_qual_vals[len(mean_qual_vals)-1-i] >= int(average_qual):\n                if mean_count_vals[len(mean_count_vals)-1-i] >= int(average_count):\n                    right_cutoff = len(mean_qual_vals)-i\n                    break\n        return(left_cutoff, right_cutoff)\n\n    def find_rev_cutoffs(dataframe):\n        mean_qual = dataframe[4:5]\n        mean_count = dataframe[0:1]\n\n        average_qual = np.round(mean_qual.mean(axis=1), 0)+2\n        average_count = np.round(mean_count.mean(axis=1), 0)\n\n        mean_qual_vals = np.array(mean_qual)[0]\n        mean_count_vals = np.array(mean_count)[0]\n\n        if int(average_qual) < 30:\n            print(\n                \"The Average Quality of these sequences may be a concern would you like to continue?\")\n            exit(0)\n\n        left_cutoff = 0\n        right_cutoff = len(mean_qual_vals)-1\n\n        for i in range(0, len(mean_qual_vals)):\n            if mean_qual_vals[i] >= int(average_qual):\n                if mean_count_vals[i] >= int(average_count):\n                    left_cutoff = i+1\n                    break\n\n        for i in range(0, len(mean_qual_vals)):\n            if mean_qual_vals[len(mean_qual_vals)-1-i] >= int(average_qual):\n                if mean_count_vals[len(mean_count_vals)-1-i] >= int(average_count):\n                    right_cutoff = len(mean_qual_vals)-i\n                    break\n\n        return(left_cutoff, right_cutoff)\n\n    if seq_format == \"single\":\n        print(\"determining left and right cutoffs based on qual score\")\n\n        input_file = str(wd)+\"/demux_summary/forward-seven-number-summaries.tsv\"\n\n        summary = pd.read_table(input_file, index_col=0, sep='\\t')\n        left_cutoff, right_cutoff = find_cutoffs(summary)\n\n        print(\"right cutoff: \"+str(right_cutoff))\n        print(\"left cutoff: \" + str(left_cutoff))\n\n        with open('cutoffs.csv', 'w', newline='') as csvfile:\n            fieldnames = ['cutoff', 'value']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            writer.writerow({'cutoff': 'left', 'value': left_cutoff})\n            writer.writerow({'cutoff': 'right', 'value': right_cutoff})\n            writer.writerow({'cutoff': 'filename', 'value': input_file})\n\n        print(left_cutoff, right_cutoff)\n\n    elif seq_format == \"paired\":\n        print(\"determining forward and revese, left and right cutoffs based on qual score\")\n        forward_file = str(wd)+\"/demux_summary/forward-seven-number-summaries.tsv\"\n        fr_summary = pd.read_table(forward_file, index_col=0, sep='\\t')\n\n        forward = find_cutoffs(fr_summary)\n\n        reverse_file = str(wd)+\"/demux_summary/reverse-seven-number-summaries.tsv\"\n        rev_summary = pd.read_table(reverse_file, index_col=0, sep='\\t')\n\n        reverse = find_rev_cutoffs(rev_summary)\n\n        print(\"forward cutoffs: \"+str(forward))\n        print(\"reverse cutoffs: \" + str(reverse))\n\n        with open('cutoffs.csv', 'w', newline='') as csvfile:\n            fieldnames = ['cutoff', 'value']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n            writer.writeheader()\n            writer.writerow({'cutoff': 'forward left', 'value': forward[0]})\n            writer.writerow({'cutoff': 'forward right', 'value': forward[1]})\n            writer.writerow({'cutoff': 'reverse left', 'value': reverse[0]})\n            writer.writerow({'cutoff': 'reverse right', 'value': reverse[1]})\n            writer.writerow({'cutoff': 'filename', 'value': forward_file})\n            writer.writerow({'cutoff': 'filename', 'value': reverse_file})\n\n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/FindCutoffs"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["WDAC", "RightField"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess Denoise {\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n    input:\n    file seq_object from ch_qiime_denoise\n    file(\"cutoffs.csv\") from ch_cutoff_vals\n    file(\"manifest_format.txt\") from ch_manifest_type_denoise\n    \n    \n    output:\n    file \"rep-seqs-dada2.qza\" into ch_rep_seqs\n    file \"table-dada2.qza\" into ch_table\n    file \"stats-dada2.qza\" into ch_dada2_stats\n    \n    \n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n    import pandas as pd \n    from pathlib import Path\n    import numpy as np \n    import csv \n    import subprocess\n\n    wd = Path.cwd()\n\n\n    seq_file = pd.read_table(\"manifest_format.txt\")\n    if seq_file.columns[0] == \"SingleEndFastqManifestPhred33V2\":\n        seq_format = \"single\"\n    else:\n        seq_format = \"paired\"\n    cutoff = pd.read_table(\"cutoffs.csv\", sep=\",\")    \n    if seq_format == 'single':\n        left = cutoff['value'][0]\n        right = cutoff['value'][1]\n        command = \"qiime dada2 denoise-single \\\n            --i-demultiplexed-seqs demux.qza \\\n            --p-trim-left \" + str(left)+\" \\\n            --p-trunc-len \" + str(right) + \" \\\n            --o-representative-sequences rep-seqs-dada2.qza \\\n            --o-table table-dada2.qza \\\n            --o-denoising-stats stats-dada2.qza\"\n    elif seq_format == 'paired':\n        forward_left = cutoff['value'][0]\n        forward_right = cutoff['value'][1]\n        rev_left = cutoff['value'][2]\n        rev_right = cutoff['value'][3]\n        command = \"qiime dada2 denoise-paired \\\n            --i-demultiplexed-seqs demux.qza \\\n            --p-trunc-len-f \" + str(forward_right)+\" \\\n            --p-trunc-len-r \" + str(rev_right) + \" \\\n            --p-trim-left-f \" + str(forward_left)+\" \\\n            --p-trim-left-r \" + str(rev_left) + \" \\\n            --o-representative-sequences rep-seqs-dada2.qza \\\n            --o-table table-dada2.qza \\\n            --o-denoising-stats stats-dada2.qza\"\n\n    subprocess.run([command], shell=True)\n    \n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/Denoise"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess FeatureVisualization{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n    input:\n    file \"stats-dada2.qza\" from ch_dada2_stats\n    file \"table-dada2.qza\" from ch_table\n    file metadata_file from ch_meta_feature_viz\n    file \"rep-seqs-dada2.qza\" from ch_rep_seqs\n\n    output:\n    file \"stats-dada2.qzv\" into ch_dada_stats_export\n    file \"table.qzv\" into ch_table_viz_obj\n    file \"table.qzv\" into ch_table_viz_export\n    file \"rep-seqs.qzv\" into ch_req_seq_vis_obj\n    file \"rep-seqs-dada2.qza\" into ch_rep_seq_tree_gen\n    file \"table-dada2.qza\" into ch_alpha_div_table\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    qiime metadata tabulate \\\n    --m-input-file stats-dada2.qza \\\n    --o-visualization stats-dada2.qzv\n\n    qiime feature-table summarize \\\n    --i-table table-dada2.qza \\\n    --o-visualization table.qzv \\\n    --m-sample-metadata-file ${metadata_file}\n\n    qiime feature-table tabulate-seqs \\\n    --i-data rep-seqs-dada2.qza \\\n    --o-visualization rep-seqs.qzv\n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/FeatureVisualization"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess TreeConstruction{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n    \n    input:\n    file \"rep-seqs-dada2.qza\" from ch_rep_seq_tree_gen\n\n    output:\n    file \"aligned-rep-seqs.qza\" into ch_aligned_rep_seqs\n    file \"masked-aligned-rep-seqs.qza\" into ch_mask_align_rep_seq\n    file \"unrooted-tree.qza\" into ch_unrooted_tree\n    file \"rooted-tree.qza\" into ch_rooted_tree\n    file \"rep-seqs-dada2.qza\" into ch_rep_seq_classify\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash \n\n    qiime phylogeny align-to-tree-mafft-fasttree \\\n    --i-sequences rep-seqs-dada2.qza \\\n    --o-alignment aligned-rep-seqs.qza \\\n    --o-masked-alignment masked-aligned-rep-seqs.qza \\\n    --o-tree unrooted-tree.qza \\\n    --o-rooted-tree rooted-tree.qza \n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/TreeConstruction"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess ExportTable{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n    \n    input:\n    file \"table.qzv\" from ch_table_viz_obj\n\n    output:\n    path \"table_viz/*\" into ch_table_viz_dir\n    path \"table_viz/*\" into ch_table_viz_dir_rare\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    qiime tools export \\\n    --input-path table.qzv \\\n    --output-path table_viz\n    \"\"\"\n\n}"], "list_proc": ["lorentzben/automate_16_nf/ExportTable"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess AlphaDiversityMeasure{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    file metadata from ch_alpha_metadata\n    file \"table-dada2.qza\" from ch_alpha_div_table\n    file \"rooted-tree.qza\" from ch_rooted_tree\n    file \"samp_depth_simple.txt\" from ch_depth\n    val user_depth from ch_user_sample_depth\n\n    output:\n    path \"core-metric-results/*\" into ch_core_beta_significance \n    path \"core-metric-results/*\" into ch_core_report\n    file \"shannon.qza\" into ch_shannon_qza\n    file \"simpson.qza\" into ch_simpson_qza \n    file \"chao1.qza\" into ch_chao_qza\n    file \"ace.qza\" into ch_ace_qza\n    file \"obs.qza\" into ch_obs_qza\n    file \"faith_pd.qza\" into ch_faith_qza\n    file \"table-dada2.qza\" into ch_table_rare_curve\n    file \"rooted-tree.qza\" into ch_tree_rare_curve\n    \n\n    \n\n    shell:\n    '''\n    #!/usr/bin/env bash\n\n    if [ !{user_depth} == 0 ];then\n        SAMP_DEPTH=$(head samp_depth_simple.txt)\n    fi\n    \n    if [ !{user_depth} != 0 ];then\n        SAMP_DEPTH=!{user_depth}\n    fi\n  \n    qiime diversity core-metrics-phylogenetic \\\n    --i-phylogeny rooted-tree.qza \\\n    --i-table table-dada2.qza \\\n    --p-sampling-depth $SAMP_DEPTH \\\n    --m-metadata-file !{metadata} \\\n    --output-dir core-metric-results \n\n    qiime diversity alpha \\\n    --i-table table-dada2.qza \\\n    --p-metric shannon \\\n    --o-alpha-diversity shannon.qza\n\n    qiime diversity alpha \\\n    --i-table table-dada2.qza \\\n    --p-metric simpson \\\n    --o-alpha-diversity simpson.qza \n\n    qiime diversity alpha \\\n    --i-table table-dada2.qza \\\n    --p-metric chao1 \\\n    --o-alpha-diversity chao1.qza\n\n    qiime diversity alpha \\\n    --i-table table-dada2.qza \\\n    --p-metric ace \\\n    --o-alpha-diversity ace.qza\n\n    qiime diversity alpha \\\n    --i-table table-dada2.qza \\\n    --p-metric observed_features \\\n    --o-alpha-diversity obs.qza \n\n    qiime diversity alpha-phylogenetic \\\n    --i-table table-dada2.qza \\\n    --i-phylogeny rooted-tree.qza \\\n    --p-metric faith_pd \\\n    --o-alpha-diversity faith_pd.qza \n    '''\n}"], "list_proc": ["lorentzben/automate_16_nf/AlphaDiversityMeasure"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess AssignTaxonomy{\n                                                          \n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    file \"rep-seqs-dada2.qza\" from ch_rep_seq_classify\n    file \"16s-whole-seq-classifier.qza\" from ch_whole_classifier\n    file \"515-806-classifier.qza\" from ch_515_classifier\n\n    output:\n    file \"taxonomy.qza\" into ch_taxonomy_phylo_tree\n    file \"taxonomy.qzv\" into ch_classified_qzv\n    \n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    if [ ! -f \"16s-whole-seq-classifier.qza\" ]; then \n    echo \"Error, download the classifier from readme\"\n    exit 1\n    fi\n    if [ ! -f \"515-806-classifier.qza\" ]; then \n    echo \"Error, download the classifier from readme\"\n    exit 1\n    fi\n\n    qiime feature-classifier classify-sklearn \\\n    --i-classifier 16s-whole-seq-classifier.qza \\\n    --i-reads rep-seqs-dada2.qza \\\n    --p-confidence 0.6 \\\n    --o-classification taxonomy.qza\n\n    qiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/AssignTaxonomy"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["DEPTH"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess CalcRareDepth{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    path \"table_viz/*\" from ch_table_viz_dir_rare\n\n    output:\n    file \"rare_depth.txt\" into ch_rare_curve_depth\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n    import pandas as pd\n    import numpy as np \n\n    sample_freq = pd.read_csv(\"table_viz/sample-frequency-detail.csv\")\n    depth = sample_freq.median()[0]\n\n    with open(\"rare_depth.txt\",'w') as file:\n        file.write(str(int(depth)))\n    \n    \"\"\"\n\n}"], "list_proc": ["lorentzben/automate_16_nf/CalcRareDepth"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess RareCurveCalc{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    file \"rare_depth.txt\" from ch_rare_curve_depth\n    file metadata from ch_metadata_rare_curve\n    file \"table-dada2.qza\" from ch_table_rare_curve\n    file \"rooted-tree.qza\" from ch_tree_rare_curve\n    val user_rare_depth from ch_user_rarefaction_depth\n\n\n    output:\n    file \"alpha-rarefaction.qzv\" into ch_alpha_rare_obj\n    path \"alpha-rareplot/*\" into ch_alpha_rare_viz\n    file \"table-dada2.qza\" into ch_table_phylo_tree\n    file \"rooted-tree.qza\" into ch_tree_lefse\n    \n\n    shell:\n    '''\n    #!/usr/bin/env bash\n\n    if [ !{user_rare_depth} == 0 ];then\n        DEPTH=$(head rare_depth.txt)\n    fi\n    \n    if [ !{user_rare_depth} != 0 ];then\n        DEPTH=!{user_rare_depth}\n    fi\n\n    \n    \n    qiime diversity alpha-rarefaction \\\n    --i-table table-dada2.qza \\\n    --i-phylogeny rooted-tree.qza \\\n    --p-max-depth $DEPTH \\\n    --m-metadata-file !{metadata} \\\n    --o-visualization alpha-rarefaction.qzv \n\n    qiime tools export \\\n    --input-path alpha-rarefaction.qzv \\\n    --output-path alpha-rareplot\n\n    '''\n}"], "list_proc": ["lorentzben/automate_16_nf/RareCurveCalc"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess AlphaDiversitySignificance{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    file metadata from ch_metadata_alpha_sig\n    file \"shannon.qza\" from ch_shannon_qza\n    file \"simpson.qza\" from ch_simpson_qza\n    file \"chao1.qza\" from ch_chao_qza\n    file \"ace.qza\" from ch_ace_qza\n    file \"obs.qza\" from ch_obs_qza\n    file \"faith_pd.qza\" from ch_faith_qza\n\n    output:\n    path \"shannon/*\" into ch_shannon_path\n    path \"simpson/*\" into ch_simpson_path\n    path \"chao1/*\" into ch_chao_path\n    path \"ace/*\" into ch_ace_path\n    path \"obs/*\" into ch_obs_path\n    path \"faith_pd/*\" into ch_faith_path\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    qiime diversity alpha-group-significance \\\n    --i-alpha-diversity shannon.qza \\\n    --m-metadata-file ${metadata} \\\n    --o-visualization shannon.qzv \n\n    qiime diversity alpha-group-significance \\\n    --i-alpha-diversity simpson.qza \\\n    --m-metadata-file ${metadata} \\\n    --o-visualization simpson.qzv\n\n    qiime diversity alpha-group-significance \\\n    --i-alpha-diversity chao1.qza \\\n    --m-metadata-file ${metadata} \\\n    --o-visualization chao1.qzv \n\n    qiime diversity alpha-group-significance \\\n    --i-alpha-diversity ace.qza \\\n    --m-metadata-file ${metadata} \\\n    --o-visualization ace.qzv \n\n    qiime diversity alpha-group-significance \\\n    --i-alpha-diversity obs.qza \\\n    --m-metadata-file ${metadata} \\\n    --o-visualization obs.qzv \n\n    qiime diversity alpha-group-significance \\\n    --i-alpha-diversity faith_pd.qza \\\n    --m-metadata-file ${metadata} \\\n    --o-visualization faith_pd.qzv \n\n    qiime tools export \\\n    --input-path shannon.qzv \\\n    --output-path shannon\n\n    qiime tools export \\\n    --input-path simpson.qzv \\\n    --output-path simpson\n\n    qiime tools export \\\n    --input-path chao1.qzv \\\n    --output-path chao1\n\n    qiime tools export \\\n    --input-path ace.qzv \\\n    --output-path ace \n\n    qiime tools export \\\n    --input-path obs.qzv \\\n    --output-path obs\n\n    qiime tools export \\\n    --input-path faith_pd.qzv \\\n    --output-path faith_pd\n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/AlphaDiversitySignificance"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess BetaDiversitySignificance{\n    publishDir \"${params.outdir}/qiime\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    val ioi from ch_ioi_beta_sig\n    file metadata from ch_metadata_beta_sig\n    path \"core-metric-results/*\" from ch_core_beta_significance \n\n    output:\n    path \"unweighted-sig/*\" into ch_u_unifrac_beta_path\n    path \"weighted-sig/*\" into ch_w_unifrac_beta_path\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    qiime diversity beta-group-significance \\\n    --i-distance-matrix core-metric-results/unweighted_unifrac_distance_matrix.qza \\\n    --m-metadata-file ${metadata} \\\n    --m-metadata-column ${ioi} \\\n    --o-visualization unweighted-unifrac-${ioi}-significance.qzv \\\n    --p-pairwise\n\n    qiime tools export \\\n    --input-path unweighted-unifrac-${ioi}-significance.qzv \\\n    --output-path unweighted-sig/\n\n    qiime diversity beta-group-significance \\\n    --i-distance-matrix core-metric-results/weighted_unifrac_distance_matrix.qza \\\n    --m-metadata-file ${metadata} \\\n    --m-metadata-column ${ioi} \\\n    --o-visualization  weighted-unifrac-${ioi}-significance.qzv \\\n    --p-pairwise\n\n    qiime tools export \\\n    --input-path weighted-unifrac-${ioi}-significance.qzv \\\n    --output-path weighted-sig/\n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/BetaDiversitySignificance"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["QIIME"], "nb_own": 1, "list_own": ["lorentzben"], "nb_wf": 1, "list_wf": ["automate_16_nf"], "list_contrib": ["lorentzben"], "nb_contrib": 1, "codes": ["\nprocess ExportSetup{\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n                                           \n                             \n    container \"docker://lorentzb/automate_16_nf\"\n\n    input:\n    file \"stats-dada2.qzv\" from ch_dada_stats_export\n    file metadata from ch_metadata_finalize \n\n    output:\n    file \"dada2_stats.tsv\" into ch_dada_stats_file\n    file \"metadata.tsv\" into ch_metadata_renamed\n\n    script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    qiime tools export \\\n    --input-path stats-dada2.qzv \\\n    --output-path stats-dada2\n\n    cp stats-dada2/metadata.tsv ./dada2_stats.tsv\n\n    cp ${metadata} ./metadata.tsv  \n    \"\"\"\n}"], "list_proc": ["lorentzben/automate_16_nf/ExportSetup"], "list_wf_names": ["lorentzben/automate_16_nf"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["louperelo"], "nb_wf": 1, "list_wf": ["longmetarg"], "list_contrib": ["louperelo"], "nb_contrib": 1, "codes": ["\nprocess assembly_MetaFlye {\n    label 'metaflye' \n\n    input:\n    file in_flye\n\n    publishDir \"$params.outdir\"\n\n    output:     \n    file 'out_flye/assembly.fasta' into (out_flye, flye_analysis)\n    \n    when:\n    params.flye == true\n\n    script:\n      \n                                                                                              \n                                                        \n      \n    \"\"\"\n    flye --$params.flyeDt $in_flye --threads $params.threads --out-dir out_flye --meta --plasmids --genome-size $params.genomeSize \n\n    \"\"\"\n}"], "list_proc": ["louperelo/longmetarg/assembly_MetaFlye"], "list_wf_names": ["louperelo/longmetarg"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Minimap2", "XTRACT"], "nb_own": 2, "list_own": ["lukasjelonek", "louperelo"], "nb_wf": 2, "list_wf": ["sequencing-technology-usage-diagram", "longmetarg"], "list_contrib": ["louperelo", "lukasjelonek"], "nb_contrib": 2, "codes": ["\nprocess map_CARD {\n    label 'mapcard' \n\n    input:\n\tfile fa from in_align\n\t\n    publishDir \"$params.outdir\", mode: 'copy'                   \n\n    output:  \n    file '*' into card_aln \n\n\twhen:\n\tparams.flye\t== false\n        \n    script:\n\t\n    \"\"\"\n    minimap2 -ax $params.readtype $params.argDb $fa -t $params.threads -c | samtools view -S -b | samtools view -b -F 4 > aln_F4.bam\n\n    \"\"\" \t\n}", "\nprocess lookup_data {\n    publishDir params.out, mode: 'copy'\n    \n    container \"quay.io/biocontainers/entrez-direct:10.2--pl526_0\"\n\n    input:\n    set val(start), val(end) from ch_years\n\n    output:\n    file 'data.csv' into ch_data\n\n    shell:\n    '''\n    #!/bin/bash\n    PLATFORM=(\"abi solid\" \"bgiseq\" \"capillary\" \"complete genomics\" \"helicos\" \"illumina\" \"ion torrent\" \"ls454\" \"oxford nanopore\" \"pacbio smrt\")\n    TYPE=(\"genomic\" \"genomic single cell\" \"metagenomic\" \"metatranscriptomic\" \"other\" \"synthetic\" \"transcriptomic\" \"transcriptomic single cell\" \"viral rna\")\n    echo \"technology,type,year,count\" > data.csv\n    for p in \"${PLATFORM[@]}\"; do\n      for t in \"${TYPE[@]}\"; do\n        for y in $(seq !{start} !{end}); do\n          COUNT=$(esearch -db sra -query \"(((\\\\\"$y/01/01\\\\\"[Modification Date] : \\\\\"$y/12/31\\\\\"[Modification Date])) AND \\\\\"$p\\\\\"[Platform]) AND \\\\\"$t\\\\\"[Source]\" | xtract -pattern ENTREZ_DIRECT -element Count)\n          echo ${p},${t},${y},${COUNT} >> data.csv\n        done\n      done\n    done\n    '''\n}"], "list_proc": ["louperelo/longmetarg/map_CARD", "lukasjelonek/sequencing-technology-usage-diagram/lookup_data"], "list_wf_names": ["louperelo/longmetarg", "lukasjelonek/sequencing-technology-usage-diagram"]}, {"nb_reuse": 1, "tools": ["shovill"], "nb_own": 1, "list_own": ["lskatz"], "nb_wf": 1, "list_wf": ["Kessel-run"], "list_contrib": ["lskatz"], "nb_contrib": 1, "codes": ["\nprocess assembly {\n\n  input:\n  set val(samplename), file(fastqs) from fastqChannel\n\n  output:\n  file(\"${samplename}.fa\") into asmChannel\n\n  shell:\n  '''\n  gbMemory=$(echo \"!{task.memory}\" | sed 's/[A-Za-z]//')\n  depth=100 # can change for debugging purposes. Default:100\n  # ensure R1 and R2 are correct\n  R1=\"!{fastqs[0]}\"\n  R2=\"!{fastqs[1]}\"\n  export TMPDIR=$PWD/tmp\n  if [ -e /scratch ]; then\n      export TMPDIR=/scratch/$USER/scratch\n  fi\n  mkdir -pv $TMPDIR\n\n  shovilltemp=$TMPDIR/!{samplename}.shovill.tmp\n  outdir=$TMPDIR/!{samplename}.shovill.out\n\n  echo \"TMPDIR is $TMPDIR\"\n\n  shovill --check\n  echo =====\n  shovill --depth $depth --tmpdir $shovilltemp --outdir $outdir --R1 $R1 --R2 $R2 --assembler spades --ram $gbMemory --cpus !{task.cpus} --force --gsize !{params.genomesize}\n  cp -v $outdir/contigs.fa ./!{samplename}.fa\n  '''\n}"], "list_proc": ["lskatz/Kessel-run/assembly"], "list_wf_names": ["lskatz/Kessel-run"]}, {"nb_reuse": 1, "tools": ["wtdbg2"], "nb_own": 1, "list_own": ["lstevens17"], "nb_wf": 1, "list_wf": ["nemAsm"], "list_contrib": ["lstevens17"], "nb_contrib": 1, "codes": ["\nprocess prelim_wtdbg2_assembly {\n    publishDir \"${params.outdir}\", mode: 'copy'\n    conda '/lustre/scratch116/tol/teams/team301/users/ls30/miniconda3/envs/wtdbg2_env'\n    \n    input:\n        tuple val(strain), path(reads)\n\n    output:\n        path \"${strain}_wtdbg2_prelim.fa\"\n\n    script:\n    \"\"\"\n    wtdbg2 -fo ${strain}_wtdbg2_prelim -t ${task.cpus} -x ccs -g 100m -i $reads\n    wtpoa-cns -t ${task.cpus} -i ${strain}_wtdbg2_prelim.ctg.lay.gz -fo ${strain}_wtdbg2_prelim.fa\n    \"\"\"\n}"], "list_proc": ["lstevens17/nemAsm/prelim_wtdbg2_assembly"], "list_wf_names": ["lstevens17/nemAsm"]}, {"nb_reuse": 1, "tools": ["TOBIAS"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["briscoe-nf-tobias"], "list_contrib": ["MJDelas", "chris-cheshire"], "nb_contrib": 2, "codes": ["\nprocess tobias_plotaggregate {\n    tag \"${motif_name}\"\n    label 'mn_cpu'\n    time = 4.h\n\n    publishDir \"${params.outdir}/${opts.publish_dir}\",\n        mode: \"copy\", \n        overwrite: true,\n        saveAs: { filename ->\n                      if (opts.publish_results == \"none\") null\n                      else filename }\n\n    container 'quay.io/biocontainers/tobias:0.12.10--py37h77a2a36_1'\n\n    input:\n        val opts\n        val motif_name\n        path motif_beds\n        path corrected_insertions\n\n    output:\n        path \"$motif_name/*.pdf\", emit: pdf\n\n    script:\n    \"\"\"\n    mkdir $motif_name\n    TOBIAS PlotAggregate --TFBS $motif_beds --signals ${corrected_insertions.join(' ')} --output $motif_name/${motif_name}_plotaggregate.pdf --share_y both --plot_boundaries --signal-on-x\n    \"\"\"\n}"], "list_proc": ["luslab/briscoe-nf-tobias/tobias_plotaggregate"], "list_wf_names": ["luslab/briscoe-nf-tobias"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["luslab-nf-modules"], "list_contrib": ["charles-plessy", "mjmansfi", "candiceh08", "sidorov-si", "amchakra", "chris-cheshire", "alexthiery", "charlotte-west", "marc-jones"], "nb_contrib": 9, "codes": ["\nprocess umitools_dedup {\n    label \"min_cores\"\n    label \"avg_mem\"\n    label \"regular_queue\"\n\n    tag \"${meta.sample_id}\"\n    \n    publishDir \"${params.outdir}/${opts.publish_dir}\",\n        mode: \"copy\", \n        overwrite: true,\n        saveAs: { filename ->\n                      if (opts.publish_results == \"none\") null\n                      else filename }\n\n                                     \n    container 'quay.io/biocontainers/mulled-v2-509311a44630c01d9cb7d2ac5727725f51ea43af:b4c5bc18f99e35cd061328b98f501aa7b9717308-0'\n\n    input:\n        val(opts)\n        tuple val(meta), path(bam), path(bai)\n       \n    output:\n        tuple val(meta), path(\"${prefix}.bam\"), path(\"${prefix}.bam.bai\"), emit: dedupBam\n        path \"*.log\", emit: report\n\n    script:\n\n           \n    prefix = opts.suffix ? \"${meta.sample_id}${opts.suffix}\" : \"${meta.sample_id}\"\n\n    args = \"--log=${prefix}.log \"\n\n    if(opts.args && opts.args != '') {\n        ext_args = opts.args\n        args += ext_args.trim()\n    }\n\n                        \n    dedup_command = \"umi_tools dedup ${args} -I ${bam[0]} -S ${prefix}.bam --output-stats=${prefix}\"\n\n          \n    if (params.verbose){\n        println (\"[MODULE] umi_tools/dedup command: \" + dedup_command)\n    }\n\n           \n    \"\"\"\n    ${dedup_command}\n    samtools index ${prefix}.bam\n    \"\"\"\n}"], "list_proc": ["luslab/luslab-nf-modules/umitools_dedup"], "list_wf_names": ["luslab/luslab-nf-modules"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["luslab-nf-modules"], "list_contrib": ["charles-plessy", "mjmansfi", "candiceh08", "sidorov-si", "amchakra", "chris-cheshire", "alexthiery", "charlotte-west", "marc-jones"], "nb_contrib": 9, "codes": ["\nprocess getcrosslinks {\n    label \"low_cores\"\n    label \"high_mem\"\n    label \"regular_queue\"\n\n    tag \"${meta.sample_id}\"\n\n    publishDir \"${params.outdir}/${opts.publish_dir}\",\n        mode: \"copy\", \n        overwrite: true,\n        saveAs: { filename ->\n                      if (opts.publish_results == \"none\") null\n                      else filename }\n\n                                 \n    container 'quay.io/biocontainers/mulled-v2-c8623b4f6522dddd48913bd12dcf405d1d4f0ce1:10e4c359b727e884f6e19ee978f89c44dbaca255-0'\n\n    input:\n      val(opts)\n      tuple val(meta), path(bam), path(bai)\n      path(fai)\n\n    output:\n      tuple val(meta), path (\"${prefix}.bed.gz\"), emit: crosslinkBed\n\n    script:\n\n      prefix = opts.suffix ? \"${meta.sample_id}${opts.suffix}\" : \"${meta.sample_id}\"\n\n             \n      \"\"\"\n      bedtools bamtobed -i ${bam[0]} > dedupe.bed\n      bedtools shift -m 1 -p -1 -i dedupe.bed -g $fai > shifted.bed\n      bedtools genomecov -dz -strand + -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"+\"}' > pos.bed\n      bedtools genomecov -dz -strand - -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"-\"}' > neg.bed\n      cat pos.bed neg.bed | sort -k1,1 -k2,2n | pigz > ${prefix}.bed.gz\n      \"\"\"\n}"], "list_proc": ["luslab/luslab-nf-modules/getcrosslinks"], "list_wf_names": ["luslab/luslab-nf-modules"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["luslab-nf-modules"], "list_contrib": ["charles-plessy", "mjmansfi", "candiceh08", "sidorov-si", "amchakra", "chris-cheshire", "alexthiery", "charlotte-west", "marc-jones"], "nb_contrib": 9, "codes": ["\nprocess seqtk_subsample {\n    label \"min_cores\"\n    label \"min_mem\"\n    label \"regular_queue\"\n\n    tag \"${meta.sample_id}\"\n\n    publishDir \"${params.outdir}/${opts.publish_dir}\",\n        mode: \"copy\", \n        overwrite: true,\n        saveAs: { filename ->\n                      if (opts.publish_results == \"none\") null\n                      else filename }\n\n    container 'quay.io/biocontainers/seqtk:1.3--hed695b0_2'\n    \n    input:\n      val opts\n      tuple val(meta), path(reads)\n\n    output:\n      tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n        \n    script:\n        args = \"\"\n        if(opts.args && opts.args != '') {\n            ext_args = opts.args\n            args += ext_args.trim()\n        }\n\n        prefix = opts.suffix ? \"${meta.sample_id}${opts.suffix}\" : \"${meta.sample_id}\"\n\n                        \n        seed = \"-s100\"\n        if(opts.seed && opts.seed != '') {\n            seed = \"-s\" + opts.seed\n        }\n\n                          \n        number = 10000\n        if(opts.base_count && opts.base_count != '') {\n            number = opts.base_count\n        }\n\n                                    \n        seqtk_command = \"seqtk sample $seed ${reads[0]} $number $args> ${prefix}.fastq\"\n        if (params.verbose){\n            println (\"[MODULE] seqtk subsample command: \" + seqtk_command)\n        }\n\n                                       \n        readList = reads.collect{it.toString()}\n        if(readList.size == 1){\n            \"\"\"\n            seqtk sample $seed ${reads[0]} $number $args> ${prefix}.fastq\n            gzip ${prefix}.fastq\n            \"\"\"\n        }\n        else {\n            \"\"\"\n            seqtk sample $seed ${reads[0]} $number $args> ${prefix}.r1.fastq\n            seqtk sample $seed ${reads[1]} $number $args > ${prefix}.r2.fastq\n            gzip ${prefix}.r1.fastq && gzip ${prefix}.r2.fastq\n            \"\"\"\n        }\n}"], "list_proc": ["luslab/luslab-nf-modules/seqtk_subsample"], "list_wf_names": ["luslab/luslab-nf-modules"]}, {"nb_reuse": 1, "tools": ["TRUmiCount", "BEDTools"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["luslab-nf-modules"], "list_contrib": ["charles-plessy", "mjmansfi", "candiceh08", "sidorov-si", "amchakra", "chris-cheshire", "alexthiery", "charlotte-west", "marc-jones"], "nb_contrib": 9, "codes": ["\nprocess icount {\n    label \"low_cores\"\n    label \"low_mem\"\n    label \"regular_queue\"\n\n    tag \"${meta.sample_id}\"\n    \n    publishDir \"${params.outdir}/${opts.publish_dir}\",\n        mode: \"copy\", \n        overwrite: true,\n        saveAs: { filename ->\n                      if (opts.publish_results == \"none\") null\n                      else filename }\n\n                                              \n    container 'quay.io/biocontainers/mulled-v2-72b6e5587e443030dafa3295501578c42d27520f:b2abdf2431759fbb85efa4b0323719d6df878912-0'\n\n    input:\n        val opts\n        tuple val(meta), path(bed)\n        path seg\n\n    output:\n        tuple val(meta), path(\"${prefix}.peaks.bed.gz\"), emit: peaks\n        tuple val(meta), path(\"${prefix}.scores.tsv\"), emit: peak_scores\n        tuple val(meta), path(\"${prefix}.clusters.bed.gz\"), emit: clusters\n    \n    script:\n        prefix = opts.suffix ? \"${meta.sample_id}${opts.suffix}\" : \"${meta.sample_id}\"\n\n               \n        \"\"\"\n        iCount peaks $seg $bed ${prefix}.peaks.bed.gz \\\n            --scores ${prefix}.scores.tsv \\\n            --half_window ${opts.half_window} \\\n            --fdr ${opts.fdr}\n\n        zcat ${prefix}.peaks.bed.gz | \\\n        bedtools merge -i stdin -s -d ${opts.half_window} -c 4,5,6 -o distinct,sum,distinct | \\\n        gzip > ${prefix}.clusters.bed.gz\n        \"\"\"\n}"], "list_proc": ["luslab/luslab-nf-modules/icount"], "list_wf_names": ["luslab/luslab-nf-modules"]}, {"nb_reuse": 1, "tools": ["TRUmiCount", "BEDTools"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["nf-clip"], "list_contrib": ["Rahul1711arora", "candiceh08", "amchakra", "BeibeiDu", "kkuret", "ojziff", "chris-cheshire", "alexthiery", "CharlotteAnne"], "nb_contrib": 9, "codes": ["\nprocess icount {\n                           \n    tag \"${sample_id}\"\n\n    publishDir \"${params.internal_outdir}/${params.internal_process_name}\",\n        mode: \"copy\", overwrite: true\n\n    input:\n      tuple val(sample_id), path(bed), path(seg)\n\n    output:\n      tuple val(sample_id), path(\"${bed.simpleName}.xl.peaks.bed.gz\"), emit: peaks\n      tuple val(sample_id), path(\"${bed.simpleName}.scores.tsv\"), emit: peak_scores\n      tuple val(sample_id), path(\"${bed.simpleName}.xl.clusters.bed.gz\"), emit: clusters\n\n    shell:\n    \"\"\"\n    iCount peaks $seg $bed ${bed.simpleName}.xl.peaks.bed.gz \\\n        --scores ${bed.simpleName}.scores.tsv \\\n        --half_window ${params.internal_half_window} \\\n        --fdr ${params.internal_fdr}\n\n    zcat ${bed.simpleName}.xl.peaks.bed.gz | \\\n    bedtools merge -i stdin -s -d ${params.internal_half_window} -c 4,5,6 -o distinct,sum,distinct | \\\n    gzip > ${bed.simpleName}.xl.clusters.bed.gz\n    \"\"\"\n}"], "list_proc": ["luslab/nf-clip/icount"], "list_wf_names": ["luslab/nf-clip"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["nf-clip"], "list_contrib": ["Rahul1711arora", "candiceh08", "amchakra", "BeibeiDu", "kkuret", "ojziff", "chris-cheshire", "alexthiery", "CharlotteAnne"], "nb_contrib": 9, "codes": ["\nprocess getcrosslinks {\n    publishDir \"${params.internal_outdir}/${params.internal_process_name}\",\n        mode: \"copy\", overwrite: true\n\n    input:\n      tuple val(sample_id), path(bam), path (fai)\n\n    output:\n      tuple val(sample_id), path (\"${bam.simpleName}.xl.bed.gz\"), emit: crosslinkBed\n\n    script:\n    \"\"\"\n    bedtools bamtobed -i $bam > dedupe.bed\n    bedtools shift -m 1 -p -1 -i dedupe.bed -g $fai > shifted.bed\n    bedtools genomecov -dz -strand + -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"+\"}' > pos.bed\n    bedtools genomecov -dz -strand - -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"-\"}' > neg.bed\n    cat pos.bed neg.bed | sort -k1,1 -k2,2n | pigz > ${bam.simpleName}.xl.bed.gz\n    \"\"\"\n}"], "list_proc": ["luslab/nf-clip/getcrosslinks"], "list_wf_names": ["luslab/nf-clip"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 1, "list_wf": ["nf-clip"], "list_contrib": ["Rahul1711arora", "candiceh08", "amchakra", "BeibeiDu", "kkuret", "ojziff", "chris-cheshire", "alexthiery", "CharlotteAnne"], "nb_contrib": 9, "codes": ["\nprocess bedtools_intersect {\n\n    publishDir \"${params.internal_outdir}/${params.internal_process_name}\",\n        mode: \"copy\", overwrite: true\n\n    input: \n        tuple val(sample_id), path(reads), path(regions_file)\n\n    output: \n        tuple val(sample_id), path(\"${sample_id}.annotated.bed\"), emit: annotatedBed\n\n    shell:\n    \"\"\"\n    bedtools intersect -a ${regions_file} -b $reads -wa -wb -s > ${sample_id}.annotated.bed\n    \"\"\"\n}"], "list_proc": ["luslab/nf-clip/bedtools_intersect"], "list_wf_names": ["luslab/nf-clip"]}, {"nb_reuse": 0, "tools": ["htseqcount"], "nb_own": 1, "list_own": ["luslab"], "nb_wf": 0, "list_wf": ["nf-core-modules"], "list_contrib": ["amchakra", "chris-cheshire", "alexthiery", "charlotte-west", "CharlotteAnne", "marc-jones"], "nb_contrib": 6, "codes": ["\nprocess HTSEQ_COUNT {\n    tag \"$meta.id\"\n    label \"min_cores\"\n    label \"low_mem\"\n    label \"regular_queue\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::htseq=0.13.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/htseq:0.13.5--py39h70b41aa_1\"\n    } else {\n        container \"quay.io/biocontainers/htseq:0.13.5--py39h70b41aa_1\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path (bai)\n    path gtf\n\n    output:\n\n    tuple val(meta), path(\"*.tsv\"), emit: counts\n    path \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    \"\"\"\n    htseq-count \\\\\n        ${options.args} \\\\\n        ${bam} \\\\\n        ${gtf} \\\\\n        --nprocesses $task.cpus \\\\\n        > \\\\\n        ${prefix}.tsv\n\n\n    htseq-count --version > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["StringTie"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nanorna-bam"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": ["\nprocess StringTie2 {\n    publishDir \"${params.outdir}/stringtie2\", mode: 'copy',\n        saveAs: { filename ->\n                      if (!filename.endsWith(\".version\")) filename\n                }\n\n    input:\n    set val(name), file(bam), file(annot) from ch_txome_reconstruction\n\n    output:\n    set val(name), file(bam), file(\"*.out.gtf\") into ch_txome_feature_count\n    file(\"*.version\") into ch_stringtie_version\n\n    script:\n    \"\"\"\n    stringtie -L -G $annot -o ${name}.out.gtf $bam\n    stringtie --version &> stringtie.version\n    \"\"\"\n}"], "list_proc": ["lwratten/nanorna-bam/StringTie2"], "list_wf_names": ["lwratten/nanorna-bam"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nanorna"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": ["\nprocess getChrSizes {\n                                                         \n\n  input:\n  file genome from refgenome\n\n  output:\n  file \"${genome.simpleName}.chrSizes.txt\" into chrsize_ch1, chrsize_ch2\n  file \"${genome.baseName}.chrSizes.ucsc.txt\" into ucsc_chrsize_ch1, ucsc_chrsize_ch2\n\n  script:\n  \"\"\"\n  samtools faidx ${genome} > ${genome}.fai\n  cut -f1,2 ${genome}.fai > ${genome.simpleName}.chrSizes.txt\n  formatUCSC.pl ${genome.simpleName}.chrSizes.txt > \\\n  ${genome.baseName}.chrSizes.ucsc.txt\n  \"\"\"\n}"], "list_proc": ["lwratten/nanorna/getChrSizes"], "list_wf_names": ["lwratten/nanorna"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nanorna"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": [" process minimapAlign {\n    input:\n    file rawdata from file_ch\n    file mmi from mmi_ch\n\n    output:\n    file \"*.sam\" into sam_ch\n\n    \"\"\"\n    minimap2 ${minimap2call} -I ${params.ram}G -t ${params.threads} ${juncbed} \\\n    $mmi ${rawdata} > ${rawdata.baseName}.sam\n    \"\"\"\n  }"], "list_proc": ["lwratten/nanorna/minimapAlign"], "list_wf_names": ["lwratten/nanorna"]}, {"nb_reuse": 1, "tools": ["Graphmap2"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nanorna"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": [" process graphmapAlign {\n    echo true\n    input:\n    file rawdata from file_ch\n    file genome from refgenome\n\n    output:\n    file \"*.sam\" into sam_ch\n\n           \n    \"\"\"\n    graphmap2 align -x rnaseq -r $rawdata -d $genome -o ${rawdata.baseName}.sam \\\n    -t ${params.threads} $graphmapIndex\n    \"\"\"\n  }"], "list_proc": ["lwratten/nanorna/graphmapAlign"], "list_wf_names": ["lwratten/nanorna"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nanorna"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": ["\nprocess sam2bam {\n  publishDir \"${params.outdir}\", mode: 'copy'\n\n  input:\n  file sam from sam_ch\n\n  output:\n  file \"*.bam\" into indexbam_ch, bam2bed12_ch, bam2bedgraph_ch, bam2bigwig_ucsc_ch\n\n  script:\n  \"\"\"\n  samtools view -Sb $sam | samtools sort -o ${sam.baseName}.bam -\n  \"\"\"\n}"], "list_proc": ["lwratten/nanorna/sam2bam"], "list_wf_names": ["lwratten/nanorna"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nanorna"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": [" process bam2bed12 {\n    input:\n    file bam from bam2bed12_ch\n\n    output:\n    file \"${bam.baseName}.bed12\" into makeUCSCbed12_ch, bed12_ch\n    file \"${bam.baseName}.sorted.bed12\" into sorted_bed12_ch\n\n    script:\n    \"\"\"\n    bamToBed -bed12 -cigar -i $bam > ${bam.baseName}.bed12\n    bedtools sort -i ${bam.baseName}.bed12 > ${bam.baseName}.sorted.bed12\n    \"\"\"\n  }"], "list_proc": ["lwratten/nanorna/bam2bed12"], "list_wf_names": ["lwratten/nanorna"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["lwratten"], "nb_wf": 1, "list_wf": ["nf_example"], "list_contrib": ["lwratten"], "nb_contrib": 1, "codes": ["\nprocess sam_to_bam {\n                                                                    \n  publishDir params.outdir, mode:'copy'\n                                           \n                                                                    \n\n  input:\n  file sam from ch_sam\n\n  output:\n  file \"*.bam\" into ch_bam\n\n  script:\n  \"\"\"\n  samtools view -Sb $sam | samtools sort -o ${sam.baseName}.bam -\n  \"\"\"\n}"], "list_proc": ["lwratten/nf_example/sam_to_bam"], "list_wf_names": ["lwratten/nf_example"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["madhyastha"], "nb_wf": 1, "list_wf": ["gatk-nextflow-sample"], "list_contrib": ["madhyastha"], "nb_contrib": 1, "codes": ["\nprocess bwa_mem {\n    container \"${containers.bwa}\"\n    cpus 8\n    memory \"64 GB\"\n\n  input:\n    file '*' from ref_indices\n    file '*' from reads\n  \n  output:\n    file \"${sample_id}.sam\" into sam_file\n  \n  script:\n  \"\"\"\n  bwa mem -M -t 16 -p -R '@RG\\\\tID:${sample_id}\\\\tSM:${sample_id}\\\\tPL:Illumina' \\\\\n        ${ref_name} \\\n        ${sample_id}_*1*.fastq.gz \\\n        > ${sample_id}.sam\n  \"\"\"\n}"], "list_proc": ["madhyastha/gatk-nextflow-sample/bwa_mem"], "list_wf_names": ["madhyastha/gatk-nextflow-sample"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["madhyastha"], "nb_wf": 1, "list_wf": ["gatk-nextflow-sample"], "list_contrib": ["madhyastha"], "nb_contrib": 1, "codes": ["\nprocess MarkDuplicates {\n    container \"${containers.gatk}\"\n    cpus 4\n    memory \"32 GB\"\n    \n  input:\n    file \"*\" from ref_indices\n    file \"${sample_id}.bam\" from bam_file\n    file \"${sample_id}.bai\" from bai_file\n\n    output:\n    file \"${sample_id}_MarkDup.bam\" into bam_markdup\n\n    script:\n    \"\"\"\n    gatk MarkDuplicates -I ${sample_id}.bam -M metrics.txt -O ${sample_id}_MarkDup.bam  \n    \"\"\"\n}"], "list_proc": ["madhyastha/gatk-nextflow-sample/MarkDuplicates"], "list_wf_names": ["madhyastha/gatk-nextflow-sample"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["madhyastha"], "nb_wf": 1, "list_wf": ["gatk-nextflow-sample"], "list_contrib": ["madhyastha"], "nb_contrib": 1, "codes": ["\nprocess BaseRecalibrator {\n    container \"${containers.gatk}\"\n    cpus 4\n    memory \"32 GB\"\n    \n    input:\n    file \"${sample_id}_MarkDup.bam\" from bam_markdup\n    file '*' from ref_indices\n    file '*' from ref_dict\n    file '*' from ref_golden_indel\n    file '*' from ref_dbsnp\n\n    output:\n    file \"${sample_id}_recal_data.table\" into BaseRecalibrator_table\n\n    script:\n    \"\"\"\n    gatk BaseRecalibrator \\\n    -I ${sample_id}_MarkDup.bam \\\n    --known-sites $dbsnp \\\n    --known-sites $golden_indel \\\n    -O ${sample_id}_recal_data.table \\\n    -R ${ref_name}\n    \"\"\"\n}"], "list_proc": ["madhyastha/gatk-nextflow-sample/BaseRecalibrator"], "list_wf_names": ["madhyastha/gatk-nextflow-sample"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["madhyastha"], "nb_wf": 1, "list_wf": ["gatk-nextflow-sample"], "list_contrib": ["madhyastha"], "nb_contrib": 1, "codes": ["\nprocess ApplyBQSR {\n    container \"${containers.gatk}\"\n    cpus 4\n    memory \"32 GB\"\n    \n    input:\n    file \"${sample_id}_MarkDup.bam\" from bam_markdup\n    file \"${sample_id}_recal_data.table\" from BaseRecalibrator_table\n\n    output:\n    file \"${sample_id}_aln-pe_bqsr.bam\" into bam_bqsr\n\n    script:\n    \"\"\"\n    gatk ApplyBQSR -I ${sample_id}_MarkDup.bam -bqsr ${sample_id}_recal_data.table -O ${sample_id}_aln-pe_bqsr.bam\n    \"\"\"\n}"], "list_proc": ["madhyastha/gatk-nextflow-sample/ApplyBQSR"], "list_wf_names": ["madhyastha/gatk-nextflow-sample"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["madhyastha"], "nb_wf": 1, "list_wf": ["gatk-nextflow-sample"], "list_contrib": ["madhyastha"], "nb_contrib": 1, "codes": ["\nprocess HaplotypeCaller {\n    container \"${containers.gatk}\"\n    cpus 4\n    memory \"32 GB\"\n    \n    input:\n    file \"${sample_id}_aln-pe_bqsr.bam\" from bam_bqsr\n    file '*' from ref_indices\n    file '*' from ref_dict\n\n    output:\n    file \"${sample_id}_haplotypecaller.g.vcf\" into haplotypecaller_gvcf\n    \n    script:\n    \"\"\"\n    gatk HaplotypeCaller -I ${sample_id}_aln-pe_bqsr.bam -O ${sample_id}_haplotypecaller.g.vcf --emit-ref-confidence GVCF -R ${ref_name}\n    \"\"\"\n}"], "list_proc": ["madhyastha/gatk-nextflow-sample/HaplotypeCaller"], "list_wf_names": ["madhyastha/gatk-nextflow-sample"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mahesh-panchal"], "nb_wf": 1, "list_wf": ["NBIS_project_template"], "list_contrib": ["mahesh-panchal"], "nb_contrib": 1, "codes": ["\nprocess FASTQC {\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(sample), path(reads)\n\n    output:\n    tuple val(sample), path(\"*.html\"), emit: html\n    tuple val(sample), path(\"*.zip\") , emit: zip\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    fastqc $args --threads $task.cpus $reads\n    \"\"\"\n\n}"], "list_proc": ["mahesh-panchal/NBIS_project_template/FASTQC"], "list_wf_names": ["mahesh-panchal/NBIS_project_template"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["mahesh-panchal"], "nb_wf": 1, "list_wf": ["NBIS_project_template"], "list_contrib": ["mahesh-panchal"], "nb_contrib": 1, "codes": ["\nprocess FASTP {\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(sample), path(reads)\n\n    output:\n    tuple val(sample), path(\"*fastp-trimmed_R{1,2}.fastq.gz\"), emit: reads\n    path \"*_fastp.json\"                                      , emit: logs\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    fastp $args -w $task.cpus -i ${reads[0]} -I ${reads[1]} \\\\\n        -o ${sample}_fastp-trimmed_R1.fastq.gz \\\\\n        -O ${sample}_fastp-trimmed_R2.fastq.gz \\\\\n        --json ${sample}_fastp.json\n    \"\"\"\n\n}"], "list_proc": ["mahesh-panchal/NBIS_project_template/FASTP"], "list_wf_names": ["mahesh-panchal/NBIS_project_template"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mahesh-panchal"], "nb_wf": 1, "list_wf": ["NBIS_project_template"], "list_contrib": ["mahesh-panchal"], "nb_contrib": 1, "codes": ["\nprocess MULTIQC {\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path config\n    path \"fastqc/*\"\n    path \"fastp/*\"\n\n    output:\n    path \"multiqc_report.html\"\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc . -c ${config} $args\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/NBIS_project_template/MULTIQC"], "list_wf_names": ["mahesh-panchal/NBIS_project_template"]}, {"nb_reuse": 1, "tools": ["TRAP"], "nb_own": 1, "list_own": ["mahesh-panchal"], "nb_wf": 1, "list_wf": ["nextflow-docker-actions-test"], "list_contrib": ["mahesh-panchal", "heuermh"], "nb_contrib": 2, "codes": ["\nprocess ADAM_TRANSFORMALIGNMENTS {\n\n    memory '2 GB'\n    container 'quay.io/biocontainers/adam:0.35.0--hdfd78af_0'\n\n    input:\n    tuple val(sample), path(sam)\n\n    output:\n    tuple val(sample), path(\"*.bam\"), emit: bam\n    stdout(emit: stdout)\n\n    script:\n    \"\"\"\n    more /etc/passwd\n    echo \"-u \\$(id -u):\\$(id -g)\"\n    env\n    # whoami ## Fails with unknown <id>\n    mkdir .spark-local\n    TMP=`realpath .spark-local`\n\n    export SPARK_LOCAL_IP=127.0.0.1\n    export SPARK_PUBLIC_DNS=127.0.0.1\n    trap \"more /etc/passwd\" ERR\n    adam-submit \\\\\n        --master local[${task.cpus}] \\\\\n        --driver-memory ${task.memory.toGiga()}g \\\\\n        --conf spark.local.dir=\\$TMP \\\\\n        --conf spark.jars.ivy=\\$TMP/.ivy2 \\\\\n        -- \\\\\n        transformAlignments -single ${sam} ${sample}.bam\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/nextflow-docker-actions-test/ADAM_TRANSFORMALIGNMENTS"], "list_wf_names": ["mahesh-panchal/nextflow-docker-actions-test"]}, {"nb_reuse": 0, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mahesh-panchal"], "nb_wf": 0, "list_wf": ["nf-core-alttemplate"], "list_contrib": ["mahesh-panchal"], "nb_contrib": 1, "codes": ["\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n    def software = 'fastqc'                                           \n    def args     = task.ext.args?: ''\n    def prefix   = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mahesh-panchal"], "nb_wf": 0, "list_wf": ["nf-core-alttemplate"], "list_contrib": ["mahesh-panchal"], "nb_contrib": 1, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = 'multiqc'                                                                          \n    \"\"\"\n    multiqc -f $task.ext.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["FastQC", "SnpSift"], "nb_own": 2, "list_own": ["nibscbioinformatics", "mahesh-panchal"], "nb_wf": 1, "list_wf": ["test_nfcore_workflow_chain", "nf-core-bagobugs"], "list_contrib": ["mahesh-panchal", "MGordon09"], "nb_contrib": 2, "codes": ["process SNPSIFT_EXTRACTFIELDS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::snpsift=4.3.1t\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpsift:4.3.1t--hdfd78af_3' :\n        'quay.io/biocontainers/snpsift:4.3.1t--hdfd78af_3' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*.snpsift.txt\"), emit: txt\n    path \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[SnpSift] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    SnpSift \\\\\n        -Xmx${avail_mem}g \\\\\n        extractFields \\\\\n        -s \",\" \\\\\n        -e \".\" \\\\\n        $args \\\\\n        $vcf \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${prefix}.snpsift.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpsift: \\$( echo \\$(SnpSift split -h 2>&1) | sed 's/^.*version //' | sed 's/(.*//' | sed 's/t//g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/SNPSIFT_EXTRACTFIELDS"], "list_wf_names": ["mahesh-panchal/test_nfcore_workflow_chain"]}, {"nb_reuse": 2, "tools": ["SAMtools", "mpileup"], "nb_own": 2, "list_own": ["mahesh-panchal", "nf-modules"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "samtools"], "list_contrib": ["abhi18av", "mahesh-panchal"], "nb_contrib": 2, "codes": ["process MAKE_BED_MASK {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.9.5 bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1a35167f7a491c7086c13835aaa74b39f1f43979:6b5cffa1187cfccf2dc983ed3b5359d49b999eb0-0' :\n        'quay.io/biocontainers/mulled-v2-1a35167f7a491c7086c13835aaa74b39f1f43979:6b5cffa1187cfccf2dc983ed3b5359d49b999eb0-0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(vcf)\n    path fasta\n    val save_mpileup\n\n    output:\n    tuple val(meta), path(\"*.bed\")    , emit: bed\n    tuple val(meta), path(\"*.mpileup\"), optional:true, emit: mpileup\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:                                                                         \n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: 10\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mpileup = save_mpileup ? \"| tee ${prefix}.mpileup\" : \"\"\n    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        $args \\\\\n        --reference $fasta \\\\\n        $bam \\\\\n        $mpileup  \\\\\n        | awk -v OFS='\\\\t' '{print \\$1, \\$2-1, \\$2, \\$4}' | awk '\\$4 < $args2' > lowcov_positions.txt\n\n    make_bed_mask.py \\\\\n        $vcf \\\\\n        lowcov_positions.txt \\\\\n        ${prefix}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess sort {\n    publishDir params.sortResultsDir, mode: params.saveMode\n    container 'quay.io/biocontainers/samtools:1.10--h2e538c0_3'\n\n    when:\n    params.sort\n\n    input:\n    file(bamRead) from ch_in_sort\n\n    output:\n    file(\"*${params.sortedBamFilePattern}\") into ch_out_sort\n\n    script:\n\n    genomeName = bamRead.toString().split(\"\\\\.\")[0]\n    \"\"\"\n    samtools sort ${bamRead} >  ${genomeName}.sort.bam\n    \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/MAKE_BED_MASK", "nf-modules/samtools/sort"], "list_wf_names": ["mahesh-panchal/test_nfcore_workflow_chain", "nf-modules/samtools"]}, {"nb_reuse": 2, "tools": ["SAMtools", "snpEff"], "nb_own": 2, "list_own": ["nibscbioinformatics", "mahesh-panchal"], "nb_wf": 2, "list_wf": ["test_nfcore_workflow_chain", "humgen"], "list_contrib": ["mahesh-panchal", "bleazard", "lescai"], "nb_contrib": 3, "codes": ["process SNPEFF_BUILD {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::snpeff=5.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snpeff:5.0--hdfd78af_1' :\n        'quay.io/biocontainers/snpeff:5.0--hdfd78af_1' }\"\n\n    input:\n    path fasta\n    path gff\n\n    output:\n    path 'snpeff_db'   , emit: db\n    path '*.config'    , emit: config\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def basename = fasta.baseName\n\n    def avail_mem = 4\n    if (!task.memory) {\n        log.info '[snpEff] Available memory not known - defaulting to 4GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    mkdir -p snpeff_db/genomes/\n    cd snpeff_db/genomes/\n    ln -s ../../$fasta ${basename}.fa\n\n    cd ../../\n    mkdir -p snpeff_db/${basename}/\n    cd snpeff_db/${basename}/\n    ln -s ../../$gff genes.gff\n\n    cd ../../\n    echo \"${basename}.genome : ${basename}\" > snpeff.config\n\n    snpEff \\\\\n        -Xmx${avail_mem}g \\\\\n        build \\\\\n        -config snpeff.config \\\\\n        -dataDir ./snpeff_db \\\\\n        -gff3 \\\\\n        -v \\\\\n        ${basename}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snpeff: \\$(echo \\$(snpEff -version 2>&1) | cut -f 2 -d ' ')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess indexrecalibrated {\n  publishDir \"$params.outdir/alignments\", mode: \"copy\"\n    tag \"$name\"\n    label 'process_medium'\n\n  input:\n  set ( sampleprefix, file(bqsrfile) ) from recalibratedforindex\n\n  output:\n  set ( sampleprefix, file(\"${bqsrfile}.bai\") ) into indexedbam\n\n  \"\"\"\n  samtools index $bqsrfile\n  \"\"\"\n}"], "list_proc": ["mahesh-panchal/test_nfcore_workflow_chain/SNPEFF_BUILD", "nibscbioinformatics/humgen/indexrecalibrated"], "list_wf_names": ["mahesh-panchal/test_nfcore_workflow_chain", "nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["mal2017"], "nb_wf": 1, "list_wf": ["sfr"], "list_contrib": ["mal2017"], "nb_contrib": 1, "codes": ["\nprocess alignment {\n\t\tconda 'bowtie2'\n\n\t\tcpus 2\n\n\t\tinput:\n\t\ttuple val(sample), path(reads) from fq_ch\n\t\tval(idx) from bt2_idx\n\n    output:\n    tuple sample,file('aln.sam') into x_aln_ch, y_aln_ch\n\n\t\tscript:\n\t\tif(params.dev)\n    \t\"\"\"\n    \tbowtie2 -x ${bt2_idx} -u ${n_reads_for_dev} -U ${reads} -k 1 -p ${task.cpus} > aln.sam\n    \t\"\"\"\n\t\telse\n\t\t\t\"\"\"\n\t\t\tbowtie2 -x ${bt2_idx} -U ${reads} -k 1 -p ${task.cpus} > aln.sam\n\t\t\t\"\"\"\n}"], "list_proc": ["mal2017/sfr/alignment"], "list_wf_names": ["mal2017/sfr"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mal2017"], "nb_wf": 1, "list_wf": ["sfr"], "list_contrib": ["mal2017"], "nb_contrib": 1, "codes": ["\nprocess x_reads {\n\t\tconda 'samtools'\n\n\t\tinput:\n\t\ttuple val(sample), path(sam_file) from x_aln_ch\n\n\t\toutput:\n\t\ttuple val(sample),stdout into x_result_ch\n\n\t\t\"\"\"\n\t\tsamtools view ${sam_file} | awk '(\\$3==\"${female_chrom}\")' | wc -l | tr -d '\\n'\n\t\t\"\"\"\n}"], "list_proc": ["mal2017/sfr/x_reads"], "list_wf_names": ["mal2017/sfr"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["mamanambiya"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya"], "nb_contrib": 1, "codes": ["\nprocess sites_only {\n    tag \"sites_only_${dataset}_${chrm}:${start}-${end}\"\n    label \"bigmem\"\n\n    input:\n        tuple val(dataset), val(chrm), val(start), val(end), val(chip), file(vcf)\n\n    output:\n        tuple val(dataset), val(chrm), val(start), val(end), val(chip), file(sites_vcf)\n\n    script:\n        sites_vcf = \"${vcf.getSimpleName()}_sites.bcf\"\n        \"\"\"\n        tabix ${vcf}\n        bcftools view ${vcf} --drop-genotypes --threads ${task.cpus} -Ob -o ${sites_vcf}\n        tabix ${sites_vcf}\n        \"\"\"\n}"], "list_proc": ["mamanambiya/popfreqs/sites_only"], "list_wf_names": ["mamanambiya/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["mamanambiya"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya"], "nb_contrib": 1, "codes": ["\nprocess concat_chrms {\n   tag \"concat_${dataset}_${chrm}_${prefix}\"\n   label \"bigmem\"\n   publishDir \"${params.outdir}/${prefix}\", mode: 'copy'\n                           \n                  \n   \n   input:\n        tuple val(dataset), val(chrm), val(vcfs), val(prefix)\n   output:\n        tuple val(dataset), val(chrm), file(vcf_out)\n   script:\n        vcf_out = \"${prefix}_${dataset}_${chrm}.bcf\"\n                        \n        if(vcfs.size() > 1){\n            \"\"\"\n            bcftools concat ${vcfs.join(' ')} |\\\n            bcftools sort -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n        else{\n            \"\"\"\n            bcftools sort ${vcfs.join(' ')} -T . -Ob -o ${vcf_out}\n            tabix ${vcf_out}\n            \"\"\"\n        }\n}"], "list_proc": ["mamanambiya/popfreqs/concat_chrms"], "list_wf_names": ["mamanambiya/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["mamanambiya"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya"], "nb_contrib": 1, "codes": ["\nprocess merge_groups {\n    tag \"merge_groups_${dataset}_${prefix}\"\n    label \"largemem\"\n                                                             \n\n    input:\n        tuple val(dataset), val(chrms), val(vcfs), val(prefix)\n    output:\n        tuple val(dataset), file(vcf_out)\n    script:\n        vcf_out = \"${prefix}.vcf.gz\"\n        if(vcfs.size() > 1){\n            \"\"\"\n            bcftools merge ${vcfs.join(' ')} |\\\n            bcftools sort -T . -Ob -o ${vcf_out}\n            \"\"\"\n        }\n        else{\n            \"\"\"\n            bcftools sort ${vcfs.join(' ')} -T . -Ob -o ${vcf_out}\n            \"\"\"\n        }\n        \n}"], "list_proc": ["mamanambiya/popfreqs/merge_groups"], "list_wf_names": ["mamanambiya/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools", "snpEff"], "nb_own": 1, "list_own": ["mamanambiya"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya"], "nb_contrib": 1, "codes": ["\nprocess snpeff_vcf {\n    tag \"snpeff_${dataset}_${chrm}\"\n    label \"snpeff_bcftools\"\n    input:\n        tuple val(dataset), val(chrm), file(vcf_file)\n    output:\n        tuple val(dataset), val(chrm), file(vcf_out)\n    script:\n        base = vcf_file.getSimpleName()\n        vcf_out = \"${base}_snpeff.vcf.gz\"\n        \"\"\"\n        snpEff \\\n            -Xmx${task.memory.toGiga()}g \\\n            ${params.snpEff_human_db} \\\n            -lof \\\n            -stats ${base}_snpeff.html \\\n            -csvStats ${base}_snpeff.csv \\\n            -dataDir ${params.snpEff_database} \\\n            ${vcf_file} |\\\n        bcftools view -Oz -o ${vcf_out}\n        tabix ${vcf_out}\n        #rm ${base}_snpeff.vcf\n        \"\"\"\n}"], "list_proc": ["mamanambiya/popfreqs/snpeff_vcf"], "list_wf_names": ["mamanambiya/popfreqs"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["mamanambiya"], "nb_wf": 1, "list_wf": ["popfreqs"], "list_contrib": ["mamanambiya"], "nb_contrib": 1, "codes": ["\nprocess annot_vcf_chrm {\n    tag \"annot_vcf_chrm_${dataset}_${chrm}\"\n    label \"medium\"\n    publishDir \"/cbio/users/mamana/exome_aibst/data/AIBST/VCF/CHRS/\", mode: 'copy'\n\n    input:\n        tuple val(dataset), val(chrm), file(annot_vcf), file(vcf_file)\n    output:\n        tuple val(dataset), val(chrm), file(vcf_chrm)\n    script:\n        base = file(vcf_file.baseName).baseName\n        vcf_chrm = \"${base}.annot.vcf.gz\"\n        \"\"\"\n        tabix ${vcf_file}\n        tabix ${annot_vcf}\n        bcftools view --regions ${chrm} ${vcf_file} --threads ${task.cpus} -Oz -o ${base}_${chrm}.annot.vcf.gz\n        bcftools annotate -a ${annot_vcf} -c INFO ${vcf_file} --threads ${task.cpus} -Oz -o ${vcf_chrm}\n        tabix ${vcf_chrm}\n        \"\"\"\n}"], "list_proc": ["mamanambiya/popfreqs/annot_vcf_chrm"], "list_wf_names": ["mamanambiya/popfreqs"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["marchoeppner"], "nb_wf": 1, "list_wf": ["outbreak-monitoring"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\nprocess runMultiQCFastq {\n\n    publishDir \"${OUTDIR}/Summary/Fastqc\", mode: 'copy'\n\n    input:\n    file('*') from fastp_logs.flatten().toList()\n\n    output:\n    file(\"fastq_multiqc*\") into runMultiQCFastqOutput\n\n    script:\n\n    \"\"\"\n    multiqc -n fastq_multiqc *.json *.html\n    \"\"\"\n}"], "list_proc": ["marchoeppner/outbreak-monitoring/runMultiQCFastq"], "list_wf_names": ["marchoeppner/outbreak-monitoring"]}, {"nb_reuse": 1, "tools": ["Pathoscope"], "nb_own": 1, "list_own": ["marchoeppner"], "nb_wf": 1, "list_wf": ["outbreak-monitoring"], "list_contrib": ["marchoeppner"], "nb_contrib": 1, "codes": ["\nprocess runPathoscopeId {\n\n   publishDir \"${OUTDIR}/Data/${id}/Pathoscope\", mode: 'copy'\n\n   input:\n   set id,file(samfile) from inputPathoscopeId\n\n   output:\n   set id,file(pathoscope_tsv) into outputPathoscopeId\n\n   script:\n\n                                          \n   pathoscope_tsv = id + \"-sam-report.tsv\"\n\n   \"\"\"\n\tpathoscope ID -alignFile $samfile -fileType sam -expTag $id\n   \"\"\"\n\n}"], "list_proc": ["marchoeppner/outbreak-monitoring/runPathoscopeId"], "list_wf_names": ["marchoeppner/outbreak-monitoring"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["marcodelapierre"], "nb_wf": 1, "list_wf": ["illumina-nf"], "list_contrib": ["marcodelapierre"], "nb_contrib": 1, "codes": ["\nprocess contigfile {\n  tag \"${dir}/${name}_${contigid}\"\n  publishDir \"${dir}/${params.outprefix}${name}\", mode: 'copy', saveAs: { filename -> \"consensus_contig_${contigid}.fasta\" }\n\n  input:\n  tuple val(dir), val(name), path('consensus_contigs_sub.fasta'), val(contigid)\n  \n  output:\n  tuple val(dir), val(name), val(contigid), path('consensus_contig.fasta')\n  \n  script:\n  \"\"\"\n  contigid=\"${contigid}\"\n  idawk=\\${contigid#NODE_}\n  idawk=\\${idawk%_RC}\n  awk -F _ -v id=\\$idawk '{ if(ok==1){if(\\$1==\">NODE\"){exit}; print} ; if(ok!=1 && \\$1==\">NODE\" && \\$2==id){ok=1; print} }' consensus_contigs_sub.fasta >consensus_contig.fasta\n\n  if [ \"\\${contigid: -3}\" == \"_RC\" ] ; then\n    samtools faidx \\\n       -i -o consensus_contig_revcom.fasta \\\n       consensus_contig.fasta \\$(grep \"^>\\${contigid%_RC}_\" consensus_contig.fasta | tr -d '>')\n    mv consensus_contig_revcom.fasta consensus_contig.fasta\n  fi\n  \"\"\"\n}"], "list_proc": ["marcodelapierre/illumina-nf/contigfile"], "list_wf_names": ["marcodelapierre/illumina-nf"]}, {"nb_reuse": 1, "tools": ["Diamond"], "nb_own": 1, "list_own": ["marcodelapierre"], "nb_wf": 1, "list_wf": ["nanopore-nf"], "list_contrib": ["marcodelapierre"], "nb_contrib": 1, "codes": ["\nprocess diamond {\ntag \"${dir}/${name}\"\npublishDir \"${dir}/${params.outsuffix}${name}\", mode: 'copy'\n\ninput:\nset dir, name, file('Denovo_subset.fa') from denovo2_ch\n\noutput:\nset dir, name, file('diamond.tsv') into diamond_ch\nset dir, name, file('diamond.xml') into diamond_xml_ch\n\nwhen:\nparams.diamond\n\nscript:\n\"\"\"\ndiamond blastx \\\n  -q Denovo_subset.fa -d ${params.diamond_db} \\\n  -f 5 -o diamond.xml \\\n  --evalue ${params.evalue} \\\n  -p ${task.cpus}\n\ndiamond blastx \\\n  -q Denovo_subset.fa -d ${params.diamond_db} \\\n  -f 6 qseqid  sseqid  pident length evalue bitscore stitle -o diamond_unsort.tsv \\\n  --evalue ${params.evalue} \\\n  -p ${task.cpus}\n\nsort -n -r -k 6 diamond_unsort.tsv >diamond.tsv\n\"\"\"\n}"], "list_proc": ["marcodelapierre/nanopore-nf/diamond"], "list_wf_names": ["marcodelapierre/nanopore-nf"]}, {"nb_reuse": 1, "tools": ["Trinity"], "nb_own": 1, "list_own": ["marcodelapierre"], "nb_wf": 1, "list_wf": ["trinity-nf"], "list_contrib": ["marcodelapierre"], "nb_contrib": 1, "codes": ["\nprocess butterfly {\n  tag \"${dir}/${name}\"\n\n  input:\n  tuple val(dir), val(name), path(reads_fa), path(\"${params.overtaskfile}\")\n\n  output:\n  tuple val(dir), val(name), path{ params.localdisk ? \"out_${reads_fa}\" : \"*inity.fasta\" }, optional: true\n\n                                                                                        \n  script:\n  \"\"\"\n  if [ \"${params.localdisk}\" == \"true\" ] ; then\n    here=\\$PWD\n    rm -rf ${params.localdir}\n    mkdir ${params.localdir}\n    cp -r \\$( readlink $reads_fa ) ${params.localdir}/\n    cd ${params.localdir}\n  fi\n\n  mem='${params.bf_mem}'\n  mem=\\${mem%B}\n  export mem=\\${mem// /}\n\n  cat << \"EOF\" >trinity.sh\nTrinity \\\n  --single \\${1} \\\n  --run_as_paired \\\n  --seqType fa \\\n  --verbose \\\n  --no_version_check \\\n  --workdir trinity_workdir \\\n  --output \\${1}.out \\\n  --max_memory \\${mem} \\\n  --CPU ${params.bf_cpus} \\\n  --trinity_complete \\\n  --full_cleanup \\\n  --no_distributed_trinity_exec\nEOF\n  chmod +x trinity.sh\n\n  if [ \"${params.localdisk}\" == \"true\" ] ; then\n    tar xzf ${reads_fa}\n    find ${params.taskoutdir}/read_partitions -name \"*inity.reads.fa\" | parallel -j ${task.cpus} ./trinity.sh {}\n    find ${params.taskoutdir}/read_partitions -name \"*inity.fasta\" | tar -cz -h -f out_${reads_fa} -T -\n    cd \\$here\n    cp ${params.localdir}/out_chunk*.tgz .\n    rm -r ${params.localdir}\n  else\n    ls *inity.reads.fa | parallel -j ${task.cpus} ./trinity.sh {}\n  fi\n  \"\"\"\n}"], "list_proc": ["marcodelapierre/trinity-nf/butterfly"], "list_wf_names": ["marcodelapierre/trinity-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["markgene"], "nb_wf": 1, "list_wf": ["cutnrun"], "list_contrib": ["sofiahaglund", "Rotholandus", "winni2k", "markgene", "ewels", "apeltzer", "tiagochst", "chuan-wang", "drpatelh"], "nb_contrib": 9, "codes": ["\nprocess Bowtie2 {\n    tag \"$name\"\n    label 'process_high'\n\n    input:\n    set val(name), file(reads) from ch_trimmed_reads\n    file index from ch_bowtie2_index.collect()\n\n    output:\n    set val(name), file(\"*.bam\") into ch_bowtie2_bam\n\n    script:\n    prefix = \"${name}.Lb\"\n    sm = \"SM:${name.split('_')[0..-2].join('_')}\"\n    pl = \"PL:ILLUMINA\"\n    lb = \"LB:${name}\"\n    rg = \"\\'@RG\\\\tID:${name}\\\\tSM:${name.split('_')[0..-2].join('_')}\\\\tPL:ILLUMINA\\\\tLB:${name}\\\\tPU:1\\'\"\n    if (params.seq_center) {\n        rg = \"\\'@RG\\\\tID:${name}\\\\tSM:${name.split('_')[0..-2].join('_')}\\\\tPL:ILLUMINA\\\\tLB:${name}\\\\tPU:1\\\\tCN:${params.seq_center}\\'\"\n    }\n    println name\n    println rg\n    println prefix\n    \"\"\"\n    bowtie2 --local -q \\\\\n    -p $task.cpus \\\\\n    --rg-id $name \\\\\n    --rg $sm --rg $pl --rg $lb --rg PU:1 \\\\\n    -x ${index}/${bowtie2_base} \\\\\n    -1 ${reads[0]} -2 ${reads[1]} \\\\\n    | samtools view -@ $task.cpus -b -h -F 0x0100 -O BAM -o ${prefix}.bam -\n    \"\"\"\n}"], "list_proc": ["markgene/cutnrun/Bowtie2"], "list_wf_names": ["markgene/cutnrun"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["markxiao"], "nb_wf": 1, "list_wf": ["PRS-dev"], "list_contrib": ["markxiao"], "nb_contrib": 1, "codes": ["\nprocess target_qc_relatedness {\n\n    publishDir \"${params.outdir}/intermediate_files\", mode: 'copy'\n\n    input:\n    tuple prefix, file(bed), file(bim), file(cov), file(fam), file(height) from target_data.copy5\n    tuple _tmp, file(irem), file(hh), file(snplist), file(fam1), file(prune_in), file(prune_out), file(het) from target_qc_standard_gwas_qc_1_output_copy4\n    tuple file(sexcheck), file(valid), file(\"tmp_hh\") from target_qc_sex_check_output\n\n    output:\n    file(\"${prefix_qc}.rel.id\") into target_qc_relatedness_output\n\n    script:\n    prefix_qc = \"${prefix}.QC\"\n    \"\"\"\n    plink --bfile ${prefix} --extract ${prune_in} --keep ${valid} --rel-cutoff 0.125 --out ${prefix_qc}\n    if test -f \"${prefix_qc}.log\";\n        then mv ${prefix_qc}.log ${params.outdir}/logs/${prefix_qc}.log.5\n    fi\n    \"\"\"  \n}"], "list_proc": ["markxiao/PRS-dev/target_qc_relatedness"], "list_wf_names": ["markxiao/PRS-dev"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["markxiao"], "nb_wf": 1, "list_wf": ["PRS-dev"], "list_contrib": ["markxiao"], "nb_contrib": 1, "codes": ["\nprocess plink_clumping {\n\n    publishDir \"${params.outdir}/intermediate_files\", mode: 'copy'\n\n    input:\n    tuple prefix, file(fam), file(bed), file(bim) from target_qc_final_output_copy1\n    file transformed from plink_update_effect_size_output\n\n\n    output:\n    tuple file(\"${prefix}.clumped\"), file(\"${prefix}.valid.snp\") into plink_clumping_output\n\n    script:\n    prefix_qc = \"${prefix}.QC\"\n    \"\"\"\n    plink --bfile ${prefix_qc} --clump-p1 1 --clump-r2 0.1 --clump-kb 250 --clump ${transformed} --clump-snp-field SNP --clump-field P --out ${prefix}\n    if test -f \"${prefix}.log\";\n        then mv ${prefix}.log ${params.outdir}/logs/${prefix}.log.7\n    fi\n    awk 'NR!=1{print \\$3}' ${prefix}.clumped >  ${prefix}.valid.snp\n    \"\"\"\n}"], "list_proc": ["markxiao/PRS-dev/plink_clumping"], "list_wf_names": ["markxiao/PRS-dev"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["markxiao"], "nb_wf": 1, "list_wf": ["PRS-dev"], "list_contrib": ["markxiao"], "nb_contrib": 1, "codes": ["\nprocess plink_accounting_for_population_stratification {\n\n    publishDir \"${params.outdir}/intermediate_files\", mode: 'copy'\n\n    input:\n    tuple prefix, file(fam), file(bed), file(bim) from target_qc_final_output_copy3\n\n    output:\n    tuple file(\"${prefix}.prune.in\"), file(\"${prefix}.prune.out\"), file(\"${prefix}.eigenvec\"), file(\"${prefix}.eigenval\") into plink_accounting_for_population_stratification_output\n\n    script:\n    prefix_qc = \"${prefix}.QC\"\n    \"\"\"\n    # First, we need to perform prunning\n    plink --bfile ${prefix_qc} --indep-pairwise 200 50 0.25 --out ${prefix}\n    if test -f \"${prefix}.log\";\n        then mv ${prefix}.log ${params.outdir}/logs/${prefix}.log.9\n    fi\n\n    # Then we calculate the first 6 PCs\n    plink --bfile ${prefix_qc} --extract ${prefix}.prune.in --pca 6 --out ${prefix}\n    if test -f \"${prefix}.log\";\n        then mv ${prefix}.log ${params.outdir}/logs/${prefix}.log.10\n    fi\n    \"\"\"\n}"], "list_proc": ["markxiao/PRS-dev/plink_accounting_for_population_stratification"], "list_wf_names": ["markxiao/PRS-dev"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_for_nextstrain"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess nextstrain_align {\n\n\tpublishDir path: \"${params.output_dir}/${cat_name}/\", mode: \"copy\"\n\n\tinput: \n\ttuple val(cat_name), path(filtered_fasta)\n\t\n\toutput: \n\ttuple val(cat_name), path(\"${cat_name}_aln.fasta\"), emit: alignment\n\n\tscript:\n\t\"\"\"\n\taugur align --sequences ${filtered_fasta} --reference-sequence ${params.alignment_ref} --output ${cat_name}_aln.fasta --nthreads ${params.threads} --fill-gaps\n\t\"\"\"\n}"], "list_proc": ["matt-sd-watson/nextflow_for_nextstrain/nextstrain_align"], "list_wf_names": ["matt-sd-watson/nextflow_for_nextstrain"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_for_nextstrain"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess nextstrain_tree_refine {\n\n\tlabel 'med_mem'\n\n\tpublishDir path: \"${params.output_dir}/${cat_name}/\", mode: \"copy\"\n\n\tinput: \n\ttuple val(cat_name), path(tree), path(metadata), path(alignment)\n\t\n\toutput: \n\ttuple val(cat_name), path(\"${cat_name}_tree_refined.nwk\"), path(\"${cat_name}_branch_lengths.json\"), emit: refined\n\n\tscript:\n\t\"\"\"\n\taugur refine \\\n  \t--tree ${tree} \\\n  \t--alignment ${alignment} \\\n  \t--metadata ${metadata} \\\n  \t--output-tree ${cat_name}_tree_refined.nwk \\\n  \t--output-node-data ${cat_name}_branch_lengths.json \\\n  \t--timetree \\\n  \t--coalescent opt \\\n  \t--date-confidence \\\n  \t--date-inference marginal \\\n  \t--clock-filter-iqd ${params.clockfilteriqd} \\\n\t--seed ${params.refineseed} \\\n  \t--keep-root > ${params.output_dir}/${cat_name}/augur_refine_${cat_name}_clock_${params.clockfilteriqd}.txt\n\t\"\"\"\t\n\n}"], "list_proc": ["matt-sd-watson/nextflow_for_nextstrain/nextstrain_tree_refine"], "list_wf_names": ["matt-sd-watson/nextflow_for_nextstrain"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_for_nextstrain"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess nextstrain_traits {\n\n\tpublishDir path: \"${params.output_dir}/${cat_name}/\", mode: \"copy\"\n\n\tinput: \n\ttuple val(cat_name), path(metadata), path(refined_tree), path(branch_lengths)\n\n\toutput: \n\ttuple val(cat_name), path(\"${cat_name}_traits.json\"), emit: traits\n\n\tscript:\n\t\"\"\"\n\taugur traits --tree ${refined_tree} --metadata ${metadata} \\\n\t--output-node-data ${cat_name}_traits.json --columns Health.Region\n\t\"\"\"\n\t\n\n}"], "list_proc": ["matt-sd-watson/nextflow_for_nextstrain/nextstrain_traits"], "list_wf_names": ["matt-sd-watson/nextflow_for_nextstrain"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_for_nextstrain"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess nextstrain_ancestral {\n\n\tpublishDir path: \"${params.output_dir}/${cat_name}/\", mode: \"copy\"\n\n\tinput: \n\ttuple val(cat_name), path(alignment), path(refined_tree), path(branch_lengths)\n\n\toutput: \n\ttuple val(cat_name), path(\"${cat_name}_nt_muts.json\"), emit: ancestral\n\n\tscript:\n\t\"\"\"\n\taugur ancestral   --tree ${refined_tree}   --alignment ${alignment} \\\n\t--output-node-data ${splitUnderscore(refined_tree.baseName)}_nt_muts.json   --inference joint\n\t\"\"\"\n\n}"], "list_proc": ["matt-sd-watson/nextflow_for_nextstrain/nextstrain_ancestral"], "list_wf_names": ["matt-sd-watson/nextflow_for_nextstrain"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_for_nextstrain"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess nextstrain_clades {\n\n\tpublishDir path: \"${params.output_dir}/${cat_name}/\", mode: \"copy\"\n\n\tinput: \n\ttuple val(cat_name), path(refined_tree), path(branch_lengths), path(nucleotide_json), path(amino_acid_json)\n\n\toutput: \n\ttuple val(cat_name), path(\"${cat_name}_clades.json\"), emit: clades\n\t\n\n\tscript:\n\t\"\"\"\n\taugur clades --tree ${refined_tree} \\\n\t--mutations ${nucleotide_json} ${amino_acid_json} \\\n\t--clades ${params.clades} --output-node-data ${cat_name}_clades.json\n\t\"\"\"\n\n}"], "list_proc": ["matt-sd-watson/nextflow_for_nextstrain/nextstrain_clades"], "list_wf_names": ["matt-sd-watson/nextflow_for_nextstrain"]}, {"nb_reuse": 1, "tools": ["Augur"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_for_nextstrain"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess nextstrain_export {\n\n\tpublishDir path: \"${params.output_dir}/all/\", mode: \"copy\"\n\n\tinput: \n\ttuple val(cat_name), path(tree), path(metadata), path(alignment), path(refined_tree), path(branch_lengths), path(nucleotide_json), path(amino_acid_json), path(traits), path(clade_json)\n\n\toutput: \n\ttuple val(cat_name), path(\"${cat_name}_ncov.json\"), emit: export_json\n\n\tscript:\n\t\"\"\"\n\tmkdir -p ${params.output_dir}/all/\n\taugur export v2   --tree ${refined_tree} \\\n\t--metadata ${metadata} \\\n\t--node-data ${branch_lengths} \\\n                    ${traits} \\\n\t\t    ${nucleotide_json} \\\n\t\t    ${amino_acid_json} \\\n\t\t    ${clade_json} \\\n\t--colors ${params.colortsv} \\\n\t--auspice-config ${params.config} \\\n   \t--output ${cat_name}_ncov.json \\\n\t--lat-longs ${params.latlong}\n\t\"\"\"\n\n}"], "list_proc": ["matt-sd-watson/nextflow_for_nextstrain/nextstrain_export"], "list_wf_names": ["matt-sd-watson/nextflow_for_nextstrain"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_misc"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n\n\tpublishDir = \"${params.out_dir}/\"\n\n\tinput: \n\tfile input_fastq\n\n\tscript: \n\t\"\"\"\n\tfastqc -t 5 ${input_fastq} -o ${params.out_dir}\n\t\"\"\"\n}"], "list_proc": ["matt-sd-watson/nextflow_misc/fastqc"], "list_wf_names": ["matt-sd-watson/nextflow_misc"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_misc"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess adapter_trim {\n\t\n\tpublishDir = \"${params.out_dir}/\"\n\n\tinput: \n\ttuple val(adapter), val(sample_id)\n\n\toutput: \n\n\tfile \"${sample_id}_adaptertrimmed.fastq.gz\"\n\t\n\tscript: \n\t\"\"\"\n\tcutadapt -q 20 -a ${adapter} -o ${sample_id}_adaptertrimmed.fastq.gz ${params.input_dir}/${sample_id}.fastq.gz\n\t\"\"\"\n}"], "list_proc": ["matt-sd-watson/nextflow_misc/adapter_trim"], "list_wf_names": ["matt-sd-watson/nextflow_misc"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["matt-sd-watson"], "nb_wf": 1, "list_wf": ["nextflow_misc"], "list_contrib": ["matt-sd-watson"], "nb_contrib": 1, "codes": ["\nprocess star_align {\n\n\tpublishDir = \"${params.out_dir}/\"\n\t\n\tinput: \n\tfile trimmed_fastq\n\n\tscript: \n\t\"\"\"\n\tSTAR --genomeDir ${params.star_ref} --readFilesIn ${trimmed_fastq} \\\n\t    --readFilesCommand gunzip -c --outSAMtype BAM SortedByCoordinate --outFileNamePrefix ${params.out_dir}/${trimmed_fastq.simpleName}/${trimmed_fastq.simpleName} \\\n   \t --alignIntronMin ${params.alignIntronMin} --alignIntronMax ${params.alignIntronMax} \\\n    \t--sjdbGTFfile ${params.star_gtf} \\\n    \t--outSAMprimaryFlag OneBestScore --twopassMode Basic \\\n    \t--outReadsUnmapped Fastx \\\n    \t--seedSearchStartLmax ${params.seedSearchStartLmax}  \\\n    \t--outFilterScoreMinOverLread ${params.outFilterScoreMinOverLread} --outFilterMatchNminOverLread ${params.outFilterMatchNminOverLread} \\\n\t--outFilterMatchNmin ${params.outFilterMatchNmin}\n\t\"\"\"\n}"], "list_proc": ["matt-sd-watson/nextflow_misc/star_align"], "list_wf_names": ["matt-sd-watson/nextflow_misc"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["matthiasheinig"], "nb_wf": 1, "list_wf": ["scworkflows"], "list_contrib": ["matthiasheinig", "wathom"], "nb_contrib": 2, "codes": ["\nprocess Macosko_pl_mapping {\n\npublishDir params.outdir, mode: 'copy',\n\tsaveAs: {filename ->\n\tif (filename =~ /${name}$/) \"STAR_mapping/$filename\"\n}\n\ninput:\n set name, paramfile from g_7_paramset_g_19\n\noutput:\n file \"${name}\" into g_19_folder\n file \"${name}\" into g_19_folder2_g_20\n\n\"\"\"\n# Read paramfile into a var\nparamline=\\$(<${paramfile})\n\n# Split line\nIFS=\\$'\\t'\nparams=(\\${paramline})\n\n# Create param vars\nread1=\\${params[1]}\nread2=\\${params[2]}\n#nestcells=\\${params[3]}\ngenomedir=\\${params[3]}\nreferencefasta=\\${params[4]}\n\n#echo \\${read1} \\${read2} \\${genomedir} \\${referencefasta} > ${name}.out.params.txt\n\n# Define STAT exec path\nSTAR=\"/PATH/TO/STAR\"\n\n# add bc # Added tmp dir\ncmd_pre=\"java -Djava.io.tmpdir=./tmpdir -jar /PATH/TO/Drop-seq_tools-2.0.0/3rdParty/picard/picard.jar FastqToSam \\\nF1=\\${read1} \\\nF2=\\${read2} \\\nSM=${name} \\\nO=${name}.unmapped.bam\"\n\n# Execute\neval \\${cmd_pre}\n\n# Create out dir\nmkdir ${name}\n# use -k to keep intermediates otherwise there is an error.\n\ncmd_map=\"bash /PATH/TO/Drop-seq_tools-2.0.0/Drop-seq_alignment.sh \\\n-g \\${genomedir} \\\n-r \\${referencefasta} \\\n-o ${name} \\\n-d /PATH/TO/bin/Drop-seq_tools-2.0.0 \\\n-t ${name} \\\n-s \\${STAR} -k \\\n./${name}.unmapped.bam\"\n\n# Execute\neval \\${cmd_map}\n\n# Write commands to file\necho \\${cmd_pre} > ./${name}/exec.pre.cmd.txt\necho \\${cmd_map} > ./${name}/exec.map.cmd.txt\n\n# Write STAR version to file\n\\${STAR} | head -8 > ./${name}/STAR.version\n\n# Copy the param file to the out directory\ncp ${paramfile} ./${name}/params.txt\n\n# Copy the unmapped bam file to out dir\ncp ./${name}.unmapped.bam ./${name}\n\n# final bam is now named ${name}/final.bam\n# Rename to ${name}.bam\nmv ./${name}/final.bam ./${name}/${name}.bam\nmv ./${name}/final.bai ./${name}/${name}.bai\n\n# RENAME the output file\n#mv ./${name}/error_detected.bam ./${name}/${name}.bam\n#mv ./${name}/error_detected.bai ./${name}/${name}.bai\n\n\n\n\"\"\"\n}"], "list_proc": ["matthiasheinig/scworkflows/Macosko_pl_mapping"], "list_wf_names": ["matthiasheinig/scworkflows"]}, {"nb_reuse": 1, "tools": ["iRefIndex"], "nb_own": 1, "list_own": ["matthiasheinig"], "nb_wf": 1, "list_wf": ["scworkflows"], "list_contrib": ["matthiasheinig", "wathom"], "nb_contrib": 2, "codes": ["\nprocess split_sample_table_cr_pipe {\n\npublishDir params.outdir, mode: 'copy',\n\tsaveAs: {filename ->\n\tif (filename =~ /.*.txt$/) \"samplefiles/$filename\"\n}\n\ninput:\n file sampletable from g_15_xlsfile_g_22\n\noutput:\n file '*.txt' into g_22_txtfile_g_20\n\n\"\"\"\n#!/usr/bin/env Rscript\n\n# Read the table\nsampletable=\"${sampletable}\"\ntable = read.table(sampletable, stringsAsFactors = F, header = T, sep=\"\\t\")\nfor(i in 1:nrow(table)){\n  \n# Parse params\nsamplename = table[i,\"Sample\"]\nncells =  table[i,\"ncells\"]\nfastqpath = table[i,\"fastqpath\"]\nrefindex = table[i,\"refindex\"]\nrefgtf = table[i,\"refgtf\"]\nchemistry = table[i,\"chemistry\"]\n\n# fn\nfn=paste0(samplename,\".txt\")\n\n# write to file\nfileConn<-file(fn)\nwriteLines(paste(samplename,ncells,fastqpath,refindex,refgtf,chemistry,sep=\"\\t\"), fileConn)\nclose(fileConn)  \n}\n\n\"\"\"\n}"], "list_proc": ["matthiasheinig/scworkflows/split_sample_table_cr_pipe"], "list_wf_names": ["matthiasheinig/scworkflows"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["matthiasheinig"], "nb_wf": 1, "list_wf": ["scworkflows"], "list_contrib": ["matthiasheinig", "wathom"], "nb_contrib": 2, "codes": ["\nprocess cellranger_count {\n\npublishDir params.outdir, mode: 'copy',\n\tsaveAs: {filename ->\n\tif (filename =~ /${samplename}$/) \"cellranger/$filename\"\n}\n\ninput:\n set samplename,paramfile from g_20_paramset_g_27\n val maxjobs from g_5_maxjobs_g_27\n val samtoolsncpus from g_10_ncpus_g_27\n\noutput:\n file \"${samplename}\" into g_27_folder_g_28, g_27_folder_g_29\n\n\"\"\"\n# Read paramfile into a var\nparamline=\\$(<${paramfile})\n\n# Split line\nIFS=\\$'\\t'\nparams=(\\${paramline})\n\n# Create param vars\nsamplename=\\${params[0]}\nncells=\\${params[1]}\nfastqpath=\\${params[2]}\nrefindex=\\${params[3]}\nchemistry=\\${params[5]}\n\n# Get cellranger version\ncellranger sitecheck > sitecheck.txt\ncrversion=`head -n2 sitecheck.txt | grep -oP '\\\\(\\\\K[^)]+'`\n\ncr_cmd=\"cellranger count --id=cellranger --fastqs=\\${fastqpath} \\\n--sample=${samplename} \\\n--transcriptome=\\${refindex} \\\n--expect-cells=\\${ncells} \\\n--jobmode=sge --maxjobs=${maxjobs} \\\n--chemistry=\\${chemistry}\"\n\n# Echo the cmd to a file\necho \\${cr_cmd} > cr_cmd.txt\n\n# Execute\neval \\${cr_cmd}\n\n# Create output directories\nmkdir -p ${samplename}/count_matrices\nmkdir -p ${samplename}/bamfile\nmkdir -p ${samplename}/statsfiles\n\n# Copy count matrices folders and h5 files from cellranger directory\ncp -R cellranger/outs/filtered_gene_bc_matrices ${samplename}/count_matrices\ncp -R cellranger/outs/raw_gene_bc_matrices ${samplename}/count_matrices\ncp cellranger/outs/filtered_gene_bc_matrices_h5.h5 ${samplename}/count_matrices\ncp cellranger/outs/raw_gene_bc_matrices_h5.h5 ${samplename}/count_matrices\n\n# Copy bamfile\ncp cellranger/outs/possorted_genome_bam.bam ${samplename}/bamfile\ncp cellranger/outs/possorted_genome_bam.bam.bai ${samplename}/bamfile\n\n# Copy statsfiles\ncp cellranger/outs/web_summary.html ${samplename}/statsfiles\ncp cellranger/outs/metrics_summary.csv ${samplename}/statsfiles\n\n# Copy analysis folder\ncp -R cellranger/outs/analysis ${samplename}\n\n# Copy molecule_info.h5 file\ncp cellranger/outs/molecule_info.h5 ${samplename}/analysis\n\n# Try to copy cloupe.cloupe file to analysis folder / is not created if mult genomes are used / do not err if file does not exist\ncp cellranger/outs/cloupe.cloupe ${samplename}/analysis 2>/dev/null || :\n\n# Echo the version to a file\necho \\${crversion} > ${samplename}/cr_version.txt\n# Cp sitecheck file\ncp sitecheck.txt ${samplename}/cr_sitecheck.txt\n# Copy the results directory\ncp cr_cmd.txt ${samplename}/cr_cmd.txt\n\n# Get samtools version\nsamtools --help | head -3 > ${samplename}/samtools_version.txt\n\n# Sort bamfile by CB tag / for velocyto\nsamt_cmd=\"samtools sort --threads ${samtoolsncpus} -t CB -O BAM -o ${samplename}/bamfile/cellsorted_possorted_genome_bam.bam ${samplename}/bamfile/possorted_genome_bam.bam\"\n\n# Execute\neval \\${samt_cmd}\n\n# Echo cmd to file\necho \\${samt_cmd} > ${samplename}/samtools_cmd.txt\n\n# Copy the param file to the out directory\ncp ${paramfile} ${samplename}/params.txt\n\n# rm cellranger folder\n#rm -R cellranger\n\n\"\"\"\n}"], "list_proc": ["matthiasheinig/scworkflows/cellranger_count"], "list_wf_names": ["matthiasheinig/scworkflows"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["mattpito", "mattpito_nf"], "nb_wf": 2, "list_wf": ["bam2RNAseq", "nf"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess bam2fq {\n\n    input:\n    file bam from params.sorted_bam\n    output:\n    file \"${bam}.fastq\" into bam2fq_ch1, bam2fq_ch2\n\n    shell:\n    \"\"\"\n    samtools bam2fq $bam > ${bam}.fastq\n    \"\"\"\n}", "\nprocess bam2fq {\n\n    input:\n    file bam from params.sorted_bam\n    output:\n    file \"${bam}.fastq\" into bam2fq_ch1, bam2fq_ch2\n\n    shell:\n    \"\"\"\n    samtools bam2fq $bam > ${bam}.fastq\n    \"\"\"\n}"], "list_proc": ["mattpito/nf/bam2fq", "mattpito_nf/bam2RNAseq/bam2fq"], "list_wf_names": ["mattpito/nf", "mattpito_nf/bam2RNAseq"]}, {"nb_reuse": 1, "tools": ["JAR3D"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess includeLocalRef {\n  input:\n  file \"*\" from local_ref_include_w_rpkg.collect()\n  file \"*\" from class_rpkg_map.collect()\n\n  output:\n  file \"eggnog.map\" into extended_eggnog_map\n  file \"data.jar\" into data_jar\n\n  publishDir params.reference_dir, pattern: '*.jar'\n\n  script:\n  \"\"\"\n  cp -L $params.megan_dir/jars/data.jar .\n  jar -xf data.jar\n  mv resources/files/eggnog.map .\n\n  if [ ! -n \"\\$(echo *.class)\" ];\n  then\n    for c in *.class;\n    do\n      if ! grep \\$'\\\\t\\${f%%.*}\\\\s\\\\|\\\\t\\${f%%.*}\\$' eggnog.map ;\n      then\n        cat \\$c >> eggnog.map\n      fi\n    done\n  fi\n\n  for f in *.fasta;\n  do\n    if ! grep \\$'\\\\t\\${f%%.*}\\\\s\\\\|\\\\t\\${f%%.*}\\$' eggnog.map ;\n    then\n      ID=\"\\$((\\$(tail -n 1 eggnog.map | cut -f1) + 1))\"\n      printf \"%i\\\\t%s\\\\n\" \"\\$ID\" \"\\${f%%.*}\" >> eggnog.map\n    fi\n  done\n\n  cp eggnog.map resources/files/\n  jar -cvf data.jar resources/*\n  rm -rf resources\n  \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/includeLocalRef"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["Diamond"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess diamondMakeDB {\n    input:\n    file 'references.fasta' from concatenated_references\n\n    output:\n    file 'references.dmnd' into diamond_database\n\n    publishDir params.reference_dir, mode: 'copy'\n\n    script:\n    \"\"\"\n    diamond makedb --in references.fasta --db references.dmnd\n    \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/diamondMakeDB"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["Diamond"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess alignFastQFiles {\n    input:\n    file fq from fastq_files_all\n    file 'references.dmnd' from diamond_database.first()\n\n    output:\n    file \"${fq.simpleName}.daa\" into daa_files optional true\n    stdout diamond_align_out\n\n    tag \"${fq.simpleName}\"\n                                        \n\n    script:\n    \"\"\"\n    diamond blastx -q ${fq} --db references.dmnd -f 100 --unal 0 -e 1e-6 --out ${fq.simpleName}.daa --threads ${task.cpus}\n    if [ ! \\$(diamond view --daa ${fq.simpleName}.daa | wc -l) -gt ${params.diamond_min_align_reads} ];\n    then\n      rm ${fq.simpleName}.daa\n      echo \"No queries were aligned for sample ${fq.simpleName}\"\n    fi\n    \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/alignFastQFiles"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["trimAl", "PRANK"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess alignReferences {\n  input:\n  file fasta from references_unique_fastas_align\n\n  output:\n  file \"${fasta.baseName}.aln\" into ref_alignments\n  file \"${fasta.baseName}.alignment.log\" into alignment_logs\n\n  publishDir \"${params.reference_dir}/${fasta.simpleName}\", mode: 'copy'\n  tag \"${fasta.simpleName}\"\n  stageInMode 'copy'\n\n  script:\n  if (params.align_method == \"prank\")\n    \"\"\"\n    prank -protein -d=$fasta -o=${fasta.baseName} -f=fasta\n    sed -i '/^>/! s/[U|*|X]/-/g' ${fasta.baseName}.best.fas\n    trimal -in ${fasta.baseName}.best.fas -out ${fasta.baseName}.aln -gt 0 -fasta\n    cp .command.out ${fasta.baseName}.alignment.log\n    \"\"\"\n  else if (params.align_method.startsWith(\"mafft\"))\n    \"\"\"\n    $params.align_method --adjustdirection --anysymbol --thread ${task.cpus}  $fasta > ${fasta.baseName}.mafft.aln\n    sed -i '/^>/! s/[U|*|X]/-/g' ${fasta.baseName}.mafft.aln\n    trimal -in ${fasta.baseName}.mafft.aln -out ${fasta.baseName}.aln -gt 0 -fasta\n    sed \"s/\\\\r/\\\\n/g\" .command.log > ${fasta.baseName}.alignment.log\n    \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/alignReferences"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["RAxML-NG"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess buildTreefromReferences {\n  input:\n  file reference_alignment from ref_alignments\n\n  output:\n  set file(\"${reference_alignment.simpleName}.treefile\"), file(\"${reference_alignment.simpleName}.modelfile\"), file(\"$reference_alignment\") into reference_trees\n  file \"${reference_alignment.simpleName}.tree.log\" into tree_logs\n\n  publishDir \"${params.reference_dir}/${reference_alignment.simpleName}\", mode: 'copy'\n  tag \"${reference_alignment.simpleName}\"\n\n  script:\n  if (params.phylo_method.startsWith(\"iqtree\"))\n    \"\"\"\n    ${params.phylo_method} -s ${reference_alignment} -m LG+G+F -nt AUTO -ntmax ${task.cpus} -pre ${reference_alignment.simpleName}\n    raxml-ng --evaluate --msa ${reference_alignment} --tree ${reference_alignment.simpleName}.treefile --threads ${task.cpus} --model LG+G+F --prefix info\n    mv info.raxml.bestModel ${reference_alignment.simpleName}.modelfile\n    mv ${reference_alignment.simpleName}.log ${reference_alignment.simpleName}.tree.log\n    \"\"\"\n  else if (params.phylo_method == \"fasttree\")\n    \"\"\"\n    FastTree -log ${reference_alignment.simpleName}.tree.log -lg ${reference_alignment} > ${reference_alignment.simpleName}.treefile\n    raxml-ng --evaluate --msa ${reference_alignment} --tree ${reference_alignment.simpleName}.treefile --threads ${task.cpus} --model LG+G+F --prefix info\n    mv info.raxml.bestModel ${reference_alignment.simpleName}.modelfile\n    \"\"\"\n  else if (params.phylo_method == \"raxml\")\n    \"\"\"\n    # raxmlHPC-PTHREADS -f e -s ${reference_alignment} -t ${reference_alignment.simpleName}.treefile -T ${task.cpus} -n file -m PROTGAMMALGF\n    raxml-ng --msa ${reference_alignment} --prefix ${reference_alignment.simpleName} --threads ${task.cpus} --model LG+G+F\n    mv ${reference_alignment.simpleName}.raxml.log ${reference_alignment.simpleName}.tree.log\n    mv ${reference_alignment.simpleName}.raxml.bestTree ${reference_alignment.simpleName}.treefile\n    mv ${reference_alignment.simpleName}.raxml.bestModel ${reference_alignment.simpleName}.modelfile\n    \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/buildTreefromReferences"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["trimAl"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess alignQueriestoRefMSA {\n  input:\n  set file(reftree), file(modelinfo), file(refalignment) from reference_trees_combined\n  each contigs from translated_contigs\n\n  output:\n  set file(\"$reftree\"), file(\"$modelinfo\"), file(\"$refalignment\"), file(\"${contigs.simpleName}.ref.aln\") into aligned_queries\n\n  tag \"${contigs.simpleName} - ${refalignment.simpleName}\"\n  publishDir \"${params.queries_dir}/${contigs.simpleName.tokenize('-')[0]}\", mode: 'copy', pattern: \"${contigs.simpleName}*\"\n\n  when:\n  \"${refalignment.simpleName}\" == \"${contigs.simpleName.tokenize('-')[1]}\"\n\n  script:\n  \"\"\"\n  trimal -in $refalignment -out ${refalignment.simpleName}.phy -phylip\n  papara -t $reftree -s ${refalignment.simpleName}.phy -q $contigs -a -n ${contigs.simpleName} -r\n  mv papara_alignment.${contigs.simpleName} ${contigs.simpleName}.ref.aln\n  \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/alignQueriestoRefMSA"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["EPA-ng"], "nb_own": 1, "list_own": ["maxemil"], "nb_wf": 1, "list_wf": ["PhyloMagnet"], "list_contrib": ["maxemil"], "nb_contrib": 1, "codes": ["\nprocess placeContigsOnRefTree {\n  input:\n  set file(reftree), file(modelinfo), file(refalignment), file(queryalignment) from aligned_queries_split\n\n  output:\n  file \"${queryalignment.simpleName}.jplace\" into placed_contigs\n\n  tag \"${queryalignment.simpleName} - ${refalignment.simpleName}\"\n  publishDir \"${params.queries_dir}/${queryalignment.simpleName.tokenize('-')[0]}\", mode: 'copy'\n\n  script:\n  \"\"\"\n  epa-ng --ref-msa $refalignment --tree $reftree --query $queryalignment --model $modelinfo --no-heur --threads ${task.cpus}\n  mv epa_result.jplace ${queryalignment.simpleName}.jplace\n  \"\"\"\n}"], "list_proc": ["maxemil/PhyloMagnet/placeContigsOnRefTree"], "list_wf_names": ["maxemil/PhyloMagnet"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n\n    conda 'bioconda::fastqc'\n\n    label 'normal'\n\n    cpus 1\n\n    publishDir \"${params.results}/fastqc\", mode: 'copy'\n\n    errorStrategy 'ignore'\n\n    input:\n        set val(name), file(reads) from reads_fastqc\n\n    output:\n        file '*_fastqc.{zip,html}' into fastqc_results\n    script:\n        \"\"\"\n        fastqc -q $reads\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/fastqc"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["MEGAHIT"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess megahit_assembly{\n    tag \"$name\"\n\n    conda 'bioconda::megahit'\n\n    label 'normal'\n\n    cpus params.megahitCPU\n\n    publishDir \"${params.results}/assembly\", mode: 'copy'\n\n    input:\n        set val(name), file(merged) from trimmed_reads_assembly\n    output:\n        set val(name), file (\"megahit_out/*.contigs.fa\") into contigs_mapping, contigs_filter\n    script:\n        \"\"\"\n        megahit -r $merged -t ${task.cpus} --out-prefix $name\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/megahit_assembly"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess bowtie_index_contigs{\n    tag \"$name\"\n\n    conda 'bioconda::bowtie2'\n\n    label 'normal'\n\n    cpus params.bowtieCPU\n\n    publishDir \"${params.results}/bowtie_index\", mode: 'copy'\n\n    input:\n        set val(name), file(contig) from contigs_mapping\n    output:\n        set val(name), file(\"*.bt2\") into bt_index\n    script:\n        \"\"\"\n        bowtie2-build --threads ${task.cpus} $contig $name\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/bowtie_index_contigs"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess align_reads_to_contigs{\n    tag \"$name\"\n\n    conda 'bioconda::bowtie2 bioconda::samtools'\n\n    label 'normal'\n\n    cpus params.bowtieCPU\n\n    publishDir \"${params.results}/alignment\", mode: 'copy'\n\n    input:\n        set val(name), file(reads), file(contig) from trimmed_reads_mapping.join(bt_index)\n    output:\n        set val(name), file(\"*.sorted.bam\") into alignment_to_index, alignment_to_coverage, alignment_to_sam\n    script:\n        outfile = name+\".sorted.bam\"\n        \"\"\"\n        bowtie2 -x $name -U $reads --very-fast --threads ${task.cpus} | samtools view -S -b -F 4 - | samtools sort - > $outfile\n        \"\"\"\n\n}"], "list_proc": ["maxibor/admapipe/align_reads_to_contigs"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess bam_index {\n    tag \"$name\"\n\n    conda 'bioconda::samtools'\n\n    label 'normal'\n\n    cpus 1\n\n    publishDir \"${params.results}/alignment\", mode: 'copy'\n\n    input:\n        set val(name), file(bam) from alignment_to_index\n    output:\n        set val(name), file(\"*.bam.bai\")\n    script:\n        \"\"\"\n        samtools index $bam\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/bam_index"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess bedtools_genomecov {\n    tag \"$name\"\n\n    conda 'bioconda::bedtools'\n\n    label 'normal'\n\n    cpus 1\n\n    publishDir \"${params.results}/coverage\", mode: 'copy'\n\n    input:\n        set val(name), file(bam) from alignment_to_coverage\n    output:\n        set val(name), file(\"*.bed\") into bedfile\n    script:\n        outfile = name+\".bed\"\n        \"\"\"\n        bedtools genomecov -ibam $bam -d > $outfile\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/bedtools_genomecov"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["Kraken"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess kraken {\n    tag \"$name\"\n\n    conda 'bioconda::kraken'\n\n    label 'normal'\n\n    cpus params.krakenCPU\n\n    publishDir \"${params.results}/kraken\", mode: 'copy'\n\n    input:\n        set val(name), file(fasta) from trimmed_reads_kraken\n    output:\n        set val(name), file(\"*.kraken\") into kraken_output\n    script:\n        outfile = name+\".kraken\"\n        \"\"\"\n        kraken --db ${params.krakendb} --threads ${task.cpus} --fastq-input $fasta > $outfile\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/kraken"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["admapipe"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess megablast {\n    tag \"$name\"\n\n    conda 'bioconda::blast'\n\n    label 'normal'\n\n    cpus params.megablastCPU\n\n    publishDir \"${params.results}/megablast\", mode: 'copy'\n\n    input:\n        set val(name), file(fasta) from fasta2megablast\n    output:\n        set val(name), file(\"*.blast\") into blast_output\n    script:\n        outfile = name+\".blast\"\n        \"\"\"\n        blastn -task megablast -num_threads ${task.cpus} -outfmt 6 -db ${params.blastdb} -query $fasta -out $outfile\n        \"\"\"\n}"], "list_proc": ["maxibor/admapipe/megablast"], "list_wf_names": ["maxibor/admapipe"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["anonymap"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess Bowtie2Align {\n    tag \"$name\"\n\n    conda 'bioconda::bowtie2 bioconda::samtools'\n\n    errorStrategy 'ignore'\n\n    label 'intenso'\n\n    publishDir \"${params.results}\", pattern: \"*.flagstat.txt\", mode: 'copy'\n\n    input:\n        set val(name), file(reads) from trimmed_reads\n        file(index) from bt_index_genome.collect()\n    output:\n        set val(name), file(\"*.aligned.sorted.bam\") into alignment_genome\n        set val(name), file(\"*.flagstat.txt\") into align1_multiqc\n    script:\n        index_name = index.toString().tokenize(' ')[0].tokenize('.')[0]\n        samfile = name+\".aligned.sam\"\n        fstat = name+\".flagstat.txt\"\n        outfile = name+\".aligned.sorted.bam\"\n        if (params.pairedEnd) {\n            \"\"\"\n            bowtie2 -x $index_name -1 ${reads[0]} -2 ${reads[1]} $bowtie_setting --threads ${task.cpus} > $samfile\n            samtools view -S -b -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n            samtools flagstat $outfile > $fstat\n            \"\"\"\n        } else {\n            \"\"\"\n            bowtie2 -x $index_name -U ${reads[0]} $bowtie_setting --threads ${task.cpus} > $samfile\n            samtools view -S -b -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n            samtools flagstat $outfile > $fstat\n            \"\"\"\n        }\n}"], "list_proc": ["maxibor/anonymap/Bowtie2Align"], "list_wf_names": ["maxibor/anonymap"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["bowtie2-lca"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": [" process bowtie2index {\n\n    label 'process_medium'\n\n    input:\n        file(fasta) from fasta_ref\n    output:\n        file(\"*.bt2\") into bowtie_db\n        val(dbname) into bt_db_name\n    script:\n        dbname = fasta.baseName\n        \"\"\"\n        bowtie2-build $fasta $dbname --threads ${task.cpus}\n        \"\"\"\n    }"], "list_proc": ["maxibor/bowtie2-lca/bowtie2index"], "list_wf_names": ["maxibor/bowtie2-lca"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["bowtie2-lca"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess sam2lca {\n    tag \"$name\"\n\n    label 'process_medium'\n\n    publishDir \"${params.outdir}/sam2lca\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(name), file(bam), file(tree), file(db_check), file(ete_check) from sam2lca_ch\n    output:\n        set val(name), file(\"*.sam2lca.*\") into sam2lca_result\n    script:\n        if (params.lca_tree){\n            tree_opt = \"-t ${tree}\"\n        } else {\n            tree_opt = \"\"\n        }\n        \"\"\"\n        samtools index $bam\n        sam2lca -d ${params.lca_db} -m ${params.lca_mapping} analyze -p ${task.cpus} $tree_opt -o ${name}.sam2lca $bam\n        \"\"\"\n    \n}"], "list_proc": ["maxibor/bowtie2-lca/sam2lca"], "list_wf_names": ["maxibor/bowtie2-lca"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n\n    input:\n        set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n        file '*_fastqc.{zip,html}' into fastqc_results\n    script:\n        \"\"\"\n        fastqc -q $reads\n        \"\"\"\n}"], "list_proc": ["maxibor/coproID/fastqc"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Bowtie", "MetaPhlAn"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 2, "list_wf": ["coproID", "metaphlan-nf"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess sam2bam {\n    tag \"$name\"\n\n    label 'expresso'\n\n    publishDir \"${params.results}/metaphlan/$name\", mode: 'copy', pattern: '*.sorted.bam'\n\n    input:\n        path(fasta) from fasta_ref_decomp_ch\n        tuple val(name), path(sam) from mpa_aln\n    output:\n        set val(name), path('*.sorted.bam') into mpa_aln_damageprofiler, mpa_aln_pydamage\n    script:\n        \"\"\"\n        samtools view -F 4 $sam | cut -f 3 | sort | uniq > mapped_refs.txt\n        samtools view -H $sam > all_refs.txt\n        grep -Ff mapped_refs.txt all_refs.txt > mapped.sam\n        samtools view -F 4 $sam >> mapped.sam\n        samtools view -h -b -@ ${task.cpus} mapped.sam | samtools sort -@ ${task.cpus} > ${name}.sorted.bam\n        \"\"\"\n}", "\nprocess metaphlan {\n    tag \"$name\"\n\n    label 'intenso'\n\n    publishDir \"${params.results}/metaphlan/$name\", mode: 'copy', pattern: '*.out'\n\n    input:\n        set val(name), file(reads) from trimmed_reads\n        val (mpa_db) from mpa_db_path_wait1\n\n    output:\n        set val(name), file('*.metaphlan.out') into metaphlan_out\n        set val(name), path('*.sam') into mpa_aln\n        path(\"*.bowtie2.log\") into bt2_log\n\n    script:\n        out = name+\".metaphlan.out\"\n        sam_out = name+\".sam\"\n        bt_out = name+\".bowtie2.log\"\n        btdb = \"${params.bt2db}/${params.mpa_db_name}\"\n        if (params.pairedEnd && !params.collapse){\n            \"\"\"\n            bowtie2 --no-unal --very-sensitive -S $sam_out -x $btdb -p ${task.cpus} -1 ${reads[0]} -2 ${reads[1]} 2> $bt_out\n            metaphlan $sam_out \\\\\n                      -o $out \\\\\n                      --bowtie2db ${params.bt2db} \\\\\n                      -x ${params.mpa_db_name} \\\\\n                      --input_type sam \\\\\n                      --nproc ${task.cpus} \\\\\n            \"\"\"    \n        } else {\n            \"\"\"\n            bowtie2 --no-unal --very-sensitive -S $sam_out -x $btdb -p ${task.cpus} -U $reads 2> $bt_out\n            metaphlan $sam_out \\\\\n                      -o $out \\\\\n                      --bowtie2db ${params.bt2db} \\\\\n                      -x ${params.mpa_db_name} \\\\\n                      --input_type sam \\\\\n                      --nproc ${task.cpus} \\\\\n            \"\"\"  \n        }\n        \n}", " process BowtieIndexGenome1 {\n        tag \"${params.name1}\"\n\n        label 'process_medium'\n\n        input:\n            file(fasta) from genome1Fasta\n        output:\n            file(\"*.bt2\") into bt1_ch\n        script:\n            \"\"\"\n            bowtie2-build $fasta ${bt1_index}\n            \"\"\"\n    }"], "list_proc": ["maxibor/metaphlan-nf/sam2bam", "maxibor/metaphlan-nf/metaphlan", "maxibor/coproID/BowtieIndexGenome1"], "list_wf_names": ["maxibor/coproID", "maxibor/metaphlan-nf"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Bowtie", "MultiQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 2, "list_wf": ["coproID", "metaphlan-nf"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess AlignToGenome1 {\n    tag \"$name\"\n\n    label 'process_medium'\n\n    publishDir \"${params.outdir}/alignments/${params.name1}\", mode: 'copy', pattern: '*.sorted.bam'\n\n    input:\n        set val(name), file(reads) from trimmed_reads_genome1\n        file(index) from bt1_ch\n    output:\n        set val(name), file(\"*.aligned.sorted.bam\") into alignment_genome1, alignment_genome1_pmd\n        set val(name), file(\"*.unaligned.sorted.bam\") into unaligned_genome1\n        set val(name), file(\"*.stats.txt\") into align1_multiqc\n    script:\n        samfile = \"aligned_\"+params.name1+\".sam\"\n        fstat = name+\"_\"+params.name1+\".stats.txt\"\n        outfile = name+\"_\"+params.name1+\".aligned.sorted.bam\"\n        outfile_unalign = name+\"_\"+params.name1+\".unaligned.sorted.bam\"\n        if (params.collapse == true || params.single_end == true) {\n            \"\"\"\n            bowtie2 -x $bt1_index -U ${reads[0]} $bowtie_setting --threads ${task.cpus} > $samfile 2> $fstat\n            samtools view -S -b -F 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n            samtools view -S -b -f 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile_unalign\n            \"\"\"\n        } else if (params.collapse == false){\n            \"\"\"\n            bowtie2 -x $bt1_index -1 ${reads[0]} -2 ${reads[1]} $bowtie_setting --threads ${task.cpus} > $samfile 2> $fstat\n            samtools view -S -b -F 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n            samtools view -S -b -f 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile_unalign\n            \"\"\"\n        }            \n}", " process pydamage {\n        tag \"$name\"\n        \n        label 'intenso'\n\n        errorStrategy 'ignore'\n\n        publishDir \"${params.results}/pydamage/$name\", mode: 'copy'\n\n        input:\n            tuple val(name), path(aln) from mpa_aln_pydamage\n        output:\n            tuple val(name), path(\"*.pydamage_results.csv\") optional true into pydamage_result_ch\n            path \"${name}/plots\", optional: true\n        script:\n            output = name\n            if (params.pydamage_plot) {\n                plot = \"--plot\"\n            } else {\n                plot = \"\"\n            }\n            \"\"\"\n            samtools index $aln\n            pydamage --force -p ${task.cpus} -m ${params.minread} -c ${params.coverage} $plot -o $output $aln\n            mv ${name}/pydamage_results.csv ${name}.pydamage_results.csv\n            \"\"\"\n    }", "\nprocess multiqc {\n\n    label 'ristretto'\n \n    publishDir \"${params.results}\", mode: 'copy'\n\n    input:\n        path('adapterRemoval/*') from adapter_removal_results_multiqc.collect().ifEmpty([])\n        path('bowtie2/*') from bt2_log.collect().ifEmpty([])\n                'damageProfiler/*'                                                     \n    output:\n        path('*multiqc_report.html')\n    script:\n        \"\"\"\n        multiqc -c ${params.multiqc_config} .\n        \"\"\"\n}"], "list_proc": ["maxibor/coproID/AlignToGenome1", "maxibor/metaphlan-nf/pydamage", "maxibor/metaphlan-nf/multiqc"], "list_wf_names": ["maxibor/coproID", "maxibor/metaphlan-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess bam2fq {\n    tag \"$name\"\n\n    label 'process_medium'\n\n    input:\n        set val(name), file(bam) from unaligned_genome1\n    output:\n        set val(name), file(\"*.fastq\") into unmapped_humans_reads\n    script:\n        if (paired_end && params.collapse == false){\n            out1 = name+\"_\"+params.name1+\".unaligned_R1.fastq\"\n            out2 = name+\"_\"+params.name1+\".unaligned_R2.fastq\"\n            \"\"\"\n            samtools fastq -1 $out1 -2 $out2 -0 /dev/null -s /dev/null -n -F 0x900 $bam\n            \"\"\"\n        } else {\n            out = name+\"_\"+params.name1+\".unaligned.fastq\"\n            \"\"\"\n            samtools fastq $bam > $out\n            \"\"\"\n        }\n}"], "list_proc": ["maxibor/coproID/bam2fq"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 2, "tools": ["Bowtie", "MultiQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 2, "list_wf": ["coproID", "metaphlan-nf"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": [" process BowtieIndexGenome2 {\n        tag \"${params.name2}\"\n\n        label 'process_medium'\n\n        input:\n            file(fasta) from genome2Fasta\n        output:\n            file(\"*.bt2\") into bt2_ch\n        script:\n            \"\"\"\n            bowtie2-build $fasta ${bt2_index}\n            \"\"\"\n    }", "\nprocess multiqc {\n\n    label 'ristretto'\n \n    publishDir \"${params.results}\", mode: 'copy'\n\n    input:\n        path('adapterRemoval/*') from adapter_removal_results_multiqc.collect().ifEmpty([])\n        path('bowtie2/*') from bt2_log.collect().ifEmpty([])\n                'damageProfiler/*'                                                     \n    output:\n        path('*multiqc_report.html')\n    script:\n        \"\"\"\n        multiqc -c ${params.multiqc_config} .\n        \"\"\"\n}"], "list_proc": ["maxibor/coproID/BowtieIndexGenome2", "maxibor/metaphlan-nf/multiqc"], "list_wf_names": ["maxibor/coproID", "maxibor/metaphlan-nf"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": [" process BowtieIndexGenome3 {\n        tag \"${params.name2}\"\n\n        label 'process_medium'\n\n        input:\n            file(fasta) from genome3Fasta\n        output:\n            file(\"*.bt2\") into bt3_ch\n        script:\n            \"\"\"\n            bowtie2-build $fasta ${bt3_index}\n            \"\"\"\n    }"], "list_proc": ["maxibor/coproID/BowtieIndexGenome3"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie", "BWA"], "nb_own": 2, "list_own": ["mblanche", "maxibor"], "nb_wf": 2, "list_wf": ["coproID", "nextflow"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess AlignToGenome2 {\n    tag \"$name\"\n\n    label 'process_medium'\n\n    publishDir \"${params.outdir}/alignments/${params.name2}\", mode: 'copy', pattern: '*.sorted.bam'\n\n    input:\n        set val(name), file(reads) from trimmed_reads_genome2\n        file(index) from bt2_ch\n    output:\n        set val(name), file(\"*.aligned.sorted.bam\") into alignment_genome2, alignment_genome2_pmd\n        set val(name), file(\"*.unaligned.sorted.bam\") into unaligned_genome2\n        set val(name), file(\"*.stats.txt\") into align2_multiqc\n    script:\n        samfile = \"aligned_\"+params.name2+\".sam\"\n        fstat = name+\"_\"+params.name2+\".stats.txt\"\n        outfile = name+\"_\"+params.name2+\".aligned.sorted.bam\"\n        outfile_unalign = name+\"_\"+params.name2+\".unaligned.sorted.bam\"\n        if (params.collapse == true || params.single_end == true) {\n            \"\"\"\n            bowtie2 -x $bt2_index -U ${reads[0]} $bowtie_setting --threads ${task.cpus} > $samfile 2> $fstat\n            samtools view -S -b -F 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n            samtools view -S -b -f 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile_unalign\n            \"\"\"\n        } else if (params.collapse == false){\n            \"\"\"\n            bowtie2 -x $bt2_index -1 ${reads[0]} -2 ${reads[1]} $bowtie_setting --threads ${task.cpus} > $samfile 2> $fstat\n            samtools view -S -b -F 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n            samtools view -S -b -f 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile_unalign\n            \"\"\"\n        }            \n}", "\nprocess bwa_mem {\n    tag \"_${id}\"\n    cpus 48\n    memory '48 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    tuple val(id), file(R1s), file(R2s) from fastqDir_ch\n    path index from bwa_index.first()\n    \n    output:\n    tuple id, path(\"*.bam\") into bam_part_ch\n    \n    script:\n    \"\"\"\n    bwa mem -5SP -t ${task.cpus} \\\n    \t${index}/${params.genome} \\\n    \t<(zcat ${R1s}) \\\n    \t<(zcat ${R2s}) \\\n\t|samtools view -@ ${task.cpus} -Shb - > ${id}.bam\n    \n    \"\"\"\n}"], "list_proc": ["maxibor/coproID/AlignToGenome2", "mblanche/nextflow/bwa_mem"], "list_wf_names": ["maxibor/coproID", "mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": [" process AlignToGenome3 {\n        tag \"$name\"\n\n        label 'process_medium'\n\n        publishDir \"${params.outdir}/alignments/${params.name1}\", mode: 'copy', pattern: '*.sorted.bam'\n\n        input:\n            set val(name), file(reads) from trimmed_reads_genome3\n            file(index) from bt3_ch\n        output:\n            set val(name), file(\"*.aligned.sorted.bam\") into alignment_genome3, alignment_genome3_pmd\n            set val(name), file(\"*.unaligned.sorted.bam\") into unaligned_genome3\n            set val(name), file(\"*.stats.txt\") into align3_multiqc\n        script:\n            samfile = \"aligned_\"+params.name3+\".sam\"\n            fstat = name+\"_\"+params.name3+\".stats.txt\"\n            outfile = name+\"_\"+params.name3+\".aligned.sorted.bam\"\n            outfile_unalign = name+\"_\"+params.name3+\".unaligned.sorted.bam\"\n            if (params.collapse == true || params.single_end == true) {\n                \"\"\"\n                bowtie2 -x $bt3_index -U ${reads[0]} $bowtie_setting --threads ${task.cpus} > $samfile 2> $fstat\n                samtools view -S -b -F 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n                samtools view -S -b -f 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile_unalign\n                \"\"\"\n            } else if (params.collapse == false){\n                \"\"\"\n                bowtie2 -x $bt3_index -1 ${reads[0]} -2 ${reads[1]} $bowtie_setting --threads ${task.cpus} > $samfile 2> $fstat\n                samtools view -S -b -F 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile\n                samtools view -S -b -f 4 -@ ${task.cpus} $samfile | samtools sort -@ ${task.cpus} -o $outfile_unalign\n                \"\"\"\n            }            \n    }"], "list_proc": ["maxibor/coproID/AlignToGenome3"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 3, "tools": ["SAMtools", "IBS"], "nb_own": 2, "list_own": ["mblanche", "maxibor"], "nb_wf": 2, "list_wf": ["coproID", "nextflow"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess index {\n    label 'index'\n    tag '_${id}'\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n    \n    publishDir \"${params.bamDir}\",\n\tmode: \"copy\"\n\n    input:\n    path(bam) from bam_ch\n\n    output:\n    tuple id, path(bam), path(\"*.bam.bai\") into bamNidx_ch\n    \n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools index -@${task.cpus} ${bam}\n    \"\"\"\n    \n\n}", " process download_bs {\n\techo true\n\tlabel \"movers\"\n\tcpus 4\n\tmemory '4G'\n\tcontainer 'mblanche/basespace-cli'\n\tqueue 'moversQ'\n\t\n\tpublishDir \"${params.outDir}/fastqs\",\n\t    mode: 'copy'\n\t\n\tinput:\n\ttuple id, val(bs) from bs_id_ch\n\t    .splitCsv(header: false)\n\t\n\toutput:\n\ttuple bs, path(\"*.fastq.gz\") into fastqs_ch\n\t\n\tscript:\n\t\"\"\"\n\tbs file download --api-server ${host} --access-token ${token} -i ${id} -o . \n\t\"\"\"\n    }", " process pmdtoolsgenome1 {\n    tag \"$name\"\n\n    publishDir \"${params.outdir}/pmdtools/${params.name1}\", mode: 'copy', pattern: '*.pmd_filtered.bam'\n\n    input:\n        set val(name), file(bam1) from alignment_genome1_pmd\n    output:\n        set val(name), file(\"*.pmd_filtered.bam\") into pmd_aligned1\n    script:\n        outfile = name+\"_\"+params.name1+\".pmd_filtered.bam\"\n        \"\"\"\n        samtools view -h -F 4 $bam1 | pmdtools -t ${params.pmdscore} --header $library | samtools view -Sb - > $outfile\n        \"\"\"\n    }"], "list_proc": ["mblanche/nextflow/index", "mblanche/nextflow/download_bs", "maxibor/coproID/pmdtoolsgenome1"], "list_wf_names": ["maxibor/coproID", "mblanche/nextflow"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["mblanche", "maxibor"], "nb_wf": 2, "list_wf": ["coproID", "nextflow"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess cleanUpBam {\n    label 'cleanUp'\n    tag \"_${id}\"\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n\n    input:\n    tuple id, path(bam), path(idx) from bam_cleanBam_ch\n    \n    output:\n    tuple id, path(\"*-cleanedUp.bam\") into cleanBam_ch\n\n    script:\n    \"\"\"\n    samtools view -@ ${task.cpus} -Shu -F 2048 ${bam} \\\n\t| samtools sort -n -@ ${task.cpus}  -o ${id}-cleanedUp.bam -\n    \"\"\"\n}", " process pmdtoolsgenome2 {\n        tag \"$name\"\n\n        publishDir \"${params.outdir}/pmdtools/${params.name2}\", mode: 'copy', pattern: '*.pmd_filtered.bam'\n\n        input:\n            set val(name), file(bam2) from alignment_genome2_pmd\n        output:\n            set val(name), file(\"*.pmd_filtered.bam\") into pmd_aligned2\n        script:\n            outfile = name+\"_\"+params.name2+\".pmd_filtered.bam\"\n            \"\"\"\n            samtools view -h -F 4 $bam2 | pmdtools -t ${params.pmdscore} --header $library | samtools view -Sb - > $outfile\n            \"\"\"\n    }"], "list_proc": ["mblanche/nextflow/cleanUpBam", "maxibor/coproID/pmdtoolsgenome2"], "list_wf_names": ["maxibor/coproID", "mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess kraken2 {\n    tag \"$name\"\n\n    label 'process_medium'\n\n    input:\n        set val(name), file(reads) from unmapped_humans_reads\n        file(krakendb) from krakendb_ch\n\n\n    output:\n        set val(name), file('*.kraken.out') into kraken_out\n        set val(name), file('*.kreport') into kraken_report\n\n    script:\n        out = name+\".kraken.out\"\n        kreport = name+\".kreport\"\n        if (paired_end && params.collapse == false){\n            \"\"\"\n            kraken2 --db ${krakendb} \\\\\n                    --threads ${task.cpus} \\\\\n                    --output $out \\\\\n                    --report $kreport \\\\\n                    --paired ${reads[0]} ${reads[1]}\n            \"\"\"    \n        } else {\n            \"\"\"\n            kraken2 --db ${krakendb} \\\\\n                    --threads ${task.cpus} \\\\\n                    --output $out \\\\\n                    --report $kreport ${reads[0]}\n            \"\"\"\n        }\n        \n}"], "list_proc": ["maxibor/coproID/kraken2"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": [" process countBp3genomes{\n    tag \"$name\"\n\n    label 'process_low'\n\n    echo true\n\n    input:\n\n        set val(name), file(abam1), file(abam2), file(abam3), file(bam1), file(bam2), file(bam3) from ( (params.adna ? pmd_aligned1.join(pmd_aligned2).join(pmd_aligned3) : alignment_genome1_pmd.join(alignment_genome2_pmd).join(alignment_genome3_pmd)).join(alignment_genome1).join(alignment_genome2).join(alignment_genome3))\n        file(genome1) from genome1Size\n        file(genome2) from genome2Size\n        file(genome3) from genome3Size\n    output:\n        set val(name), file(\"*.bpc.csv\") into bp_count\n        set val(name), file(\"*\"+params.name1+\".ancient.filtered.bam\") optional true into ancient_filtered_bam1\n        set val(name), file(\"*\"+params.name2+\".ancient.filtered.bam\") optional true into ancient_filtered_bam2\n        set val(name), file(\"*\"+params.name3+\".ancient.filtered.bam\") optional true into ancient_filtered_bam3\n    script:\n        outfile = name+\".bpc.csv\"\n        organame1 = params.name1\n        organame2 = params.name2\n        organame3 = params.name3\n\n        obam1 = name+\"_\"+organame1+\".filtered.bam\"\n        obam2 = name+\"_\"+organame2+\".filtered.bam\"\n        obam3 = name+\"_\"+organame3+\".filtered.bam\"\n        if (params.adna) {\n            aobam1 = name+\"_\"+organame1+\".ancient.filtered.bam\"\n            aobam2 = name+\"_\"+organame2+\".ancient.filtered.bam\"\n            aobam3 = name+\"_\"+organame3+\".ancient.filtered.bam\"\n            \"\"\"\n            samtools index $bam1\n            samtools index $bam2\n            samtools index $bam3\n            samtools index $abam1\n            samtools index $abam2\n            samtools index $abam3\n            normalizedReadCount -n $name \\\\\n                                -b1 $bam1 \\\\\n                                -ab1 $abam1 \\\\\n                                -b2 $bam2 \\\\\n                                -ab2 $abam2 \\\\\n                                -b3 $bam3 \\\\\n                                -ab3 $abam3 \\\\\n                                -g1 $genome1 \\\\\n                                -g2 $genome2 \\\\\n                                -g3 $genome3 \\\\\n                                -r1 $organame1 \\\\\n                                -r2 $organame2 \\\\\n                                -r3 $organame3 \\\\\n                                -i ${params.identity} \\\\\n                                -o $outfile \\\\\n                                -ob1 $obam1 \\\\\n                                -aob1 $aobam1 \\\\\n                                -ob2 $obam2 \\\\\n                                -aob2 $aobam2 \\\\\n                                -ob3 $obam3 \\\\\n                                -aob3 $aobam3 \\\\\n                                -ed1 ${params.endo1} \\\\\n                                -ed2 ${params.endo2} \\\\\n                                -ed3 ${params.endo3} \\\\\n                                -p ${task.cpus}\n            \"\"\"\n\n        } else {\n            \"\"\"\n            samtools index $bam1\n            samtools index $bam2\n            samtools index $bam3\n            normalizedReadCount -n $name \\\\\n                                -b1 $bam1 \\\\\n                                -b2 $bam2 \\\\\n                                -b3 $bam3 \\\\\n                                -g1 $genome1 \\\\\n                                -g2 $genome2 \\\\\n                                -g3 $genome3 \\\\\n                                -r1 $organame1 \\\\\n                                -r2 $organame2 \\\\\n                                -r3 $organame3 \\\\\n                                -i ${params.identity} \\\\\n                                -o $outfile \\\\\n                                -ob1 $obam1 \\\\\n                                -ob2 $obam2 \\\\\n                                -ob3 $obam3 \\\\\n                                -ed1 ${params.endo1} \\\\\n                                -ed2 ${params.endo2} \\\\\n                                -ed3 ${params.endo3} \\\\\n                                -p ${task.cpus}\n            \"\"\"\n        }\n    }"], "list_proc": ["maxibor/coproID/countBp3genomes"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 1, "tools": ["SAMtools", "kraken2", "MultiQC", "Bowtie", "FastQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    sourcepredict -h  > v_sourcepredict.txt\n    samtools --version > v_samtools.txt\n    kraken2 --version > v_kraken2.txt\n    bowtie2 --version > v_bowtie2.txt\n    python --version > v_python.txt\n    AdapterRemoval --version 2> v_adapterremoval.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["maxibor/coproID/get_software_versions"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["coproID"], "list_contrib": ["apeltzer", "maxibor", "maxulysse", "jfy133"], "nb_contrib": 4, "codes": ["\nprocess multiqc {\n\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n    input:\n        file (ar:'adapter_removal/*') from adapter_removal_results.collect()\n        file (al1: 'alignment/*') from align1_multiqc.collect()\n        file ('fastqc/*') from fastqc_results.collect()\n        file ('DamageProfiler/*') from dmgProf1_ch.collect()\n        file ('DamageProfiler/*') from dmgProf2_ch.collect()\n        file ('software_versions/*') from ch_software_versions_yaml.collect()\n        file(multiqc_conf) from ch_multiqc_config\n        file logo from coproid_logo\n    output:\n        file 'multiqc_report.html' into multiqc_report\n\n    script:\n        \"\"\"\n        multiqc -f -d adapter_removal alignment fastqc DamageProfiler software_versions software_versions -c $multiqc_conf\n        \"\"\"\n}"], "list_proc": ["maxibor/coproID/multiqc"], "list_wf_names": ["maxibor/coproID"]}, {"nb_reuse": 1, "tools": ["Taxa", "M-TRACK", "getnumber"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["dada2-nf"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess dada2 {\n    tag \"$name\"\n\n    label 'dada'\n\n    publishDir \"${params.results}/dada\", mode: 'copy'\n\n    input:\n        set val(name), file(fq) from trimmed_reads\n        file(silva) from silva_db.first()\n        file(silva_species) from silva_species_db.first()\n    output:\n        set val(name), file(\"*.species_dada2.csv\") into dada_out\n        set val(name), file(\"*.dada2.csv\") into dada_classify\n        set val(name), file(\"*.read_count.csv\") into dada_read_count_table\n    script:\n        outname = name+\".dada2.csv\"\n        species_out = name+\".species_dada2.csv\"\n        read_count_name = name+\".read_count.csv\"\n         if (!params.pairedEnd || params.collapse)  {\n            \"\"\"\n            #!/usr/bin/env Rscript\n            \n            library(dada2)\n            library(plyr)\n\n            setwd(\".\")\n\n            sample.name = \"$name\"\n\n            fwd = \"${fq[0]}\"\n\n            silva_db = \"${params.silva_db}\"\n            silva_species_db = \"${params.silva_species_db}\"\n\n            # Dereplication            \n            derepFs <- derepFastq(fwd, verbose=TRUE)\n\n            # Learning error rate\n            err_forward_reads <- learnErrors(derepFs, multithread = ${task.cpus})\n\n            # Sample Inference\n            dadaFs <- dada(derepFs, err=err_forward_reads, multithread=${task.cpus})\n\n            # Make sequence table\n            seqtab <- makeSequenceTable(dadaFs)\n\n            # Remove chimeras\n            seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=${task.cpus}, verbose=TRUE)\n\n             # Track read numbers\n            getN <- function(x) sum(getUniques(x))\n            track <- cbind(getN(dadaFs), rowSums(seqtab.nochim))\n            colnames(track) <- c(\"denoisedF\", \"nonchim\")\n            rownames(track) <- sample.name\n            write.csv(track, \"${read_count_name}\")\n\n            # Assign taxonomy \n            taxa <- assignTaxonomy(seqtab.nochim, silva_db, tryRC = TRUE, taxLevels = c(\"Genus\",\"Species\"), multithread=${task.cpus})\n            write.csv(taxa, \"$outname\")\n\n            # Assign species \n            taxa2 = addSpecies(taxtab = taxa, refFasta = silva_species_db)\n\n            rownames(mergers) = mergers[,'sequence']\n            \n            # Write results to disk \n            print(taxa2)\n\n            spec = taxa2[,c(ncol(taxa2)-1,ncol(taxa2))]\n            colnames(spec) = c('Genus','Species')\n            spec_abund = merge(mergers, spec, by = 'row.names')[,c('Genus','Species','abundance')]\n\n\n            write.csv(spec_abund, \"$species_out\")\n            \"\"\"\n        } else {\n            \"\"\"\n            #!/usr/bin/env Rscript\n            \n            library(dada2)\n            library(plyr)\n\n            setwd(\".\")\n\n            sample.name = \"$name\"\n\n            fwd = \"${fq[0]}\"\n            rev = \"${fq[1]}\"\n\n            silva_db = \"${silva}\"\n            silva_species_db = \"${silva_species}\"\n\n            # Dereplication            \n            derepFs <- derepFastq(fwd, verbose=TRUE)\n            derepRs <- derepFastq(rev, verbose=TRUE)\n\n            # Learning error rate\n            err_forward_reads <- learnErrors(derepFs, multithread = ${task.cpus})\n            err_reverse_reads <- learnErrors(derepRs, multithread=${task.cpus})\n\n            # Sample Inference\n            dadaFs <- dada(derepFs, err=err_forward_reads, multithread=${task.cpus})\n            dadaRs <- dada(derepRs, err=err_reverse_reads, multithread=${task.cpus})\n\n            # Merge paired reads \n            mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)\n\n            # Make sequence table\n            seqtab <- makeSequenceTable(mergers)\n\n            # Remove chimeras\n            seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=${task.cpus}, verbose=TRUE)\n\n            # Track read numbers\n            getN <- function(x) sum(getUniques(x))\n            track <- cbind(getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))\n            colnames(track) <- c(\"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\n            rownames(track) <- sample.name\n            write.csv(track, \"${read_count_name}\")\n\n            # Assign taxonomy \n            taxa <- assignTaxonomy(seqtab.nochim, silva_db, tryRC = TRUE, taxLevels = c(\"Genus\",\"Species\"), multithread=${task.cpus})\n            write.csv(taxa, \"$outname\")\n\n            # Assign species \n            taxa2 = addSpecies(taxtab = taxa, refFasta = silva_species_db)\n\n            rownames(mergers) = mergers[,'sequence']\n            \n            # Write results to disk \n            print(taxa2)\n\n            spec = taxa2[,c(ncol(taxa2)-1,ncol(taxa2))]\n            colnames(spec) = c('Genus','Species')\n            spec_abund = merge(mergers, spec, by = 'row.names')[,c('Genus','Species','abundance')]\n\n\n            write.csv(spec_abund, \"$species_out\")\n            \"\"\"\n        }\n        \n}"], "list_proc": ["maxibor/dada2-nf/dada2"], "list_wf_names": ["maxibor/dada2-nf"]}, {"nb_reuse": 1, "tools": ["Bowtie", "MetaPhlAn"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["humann-nf"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess metaphlan {\n    tag \"$name\"\n\n    label 'intenso'\n\n    conda (params.enable_conda ? \"bioconda::metaphlan=3.0.13\" : null)\n    if (workflow.containerEngine == 'singularity') {\n        container \"https://depot.galaxyproject.org/singularity/metaphlan:3.0.13--pyhb7b1952_0\"\n    } else {\n        container \"quay.io/biocontainers/metaphlan:3.0.13--pyhb7b1952_0\"\n    }\n\n    publishDir \"${params.results}/metaphlan/$name\", mode: 'copy', pattern: '*.out'\n\n    input:\n        set val(name), file(reads) from trimmed_reads_mpa\n        val (mpa_db) from mpa_db_path_wait.ifEmpty([])\n\n    output:\n        set val(name), file('*.metaphlan.out') into metaphlan_out\n        path(\"*.bowtie2.log\") into bt2_log\n\n    script:\n        out = name+\".metaphlan.out\"\n        sam_out = name+\".sam\"\n        bt_out = name+\".bowtie2.log\"\n        if (params.pairedEnd && !params.collapse){\n            \"\"\"\n            bowtie2 --no-unal --very-sensitive -S $sam_out -x ${params.bt2db}/${params.mpa_db_name} -p ${task.cpus} -1 ${reads[0]} -2 ${reads[1]} 2> $bt_out\n            metaphlan $sam_out \\\\\n                      -o $out \\\\\n                      --bowtie2db ${params.bt2db} \\\\\n                      -x ${params.mpa_db_name} \\\\\n                      --input_type sam \\\\\n                      --nproc ${task.cpus} \\\\\n            \"\"\"    \n        } else {\n            \"\"\"\n            bowtie2 --no-unal --very-sensitive -S $sam_out -x ${params.bt2db}/${params.mpa_db_name} -p ${task.cpus} -U $reads 2> $bt_out\n            metaphlan $sam_out \\\\\n                      -o $out \\\\\n                      --bowtie2db ${params.bt2db} \\\\\n                      -x ${params.mpa_db_name} \\\\\n                      --input_type sam \\\\\n                      --nproc ${task.cpus} \\\\\n            \"\"\"  \n        } \n}"], "list_proc": ["maxibor/humann-nf/metaphlan"], "list_wf_names": ["maxibor/humann-nf"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["kraken-nf"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess kraken2 {\n    tag \"$name\"\n\n    label 'intenso'\n\n    errorStrategy 'ignore'\n\n    publishDir \"${params.results}/kraken\", mode: 'copy', pattern: '*.kraken2_minimizer_report'\n\n    input:\n        set val(name), path(reads) from trimmed_reads\n        path db from krakendb\n\n    output:\n        set val(name), file('*.kraken.out') into kraken_out\n        set val(name), file('*.kraken2_minimizer_report') into kraken_report\n\n    script:\n        out = name+\".kraken.out\"\n        kreport = name+\".kraken2_minimizer_report\"\n        if (params.pairedEnd && !params.collapse){\n            \"\"\"\n            kraken2 --db $db --threads ${task.cpus} --output $out --report-minimizer-data --report $kreport --paired ${reads[0]} ${reads[1]}\n            \"\"\"    \n        } else {\n            \"\"\"\n            kraken2 --db $db --threads ${task.cpus} --output $out --report-minimizer-data --report $kreport ${reads[0]}\n            \"\"\"\n        }\n        \n}"], "list_proc": ["maxibor/kraken-nf/kraken2"], "list_wf_names": ["maxibor/kraken-nf"]}, {"nb_reuse": 1, "tools": ["Bracken"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["kraken-nf"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": [" process bracken {\n        tag \"$prefix\"\n\n        label 'expresso'\n\n        publishDir \"${params.results}/bracken\", mode: 'copy', pattern: '*bracken_report'\n\n        input:\n        tuple val(prefix), path(kraken_report) from kraken_report_bracken\n        path db from bracken_db\n\n        output:\n        tuple prefix, path(\"*.bracken_report\") into bracken_report\n        tuple prefix, path(\"*.new_bracken_report\") into bracken_new_report\n\n        script:\n        bracken_report = prefix+\".bracken_report\"\n        bracken_new_report = prefix+\".new_bracken_report\"\n        \"\"\"\n        bracken \\\\\n            -d $db \\\\\n            -i $kraken_report\\\\\n            -o $bracken_report \\\\\n            -w $bracken_new_report \\\\\n            -r ${params.minimum_read_length} \\\\\n            -l ${params.bracken_level} \\\\\n            -t ${params.bracken_threshold} \\\\\n        \"\"\"\n    }"], "list_proc": ["maxibor/kraken-nf/bracken"], "list_wf_names": ["maxibor/kraken-nf"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["kraken-nf"], "list_contrib": ["maxibor"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n\n    publishDir \"${params.results}/multiqc\", mode: 'copy'\n\n    label 'ristretto'\n\n    input:\n        path('adapterRemoval/*') from adapter_removal_results_multiqc.collect().ifEmpty([])\n        path('kraken/*') from kraken_report_multiqc_file.collect().ifEmpty([])\n    output:\n        path('*multiqc_report.html')\n    script:\n        \"\"\"\n        multiqc .\n        \"\"\"\n\n}"], "list_proc": ["maxibor/kraken-nf/multiqc"], "list_wf_names": ["maxibor/kraken-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["madman"], "list_contrib": ["jfy133", "maxibor", "alexhbnr"], "nb_contrib": 3, "codes": ["process pydamage {\n    tag \"$name\"\n    \n    label 'process_high'\n    label 'process_ignore'\n\n    publishDir \"${params.outdir}/pydamage/$name\", mode: 'copy'\n\n    input:\n        tuple val(name), path(bam)\n    output:\n        tuple val(name), path(\"*.pydamage_results.csv\"), emit: csv\n        path \"${name}/plots\", optional: true, emit: plot\n    script:\n        output = name\n        if (params.pydamage_plot) {\n            plot = \"--plot\"\n        } else {\n            plot = \"\"\n        }\n        \"\"\"\n        samtools index $bam\n        pydamage --force -p ${task.cpus} -m ${params.minread} -c ${params.coverage} -w ${params.wlen} $plot -o $output $bam\n        mv ${name}/pydamage_results.csv ${name}.pydamage_results.csv\n        \"\"\"\n}"], "list_proc": ["maxibor/madman/pydamage"], "list_wf_names": ["maxibor/madman"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["madman"], "list_contrib": ["jfy133", "maxibor", "alexhbnr"], "nb_contrib": 3, "codes": ["process align_reads_to_contigs {\n    tag \"$name\"\n\n    label 'process_high'\n    label 'process_ignore'\n\n    publishDir \"${params.outdir}/alignment/${name}\", mode: 'copy'\n\n    input:\n        tuple val(name), path(contigs), path(reads)\n    output:\n        tuple val(name), file(\"*.sorted.bam\")\n    script:\n        outfile = name+\".sorted.bam\"\n        if (!params.single_end) {\n            \"\"\"\n            bowtie2-build --threads ${task.cpus} $contigs $name\n            bowtie2 -x $name -1 ${reads[0]} -2 ${reads[1]} --very-sensitive -N 1 --threads ${task.cpus} | samtools view -S -b -F 4 - | samtools sort - > $outfile\n            \"\"\"\n        } else {\n            \"\"\"\n            bowtie2-build --threads ${task.cpus} $contigs $name\n            bowtie2 -x $name -U $reads --very-sensitive -N 1 --threads ${task.cpus} | samtools view -S -b -F 4 - | samtools sort - > $outfile\n            \"\"\"\n        }\n}"], "list_proc": ["maxibor/madman/align_reads_to_contigs"], "list_wf_names": ["maxibor/madman"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["madman"], "list_contrib": ["jfy133", "maxibor", "alexhbnr"], "nb_contrib": 3, "codes": ["process fastp {\n    tag \"$name\"\n\n    label 'process_medium'\n    label 'process_mantory'\n    input:\n        tuple val(name), path(reads)\n\n    output:\n        tuple val(name), path(\"*.fq.gz\"), emit: trimmed_reads\n        path \"*.json\", emit: settings\n\n    script:\n        if (!params.single_end) {\n            out1 = name+\".pair1.trimmed.fq.gz\"\n            out2 = name+\".pair2.trimmed.fq.gz\"\n            \"\"\"\n            fastp --in1 ${reads[0]} --in2 ${reads[1]} --out1 $out1 --out2 $out2 -A -g --poly_g_min_len \"${params.complexity_filter_poly_g_min}\" -Q -L -w ${task.cpus} --json ${name}.json \n            \"\"\"\n        } else {\n            se_out = name+\".trimmed.fq.gz\"\n            \"\"\"\n            fastp --in1 ${reads[0]} --out1 $se_out -A -g --poly_g_min_len \"${params.complexity_filter_poly_g_min}\" -Q -L -w ${task.cpus} --json ${name}.json \n            \"\"\"\n        }\n}"], "list_proc": ["maxibor/madman/fastp"], "list_wf_names": ["maxibor/madman"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["madman"], "list_contrib": ["jfy133", "maxibor", "alexhbnr"], "nb_contrib": 3, "codes": ["process prokka {\n    tag \"$name\"\n\n    label 'process_high'\n    label 'process_ignore'\n\n    publishDir \"${params.outdir}/prokka\", mode: 'copy'\n\n    input:\n        tuple val(name), path(contigs)\n    output:\n        path(\"${name}\")\n    script:\n        \"\"\"\n        prokka --metagenome --cpus ${task.cpus} --outdir $name --prefix $name $contigs\n        \"\"\"\n}"], "list_proc": ["maxibor/madman/prokka"], "list_wf_names": ["maxibor/madman"]}, {"nb_reuse": 1, "tools": ["QUAST"], "nb_own": 1, "list_own": ["maxibor"], "nb_wf": 1, "list_wf": ["madman"], "list_contrib": ["jfy133", "maxibor", "alexhbnr"], "nb_contrib": 3, "codes": ["process quast {\n    tag \"$name\"\n\n    label 'process_medium' \n    label 'process_ignore'\n\n    publishDir \"${params.outdir}/quast/${outdir}\", mode: 'copy'\n\n    input:\n        tuple val(name), path(contigs)\n        val(step)\n    output:\n        path(\"*_quast_${step}\")\n    script:\n        outdir = name+\"_quast_${step}\"\n        \"\"\"\n        quast -o $outdir -t ${task.cpus} $contigs\n        \"\"\"\n}"], "list_proc": ["maxibor/madman/quast"], "list_wf_names": ["maxibor/madman"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess make_chr_size {\n    container 'mblanche/bwa-samtools'\n\n    input:\n    tuple val(id), path(bam) from bam_ch\n\n    output:\n    tuple val(id), path('chr_size.tsv') into chrSizes_ch\n\n    script:\n    \"\"\"\n    samtools view -H ${bam} | \\\n\tawk -v OFS='\\t' '/^@SQ/{split(\\$2,chr,\":\");split(\\$3,ln,\":\");print chr[2],ln[2]}' \\\n\t> chr_size.tsv\n    \"\"\"\n}"], "list_proc": ["mblanche/nextflow/make_chr_size"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess merge_bam {\n    tag \"_${id}\"\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    tuple id, path(bam_part) from bam_parts_ch\n\t.map {id, file ->\n\t    if ( id.contains(\"-rep\") ){\n\t\tdef key = id.replaceFirst(/(.*)-rep.*/,'$1')\n\t\treturn tuple(key, file)\n\t    } else {\n\t\treturn( tuple(id,file) )\n\t    }\n\t}\n\t.groupTuple()\n\n    output:\n    tuple id, path(\"*.bam\") into merged_bam_sort_ch\n\n    script:\n    bam_files = bam_part.sort()\n    if (bam_files.size() >1) {\n\t\"\"\"\n\tsamtools merge -@ ${task.cpus} ${id}_MB.bam ${bam_part}\n\t\"\"\"\n    } else {\n\t\"\"\"\n\tln -s ${bam_part} ${id}_MB.bam\n\t\"\"\"\n    }\n}"], "list_proc": ["mblanche/nextflow/merge_bam"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess bam_sort {\n    tag \"bam_sort_${id}\"\n    cpus 48\n    memory '150 GB'\n    container 'mblanche/bwa-samtools'\n    \n    publishDir \"${params.outDir}/bam\",\n\tmode: 'copy',\n\tpattern: \"${id}.bam\"\n        \n    input:\n    tuple id, path(bam) from merged_bam_sort_ch\n    \n    output:\n    tuple id, path(\"${id}.bam\"),path(\"${id}.bam.bai\") into bam_bigwig_ch, bam_chicago_ch\n\n    script:\n    \"\"\"\n    samtools sort -m 2G \\\n\t-@ ${task.cpus} \\\n\t-o ${id}.bam \\\n\t${bam} \n\n    samtools index -@${task.cpus} ${id}.bam\n    \"\"\"\n}"], "list_proc": ["mblanche/nextflow/bam_sort"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["IBS"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess get_bs_dataset {\n    echo true\n    label 'movers'\n    cpus 4\n    memory '2G'\n    container 'mblanche/basespace-cli'\n    \n    publishDir \"${HOME}/ebs/ref_push/${params.expDir}/${params.expName}/fastqs\",\n\tmode: 'copy'\n    \n    input:\n    tuple bs, id, filePath from bs_id_ch\n\t.splitCsv(header: true)\n\t.map {row -> tuple(row.biosample,row.Id,row.FilePath) }\n\n    output:\t\t\t\n    path \"*.fastq.gz\" into fastq_ch\n    \n    script:\n    \"\"\"\n    bs file download -i ${id} -o .\n    \n    new_fq=\\$(echo ${filePath}| perl -pe 's/.+?(_.+)/${bs}\\$1/')\n\n    if [[ ${filePath} != \\$new_fq ]]\n    then\n    mv ${filePath} \\$new_fq\n    fi\n    \"\"\"\n}"], "list_proc": ["mblanche/nextflow/get_bs_dataset"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["HINT"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess hint_pre {\n    tag \"_${id}\"\n    cpus 48\n    memory '48 GB'\n    container 'suwangbio/hint'\n    \n    input:\n    tuple val(id), file(R1s), file(R2s) from fastqs_ch\n    \t.mix(fastqDir_ch)\n\t.mix(genewiz_ch)\n\n    path index from bwa_index.first()\n    \n    output:\n    tuple id, path(\"*.bam\"), path(\"*.tsv\") into  pairtools_parse_ch\n    \n    script:\n    \"\"\"\n\n    hint pre -d <(zcat ${R1s|head -n 400000}),<(zcat ${R2s|head -n 400000}) \\\n\t-i ${index}/${params.genome} \\\n\t--refdir /path/to/refData/hg19 \\\n\t-g hg19 \\\n\t--informat fastq \\\n\t--outformat cooler \\\n\t-n test \\\n\t-o /path/to/outputdir \\\n\t--pairtoolspath /path/to/pairtools \\\n\t--samtoolspath /path/to/samtools \\\n\t--coolerpath /path/to/cooler\\\n    \"\"\"\n}"], "list_proc": ["mblanche/nextflow/hint_pre"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess bam_merge_lane {\n    tag \"_${id}\"\n    cpus 14\n    memory '50 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    tuple id, path(bam) from bam_lane_ch\n\t.groupTuple()\n    \n    output:\n    tuple id, path(\"*_ML.bam\") into bam_ML_ch, bam_sort4bw_ch\n\n    script:\n    if (bam.sort().size() >1) {\n\t\"\"\"\n\tsamtools merge -@ ${task.cpus} ${id}_ML.bam ${bam}\n\t\"\"\"\n    } else {\n\t\"\"\"\n\tln -sf ${bam} ${id}_ML.bam\n\t\"\"\"\n    }\n    \n}"], "list_proc": ["mblanche/nextflow/bam_merge_lane"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess bam_sort_4bw {\n    tag \"bam_sort_${id}\"\n    cpus 48\n    memory '150 GB'\n    container 'mblanche/bwa-samtools'\n    \n    publishDir \"${outDir}/bam\",\n\tmode: 'copy',\n\tpattern: \"${id}.bam\"\n        \n    input:\n    tuple id, path(bam) from bam_sort4bw_ch\n    \n    output:\n    tuple groupID, path(\"${id}.bam\"),path(\"*.bai\") into bam_bigwig_ch\n\n    script:\n    groupID = id.replaceFirst(/-rep.+/,\"\")\n    \"\"\"\n    samtools sort -m 2G \\\n\t-@ ${task.cpus} \\\n\t-o ${id}.bam \\\n\t${bam} \n\n    samtools index -@ ${task.cpus} ${id}.bam\n    \"\"\"\n}"], "list_proc": ["mblanche/nextflow/bam_sort_4bw"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess index_bam {\n    label 'index'\n    tag \"_${id}\"\n    cpus 48\n    memory '100 GB'\n    container 'mblanche/bwa-samtools'\n\n    input:\n    path(bam) from bam_ch\n    \n    output:\n    tuple id, path(bam), path(\"*.bai\") into bam_capStats_ch, bam_cleanBam_ch, bam_mapFile_ch\n\n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools index -@${task.cpus} ${bam}\n    \"\"\"\n}"], "list_proc": ["mblanche/nextflow/index_bam"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mblanche"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess merge_lane {\n    echo true\n    tag \"_${id}\"\n    cpus 14\n    memory '50 GB'\n    container 'mblanche/pairtools'\n    \n    input:\n    tuple id, path(sam) from bam_part_ch\n\t.map {id, file ->\n            def key = id.tokenize('_').get(0)\n            return tuple(key, file)\n\t}\n\t.groupTuple()\n\n    output:\n    tuple id, path(\"*.bam\"), path(\"*.bai\") into bam_ch\n\n    script:\n    if (sam.sort().size() >1) {\n\t\"\"\"\n\tsamtools merge -@ ${task.cpus} - ${sam} \\\n\t    |samtools sort -@ ${task.cpus} -o ${id}_MB.bam \\\n\t    && samtools index -@ ${task.cpus} ${id}_MB.bam\n\t\"\"\"\n    } else {\n\t\"\"\"\n\tln -s ${sam} ${id}_MB.bam\n\tsamtools index -@ ${task.cpus} ${id}_MB.bam\n\t\"\"\"\n    }\n    \n}"], "list_proc": ["mblanche/nextflow/merge_lane"], "list_wf_names": ["mblanche/nextflow"]}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 0, "list_wf": ["nf-core-gatkcohortcall"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess SID_VariantRecalibrator {\n\n\tlabel 'memory_max'\n    label 'cpus_1'\n\t\n\ttag \"${params.cohort}\"\n\n    input:\n\ttuple file(gvcf), file(gvcf_idx) from vcf_sid_ch\n    file genome from ch_fasta\n    file genomefai from ch_fastaFai\n    file genomedict from Channel.value(file(params.dict ))\n    file dbsnp_resource_vcf from Channel.value(file(params.dbsnp ))\n    file dbsnp_resource_vcf_idx from Channel.value(file(params.dbsnpIndex ))\n    file knownIndels_file from Channel.value(file(params.mills ))\n    file knownIndels_idx_file from Channel.value(file(params.millsIndex ))\n    file axiomPoly_resource_vcf from Channel.value(file(params.axiomPoly ))\n                                                                                 \n    \n\n\toutput:\n    tuple file(\"${params.cohort}.sid.recal\"),file(\"${params.cohort}.sid.recal.idx\"),file(\"${params.cohort}.sid.tranches\") into sid_recal_ch\n\n    script:\n\t\"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g  \\\n      VariantRecalibrator \\\n      -R ${genome} \\\n      -V ${gvcf} \\\n      --output ${params.cohort}.sid.recal \\\n      --tranches-file ${params.cohort}.sid.tranches \\\n      --trust-all-polymorphic \\\n      -an QD -an DP -an FS -an SOR -an ReadPosRankSum -an MQRankSum  \\\n      -mode INDEL \\\n      --max-gaussians 4 \\\n      --resource:mills,known=false,training=true,truth=true,prior=12 ${knownIndels_file} \\\n      --resource:dbsnp,known=true,training=false,truth=false,prior=2 ${dbsnp_resource_vcf} \\\n      # --resource:axiomPoly,known=false,training=true,truth=false,prior=10 ${axiomPoly_resource_vcf} \\\n\t  # Can add \"-an InbreedingCoeff\" if more than 10 samples\n\t\"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 0, "list_wf": ["nf-core-gatkcohortcall"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess SNV_VariantRecalibrator {\n\n\tlabel 'memory_max'\n    label 'cpus_1'\n\t\n\ttag \"${params.cohort}\"\n\n    input:\n\ttuple file(gvcf), file(gvcf_idx) from  vcf_snv_ch\n    file genome from ch_fasta\n    file genomefai from ch_fastaFai\n    file genomedict from Channel.value(file(params.dict ))\n    file dbsnp_resource_vcf from Channel.value(file(params.dbsnp ))\n    file dbsnp_resource_vcf_idx from Channel.value(file(params.dbsnpIndex ))\n    file hapmap_resource_vcf from Channel.value(file(params.hapmap ))\n    file hapmap_resource_vcfIndex from Channel.value(file(params.hapmapIndex ))\n    file omni_resource_vcf from Channel.value(file(params.omni))\n    file omni_resource_vcfIndex from Channel.value(file(params.omniIndex))\n    file one_thousand_genomes_resource_vcf from Channel.value(file(params.onekg))\n    file one_thousand_genomes_resource_vcfIndex from Channel.value(file(params.onekgIndex))\n\n\n\n\toutput:\n    tuple file(\"${params.cohort}.snv.recal\"),file(\"${params.cohort}.snv.recal.idx\"),file(\"${params.cohort}.snv.tranches\") into snv_recal_ch\n\n    script:\n\t\"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g  \\\n      VariantRecalibrator \\\n      -R ${genome} \\\n      -V ${gvcf} \\\n      --output ${params.cohort}.snv.recal \\\n      --tranches-file ${params.cohort}.snv.tranches \\\n      --trust-all-polymorphic \\\n      -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP  \\\n      -mode SNP \\\n      --max-gaussians 6 \\\n      --resource:hapmap,known=false,training=true,truth=true,prior=15 ${hapmap_resource_vcf} \\\n      --resource:omni,known=false,training=true,truth=true,prior=12 ${omni_resource_vcf} \\\n      --resource:1000G,known=false,training=true,truth=false,prior=10 ${one_thousand_genomes_resource_vcf} \\\n      --resource:dbsnp,known=true,training=false,truth=false,prior=7 ${dbsnp_resource_vcf}\n\t\n     # Can add \"-an InbreedingCoeff\" if more than 10 samples\n\t\"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 0, "list_wf": ["nf-core-gatkcohortcall"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess ApplyRecalibration {\n\n\tlabel 'memory_max'\n    label 'cpus_1'\n\t\n\ttag \"${params.cohort}\"\n\n\tpublishDir path:\"${params.outdir}/VariantRecalibration/\", mode: params.publishDirMode, pattern: '*.{vcf,idx}'\n\n    input:\n\ttuple file (input_vcf), file (input_vcf_idx) from vcf_recal_ch\n\ttuple file (indels_recalibration), file (indels_recalibration_idx), file (indels_tranches) from sid_recal_ch\n\ttuple file (snps_recalibration), file (snps_recalibration_idx), file (snps_tranches) from snv_recal_ch\n\n\toutput:\n    tuple file(\"${params.cohort}.recalibrated.vcf\"),file(\"${params.cohort}.recalibrated.vcf.idx\") into vcf_final_ch\n\n    script:\n\t\"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g  \\\n      ApplyVQSR \\\n      -O tmp.indel.recalibrated.vcf \\\n      -V ${input_vcf} \\\n      --recal-file ${indels_recalibration} \\\n      --tranches-file ${indels_tranches} \\\n      --truth-sensitivity-filter-level 99.0 \\\n      --exclude-filtered \\\n      --create-output-variant-index true \\\n      -mode INDEL\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g  \\\n      ApplyVQSR \\\n      -O ${params.cohort}.recalibrated.vcf \\\n      -V tmp.indel.recalibrated.vcf \\\n      --recal-file ${snps_recalibration} \\\n      --tranches-file ${snps_tranches} \\\n      --truth-sensitivity-filter-level 99.5 \\\n      --exclude-filtered \\\n      --create-output-variant-index true \\\n      -mode SNP\n\t\t\n\t\"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["BCFtools", "BWA", "SAMtools", "MultiQC", "FastQC", "QualiMap", "GATK", "VCFtools"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess GetSoftwareVersions {\n    publishDir path:\"${params.outdir}/pipeline_info\", mode: params.publishDirMode\n\n    output:\n        file 'software_versions_mqc.yaml' into yamlSoftwareVersion\n\n    when: !('versions' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools version > v_bcftools.txt 2>&1 || true\n    vcftools -h | head -n2 | tail -n1 > v_vcftools.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    fastqc --version > v_fastqc.txt 2>&1 || true\n    gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt  || true\n    # R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt\n    samtools --version &> v_samtools.txt 2>&1 || true\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/GetSoftwareVersions"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["QIIME", "BWA"], "nb_own": 2, "list_own": ["mbosio85", "nibscbioinformatics"], "nb_wf": 1, "list_wf": ["ngs_variant_calling", "nf-core-buggybarcodes"], "list_contrib": ["mbosio85", "MGordon09"], "nb_contrib": 2, "codes": ["\nprocess QIIME2_VSEARCH_JOINPAIRS {\n    tag \"$trim_qza\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.2\"\n\n    input:\n    path trim_qza                                     \n                                                                                  \n\n    output:\n    path \"*.qza\"                , emit: qza\n    path \"*.log\"                , emit: log\n    path \"*.version.txt\"        , emit: version\n\n    script:\n    def software      = getSoftwareName(task.process)\n    \"\"\"\n\n    qiime vsearch join-pairs \\\\\n        --i-demultiplexed-seqs $trim_qza \\\\\n        --o-joined-sequences demux_joined.qza \\\\\n        --p-threads $task.cpus \\\\\n        $options.args \\\\\n        > vsearch_joinpairs.log\n    echo \\$(qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\") > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BuildBWAindexes {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/BWAIndex/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.*\") into bwaIndexes\n\n  when: !(params.bwaIndex) && params.fasta && 'mapping' in step\n\n  script:\n  \"\"\"\n  bwa index ${fasta}\n  \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/BuildBWAindexes"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["QIIME", "GATK"], "nb_own": 2, "list_own": ["mbosio85", "nibscbioinformatics"], "nb_wf": 1, "list_wf": ["ngs_variant_calling", "nf-core-buggybarcodes"], "list_contrib": ["mbosio85", "MGordon09"], "nb_contrib": 2, "codes": ["\nprocess QIIME2_TOOLS_EXPORT {\n\ttag \"$qiime_qzv\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.2\"\n\n\tinput:\n\tpath qiime_qzv\n\n\toutput:\n\tpath \"export/*\"     , emit: folder\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software     = getSoftwareName(task.process)\n\t\"\"\"\n\tqiime tools export \\\\\n\t    --input-path $qiime_qzv \\\\\n\t\t--output-path export\n    echo \\$(qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\") > ${software}.version.txt\n\t\"\"\"\n}", "\nprocess BuildDict {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta.baseName}.dict\") into dictBuilt\n\n  when: !(params.dict) && params.fasta \n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${fasta} \\\n  --OUTPUT ${fasta.baseName}.dict\n  \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/BuildDict"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FastQC"], "nb_own": 2, "list_own": ["mbosio85", "nibscbioinformatics"], "nb_wf": 1, "list_wf": ["nf-core-conva", "ngs_variant_calling"], "list_contrib": ["mbosio85", "kaurravneet4123"], "nb_contrib": 2, "codes": ["\nprocess BuildFastaFai {\n  tag {fasta}\n\n  publishDir params.outdir, mode: params.publishDirMode,\n    saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n  input:\n    file(fasta) from ch_fasta\n\n  output:\n    file(\"${fasta}.fai\") into fastaFaiBuilt\n\n  when: !(params.fastaFai) && params.fasta \n\n  script:\n  \"\"\"\n  samtools faidx ${fasta}\n  \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["mbosio85/ngs_variant_calling/BuildFastaFai"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess BaseRecalibrator {\n    label 'memory_max'\n    label 'cpus_1'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fastaFai\n        file(knownIndels) from ch_knownIndels\n        file(knownIndelsIndex) from ch_knownIndelsIndex\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.recal.table\") into tableGatherBQSRReports\n\n    when: step == 'mapping'\n\n    script:\n    known = knownIndels.collect{\"--known-sites ${it}\"}.join(' ')\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${intervalBed.baseName}_${idSample}.recal.table \\\n        --tmp-dir /tmp \\\n        -R ${fasta} \\\n        -L ${intervalBed} \\\n        --known-sites ${dbsnp} \\\n        ${known} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/BaseRecalibrator"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.recal.bam\") into bamMergeBamRecal\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${intervalBed.baseName}_${idSample}.recal.bam \\\n        -L ${intervalBed} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/ApplyBQSR"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publishDirMode\n\n    input:\n        set idPatient, idSample, file(bam) from bamMergeBamRecal\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bai\") into bamRecal\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into (bamRecalBamQC, bamRecalSamToolsStats)\n        set idPatient, idSample, val(\"${idSample}.recal.bam\"), val(\"${idSample}.recal.bai\") into (bamRecalTSV, bamRecalSampleTSV)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    mv ${idSample}.recal.bam.bai ${idSample}.recal.bai\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/MergeBamRecal"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_1'\n\n    tag {idSample + \"-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), \n            file(\"${intervalBed.baseName}_${idSample}.bam\"),file(\"${intervalBed.baseName}_${idSample}.bai\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms1000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -bamout ${intervalBed.baseName}_${idSample}.bam \\\n        -ERC GVCF \\\n        --native-pair-hmm-threads ${task.cpus} \\\n        --use-jdk-deflater \\\n        --use-jdk-inflater \\\n\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/HaplotypeCaller"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess CNN_scoring {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_1'\n    label 'CNN'\n    container 'broadinstitute/gatk:4.1.4.1'\n\n\n    tag {idSample + \"CNN-\" + intervalBed.baseName}\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(vcf), file(bam), file(bamidx) from vcfGenotypeVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnpIndex\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fastaFai\n        \n\n    output:\n    set  idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.cnn_annotated.vcf.gz\") into vcfCNNvcfs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile -I ${vcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        CNNScoreVariants \\\n        -R ${fasta} \\\n        -V ${vcf} \\\n        -L ${intervalBed} \\\n        -O ${intervalBed.baseName}_${idSample}.cnn_annotated.vcf.gz \\\n        -I ${bam} \\\n        --tensor-type read_tensor \\\n        --transfer-batch-size 32 \\\n        --inference-batch-size 8 \\\n        --intra-op-threads 0 \\\n        --inter-op-threads ${task.cpus} \\\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/CNN_scoring"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mbosio85"], "nb_wf": 1, "list_wf": ["ngs_variant_calling"], "list_contrib": ["mbosio85"], "nb_contrib": 1, "codes": ["\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publishDirMode\n\n    input:\n        file (multiqcConfig) from Channel.value(params.multiqc_config ? file(params.multiqc_config) : \"\")\n        file (versions) from yamlSoftwareVersion\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from markDuplicatesReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        val custom_runName from custom_runName\n\n    output:\n        set file(\"${custom_runName}/*multiqc_report.html\"), file(\"${custom_runName}/*multiqc_data\") into multiQCOut\n\n    when: !('multiqc' in skipQC) || !('haplotypecaller' in tools)\n\n    script:\n    \"\"\"\n    mkdir -p ${custom_runName}\n    multiqc -f -v --outdir ${custom_runName} .\n    \"\"\"\n}"], "list_proc": ["mbosio85/ngs_variant_calling/MultiQC"], "list_wf_names": ["mbosio85/ngs_variant_calling"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["mbradyneeley"], "nb_wf": 1, "list_wf": ["RNASeq_Pipeline"], "list_contrib": ["mbradyneeley"], "nb_contrib": 1, "codes": ["\nprocess alignReads {\n\n    tag \"$sample_id\"\n\n    publishDir \"${params.bams}\", mode: 'copy', pattern: '*.bam'\n                            \n\n    input:\n                                                              \n                                                        \n        tuple val(sample_id), path(fqs) from trimmedFQs2\n\n    output:\n        path(\"*.sortedByCoord.out.bam\") into aligned_ch \n    \n    script:\n        fq1 = fqs[0]\n        fq2 = fqs[1]\n                                                                                                   \n        \"\"\"\n        module load gcc/8.3.0\n        STAR --genomeDir $params.genomeRef \\\n             --runThreadN 24 \\\n             --readFilesIn $fq1 $fq2 \\\n             --twopassMode Basic \\\n             --outSAMtype BAM SortedByCoordinate \\\n             --quantMode TranscriptomeSAM \\\n             --outWigType bedGraph \\\n             --outWigStrand Unstranded \\\n             --outFileNamePrefix ${sample_id} \\\n             --outTmpKeep All \\\n             --outStd Log \\\n             --limitBAMsortRAM 40000000000 \\\n             --readFilesCommand zcat\n        \"\"\"\n}"], "list_proc": ["mbradyneeley/RNASeq_Pipeline/alignReads"], "list_wf_names": ["mbradyneeley/RNASeq_Pipeline"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["mbradyneeley"], "nb_wf": 1, "list_wf": ["RNASeq_Pipeline"], "list_contrib": ["mbradyneeley"], "nb_contrib": 1, "codes": ["\nprocess countFeatures {\n\n    publishDir params.outDir\n\n    beforeScript \"echo ${alignedReads}\"\n\n    input:\n        path(bam) from aligned_ch1\n\n    output:\n                                                       \n        path '*.counts.txt' into counts_ch1\n\n    script:\n                        \n        \"\"\" \n        featureCounts -T 24 -p -s 2  --largestOverlap -a ${params.refGTF} -o ${bam.simpleName}.counts.txt $bam\n        \"\"\"\n}"], "list_proc": ["mbradyneeley/RNASeq_Pipeline/countFeatures"], "list_wf_names": ["mbradyneeley/RNASeq_Pipeline"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mbradyneeley"], "nb_wf": 1, "list_wf": ["RNASeq_Pipeline"], "list_contrib": ["mbradyneeley"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n\n    publishDir params.multi_qc\n\n    input:\n        val flag from done_ch\n\n    output:\n        file(\"*.html\") into mult_ch\n\n    script:\n        \"\"\"\n        ml multiqc\n        multiqc /uufs/chpc.utah.edu/common/home/u0854535/workflows/RNASeq/RNASeq_Pipeline/kingspeak/out /scratch/general/lustre/u0854535/1frost/results/bam\n        \"\"\"\n\n}"], "list_proc": ["mbradyneeley/RNASeq_Pipeline/multiqc"], "list_wf_names": ["mbradyneeley/RNASeq_Pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mchimenti"], "nb_wf": 1, "list_wf": ["ATAC_seq_nextflow"], "list_contrib": ["mchimenti"], "nb_contrib": 1, "codes": ["\nprocess bwa {\n\tpublishDir \"aligned_BAMs\"\n\tmaxForks 8\n\ttag \"Sample: $pair_id\"\n\n\tinput: \n\tset val(pair_id), file(read1), file(read2) from trim_read_ch\n\n\toutput:\n\tset pair_id, file(\"${pair_id}_sort.bam\") into bam_ch\t\n\n\tscript:\n\t\"\"\"\n\t$bwa mem -c 250 -t 1 -v 3 ${genome} ${read1} ${read2} | samtools view -u - | samtools sort -n -o ${pair_id}_sort.bam  \n\t\"\"\" \n\n}"], "list_proc": ["mchimenti/ATAC_seq_nextflow/bwa"], "list_wf_names": ["mchimenti/ATAC_seq_nextflow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mckennalab"], "nb_wf": 1, "list_wf": ["NextLineage"], "list_contrib": ["femiliani", "aaronmck"], "nb_contrib": 2, "codes": ["\nprocess FastqcReport {\n    beforeScript 'chmod o+rw .'\n    publishDir \"$results_path/02_fastqc\"\n\n    input:\n    set sampleId, reference, targets, read1, read2 from sample_table_fastqc\n    \n    output:\n    set sampleId, reference, targets, read1, read2, \"${sampleId}_fastqc\" into fastqc_results\n    \n    script:\n    \n    \"\"\"\n    mkdir ${sampleId}_fastqc\n    /dartfs/rc/lab/M/McKennaLab/projects/nextflow_lineage/tools/FastQC/fastqc -o ${sampleId}_fastqc ${read1} ${read2}\n    \"\"\"\n}"], "list_proc": ["mckennalab/NextLineage/FastqcReport"], "list_wf_names": ["mckennalab/NextLineage"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess RunFastQC {\n  tag {idPatient + \"-\" + idRun}\n\n  publishDir \"${directoryMap.fastQC}/${idRun}\", mode: 'link'\n\n  input:\n    set idPatient, status, idSample, idRun, file(fastqFile1), file(fastqFile2) from fastqFilesforFastQC\n\n  output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  when: step == 'mapping' && !params.noReports\n\n  script:\n  \"\"\"\n  fastqc -t 2 -q ${fastqFile1} ${fastqFile2}\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/RunFastQC"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess MapReads {\n  tag {idPatient + \"-\" + idRun}\n\n  input:\n    set idPatient, status, idSample, idRun, file(fastqFile1), file(fastqFile2) from fastqFiles\n    set file(genomeFile), file(bwaIndex) from Channel.value([referenceMap.genomeFile, referenceMap.bwaIndex])\n\n  output:\n    set idPatient, status, idSample, idRun, file(\"${idRun}.bam\") into mappedBam\n\n  when: step == 'mapping' && !params.onlyQC\n\n  script:\n  CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n  readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                              \n  extra = status == 1 ? \"-B 3\" : \"\"\n  \"\"\"\n  bwa mem -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M \\\n  ${genomeFile} ${fastqFile1} ${fastqFile2} | \\\n  samtools sort --threads ${task.cpus} -m 2G - > ${idRun}.bam\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/MapReads"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 2, "tools": ["SAMtools", "seqtk"], "nb_own": 2, "list_own": ["melnel000", "vpeddu"], "nb_wf": 2, "list_wf": ["ev-meta", "Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "vpeddu", "J35P312"], "nb_contrib": 13, "codes": ["\nprocess Collect_unassigned_results{ \n                                \npublishDir \"${params.OUTPUT}/Minimap2/${base}\", mode: 'symlink'\ncontainer \"vpeddu/nanopore_metagenomics\"\nbeforeScript 'chmod o+rw .'\ncpus 4\ninput: \n    tuple val(base), file(unclassified_fastq), file(depleted_fastq)\n    file filter_unassigned_reads\n    \noutput: \n    tuple val(\"${base}\"), file (\"${base}.merged.unclassified.fastq.gz\")\n\nscript:\n    \"\"\"\n    #!/bin/bash\n\n    #cat *.unclassified_reads.txt | sort | uniq > unique_unclassified_read_ids.txt\n    python3 ${filter_unassigned_reads}\n    /usr/local/miniconda/bin/seqtk subseq ${depleted_fastq} true_unassigned_reads.txt | gzip > ${base}.merged.unclassified.fastq.gz\n    \n    \"\"\"\n}", "\nprocess MergeBams {\n  tag {idPatient + \"-\" + idSample}\n\n  input:\n    set idPatient, status, idSample, idRun, file(bam) from groupedBam\n\n  output:\n    set idPatient, status, idSample, file(\"${idSample}.bam\") into mergedBam\n\n  when: step == 'mapping' && !params.onlyQC\n\n  script:\n  \"\"\"\n  samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n  \"\"\"\n}"], "list_proc": ["vpeddu/ev-meta/Collect_unassigned_results", "melnel000/Sarek_CBIO/MergeBams"], "list_wf_names": ["vpeddu/ev-meta", "melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["MarkDuplicates (IP)", "GATK"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess MarkDuplicates {\n  tag {idPatient + \"-\" + idSample}\n\n  publishDir params.outDir, mode: 'link',\n    saveAs: {\n      if (it == \"${bam}.metrics\") \"${directoryMap.markDuplicatesQC}/${it}\"\n      else \"${directoryMap.duplicateMarked}/${it}\"\n    }\n\n  input:\n    set idPatient, status, idSample, file(bam) from mergedBam\n\n  output:\n    set idPatient, file(\"${idSample}_${status}.md.bam\"), file(\"${idSample}_${status}.md.bai\") into duplicateMarkedBams\n    set idPatient, status, idSample, val(\"${idSample}_${status}.md.bam\"), val(\"${idSample}_${status}.md.bai\") into markDuplicatesTSV\n    file (\"${bam}.metrics\") into markDuplicatesReport\n\n  when: step == 'mapping' && !params.onlyQC\n\n  script:\n  \"\"\"\n  gatk --java-options -Xmx${task.memory.toGiga()}g \\\n  MarkDuplicates \\\n  --MAX_RECORDS_IN_RAM 50000 \\\n  --INPUT ${bam} \\\n  --METRICS_FILE ${bam}.metrics \\\n  --TMP_DIR . \\\n  --ASSUME_SORT_ORDER coordinate \\\n  --CREATE_INDEX true \\\n  --OUTPUT ${idSample}_${status}.md.bam\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/MarkDuplicates"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess CreateRecalibrationTable {\n  tag {idPatient + \"-\" + idSample}\n\n  publishDir directoryMap.duplicateMarked, mode: 'link', overwrite: false\n\n  input:\n    set idPatient, status, idSample, file(bam), file(bai) from mdBam                \n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex), file(knownIndels), file(knownIndelsIndex), file(intervals) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.dbsnp,\n      referenceMap.dbsnpIndex,\n      referenceMap.knownIndels,\n      referenceMap.knownIndelsIndex,\n      referenceMap.intervals,\n    ])\n\n  output:\n    set idPatient, status, idSample, file(\"${idSample}.recal.table\") into recalibrationTable\n    set idPatient, status, idSample, val(\"${idSample}_${status}.md.bam\"), val(\"${idSample}_${status}.md.bai\"), val(\"${idSample}.recal.table\") into recalibrationTableTSV\n\n  when: ( step == 'mapping' ) && !params.onlyQC\n\n  script:\n  known = knownIndels.collect{ \"--known-sites ${it}\" }.join(' ')\n  \"\"\"\n  gatk --java-options -Xmx${task.memory.toGiga()}g \\\n  BaseRecalibrator \\\n  --input ${bam} \\\n  --output ${idSample}.recal.table \\\n\t--TMP_DIR /tmp \\\n  -R ${genomeFile} \\\n  -L ${intervals} \\\n  --known-sites ${dbsnp} \\\n  ${known} \\\n  --verbosity INFO\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/CreateRecalibrationTable"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess RecalibrateBam {\n  tag {idPatient + \"-\" + idSample}\n\n  publishDir directoryMap.recalibrated, mode: 'link'\n\n  input:\n    set idPatient, status, idSample, file(bam), file(bai), file(recalibrationReport) from recalibrationTable\n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(intervals) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.intervals,\n    ])\n\n  output:\n    set idPatient, status, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bai\") into recalibratedBam, recalibratedBamForStats\n    set idPatient, status, idSample, val(\"${idSample}.recal.bam\"), val(\"${idSample}.recal.bai\") into recalibratedBamTSV\n\n                                                                             \n                                 \n  when: !params.onlyQC\n\n  script:\n  \"\"\"\n  gatk --java-options -Xmx${task.memory.toGiga()}g \\\n  ApplyBQSR \\\n  -R ${genomeFile} \\\n  --input ${bam} \\\n  --output ${idSample}.recal.bam \\\n  -L ${intervals} \\\n\t--create-output-bam-index true \\\n\t--bqsr-recal-file ${recalibrationReport}\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/RecalibrateBam"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess RunHaplotypecaller {\n  tag {idSample + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSample, file(bam), file(bai), file(intervalBed), recalTable from bamsForHC                                              \n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.dbsnp,\n      referenceMap.dbsnpIndex\n    ])\n\n  output:\n    set val(\"gvcf-hc\"), idPatient, idSample, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into hcGenomicVCF\n    set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into vcfsToGenotype\n\n  when: 'haplotypecaller' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n      HaplotypeCaller \\\n      -R ${genomeFile} \\\n      -I ${bam} \\\n      -L ${intervalBed} \\\n      --dbsnp ${dbsnp} \\\n      -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n      --emit-ref-confidence GVCF\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/RunHaplotypecaller"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess RunGenotypeGVCFs {\n  tag {idSample + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSample, file(intervalBed), file(gvcf) from vcfsToGenotype\n    set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([\n      referenceMap.genomeFile,\n      referenceMap.genomeIndex,\n      referenceMap.genomeDict,\n      referenceMap.dbsnp,\n      referenceMap.dbsnpIndex\n    ])\n\n  output:\n    set val(\"haplotypecaller\"), idPatient, idSample, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into hcGenotypedVCF\n\n  when: 'haplotypecaller' in tools && !params.onlyQC\n\n  script:\n                                                                                 \n  \"\"\"\n  gatk IndexFeatureFile -F ${gvcf}\n\n  gatk --java-options -Xmx${task.memory.toGiga()}g \\\n  GenotypeGVCFs \\\n  -R ${genomeFile} \\\n  -L ${intervalBed} \\\n  --dbsnp ${dbsnp} \\\n  -V ${gvcf} \\\n  -O ${intervalBed.baseName}_${idSample}.vcf\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/RunGenotypeGVCFs"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["BCFtools", "BWA", "SAMtools", "FreeBayes", "MultiQC", "FastQC", "QualiMap", "GATK", "VCFtools"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess GetVersionAll {\n  publishDir directoryMap.multiQC, mode: 'link'\n\n  input:\n    file(versions) from Channel.fromPath(\"${directoryMap.version}/*\").collect().ifEmpty(file (\"empty\"))\n\n  output:\n    file (\"tool_versions_mqc.yaml\") into versionsForMultiQC\n\n  when: !params.noReports\n\n  script:\n  \"\"\"\n  bcftools version > v_bcftools.txt 2>&1 || true\n  bwa &> v_bwa.txt 2>&1 || true\n  configManta.py --version > v_manta.txt 2>&1 || true\n  configureStrelkaGermlineWorkflow.py --version > v_strelka.txt 2>&1 || true\n  echo \"${workflow.manifest.version}\" &> v_sarek.txt 2>&1 || true\n  echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n  fastqc -v > v_fastqc.txt 2>&1 || true\n  freebayes --version > v_freebayes.txt 2>&1 || true\n  gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true\n  multiqc --version &> v_multiqc.txt 2>&1 || true\n  qualimap --version &> v_qualimap.txt 2>&1 || true\n  samtools --version &> v_samtools.txt 2>&1 || true\n  vcftools --version &> v_vcftools.txt 2>&1 || true\n\n  scrape_tool_versions.py &> tool_versions_mqc.yaml\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/GetVersionAll"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess RunMultiQC {\n  publishDir directoryMap.multiQC, mode: 'link'\n\n  input:\n    file (multiqcConfig) from createMultiQCconfig()\n    file (reports) from reportsForMultiQC\n    file (versions) from versionsForMultiQC\n\n  output:\n    set file(\"*multiqc_report.html\"), file(\"*multiqc_data\") into multiQCReport\n\n  when: !params.noReports\n\n  script:\n  \"\"\"\n  multiqc -f -v .\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/RunMultiQC"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess BuildBWAindexes {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaForBWA\n\n  output:\n    file(\"*.{amb,ann,bwt,pac,sa}\") into bwaIndexes\n\n  script:\n\n  \"\"\"\n  bwa index ${f_reference}\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/BuildBWAindexes"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess BuildReferenceIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaReference\n\n  output:\n    file(\"*.dict\") into ch_referenceIndex\n\n  script:\n  \"\"\"\n  gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n  CreateSequenceDictionary \\\n  --REFERENCE ${f_reference} \\\n  --OUTPUT ${f_reference.baseName}.dict\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/BuildReferenceIndex"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess BuildSAMToolsIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_fastaForSAMTools\n\n  output:\n    file(\"*.fai\") into ch_samtoolsIndex\n\n  script:\n  \"\"\"\n  samtools faidx ${f_reference}\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/BuildSAMToolsIndex"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["IGVtools"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess BuildVCFIndex {\n  tag {f_reference}\n\n  publishDir params.outDir, mode: 'link'\n\n  input:\n    file(f_reference) from ch_vcfFile\n\n  output:\n    file(\"${f_reference}.idx\") into ch_vcfIndex\n\n  script:\n  \"\"\"\n  igvtools index ${f_reference}\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/BuildVCFIndex"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["FreeBayes"], "nb_own": 1, "list_own": ["melnel000"], "nb_wf": 1, "list_wf": ["Sarek_CBIO"], "list_contrib": ["Sebastian-D", "arontommi", "maxulysse", "alneberg", "ewels", "waffle-iron", "malinlarsson", "szilvajuhos", "marcelm", "pditommaso", "pallolason", "J35P312"], "nb_contrib": 12, "codes": ["\nprocess RunFreeBayes {\n  tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n\n  input:\n    set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFFB\n    file(genomeFile) from Channel.value(referenceMap.genomeFile)\n    file(genomeIndex) from Channel.value(referenceMap.genomeIndex)\n\n  output:\n    set val(\"freebayes\"), idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into freebayesOutput\n\n  when: 'freebayes' in tools && !params.onlyQC\n\n  script:\n  \"\"\"\n  freebayes \\\n    -f ${genomeFile} \\\n    --pooled-continuous \\\n    --pooled-discrete \\\n    --genotype-qualities \\\n    --report-genotype-likelihood-max \\\n    --allele-balance-priors-off \\\n    --min-alternate-fraction 0.03 \\\n    --min-repeat-entropy 1 \\\n    --min-alternate-count 2 \\\n    -t ${intervalBed} \\\n    ${bamTumor} \\\n    ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n  \"\"\"\n}"], "list_proc": ["melnel000/Sarek_CBIO/RunFreeBayes"], "list_wf_names": ["melnel000/Sarek_CBIO"]}, {"nb_reuse": 1, "tools": ["Centrifuge"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess centrifuge_fastqs {\n  container 'quay.io/biocontainers/centrifuge:1.0.4_beta--h9a82719_6'\n\n  conda 'bioconda::centrifuge=1.0.4_beta'\n\n  tag \"${sampleId}\"\n  \n  cpus params.cores\n  \n  publishDir \"$params.outdir/centrifuge/$sampleId\", mode: 'copy', pattern: \"*\"\n  \n  when: params.tax\n\n  input: set sampleId, file(fastq) from fastq_tax_ch\n  \n  output: tuple(sampleId, file(\"${sampleId}*\")) into centrifuge_reports_ch\n  \n  script:\n  \"\"\"\n  centrifuge -p ${task.cpus} -x $params.cendb/$params.cenin -U $fastq -S ${sampleId}-out.txt\n  centrifuge-kreport -x $params.cendb/$params.cenin ${sampleId}-out.txt > ${sampleId}-kreport.txt\n  \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/centrifuge_fastqs"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess assembly_with_flye {\n  container 'quay.io/biocontainers/flye:2.8.3--py27h6a42192_1' \n\n  conda 'bioconda::flye=2.8.3'\n\n  tag \"${sampleId}\"\n\n  cpus params.cores\n\n  publishDir \"$params.outdir/assembly/$sampleId\", mode: 'copy', pattern: \"*\"\n\n  when: params.assembly\n\n  input: set sampleId, file(fastq) from fastq_assembly_ch\n  \n  output: \n  tuple(sampleId, file(\"assembly.fasta\")) into fasta_ch\n  tuple(sampleId, file(\"assembly_graph.gfa\")) into gfa_ch\n  tuple(sampleId, file(\"assembly_info.txt\")) into assembly_info_ch\n  \n  script:\n  if (params.meta)\n    \"\"\"\n    flye --nano-raw $fastq -t ${task.cpus} --meta -o . \n    \"\"\"\n  else\n    \"\"\"\n    flye --nano-raw $fastq -t ${task.cpus} -o .\n    \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/assembly_with_flye"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess mapping_reads_against_assembly {\n  container 'quay.io/biocontainers/minimap2:2.20--h5bf99c6_0'\n  \n  conda 'bioconda::minimap2=2.20'\n  \n  tag \"${sampleId}\"\n\n  cpus params.cores\n\n  publishDir \"$params.outdir/mapping/$sampleId\", mode: 'copy', pattern: \"*\"\n  \n  when: (params.map && params.assembly)\n  \n  input: \n  set sampleId, file(fastq) from fastq_mapping_assembly_ch\n  set sampleId, file(fasta) from assemblies_map_ch\n  \n  output: \n  tuple(sampleId, file(\"${sampleId}-assembly.bam\")) into bam_assembly_ch\n  \n  script:\n  \"\"\"\n  minimap2 -t ${task.cpus} -ax map-ont $fasta $fastq | samtools sort -@ ${task.cpus} -o ${sampleId}-assembly.bam\n  \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/mapping_reads_against_assembly"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess bam_to_bed{\n  container 'quay.io/biocontainers/bedtools:2.30.0--h7d7f7ad_1'\n\n  conda 'bioconda::bedtools=2.30.0'\n  \n  tag \"${sampleId}\"\n\n  publishDir \"$params.outdir/coverage/$sampleId\", mode: 'copy', pattern: \"*\"\n\n  when: (params.map && params.reference)\n  \n  input: set sampleId, file(bam) from bed_bam_ch\n\n  output: tuple(sampleId, file(\"${bam.baseName}.tsv\")) into cov_tsv_ch\n  \n  script:\n  \"\"\"\n  bedtools genomecov -d -ibam $bam > ${bam.baseName}.tsv\n  \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/bam_to_bed"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["Rgin"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess rgi {\n  container 'docker-daemon:metagenlab/rgi:5.2.0-3.1.2'\n\n  conda 'bioconda::rgi=5.2.0'\n  \n  tag \"${sampleId}\"\n\n  publishDir \"$params.outdir/resistance\", mode: 'copy', pattern: \"*\"\n\n  cpus params.cores\n  \n  when params.res\n\n  input: set sampleId, file(fasta) from assemblies_resistance_ch\n  \n  output: \n  tuple(sampleId, file(\"${sampleId}.txt\")) into rgi_txt_ch\n  tuple(sampleId, file(\"${sampleId}.json\")) into rgi_json_ch\n  \n  script:\n  \"\"\"\n  rgi load -i $params.card/card.json --card_annotation $params.card/card_database_*.fasta \\\n  --wildcard_annotation $params.card/wildcard_database_*.fasta \\\n  --wildcard_index $params.card/wildcard/index-for-model-sequences.txt\n  rgi main -i $fasta -o ${sampleId} -t contig -a BLAST -n ${task.cpus} --split_prodigal_jobs --clean\n  \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/rgi"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["Rgin"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess rgi_heatmap {\n  container 'docker-daemon:metagenlab/rgi:5.2.0-3.1.2'\n\n  conda 'bioconda::rgi=5.2.0'\n\n  publishDir \"$params.outdir/resistance\", mode: 'copy', pattern: \"*\"\n\n  input: file json from rgi_json_ch.collect()\n  \n  output: file \"heatmap-all.png\" into rgi_heatmap_ch\n  \n  script:\n  \"\"\"\n  rgi heatmap -i . -o heatmap-all.png -cat drug_class -clus samples\n  \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/rgi_heatmap"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["metagenlab"], "nb_wf": 1, "list_wf": ["nanopore"], "list_contrib": ["idfarbanecha"], "nb_contrib": 1, "codes": ["\nprocess prokka_annotation {\n  container 'quay.io/biocontainers/prokka:1.14.6--pl526_0'\n  \n  conda 'bioconda::prokka=1.14.6'\n  \n  tag \"${sampleId}\"\n\n  publishDir \"$params.outdir/annotation/$sampleId\", mode: 'copy', pattern: \"*\"\n\n  cpus params.cores\n  \n  when params.res\n\n  input: set sampleId, file(fasta) from assemblies_prokka_ch\n  \n  output: tuple(sampleId, file(\"PROKKA/${sampleId}\")) into prokka_out_ch\n  \n  script:\n  \"\"\"\n  prokka --outdir PROKKA --prefix ${sampleId} $fasta\n  \"\"\"\n}"], "list_proc": ["metagenlab/nanopore/prokka_annotation"], "list_wf_names": ["metagenlab/nanopore"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["busco"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess busco {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/busco\" , mode: 'copy'\n\n    input:\n    tuple val(id), path(genome)\n    path (lineage)\n    path busco_db, stageAs: 'busco_downloads'\n\n    output:\n    path \"${id}/logs\"\n    path \"${id}/exit_info.txt\", optional: true\n    path \"${id}/short_summary.*\", optional: true\n    path \"${id}/short_summary.specific.*.${id}.txt\", optional: true, emit: summary\n    \n\n    script:\n    if( params.lineage == 'auto' ) {\n        param_auto_lineage = '--auto-lineage'\n        param_lineage_dataset = ''\n    }\n    else if ( params.lineage == 'auto-prok' ) {\n        param_auto_lineage = '--auto-lineage-prok'\n        param_lineage_dataset = ''\n    }\n    else if ( params.lineage == 'auto-euk' ) {\n        param_auto_lineage = '--auto-lineage-euk'\n        param_lineage_dataset = ''\n    }\n    else {\n        param_auto_lineage = ''\n        param_lineage_dataset = \"-l ${params.lineage}\"\n    }\n\n    if( params.busco_db == 'none' ) {\n        param_offline = ''\n    }\n    else {\n        param_offline = '--offline'\n    }\n\n    \"\"\"\n    # Avoid the \"FileExistsError\" in inline mode\n    if [ \"${param_offline}\" = \"\" ]; then\n        rm busco_downloads\n    fi\n\n    set +e\n    busco \\\n        -i ${genome} \\\n        -o ${id} \\\n        -m genome \\\n        ${param_lineage_dataset} \\\n        ${param_auto_lineage} \\\n        ${param_offline} \\\n        --cpu ${task.cpus}\n    BUSCO_EXIT=\\$?\n      \n    BUSCO_LOG=${id}/logs/busco.log\n    if [ \"\\$BUSCO_EXIT\" -eq 1 ] && [ -f \\$BUSCO_LOG ]; then\n        \n        EXIT_MSG='SystemExit: Augustus did not recognize any genes'\n        grep -q \"\\$EXIT_MSG\" \\$BUSCO_LOG\n        if [ \"\\$?\" -eq 0 ]; then\n            echo \"Augustus did not recognize any genes.\" >> ${id}/exit_info.txt\n            exit 0\n        fi\n\n        EXIT_MSG='SystemExit: Placements failed'\n        grep -q \"\\$EXIT_MSG\" \\$BUSCO_LOG\n        if [ \"\\$?\" -eq 0 ]; then\n            echo \"Placements failed.\" >> ${id}/exit_info.txt\n            exit 0\n        fi\n\n    fi\n\n    exit \\$BUSCO_EXIT\n    \"\"\"\n}"], "list_proc": ["metashot/busco/busco"], "list_wf_names": ["metashot/busco"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["kraken2"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess kraken2 {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/kraken2/\" , mode: 'copy'\n\n    input:\n    tuple val(id), path(reads)\n    path kraken2_db\n    \n    output:\n    tuple val(id), path(\"${id}.kraken2.report\"), emit: report\n    tuple val(id), path(\"${id}.kraken2.mpa\"), emit: mpa\n\n    script:\n    input = params.single_end ? \"\\\"$reads\\\"\" :  \"--paired \\\"${reads[0]}\\\" \\\"${reads[1]}\\\"\"\n    report_zero_counts = params.report_zero_counts ? \"--report-zero-counts\" : \"\"\n    \"\"\"\n    kraken2 \\\n        --db $kraken2_db \\\n        --threads ${task.cpus} \\\n        $report_zero_counts \\\n        --report ${id}.kraken2.report \\\n        $input > /dev/null\n\n    kreport2mpa.py \\\n        -r ${id}.kraken2.report \\\n        -o ${id}.kraken2.mpa \\\n        --display-header\n    \"\"\"\n}"], "list_proc": ["metashot/kraken2/kraken2"], "list_wf_names": ["metashot/kraken2"]}, {"nb_reuse": 1, "tools": ["Bracken"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["kraken2"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess bracken {\n    tag \"${level}-${id}\"\n\n    publishDir \"${params.outdir}/bracken/${id}\" , mode: 'copy'\n\n    input:\n    tuple val(level), val(id), path(kraken2_report)\n    path kraken2_db\n\n    output:\n    tuple val(level), path(\"${id}.${level}.bracken\"), emit: bracken\n    tuple val(level), path(\"${id}.${level}.bracken.report\"), emit: report\n    tuple val(level), path(\"${id}.${level}.bracken.mpa\"), emit: mpa\n    \n    script:\n    \"\"\"\n    bracken \\\n        -d $kraken2_db \\\n        -r ${params.read_len} \\\n        -l $level \\\n        -i $kraken2_report \\\n        -o ${id}.${level}.bracken \\\n        -w ${id}.${level}.bracken.report\n\n    kreport2mpa.py \\\n        -r ${id}.${level}.bracken.report \\\n        -o ${id}.${level}.bracken.mpa \\\n        --display-header\n    \"\"\"\n}"], "list_proc": ["metashot/kraken2/bracken"], "list_wf_names": ["metashot/kraken2"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["mag-illumina"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess metabat2 {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}\" , mode: 'copy' ,\n        pattern: \"{bins, unbinned}/*.fa\"\n\n    publishDir \"${params.outdir}\" , mode: 'copy' ,\n        saveAs: {filename ->\n            if (filename == \"metabat2_depth.txt\") \"metabat2/${id}/depth.txt\"\n            else if (filename == \"metabat2.log\") \"metabat2/${id}/log.txt\"\n        }\n\n    input:\n    tuple val(id), path(reads), path(scaffolds)\n    \n    output:\n    path \"bins/*.fa\", optional: true\n    path \"unbinned/*.fa\", optional: true\n    path \"metabat2_depth.txt\"\n    path \"metabat2.log\"\n    \n    script:\n    input = params.single_end ? \"-U \\\"$reads\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    thread_mem_GB = (Math.floor(task.memory.toGiga() / task.cpus) - 2) as int\n    thread_mem_GB_1 = thread_mem_GB > 1 ? thread_mem_GB : 1\n    \"\"\"\n    mkdir -p bowtie_db\n\n    bowtie2-build \\\n        --threads ${task.cpus} \\\n        $scaffolds \\\n        bowtie_db/scaffolds\n\n    bowtie2 \\\n        -x bowtie_db/scaffolds \\\n        $input \\\n        --threads ${task.cpus} \\\n        -S map.sam\n\n    samtools sort \\\n        -@ ${task.cpus} \\\n        -m ${thread_mem_GB_1}G \\\n        -o map.bam \\\n        map.sam\n\n    rm -f map.sam\n\n    jgi_summarize_bam_contig_depths \\\n        --outputDepth metabat2_depth.txt \\\n        map.bam\n\n    rm -f map.bam\n\n    mkdir -p bins\n    \n    metabat2 \\\n        -i $scaffolds \\\n        -a metabat2_depth.txt \\\n        -o bins/${id}.bin \\\n        -v \\\n        -t ${task.cpus} \\\n        --seed 1 \\\n        --unbinned \\\n        -m ${params.min_contig_size} &> metabat2.log\n\n    mkdir -p unbinned\n\n    if [ -f bins/${id}.bin.unbinned.fa ]; then\n        mv bins/${id}.bin.unbinned.fa unbinned/${id}.unbinned.fa\n    fi\n    \n    if [ -f bins/${id}.bin.lowDepth.fa ]; then\n        mv bins/${id}.bin.lowDepth.fa unbinned/${id}.lowDepth.fa\n    fi\n    \n    if [ -f bins/${id}.bin.tooShort.fa ]; then\n        mv bins/${id}.bin.tooShort.fa unbinned/${id}.tooShort.fa\n    fi\n    \"\"\"\n}"], "list_proc": ["metashot/mag-illumina/metabat2"], "list_wf_names": ["metashot/mag-illumina"]}, {"nb_reuse": 1, "tools": ["MEGAHIT"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["mag-illumina"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess megahit {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/megahit\" , mode: 'copy' ,\n        enabled: params.save_assembler_output ,\n        pattern: \"${id}/*\"\n\n    publishDir \"${params.outdir}/scaffolds\" , mode: 'copy' ,\n        pattern: \"${id}.fa\"\n\n    input:\n    tuple val(id), path(reads)\n\n    output:\n    tuple val(id), path(\"${id}.fa\"), emit: scaffolds\n    path \"${id}/*\", optional: true\n\n\n    script:\n    task_memory_GB = Math.floor(0.8 * task.memory.toGiga()) as int\n    param_megahit_k = params.megahit_k == 'default' ? \"\" : \"--k-list ${params.megahit_k}\"\n    input = params.single_end ? \"-r \\\"$reads\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    param_save_assembler_output = params.save_assembler_output ? \"Y\" : \"N\"\n    \"\"\"\n    megahit \\\n        $input \\\n        -t ${task.cpus} \\\n        ${param_megahit_k} \\\n        --memory $task_memory_GB \\\n        -o ${id}\n    cp ${id}/final.contigs.fa ${id}.fa\n\n    if [ \"${param_save_assembler_output}\" = \"N\" ]; then\n        rm -rf ${id}\n    fi\n    \"\"\"\n}"], "list_proc": ["metashot/mag-illumina/megahit"], "list_wf_names": ["metashot/mag-illumina"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["prok-annotate"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess prokka {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/prokka\" , mode: 'copy'\n\n    input:\n    tuple val(id), path(genome)\n\n    output:\n    path \"${id}/*\"\n\n    script:\n    param_metagenome = params.metagenome ? '--metagenome' : ''\n    \"\"\"\n    prokka \\\n        $genome \\\n        --outdir ${id} \\\n        --prefix ${id} \\\n        --kingdom ${params.prokka_kingdom} \\\n        $param_metagenome \\\n        --cpus ${task.cpus} \\\n        --centre X --compliant\n    \"\"\"\n}"], "list_proc": ["metashot/prok-annotate/prokka"], "list_wf_names": ["metashot/prok-annotate"]}, {"nb_reuse": 1, "tools": ["Roary"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["prok-pan"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess roary {\n    publishDir \"${params.outdir}\" , mode: 'copy'\n\n    input:\n    path(gffs)\n    \n    output:\n    path \"roary/*\"\n    path \"roary/core_gene_alignment.aln\", emit: aln\n    path \"roary/gene_presence_absence.csv\", emit: gene_pa\n    path \"roary/accessory_binary_genes.fa.newick\", emit: accessory_tree\n   \n    script:\n    if( params.mode == 'Viruses') {\n        param_translation = '-t 1'\n    }\n    else if ( params.mode == 'Mitochondria' ) {\n        param_translation = '-t 4'\n    }\n    else {\n        param_translation = '-t 11'\n    }\n\n    param_dont_split_paralogs = params.dont_split_paralogs ? '-s' : ''\n    \"\"\"\n    roary \\\n        -e -n \\\n        ${param_translation} \\\n        -p ${task.cpus} \\\n        -f roary \\\n        -i ${params.min_ident} \\\n        -cd ${params.min_prev_core} \\\n        ${param_dont_split_paralogs} \\\n        ${gffs}\n    \"\"\"\n}"], "list_proc": ["metashot/prok-pan/roary"], "list_wf_names": ["metashot/prok-pan"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["prok-pan"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess prokka {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/prokka\" , mode: 'copy'\n\n    input:\n    tuple val(id), path(genome)\n    path (proteins)\n\n    output:\n    path \"${id}/*\"\n    tuple val(id), path (\"${id}/${id}.gff\"), emit: gff\n\n    script:\n    param_proteins = params.proteins != 'none' ? \"--proteins $proteins\" : ''\n    \"\"\"\n    prokka \\\n        $genome \\\n        --outdir ${id} \\\n        --prefix ${id} \\\n        --kingdom ${params.mode} \\\n        $param_proteins \\\n        --cpus ${task.cpus} \\\n        --centre X --compliant\n    \"\"\"\n}"], "list_proc": ["metashot/prok-pan/prokka"], "list_wf_names": ["metashot/prok-pan"]}, {"nb_reuse": 1, "tools": ["dRep"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["prok-quality"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess drep {\n    publishDir \"${params.outdir}\" , mode: 'copy' ,\n        pattern: 'filtered_repr/*'\n\n    publishDir \"${params.outdir}\" , mode: 'copy' ,\n        pattern: 'drep/{data_tables,figures,log}/*'\n\n    input:\n    path 'genomeinfo.csv'\n    path(genomes)\n\n    output:\n    path 'filtered_repr/*'\n    path 'drep/{data_tables,figures,log}/*'\n    path 'drep/data_tables/Cdb.csv', emit: cdb\n    path 'drep/data_tables/Wdb.csv', emit: wdb\n\n    script:   \n    \"\"\"\n    mkdir genomes_dir\n    mv $genomes genomes_dir\n    dRep dereplicate \\\n        drep \\\n        --genomeInfo genomeinfo.csv \\\n        -p ${task.cpus} \\\n        -nc ${params.min_overlap} \\\n        -sa ${params.ani_thr} \\\n        -l 1 \\\n        -comp ${params.min_completeness} \\\n        -con ${params.max_contamination} \\\n        -strW 0 \\\n        -g genomes_dir/*\n\n    mv drep/dereplicated_genomes filtered_repr\n    \"\"\"\n}"], "list_proc": ["metashot/prok-quality/drep"], "list_wf_names": ["metashot/prok-quality"]}, {"nb_reuse": 1, "tools": ["Barrnap"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["prok-quality"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess barrnap {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/barrnap/${id}\" , mode: 'copy'\n\n    input:\n    tuple val(id), path(genome)\n\n    output:\n    path '*.rRNA.{bac,arc}.gff', emit: gff\n    path '*.rRNA.{bac,arc}.fa', emit: fa\n\n    script:\n    \"\"\"\n    barrnap \\\n        --kingdom bac \\\n        --outseq ${id}.rRNA.bac.fa \\\n        --threads ${task.cpus} \\\n        ${genome} > ${id}.rRNA.bac.gff\n\n    barrnap \\\n        --kingdom arc \\\n        --outseq ${id}.rRNA.arc.fa \\\n        --threads ${task.cpus} \\\n        ${genome} > ${id}.rRNA.arc.gff\n    \"\"\"\n}"], "list_proc": ["metashot/prok-quality/barrnap"], "list_wf_names": ["metashot/prok-quality"]}, {"nb_reuse": 1, "tools": ["snippy"], "nb_own": 1, "list_own": ["metashot"], "nb_wf": 1, "list_wf": ["prok-snp"], "list_contrib": ["davidealbanese"], "nb_contrib": 1, "codes": ["\nprocess snippy {\n    tag \"${id}\"\n\n    publishDir \"${params.outdir}/snippy\" , mode: 'copy'\n\n    input:\n    tuple val(id), path(input)\n    path (ref)\n    path (targets)\n    \n    output:\n    tuple val(id), path(\"${id}\"), emit: snps\n\n    script:\n    param_input = params.single_end ? \" --se \\\"$input\\\"\" : \"--pe1 \\\"${input[0]}\\\" --pe2 \\\"${input[1]}\\\"\"\n    param_targets = params.targets != 'none' ? \"--targets $targets\" : ''\n    param_report = params.report ? '--report' : ''\n    \"\"\"\n    snippy \\\n        --cpus ${task.cpus} \\\n        --outdir ${id} \\\n        --ref ${ref} \\\n        $param_input \\\n        $param_targets \\\n        $param_report\n    \"\"\"\n}"], "list_proc": ["metashot/prok-snp/snippy"], "list_wf_names": ["metashot/prok-snp"]}, {"nb_reuse": 1, "tools": ["cohorts", "ANCESTRYMAP"], "nb_own": 1, "list_own": ["michael-ta"], "nb_wf": 1, "list_wf": ["longitudinal-GWAS-pipeline"], "list_contrib": ["michael-ta"], "nb_contrib": 1, "codes": ["\nprocess p3_cohort_samplelist {\n  scratch true\n  label 'small'\n\n  input:\n    file samplelist from p2_out_file\n    path covarfile, stageAs: 'covariates.tsv' from \"${params.covarfile}\"\n  output:\n    file \"${params.ancestry}_*_filtered.tsv\" into gwas_samplelist\n\n  script:\n                                                              \n    \"\"\"\n    #!/usr/bin/env python3\n    import pandas as pd\n    import time\n\n    ancestry = \"${params.ancestry}\"\n    study_id_colname = \"${params.study_id}\"\n\n    ancestry_df = pd.read_hdf(\"${samplelist}\", key=\"ancestry_keep\")\n    outlier_df = pd.read_hdf(\"${samplelist}\", key=\"outliers\")\n    pcs_df = pd.read_hdf(\"${samplelist}\", key=\"pcs\")\n    kin_df = pd.read_hdf(\"${samplelist}\", key=\"kin\")\n    data_df = pd.read_csv('covariates.tsv', sep=\"\\\\t\", engine='c')\n\n    cohorts = data_df[study_id_colname].unique().tolist()\n\n    kin_df = kin_df[kin_df.KINSHIP >= ${params.kinship}]\n    # TODO: address case when single cohort present\n    # TODO: currently does not address longitudinal covariates\n    for cohort in cohorts:\n      print(f'---- {cohort} ----')\n      samples = data_df[ (data_df.IID.isin(ancestry_df.IID)) &\n                         (data_df[study_id_colname] == cohort) &\n                        ~(data_df.IID.isin(outlier_df.IID)) ].copy(deep=True)\n      #samples = samples.merge(pcs_df, left_on=\"IID\", right_on=\"#IID\", how=\"inner\")\n      #samples.drop(columns=\"#IID\", inplace=True)\n      r = kin_df[(kin_df['#IID1'].isin(samples.IID)) & (kin_df.IID2.isin(samples.IID))].copy()\n      samples = samples[~samples.IID.isin(r.IID2)].copy()\n      samples.to_csv(f\"{ancestry}_{cohort}_filtered.tsv\", sep=\"\\t\", index=False)\n      print(f'Samples removed (outliers) = {data_df.IID.isin(outlier_df.IID).sum()}')\n      print(f'Samples removed (kinship) = {r.shape[0]}')\n      print(f'Samples remaining = {len(samples)}')\n      print('')\n\n    time.sleep(5)\n    \"\"\"\n}"], "list_proc": ["michael-ta/longitudinal-GWAS-pipeline/p3_cohort_samplelist"], "list_wf_names": ["michael-ta/longitudinal-GWAS-pipeline"]}, {"nb_reuse": 1, "tools": ["cohorts"], "nb_own": 1, "list_own": ["michael-ta"], "nb_wf": 1, "list_wf": ["longitudinal-GWAS-pipeline"], "list_contrib": ["michael-ta"], "nb_contrib": 1, "codes": ["\nprocess p3_merge_pca {\n  scratch true\n  label 'small'\n  \n  input:\n    \n    set file(samplelist), file(cohort_pca) from p3_cohort_pca_out\n    \n  output:\n    file \"${params.ancestry}_*_filtered.pca.tsv\" into gwas_samplelist_pca\n   \n   script:\n     \n     \"\"\"\n     #!/usr/bin/env python3\n     import pandas as pd\n     import time   \n     import os\n     \n     print(os.listdir())\n     sample_fn = \"${samplelist.getName()}\"\n     cohort = sample_fn[:-(len('_filtered.tsv'))]\n     pc_fn = cohort + '.pca.eigenvec'\n\n     print(pc_fn, cohort)\n     pc_df = pd.read_csv(pc_fn, sep=\"\\t\")\n     samples_df = pd.read_csv(sample_fn, sep=\"\\t\")\n     pc_df.rename(columns={\"#IID\": \"IID\"}, inplace=True)\n     samples_df = samples_df.merge(pc_df, on=\"IID\")\n     \n     samples_df.to_csv(cohort + '_filtered.pca.tsv', sep=\"\\t\", index=False)\n     time.sleep(5)\n     \"\"\"\n}"], "list_proc": ["michael-ta/longitudinal-GWAS-pipeline/p3_merge_pca"], "list_wf_names": ["michael-ta/longitudinal-GWAS-pipeline"]}, {"nb_reuse": 1, "tools": ["RDFScape", "PHENO", "cohorts"], "nb_own": 1, "list_own": ["michael-ta"], "nb_wf": 1, "list_wf": ["longitudinal-GWAS-pipeline"], "list_contrib": ["michael-ta"], "nb_contrib": 1, "codes": ["\nprocess p3_gwas_viz {\n  scratch true\n  label 'medium'\n\n  publishDir \"${GWAS_OUTPUT_DIR}/${params.out}_${params.datetime}/Plots\", mode: 'copy', overwrite: true\n\n  input:\n    file x from gwas_results.mix(gallop_results).collect()\n  output:\n    file \"*.png\" into gwas_plots\n\n  script:\n    \"\"\"\n    #!/usr/bin/env python3\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import os\n    from qmplot import manhattanplot\n    from qmplot import qqplot\n    from multiprocessing import Pool\n\n    files = list(filter(lambda x: os.path.splitext(x)[-1] in ['.gallop', '.linear'], os.listdir()))\n    dfs = []\n  \n    # group by phenotype\n    lt_flag = \"${params.longitudinal_flag}\" == \"true\"\n    suffix = \"${params.out}\"\n    threads = int(\"${task.cpus}\")\n\n    def plot_summary_stats(data, cohort, outcome, lt_flag=False):\n      xtick = set(['chr' + i for i in list(map(str, range(1, 10))) + ['11', '13', '15', '18', '21', 'X']])\n      if lt_flag:\n        manhattanplot(data=data,\n                      title=f\"Manhattan Intercept {cohort} {outcome}\",\n                      pv=\"Pi\",\n                      figname=f\"{cohort}_{outcome}_manhattan_intercept.gallop.png\",\n                      xtick_label_set=xtick)\n        \n        manhattanplot(data=data,\n                      title=f\"Manhattan Slope {cohort} {outcome}\",\n                      pv=\"P\",\n                      figname=f\"{cohort}_{outcome}_manhattan_slope.gallop.png\",\n                      xtick_label_set=xtick)\n        f, ax = plt.subplots(figsize=(6, 6), facecolor=\"w\", edgecolor=\"k\")\n        qqplot(data=data[\"Pi\"],\n               marker=\"o\",\n               title=f\"QQ Intercept {cohort} {outcome}\",\n               xlabel=r\"Expected -log(P)\",\n               ylabel=r\"Observed -log(P)\",\n               dpi=300,\n               ax=ax,\n               figname=f\"{cohort}_{outcome}_qq_intercept.gallop.png\")\n        f, ax = plt.subplots(figsize=(6, 6), facecolor=\"w\", edgecolor=\"k\")\n        qqplot(data=data[\"P\"],\n               marker=\"o\",\n               title=f\"QQ Slope {cohort} {outcome}\",\n               xlabel=r\"Expected -log(P)\",\n               ylabel=r\"Observed -log(P)\",\n               dpi=300,\n               ax=ax,\n               figname=f\"{cohort}_{outcome}_qq_slope.gallop.png\")\n      else:\n        manhattanplot(data=data,\n                      title=f\"Manhattan Intercept {cohort} {outcome}\",\n                      pv=\"P\",\n                      figname=f\"{cohort}_{outcome}_manhattan.linear.png\",\n                      xtick_label_set=xtick)\n        f, ax = plt.subplots(figsize=(6, 6), facecolor=\"w\", edgecolor=\"k\")\n        qqplot(data=data[\"P\"],\n               marker=\"o\",\n               title=f\"QQ Intercept {cohort} {outcome}\",\n               xlabel=r\"Expected -log(P)\",\n               ylabel=r\"Observed -log(P)\",\n               dpi=300,\n               ax=ax,\n               figname=f\"{cohort}_{outcome}_qq.linear.png\")\n\n    plot_df = dict()\n    for f in files:\n      fc = f.split('.')\n      pheno = fc[-2]\n      cohort = '_'.join(fc[0].split('_')[:-1])\n      try:\n        plot_df[f'{cohort}+{pheno}'].append(f)\n      except KeyError:\n        plot_df[f'{cohort}+{pheno}'] = [f]\n\n    def read_table(fn):\n      'converts a filename to a pandas dataframe'\n      return pd.read_table(fn, sep=\"\\t\")\n\n    for cohort in plot_df.keys():\n      df = None\n      c, outcome = cohort.split('+')\n      \n      with Pool(processes=threads) as pool:\n\n        # have your pool map the file names to dataframes\n        df_list = pool.map(read_table, plot_df[cohort])\n\n        # reduce the list of dataframes to a single dataframe\n        df = pd.concat(df_list, ignore_index=True)\n      \n      df = df.dropna(how=\"any\", axis=0)  # clean data\n      df['chr_order'] = df['#CHROM'].str.replace('chr', '')\n      df['chr_order'] = df['chr_order'].astype(int)\n      df = df.sort_values(by=['chr_order', 'POS'])\n\n      # generate manhattan plot and set an output file.\n      plot_summary_stats(data=df, cohort=f'{c}.{suffix}', outcome=outcome, lt_flag=lt_flag)\n    \"\"\"\n}"], "list_proc": ["michael-ta/longitudinal-GWAS-pipeline/p3_gwas_viz"], "list_wf_names": ["michael-ta/longitudinal-GWAS-pipeline"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Trimmomatic", "BEDTools"], "nb_own": 2, "list_own": ["michellejlin", "michaelbale"], "nb_wf": 2, "list_wf": ["covid_swift_pipeline", "JLab-ATACFlow"], "list_contrib": ["michellejlin", "vpeddu", "michaelbale"], "nb_contrib": 3, "codes": ["\nprocess BamSorting { \n    container \"quay.io/greninger-lab/swift-pipeline:latest\"\n\n\t                                     \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n      tuple val (base), file(\"${base}.bam\"),file(\"${base}_summary2.csv\")                      \n    output:\n      tuple val (base), file(\"${base}.sorted.bam\"),file(\"${base}.sorted.bam.bai\"),file(\"${base}_summary3.csv\"),env(bamsize)                      \n    \n    publishDir \"${params.OUTDIR}inprogress_summary\", mode: 'copy', pattern: '*summary.csv'\n    publishDir params.OUTDIR, mode: 'copy', pattern: '*.sorted.bam'\n\n    script:\n    \"\"\"\n    #!/bin/bash\n    /usr/local/miniconda/bin/samtools sort -@ ${task.cpus} ${base}.bam > ${base}.sorted.bam\n    /usr/local/miniconda/bin/samtools index ${base}.sorted.bam\n\n    clipped_reads=0\n    /usr/local/miniconda/bin/bedtools genomecov -d -ibam ${base}.sorted.bam > ${base}_coverage.txt\n    meancoverage=\\$(cat ${base}_coverage.txt | awk '{sum+=\\$3} END { print sum/NR}')\n    bamsize=\\$((\\$(wc -c ${base}.sorted.bam | awk '{print \\$1'})+0))\n    echo \"bamsize: \\$bamsize\"\n    if (( \\$bamsize > 92 ))\n    then\n        # Spike protein coverage\n        awk '\\$2 ~ /21563/,\\$2 ~ /25384/' ${base}_coverage.txt > ${base}_spike_coverage.txt\n        avgcoverage=\\$(cat ${base}_spike_coverage.txt | awk '{sum+=\\$3} END { print sum/NR}')\n        proteinlength=\\$((25384-21563+1))\n        cov100=\\$((100*\\$(cat ${base}_spike_coverage.txt | awk '\\$3>=100' | wc -l)/3822))\n        cov200=\\$((100*\\$(cat ${base}_spike_coverage.txt | awk '\\$3>=200' | wc -l)/3822))\n        mincov=\\$(sort -nk 3 ${base}_spike_coverage.txt | head -n 1 | cut -f3)\n    else\n        avgcoverage=0\n        cov100=0\n        cov200=0\n        mincov=0\n    fi\n\n    cp ${base}_summary2.csv ${base}_summary3.csv\n    printf \",\\$clipped_reads,\\$meancoverage,\\$avgcoverage,\\$cov100,\\$cov200,\\$mincov\" >> ${base}_summary3.csv\n\n\n    \"\"\"\n}", "\nprocess trim {\n\ttag \"Trimmomatic on ${pair_id}\"\n\tlabel 'med_mem'\n\n\tinput:\n\ttuple val(pair_id), path(reads) from reads_ch\n\t\n\toutput:\n\tpath(\"${pair_id}_trim.log\") into trimmomaticLogs_ch\n\ttuple pair_id, path(\"${pair_id}*.fastq.gz\") into trimmedReads_ch, tReadsFqc_ch\n\n\t                               \n\t                                                                         \n\tscript:\n\t\"\"\"\n\ttrimmomatic PE \\\n\t  -threads $task.cpus \\\n\t  ${reads[0]} \\\n\t  ${reads[1]} \\\n\t  -baseout ${pair_id}_trim \\\n\t  LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 2> ${pair_id}_trim.log\n\t\n\tmv ${pair_id}_trim_1P ${pair_id}_trim_R1.fastq\n\tmv ${pair_id}_trim_2P ${pair_id}_trim_R2.fastq\n\tgzip ${pair_id}_trim_R1.fastq\n\tgzip ${pair_id}_trim_R2.fastq\n\t\"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/BamSorting", "michaelbale/JLab-ATACFlow/trim"], "list_wf_names": ["michellejlin/covid_swift_pipeline", "michaelbale/JLab-ATACFlow"]}, {"nb_reuse": 3, "tools": ["FastQC", "Minimap2"], "nb_own": 3, "list_own": ["michellejlin", "michaelbale", "microgenlab"], "nb_wf": 3, "list_wf": ["porefile", "covid_swift_pipeline", "JLab-ATACFlow"], "list_contrib": ["michellejlin", "vpeddu", "michaelbale", "iferres"], "nb_contrib": 4, "codes": ["\nprocess Fastqc_SE {\n    container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n\n                                         \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n        tuple val(base),file(\"${base}.trimmed.fastq.gz\")                        \n    output: \n        file(\"*fastqc*\")                  \n\n    publishDir \"${params.OUTDIR}fastqc\", mode: 'copy'\n\n    script:\n    \"\"\"\n    #!/bin/bash\n\n    /usr/local/bin/fastqc ${base}.trimmed.fastq.gz\n    \"\"\"\n}", "\nprocess fastqc {\n\t\n\ttag \"FASTQC on ${sample_id}\"\n\tlabel 'small_mem'\n\t\n\tinput:\n\ttuple val(sample_id), path(reads) from tReadsFqc_ch\n\n\toutput:\n\tpath(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\tscript:\n\t\"\"\"\n\tmkdir fastqc_${sample_id}_logs\n\tfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n\t\"\"\"  \n}", "\nprocess AutoMap {\n\ttag \"$barcode_id\"\n\tlabel \"big_cpus\"\n\n\tinput:\n\ttuple val(barcode_id), file(\"Filt_${barcode_id}.fastq\")\n\n\toutput:\n\ttuple val(barcode_id), file(\"Filt_${barcode_id}.fastq\"), file(\"overlap_${barcode_id}.paf\")\n\n\tshell:\n\t\"\"\"\n\tminimap2 \\\n\t\t-x ava-ont \\\n\t\t-t ${task.cpus} \\\n\t\t-g 500 \\\n\t\t-f${params.minimap2_f} \\\n\t\tFilt_${barcode_id}.fastq Filt_${barcode_id}.fastq > overlap_${barcode_id}.paf\n\t\"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/Fastqc_SE", "michaelbale/JLab-ATACFlow/fastqc", "microgenlab/porefile/AutoMap"], "list_wf_names": ["microgenlab/porefile", "michellejlin/covid_swift_pipeline", "michaelbale/JLab-ATACFlow"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Bowtie", "Minimap2", "BEDTools"], "nb_own": 3, "list_own": ["michellejlin", "michaelbale", "microgenlab"], "nb_wf": 3, "list_wf": ["porefile", "covid_swift_pipeline", "JLab-ATACFlow"], "list_contrib": ["michellejlin", "vpeddu", "michaelbale", "iferres"], "nb_contrib": 4, "codes": ["\nprocess Clipping { \n    container \"quay.io/greninger-lab/swift-pipeline:latest\"\n\n                                         \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n        tuple val (base), file(\"${base}.sorted.sam\"),file(\"${base}_summary2.csv\")                     \n        file MASTERFILE\n    output:\n        tuple val (base), file(\"${base}.clipped.bam\"), file(\"${base}.clipped.bam.bai\"),file(\"${base}_summary3.csv\"),env(bamsize)                      \n        tuple val (base), file(\"${base}.clipped.bam\"), file(\"${base}.clipped.bam.bai\"),env(bamsize)                       \n        tuple val (base), file(\"${base}.clipped.bam\"), file(\"${base}.clipped.bam.bai\"),env(bamsize)                       \n\n    publishDir params.OUTDIR, mode: 'copy', pattern: '*.clipped.bam'\n    publishDir \"${params.OUTDIR}inprogress_summary\", mode: 'copy', pattern: '*summary3.csv'\n\n    script:\n        \"\"\"\n        #!/bin/bash\n        ls -latr\n        /./root/.local/bin/primerclip -s ${MASTERFILE} ${base}.sorted.sam ${base}.clipped.sam\n        #/usr/local/miniconda/bin/samtools sort -@ ${task.cpus} -n -O sam ${base}.clipped.sam > ${base}.clipped.sorted.sam\n        #/usr/local/miniconda/bin/samtools view -@ ${task.cpus} -Sb ${base}.clipped.sorted.sam > ${base}.clipped.unsorted.bam\n        #/usr/local/miniconda/bin/samtools sort -@ ${task.cpus} -o ${base}.clipped.unsorted.bam ${base}.clipped.bam\n        /usr/local/miniconda/bin/samtools sort -@ ${task.cpus} ${base}.clipped.sam -o ${base}.clipped.bam\n        /usr/local/miniconda/bin/samtools index ${base}.clipped.bam\n        clipped_reads=\\$(/usr/local/miniconda/bin/samtools flagstat ${base}.clipped.bam | grep \"mapped (\" | awk '{print \\$1}')\n        echo \"clipped reads: \\$clipped_reads\"\n        /usr/local/miniconda/bin/bedtools genomecov -d -ibam ${base}.clipped.bam > ${base}_coverage.txt\n        meancoverage=\\$(cat ${base}_coverage.txt | awk '{sum+=\\$3} END { print sum/NR}')\n        bamsize=\\$((\\$(wc -c ${base}.clipped.bam | awk '{print \\$1'})+0))\n        echo \"bamsize: \\$bamsize\"\n        if (( \\$bamsize > 92 ))\n        then\n            # Spike protein coverage\n            awk '\\$2 ~ /21563/,\\$2 ~ /25384/' ${base}_coverage.txt > ${base}_spike_coverage.txt\n            avgcoverage=\\$(cat ${base}_spike_coverage.txt | awk '{sum+=\\$3} END { print sum/NR}')\n            proteinlength=\\$((25384-21563+1))\n            cov100=\\$((100*\\$(cat ${base}_spike_coverage.txt | awk '\\$3>=100' | wc -l)/3822))\n            cov200=\\$((100*\\$(cat ${base}_spike_coverage.txt | awk '\\$3>=200' | wc -l)/3822))\n            mincov=\\$(sort -nk 3 ${base}_spike_coverage.txt | head -n 1 | cut -f3)\n        else\n            avgcoverage=0\n            cov100=0\n            cov200=0\n            mincov=0\n        fi\n        \n        cp ${base}_summary2.csv ${base}_summary3.csv\n        printf \",\\$clipped_reads,\\$meancoverage,\\$avgcoverage,\\$cov100,\\$cov200,\\$mincov\" >> ${base}_summary3.csv\n        \"\"\"\n    }", "\nprocess bowtieAlign {\n\ttag \"Aliging $pair_id to ${params.bt2_index}\"\n\tlabel 'big_mem'\n\n\tinput:\n\tval(idx) from params.bt2_index\n\ttuple val(pair_id), path(reads) from trimmedReads_ch\n\n\toutput:\n\tpath(\"${pair_id}_bt2.log\") into bt2Logs_ch\n\ttuple pair_id, file(\"${pair_id}_init.bam\") into bt2Bam_ch\n\n\t                         \n\tscript:\n\t\"\"\"\n\tbowtie2 -p $task.cpus -x ${idx} --no-mixed --no-unal --no-discordant --local --very-sensitive-local -X 1000 -k 4 --mm -1 ${reads[0]} -2 ${reads[1]} 2> ${pair_id}_bt2.log | samtools view -bS -q 30 - > ${pair_id}_init.bam\n\t\"\"\"\n\n}", "\nprocess MakeMinimapDB {\n\tlabel \"big_cpus\"\n\n\tinput:\n\tpath(\"silva_SSU_tax.fasta\")\n\n\toutput:\n\tpath(\"silva_k${params.minimap2_k}.mmi\")\n\n\tshell:\n\t\"\"\"\n\tminimap2 \\\n\t\t-t ${task.cpus} \\\n\t\t-k ${params.minimap2_k} \\\n\t\t-d silva_k${params.minimap2_k}.mmi \\\n\t\tsilva_SSU_tax.fasta\n\t\"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/Clipping", "michaelbale/JLab-ATACFlow/bowtieAlign", "microgenlab/porefile/MakeMinimapDB"], "list_wf_names": ["microgenlab/porefile", "michellejlin/covid_swift_pipeline", "michaelbale/JLab-ATACFlow"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-ATACFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": [" process callHMMRATAC {\n\t    tag \"Calling peaks using HMMRATAC\"\n\t\tpublishDir \"$params.outdir/peakcalls/HMMRATACCalls-narrowPeak\", mode: 'copy', pattern: '*.narrowPeak'\n\t\tlabel 'largeStore'\n\t\t\n\t\t\n\t\tinput:\n\t\ttuple val(sampleID), path(bam) from bamForPeaks_ch\n\t\tpath(genomeInfo) from params.genomeInfo\n\t\t\n\t\t\n\t\t\n\t\toutput:\n\t\ttuple sampleID, file(\"${sampleID}_peaks.narrowPeak\") into narrowPeaks_ch\n\t\t\t\t\n\t\tscript:\n\t\t\"\"\"\n\t\tsambamba index ${bam}\t\t\n\t\tjava -jar ${baseDir}/bin/HMMRATAC_V1.2.10_exe.jar -b ${bam} -i ${sampleID}_final.bam.bai -g ${genomeInfo} -o ${sampleID}\n\t\tawk -v OFS='\\t' '{print \\$1, \\$2, \\$3, \\$4, \"1\", \"1\", \\$13, \"-1\", \"-1\"}' ${sampleID}_peaks.gappedPeak > ${sampleID}_peaks.narrowPeak\n\t\t\"\"\"\n\t}"], "list_proc": ["michaelbale/JLab-ATACFlow/callHMMRATAC"], "list_wf_names": ["michaelbale/JLab-ATACFlow"]}, {"nb_reuse": 3, "tools": ["DEFormats", "Sambamba", "Minimap2"], "nb_own": 2, "list_own": ["michaelbale", "microgenlab"], "nb_wf": 2, "list_wf": ["porefile", "JLab-ATACFlow"], "list_contrib": ["michaelbale", "iferres"], "nb_contrib": 2, "codes": ["\nprocess makeBigwig{\n\n\ttag \"Creating ${sampleID} bigwig\"\n\tpublishDir \"$params.outdir/bigwig\", mode: 'copy'\n\tlabel 'big_mem'\n\n\tinput:\n\ttuple val(sampleID), file(finalBam) from finalBam_ch\n\t\n\toutput:\n\ttuple val(sampleID), file(\"${sampleID}_CPMnorm.bw\") into bigwig_ch, bigwig2_ch, bigwig3_ch\n\tval(sampleID) into labels_ch\n\tfile(\"${sampleID}_CPMnorm.bw\") into forGEnrichPlot_ch\n\n\t                         \n\tscript:\n\t\"\"\"\n\tsambamba index $finalBam\n\tbamCoverage -p $task.cpus --bam ${finalBam} -o ${sampleID}_CPMnorm.bw -bs 10 --extendReads --smoothLength 50 --normalizeUsing CPM --ignoreForNormalization chrX chrY  --skipNonCoveredRegions \n\t\"\"\"\n}", "\nprocess MergeResults{\n\ttag \"$selected_wf\"\n\tlabel \"small_cpus\"\n\tlabel \"small_mem\"\n\t\n\tpublishDir \"$params.outdir/Merged_Results\", mode: \"copy\"\n\n\tinput:\n\ttuple val(selected_wf), file(\"*\")\n\n\toutput:\n\ttuple file(\"${selected_wf}_COUNTS.tsv\"), file(\"${selected_wf}_TAXCLA.tsv\")\n\n\tshell:\n\t\"\"\"\n\t#!/usr/bin/env Rscript\n\t\n\t# Read *.info files\n\tinfos <- list.files(pattern = \"[.]info\\$\")\n\tlp <- lapply(setNames(infos, infos), function(x) {\n\ta <- try(read.csv(x, sep = \"\\\\t\", header = FALSE))\n\tif (class(a) != \"try-error\"){\n\t\tsetNames(a\\$V2, a\\$V1)\n\t}else{\n\t\tNULL\n\t}\n\t})\n\n\t# Get present taxa\n\tlvls <- unique(unlist(lapply(lp, names)))\n\n\t# Create new names (TAXA_*)\n\tlnlv <- length(lvls)\n\twdth <- nchar(lnlv)\n\tbc <- paste0(\"TAXA_\", formatC(seq_along(lvls),\n\t\t\t\t\t\t\t\twidth = wdth,\n\t\t\t\t\t\t\t\tformat = 'd',\n\t\t\t\t\t\t\t\tflag = '0'))\n\n\t# Parse tax paths for each TAXA\n\trr <- data.frame(otu = bc, raw = lvls)\n\tspl <- strsplit(setNames(rr\\$raw, rr\\$otu), \";\")\n\trks <- c(\"[D]\", \"[P]\", \"[C]\", \"[O]\", \"[F]\", \"[G]\", \"[S]\")\n\ttax <- c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\")\n\trks <- setNames(rks, tax)\n\ttt <- lapply(spl, function(x) {\n\tunlist(lapply(rks, function(y){\n\t\tgrep(y, x, fixed = TRUE, value = TRUE)\n\t}))\n\t})\n\ttt <- lapply(tt, function(y) gsub(\"[ ]*\\\\\\\\[\\\\\\\\w{1,2}\\\\\\\\][ ]\", \"\", y))\n\n\t# Create TAX table\n\ttaxt <- matrix(NA_character_, \n\t\t\t\tnrow = length(tt), \n\t\t\t\tncol = length(tax), \n\t\t\t\tdimnames = list(names(tt), tax))\n\tfor (i in seq_along(tt)){\n\ttaxt[names(tt)[i], names(tt[[i]])] <- tt[[i]]\n\t}\n\n\n\tre <- as.data.frame(cbind(rr, taxt))\n\n\t# Rename OTU counts\n\tlp <- lapply(lp, function(x){\n\t\tif (length(x)){\n\t\t\tsetNames(x, re\\$otu[match(names(x), re\\$raw)])\n\t\t}\n\t})\n\n\t# Create \"OTU\" (TAXA) Table\n\totut <- matrix(0L, nrow = length(re\\$otu), ncol = length(lp),\n\t\t\t\tdimnames = list(re\\$otu, names(lp)))\n\tfor (i in seq_along(lp)){\n\totut[names(lp[[i]]), names(lp)[i]] <- lp[[i]]\n\t}\n\tcolnames(otut) <- sub(\"[.]info\\$\", \"\", colnames(otut))\n\n\n\t# Write Taxa Classification Table and Counts Table\n\twrite.table(taxt, \"${selected_wf}_TAXCLA.tsv\", quote = FALSE, sep = \"\\\\t\")\n\twrite.table(otut, \"${selected_wf}_COUNTS.tsv\", quote = FALSE, sep = \"\\\\t\")\n\t\"\"\"\n}", "\nprocess AutoMap {\n\ttag \"$barcode_id\"\n\tlabel \"big_cpus\"\n\n\tinput:\n\ttuple val(barcode_id), file(\"Filt_${barcode_id}.fastq\")\n\n\toutput:\n\ttuple val(barcode_id), file(\"Filt_${barcode_id}.fastq\"), file(\"overlap_${barcode_id}.paf\")\n\n\tshell:\n\t\"\"\"\n\tminimap2 \\\n\t\t-x ava-ont \\\n\t\t-t ${task.cpus} \\\n\t\t-g 500 \\\n\t\t-f${params.minimap2_f} \\\n\t\tFilt_${barcode_id}.fastq Filt_${barcode_id}.fastq > overlap_${barcode_id}.paf\n\t\"\"\"\n}"], "list_proc": ["michaelbale/JLab-ATACFlow/makeBigwig", "microgenlab/porefile/MergeResults", "microgenlab/porefile/AutoMap"], "list_wf_names": ["microgenlab/porefile", "michaelbale/JLab-ATACFlow"]}, {"nb_reuse": 3, "tools": ["Minimap2", "THREADER", "MultiQC"], "nb_own": 3, "list_own": ["michaelbale", "microgenlab", "mikek498"], "nb_wf": 3, "list_wf": ["porefile", "artic-gls", "JLab-ATACFlow"], "list_contrib": ["m-bull", "michaelbale", "mikek498", "iferres"], "nb_contrib": 4, "codes": ["\nprocess multiqc {\n\tpublishDir \"$params.outdir/results\", mode:'copy'\n\tlabel 'small_mem'\n\n\tinput:\n\tpath('*') from fastqc_ch\n\t  .mix(idxstats_ch)\n\t  .mix(picardISStats_ch)\n\t  .mix(picardDupStats_ch)\n\t  .mix(trimmomaticLogs_ch)\n\t  .mix(bt2Logs_ch)\n\t  .collect()\n\t\n\toutput:\n\tpath('multiqc_report.html')\n\n\tscript:\n\t\"\"\"\n\tmultiqc .\n\t\"\"\"\n}", "\nprocess makeIvarBedfile {\n    cpus 16\n    memory '60 GB'\n    tag { schemeRepo }\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"ivar.bed\", mode: 'copy'\n\n    input:\n    file(schemeRepo)\n\n    output:\n    file(\"ivar.bed\")\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n  \n    import csv\n\n    bedrows = []\n    with open(\"${schemeRepo}/${params.schemeDir}/${params.scheme}/${params.schemeVersion}/nCoV-2019.scheme.bed\", newline='') as bedfile:\n        reader = csv.reader(bedfile, delimiter='\\t')\n        for row in reader:\n            row[4] = '60'\n            if row[3].endswith('LEFT'):\n                 row.append('+')\n            else: \n                row.append('-')\n            bedrows.append(row)\n\n    with open('ivar.bed', 'w', newline='') as bedfile:\n        writer = csv.writer(bedfile, delimiter='\\t')\n        for row in bedrows:\n            writer.writerow(row)\n    \"\"\"\n}", "\nprocess MakeMinimapDB {\n\tlabel \"big_cpus\"\n\n\tinput:\n\tpath(\"silva_SSU_tax.fasta\")\n\n\toutput:\n\tpath(\"silva_k${params.minimap2_k}.mmi\")\n\n\tshell:\n\t\"\"\"\n\tminimap2 \\\n\t\t-t ${task.cpus} \\\n\t\t-k ${params.minimap2_k} \\\n\t\t-d silva_k${params.minimap2_k}.mmi \\\n\t\tsilva_SSU_tax.fasta\n\t\"\"\"\n}"], "list_proc": ["michaelbale/JLab-ATACFlow/multiqc", "mikek498/artic-gls/makeIvarBedfile", "microgenlab/porefile/MakeMinimapDB"], "list_wf_names": ["microgenlab/porefile", "mikek498/artic-gls", "michaelbale/JLab-ATACFlow"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": ["\tprocess trimSE {\n\t\ttag \"Trimmomatic on ${pair_id}\"\n\t\tlabel 'med_mem'\n\n\t\tinput:\n\t\ttuple val(pair_id), path(reads) from reads_ch\n\t\t\n\t\toutput:\n\t\tpath(\"${pair_id}_trim.log\") into trimmomaticLogs_ch\n\t\ttuple pair_id, path(\"${pair_id}*.fastq.gz\") into trimmedReads_ch, tReadsFqc_ch\n\n\t\t                               \n\t\t                                                                         \n\t\tscript:\n\t\t\"\"\"\n\t\ttrimmomatic SE \\\n\t\t  -threads $task.cpus \\\n\t\t  ${reads[0]} \\\n\t\t  ${pair_id}_trim_1P \\\n\t\t  LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 2> ${pair_id}_trim.log\n\t\t\n\t\tmv ${pair_id}_trim_1P ${pair_id}_trim_R1.fastq\n\t\tgzip ${pair_id}_trim_R1.fastq\n\t\t\"\"\"\n\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/trimSE"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 2, "tools": ["FastQC", "mosdepth"], "nb_own": 2, "list_own": ["michaelbale", "mikecormier"], "nb_wf": 2, "list_wf": ["JLab-IPSeqFlow", "neoseq-seqcover-nf"], "list_contrib": ["michaelbale", "mikecormier"], "nb_contrib": 2, "codes": ["\tprocess fastqcSE {\n\t\t\n\t\ttag \"FASTQC on ${sample_id}\"\n\t\tlabel 'small_mem'\n\t\t\n\t\tinput:\n\t\ttuple val(sample_id), path(reads) from tReadsFqc_ch\n\n\t\toutput:\n\t\tpath(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\t\tscript:\n\t\t\"\"\"\n\t\tmkdir fastqc_${sample_id}_logs\n\t\tfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n\t\t\"\"\"  \n\t}", "\nprocess run_mosdepth {\n    container \"mikecormier/neoseq-seqcover-nf:v0.3.0\"\n    publishDir \"${params.outdir}/mosdepth\"\n    cpus params.cpus\n\n    input:\n    path(cram)\n    path(crai)\n    path(reference)\n\n    output:\n    path(\"*.d4\"), emit: d4\n\n    script:\n    \"\"\"\n    mosdepth -f $reference -x -t ${task.cpus} --d4 ${cram.getSimpleName()} $cram\n    \"\"\"\n}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/fastqcSE", "mikecormier/neoseq-seqcover-nf/run_mosdepth"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow", "mikecormier/neoseq-seqcover-nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie", "THREADER"], "nb_own": 2, "list_own": ["michaelbale", "mikek498"], "nb_wf": 2, "list_wf": ["JLab-IPSeqFlow", "artic-gls"], "list_contrib": ["michaelbale", "mikek498", "m-bull"], "nb_contrib": 3, "codes": ["\tprocess bowtieAlignSE {\n\t\ttag \"Aliging $pair_id to ${params.bt2_index}\"\n\t\tlabel 'big_mem'\n\n\t\tinput:\n\t\tval(idx) from params.bt2_index\n\t\ttuple val(pair_id), path(reads) from trimmedReads_ch\n\n\t\toutput:\n\t\tpath(\"${pair_id}_bt2.log\") into bt2Logs_ch\n\t\ttuple pair_id, file(\"${pair_id}_init.bam\") into bt2Bam_ch\n\n\t\t                         \n\t\tscript:\n\t\t\"\"\"\n\t\tbowtie2 -p $task.cpus -x ${idx} --no-mixed --no-unal --no-discordant --local --very-sensitive-local -X 1000 -k 4 --mm -U ${reads} 2> ${pair_id}_bt2.log | samtools view -bS -q 30 - > ${pair_id}_init.bam\n\t\t\"\"\"\n\n\t}", "\nprocess makeIvarBedfile {\n    cpus 16\n    memory '60 GB'\n    tag { schemeRepo }\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"ivar.bed\", mode: 'copy'\n\n    input:\n    file(schemeRepo)\n\n    output:\n    file(\"ivar.bed\")\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n  \n    import csv\n\n    bedrows = []\n    with open(\"${schemeRepo}/${params.schemeDir}/${params.scheme}/${params.schemeVersion}/nCoV-2019.scheme.bed\", newline='') as bedfile:\n        reader = csv.reader(bedfile, delimiter='\\t')\n        for row in reader:\n            row[4] = '60'\n            if row[3].endswith('LEFT'):\n                 row.append('+')\n            else: \n                row.append('-')\n            bedrows.append(row)\n\n    with open('ivar.bed', 'w', newline='') as bedfile:\n        writer = csv.writer(bedfile, delimiter='\\t')\n        for row in bedrows:\n            writer.writerow(row)\n    \"\"\"\n}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/bowtieAlignSE", "mikek498/artic-gls/makeIvarBedfile"], "list_wf_names": ["mikek498/artic-gls", "michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Sambamba", "AIVAR"], "nb_own": 2, "list_own": ["michaelbale", "mikek498"], "nb_wf": 2, "list_wf": ["JLab-IPSeqFlow", "artic-gls"], "list_contrib": ["michaelbale", "mikek498", "m-bull"], "nb_contrib": 3, "codes": ["\tprocess makeBigwigSE{\n\n\t\ttag \"Creating ${sampleID} bigwig\"\n\t\tpublishDir \"$params.outdir/bigwig\", mode: 'copy'\n\t\tlabel 'big_mem'\n\n\t\tinput:\n\t\ttuple val(sampleID), file(finalBam) from finalBam_ch\n\t\t\n\t\toutput:\n\t\ttuple val(sampleID), file(\"${sampleID}_CPMnorm.bw\") into bigwig_ch, bigwig2_ch, bigwig3_ch\n\t\tval(sampleID) into labels_ch\n\t\tfile(\"${sampleID}_CPMnorm.bw\") into forGEnrichPlot_ch\n\n\t\t                         \n\t\tscript:\n\t\t\"\"\"\n\t\tsambamba index $finalBam\n\t\tbamCoverage -p $task.cpus --bam ${finalBam} -o ${sampleID}_CPMnorm.bw -bs 10 --smoothLength 50 --normalizeUsing CPM --ignoreForNormalization chrX chrY  --skipNonCoveredRegions \n\t\t\"\"\"\n\t}", "\nprocess makeConsensus {\n    cpus 16\n    memory '60 GB'\n    tag { sampleName }\n\n    publishDir \"${params.outdir}/climb_upload/${params.runPrefix}/${sampleName}\", pattern: \"${sampleName}.primertrimmed.consensus.fa\", mode: 'copy'\n\n    input:\n        tuple(sampleName, path(bam))\n\n    output:\n        tuple(sampleName, path(\"${sampleName}.primertrimmed.consensus.fa\"))\n\n    script:\n        \"\"\"\n        samtools mpileup -A -d ${params.mpileupDepth} -Q0 ${bam} | \\\n        ivar consensus -t ${params.ivarFreqThreshold} -m ${params.ivarMinDepth} \\\n        -n N -p ${sampleName}.primertrimmed.consensus\n        \"\"\"\n}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/makeBigwigSE", "mikek498/artic-gls/makeConsensus"], "list_wf_names": ["mikek498/artic-gls", "michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 2, "tools": ["MultiQC", "nanopolish"], "nb_own": 2, "list_own": ["michaelbale", "mikek498"], "nb_wf": 2, "list_wf": ["JLab-IPSeqFlow", "artic-gls"], "list_contrib": ["michaelbale", "mikek498", "m-bull"], "nb_contrib": 3, "codes": ["\nprocess nanopolishIndex {\n   tag params.runPrefix\n\n   label 'largemem'\n\n   cpus 1\n\n   publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${fastqPass}.index*\", mode: \"copy\"\n\n   input:\n   tuple file(fastqPass), file(sequencingSummary), file(runDirectory)\n\n   output:\n   file \"${fastqPass}.index*\"\n\n   script:\n   \"\"\"\n   ln -s ${runDirectory}/fast5_pass .\n   nanopolish index -s ${sequencingSummary} -d fast5_pass ${fastqPass}\n   \"\"\"\n}", "\tprocess multiqcSE {\n\t\tpublishDir \"$params.outdir/results\", mode:'copy'\n\t\tlabel 'small_mem'\n\n\t\tinput:\n\t\tpath('*') from fastqc_ch\n\t\t  .mix(idxstats_ch)\n\t\t  .mix(picardDupStats_ch)\n\t\t  .mix(trimmomaticLogs_ch)\n\t\t  .mix(bt2Logs_ch)\n\t\t  .collect()\n\t\t\n\t\toutput:\n\t\tpath('multiqc_report.html')\n\n\t\tscript:\n\t\t\"\"\"\n\t\tmultiqc .\n\t\t\"\"\"\n\t}"], "list_proc": ["mikek498/artic-gls/nanopolishIndex", "michaelbale/JLab-IPSeqFlow/multiqcSE"], "list_wf_names": ["mikek498/artic-gls", "michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": ["\tprocess trim {\n\t\ttag \"Trimmomatic on ${pair_id}\"\n\t\tlabel 'med_mem'\n\n\t\tinput:\n\t\ttuple val(pair_id), path(reads) from reads_ch\n\t\t\n\t\toutput:\n\t\tpath(\"${pair_id}_trim.log\") into trimmomaticLogs_ch\n\t\ttuple pair_id, path(\"${pair_id}*.fastq.gz\") into trimmedReads_ch, tReadsFqc_ch\n\n\t\t                               \n\t\t                                                                         \n\t\tscript:\n\t\t\"\"\"\n\t\ttrimmomatic PE \\\n\t\t  -threads $task.cpus \\\n\t\t  ${reads[0]} \\\n\t\t  ${reads[1]} \\\n\t\t  -baseout ${pair_id}_trim \\\n\t\t  LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 2> ${pair_id}_trim.log\n\t\t\n\t\tmv ${pair_id}_trim_1P ${pair_id}_trim_R1.fastq\n\t\tmv ${pair_id}_trim_2P ${pair_id}_trim_R2.fastq\n\t\tgzip ${pair_id}_trim_R1.fastq\n\t\tgzip ${pair_id}_trim_R2.fastq\n\t\t\"\"\"\n\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/trim"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": ["\tprocess fastqc {\n\t\t\n\t\ttag \"FASTQC on ${sample_id}\"\n\t\tlabel 'small_mem'\n\t\t\n\t\tinput:\n\t\ttuple val(sample_id), path(reads) from tReadsFqc_ch\n\n\t\toutput:\n\t\tpath(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\t\tscript:\n\t\t\"\"\"\n\t\tmkdir fastqc_${sample_id}_logs\n\t\tfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n\t\t\"\"\"  \n\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/fastqc"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": ["\tprocess bowtieAlign {\n\t\ttag \"Aliging $pair_id to ${params.bt2_index}\"\n\t\tlabel 'big_mem'\n\n\t\tinput:\n\t\tval(idx) from params.bt2_index\n\t\ttuple val(pair_id), path(reads) from trimmedReads_ch\n\n\t\toutput:\n\t\tpath(\"${pair_id}_bt2.log\") into bt2Logs_ch\n\t\ttuple pair_id, file(\"${pair_id}_init.bam\") into bt2Bam_ch\n\n\t\t                         \n\t\tscript:\n\t\t\"\"\"\n\t\tbowtie2 -p $task.cpus -x ${idx} --no-mixed --no-unal --no-discordant --local --very-sensitive-local -X 1000 -k 4 --mm -1 ${reads[0]} -2 ${reads[1]} 2> ${pair_id}_bt2.log | samtools view -bS -q 30 - > ${pair_id}_init.bam\n\t\t\"\"\"\n\n\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/bowtieAlign"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": ["\tprocess makeBigwig{\n\n\t\ttag \"Creating ${sampleID} bigwig\"\n\t\tpublishDir \"$params.outdir/bigwig\", mode: 'copy'\n\t\tlabel 'big_mem'\n\n\t\tinput:\n\t\ttuple val(sampleID), file(finalBam) from finalBam_ch\n\t\t\n\t\toutput:\n\t\ttuple val(sampleID), file(\"${sampleID}_CPMnorm.bw\") into bigwig_ch, bigwig2_ch, bigwig3_ch\n\t\tval(sampleID) into labels_ch\n\t\tfile(\"${sampleID}_CPMnorm.bw\") into forGEnrichPlot_ch\n\n\t\t                         \n\t\tscript:\n\t\t\"\"\"\n\t\tsambamba index $finalBam\n\t\tbamCoverage -p $task.cpus --bam ${finalBam} -o ${sampleID}_CPMnorm.bw -bs 10 --extendReads --smoothLength 50 --normalizeUsing CPM --ignoreForNormalization chrX chrY  --skipNonCoveredRegions \n\t\t\"\"\"\n\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/makeBigwig"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": [" process  plotPCA {\n        tag \"Creating bin-based Multi-Bam Summary\"\n        publishDir \"$params.outdir/results\", mode: 'copy'\n\tlabel 'massive_mem'\n    \n        input:\n        path(files) from forPCA_ch.collect()\n        val(name) from params.name\n        val(pcaTitle) from params.PCATitle\n\n        output:\n        file(\"${name}_PCA.png\") into results_ch\n\n\t\t                         \n        script:\n        \"\"\"\n        for i in ${files}\n        do\n          sambamba index \\$i\n        done\n        multiBamSummary bins -p $task.cpus -b $files -o ${name}_matrix.npz --smartLabels --extendReads\n        plotPCA \\\n          -in ${name}_matrix.npz \\\n          -o ${name}_PCA.png \\\n          -T \"$pcaTitle\"\n        \"\"\"\n\t\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/plotPCA"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["michaelbale"], "nb_wf": 1, "list_wf": ["JLab-IPSeqFlow"], "list_contrib": ["michaelbale"], "nb_contrib": 1, "codes": ["\tprocess multiqc {\n\t\tpublishDir \"$params.outdir/results\", mode:'copy'\n\t\tlabel 'small_mem'\n\n\t\tinput:\n\t\tpath('*') from fastqc_ch\n\t\t  .mix(idxstats_ch)\n\t\t  .mix(picardISStats_ch)\n\t\t  .mix(picardDupStats_ch)\n\t\t  .mix(trimmomaticLogs_ch)\n\t\t  .mix(bt2Logs_ch)\n\t\t  .collect()\n\t\t\n\t\toutput:\n\t\tpath('multiqc_report.html')\n\n\t\tscript:\n\t\t\"\"\"\n\t\tmultiqc .\n\t\t\"\"\"\n\t}"], "list_proc": ["michaelbale/JLab-IPSeqFlow/multiqc"], "list_wf_names": ["michaelbale/JLab-IPSeqFlow"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["michellejlin"], "nb_wf": 1, "list_wf": ["covid_swift_pipeline"], "list_contrib": ["michellejlin", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess Trimming { \n    container \"quay.io/biocontainers/trimmomatic:0.35--6\"\n\n                                         \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n        tuple val(base), file(R1), file(R2)                      \n        file ADAPTERS\n    output: \n        tuple val(base), file(\"${base}.trimmed.fastq.gz\"),file(\"${base}_summary.csv\")                   \n        tuple val(base), file(R1),file(R2),file(\"${base}.R1.paired.fastq.gz\"), file(\"${base}.R2.paired.fastq.gz\"),file(\"${base}.R1.unpaired.fastq.gz\"), file(\"${base}.R2.unpaired.fastq.gz\")                    \n        tuple val(base), file(\"${base}.trimmed.fastq.gz\")                    \n\n    publishDir \"${params.OUTDIR}trimmed_fastqs\", mode: 'copy',pattern:'*.trimmed.fastq*'\n\n    script:\n    \"\"\"\n    #!/bin/bash\n\n    trimmomatic PE -threads ${task.cpus} ${R1} ${R2} ${base}.R1.paired.fastq.gz ${base}.R1.unpaired.fastq.gz ${base}.R2.paired.fastq.gz ${base}.R2.unpaired.fastq.gz \\\n    ILLUMINACLIP:${ADAPTERS}:2:30:10:1:true LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:75\n\n    num_r1_untrimmed=\\$(gunzip -c ${R1} | wc -l)\n    num_r2_untrimmed=\\$(gunzip -c ${R2} | wc -l)\n    num_untrimmed=\\$((\\$((num_r1_untrimmed + num_r2_untrimmed))/4))\n\n    num_r1_paired=\\$(gunzip -c ${base}.R1.paired.fastq.gz | wc -l)\n    num_r2_paired=\\$(gunzip -c ${base}.R2.paired.fastq.gz | wc -l)\n    num_paired=\\$((\\$((num_r1_paired + num_r2_paired))/4))\n\n    num_r1_unpaired=\\$(gunzip -c ${base}.R1.unpaired.fastq.gz | wc -l)\n    num_r2_unpaired=\\$(gunzip -c ${base}.R2.unpaired.fastq.gz | wc -l)\n    num_unpaired=\\$((\\$((num_r1_unpaired + num_r2_unpaired))/4))\n\n    num_trimmed=\\$((num_paired + num_unpaired))\n    \n    percent_trimmed=\\$((100-\\$((100*num_trimmed/num_untrimmed))))\n    \n    echo Sample_Name,Raw_Reads,Trimmed_Paired_Reads,Trimmed_Unpaired_Reads,Total_Trimmed_Reads,Percent_Trimmed,Mapped_Reads,Clipped_Mapped_Reads,Mean_Coverage,Spike_Mean_Coverage,Spike_100X_Cov_Percentage,Spike_200X_Cov_Percentage,Lowest_Spike_Cov,Percent_N > ${base}_summary.csv\n    printf \"${base},\\$num_untrimmed,\\$num_paired,\\$num_unpaired,\\$num_trimmed,\\$percent_trimmed\" >> ${base}_summary.csv\n\n    cat *paired.fastq.gz > ${base}.trimmed.fastq.gz\n    \n    \"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/Trimming"], "list_wf_names": ["michellejlin/covid_swift_pipeline"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["michellejlin"], "nb_wf": 1, "list_wf": ["covid_swift_pipeline"], "list_contrib": ["michellejlin", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess Fastqc {\n    container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n\n                                         \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n    tuple val(base), file(R1),file(R2),file(\"${base}.R1.paired.fastq.gz\"), file(\"${base}.R2.paired.fastq.gz\"),file(\"${base}.R1.unpaired.fastq.gz\"), file(\"${base}.R2.unpaired.fastq.gz\")                     \n    output: \n    file(\"*fastqc*\")                  \n\n    publishDir \"${params.OUTDIR}fastqc\", mode: 'copy'\n\n    script:\n    \"\"\"\n    #!/bin/bash\n\n    /usr/local/bin/fastqc ${R1} ${R2} ${base}.R1.paired.fastq.gz ${base}.R2.paired.fastq.gz\n\n    \"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/Fastqc"], "list_wf_names": ["michellejlin/covid_swift_pipeline"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["michellejlin"], "nb_wf": 1, "list_wf": ["covid_swift_pipeline"], "list_contrib": ["michellejlin", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess Trimming_SE { \n    container \"quay.io/biocontainers/trimmomatic:0.35--6\"\n\n                                         \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n        file R1                     \n        file ADAPTERS\n    output: \n        tuple env(base),file(\"*.trimmed.fastq.gz\"),file(\"*summary.csv\")                      \n        tuple env(base),file(\"*.trimmed.fastq.gz\")                       \n        tuple env(base),file(\"*.trimmed.fastq.gz\")                       \n\n    publishDir \"${params.OUTDIR}trimmed_fastqs\", mode: 'copy',pattern:'*.trimmed.fastq*'\n\n    script:\n    \"\"\"\n    #!/bin/bash\n\n    base=`basename ${R1} \".fastq.gz\"`\n\n    echo \\$base\n\n    trimmomatic SE -threads ${task.cpus} ${R1} \\$base.trimmed.fastq.gz \\\n    ILLUMINACLIP:${ADAPTERS}:2:30:10:1:true LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:75\n\n    num_untrimmed=\\$((\\$(gunzip -c ${R1} | wc -l)/4))\n    num_trimmed=\\$((\\$(gunzip -c \\$base'.trimmed.fastq.gz' | wc -l)/4))\n    \n    percent_trimmed=\\$((100-\\$((100*num_trimmed/num_untrimmed))))\n    \n    echo Sample_Name,Raw_Reads,Trimmed_Reads,Percent_Trimmed,Mapped_Reads,Clipped_Mapped_Reads,Mean_Coverage,Spike_Mean_Coverage,Spike_100X_Cov_Percentage,Spike_200X_Cov_Percentage,Lowest_Spike_Cov,Percent_N > \\$base'_summary.csv'\n    printf \"\\$base,\\$num_untrimmed,\\$num_trimmed,\\$percent_trimmed\" >> \\$base'_summary.csv'\n    \n    ls -latr\n    \"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/Trimming_SE"], "list_wf_names": ["michellejlin/covid_swift_pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["michellejlin"], "nb_wf": 1, "list_wf": ["covid_swift_pipeline"], "list_contrib": ["michellejlin", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess NameSorting { \n    container \"quay.io/biocontainers/samtools:1.3--h0592bc0_3\"\n\n                                         \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n        tuple val (base), file(\"${base}.bam\"),file(\"${base}_summary2.csv\")                      \n    output:\n        tuple val (base), file(\"${base}.sorted.sam\"),file(\"${base}_summary2.csv\")                     \n\n    publishDir \"${params.OUTDIR}inprogress_summary\", mode: 'copy', pattern: '*summary.csv'\n\n    script:\n    \"\"\"\n    #!/bin/bash\n    samtools sort -@ ${task.cpus} -n -O sam ${base}.bam > ${base}.sorted.sam\n\n    \"\"\"\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/NameSorting"], "list_wf_names": ["michellejlin/covid_swift_pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BCFtools", "MAFFT", "BEDTools"], "nb_own": 1, "list_own": ["michellejlin"], "nb_wf": 1, "list_wf": ["covid_swift_pipeline"], "list_contrib": ["michellejlin", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess GenerateConsensus {\n    container \"quay.io/greninger-lab/swift-pipeline:latest\"\n\n\t                                     \n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n        tuple val (base), file(BAMFILE),file(INDEX_FILE),file(\"${base}_summary3.csv\"),val(bamsize)                      \n        file REFERENCE_FASTA\n        file TRIM_ENDS\n        file FIX_COVERAGE\n        file VCFUTILS\n        file REFERENCE_FASTA_FAI\n        file SPLITCHR\n    output:\n        file(\"${base}_swift.fasta\")\n        file(\"${base}_bcftools.vcf\")\n        file(INDEX_FILE)\n        file(\"${base}_summary.csv\")\n        tuple val(base), val(bamsize), file(\"${base}_pre_bcftools.vcf\")              \n\n    publishDir params.OUTDIR, mode: 'copy'\n\n    shell:\n    '''\n    #!/bin/bash\n    ls -latr\n\n    R1=!{base}\n\n    echo \"bamsize: !{bamsize}\"\n\n    #if [ -s !{BAMFILE} ]\n    # More reliable way of checking bam size, because of aliases\n    if (( !{bamsize} > 92 ))\n    then\n        # Parallelize pileup based on number of cores\n        splitnum=$(($((29903/!{task.cpus}))+1))\n        perl !{VCFUTILS} splitchr -l $splitnum !{REFERENCE_FASTA_FAI} | \\\\\n        #cat !{SPLITCHR} | \\\\\n            xargs -I {} -n 1 -P !{task.cpus} sh -c \\\\\n                \"/usr/local/miniconda/bin/bcftools mpileup \\\\\n                    -f !{REFERENCE_FASTA} -r {} \\\\\n                    --count-orphans \\\\\n                    --no-BAQ \\\\\n                    --max-depth 50000 \\\\\n                    --max-idepth 500000 \\\\\n                    --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR \\\\\n                !{BAMFILE} | /usr/local/miniconda/bin/bcftools call -A -m -Oz - > tmp.{}.vcf.gz\"\n        \n        # Concatenate parallelized vcfs back together\n        cat *.vcf.gz > \\${R1}_catted.vcf.gz\n\n        # Index and call variants from vcf\n        /usr/local/miniconda/bin/tabix \\${R1}_catted.vcf.gz\n        gunzip \\${R1}_catted.vcf.gz\n        cat \\${R1}_catted.vcf | awk '$1 ~ /^#/ {print $0;next} {print $0 | \"sort -k1,1 -k2,2n\"}' > \\${R1}_pre_bcftools.vcf\n        \n        # Make sure variants are majority variants for consensus calling\n        /usr/local/miniconda/bin/bcftools filter -i '(DP4[0]+DP4[1]) < (DP4[2]+DP4[3]) && ((DP4[2]+DP4[3]) > 0)' --threads !{task.cpus} \\${R1}_pre_bcftools.vcf -o \\${R1}_pre2.vcf\n        /usr/local/miniconda/bin/bcftools filter -e 'IMF < 0.5' \\${R1}_pre2.vcf -o \\${R1}.vcf\n\n        # Index and generate consensus from vcf with majority variants\n        /usr/local/miniconda/bin/bgzip \\${R1}.vcf\n        /usr/local/miniconda/bin/tabix \\${R1}.vcf.gz \n        cat !{REFERENCE_FASTA} | /usr/local/miniconda/bin/bcftools consensus \\${R1}.vcf.gz > \\${R1}.consensus.fa\n\n        # Create coverage file from bam for whole genome, then pipe anything that has less than 6 coverage to bed file,\n        # to be masked later\n        /usr/local/miniconda/bin/bedtools genomecov \\\\\n            -bga \\\\\n            -ibam !{BAMFILE} \\\\\n            -g !{REFERENCE_FASTA} \\\\\n            | awk '\\$4 < 6' | /usr/local/miniconda/bin/bedtools merge > \\${R1}.mask.bed\n        # Get rid of anything outside of the genome we care about, to prevent some sgrnas from screwing with masking\n        awk '{ if(\\$3 > 200 && \\$2 < 29742) {print}}' \\${R1}.mask.bed > a.tmp && mv a.tmp \\${R1}.mask.bed\n\n        # Mask refseq fasta for low coverage areas based on bed file\n        /usr/local/miniconda/bin/bedtools maskfasta \\\\\n            -fi !{REFERENCE_FASTA} \\\\\n            -bed \\${R1}.mask.bed \\\\\n            -fo ref.mask.fasta\n        \n        # Align to Wuhan refseq and unwrap fasta\n        cat ref.mask.fasta \\${R1}.consensus.fa > align_input.fasta\n        /usr/local/miniconda/bin/mafft --auto --thread !{task.cpus} align_input.fasta > repositioned.fasta\n        awk '/^>/ { print (NR==1 ? \"\" : RS) $0; next } { printf \"%s\", $0 } END { printf RS }' repositioned.fasta > repositioned_unwrap.fasta\n        \n        # Trim ends and aligns masking of refseq to our consensus\n        python3 !{TRIM_ENDS} \\${R1}\n\n        # Find percent ns, doesn't work, fix later in python script\n        num_bases=$(grep -v \">\" \\${R1}_swift.fasta | wc | awk '{print $3-$1}')\n        num_ns=$(grep -v \">\" \\${R1}_swift.fasta | awk -F\"n\" '{print NF-1}')\n        percent_n=$(awk -v num_ns=$num_ns -v num_bases=$num_bases 'BEGIN { print ( num_ns * 100 / num_bases ) }')\n        echo \"num_bases=$num_bases\"\n        echo \"num_ns=$num_ns\"\n        echo \"percent_n=$percent_n\"\n        gunzip \\${R1}.vcf.gz\n        mv \\${R1}.vcf \\${R1}_bcftools.vcf\n        #/usr/local/miniconda/bin/samtools view !{BAMFILE} -@ !{task.cpus} | awk -F: '$12 < 600' > \\${R1}'.clipped.cleaned.bam'\n    else\n       echo \"Empty bam detected. Generating empty consensus fasta file...\"\n       # Generate empty stats for empty bam\n       printf '>!{base}\\n' > \\${R1}_swift.fasta\n       printf 'n%.0s' {1..29539} >> \\${R1}_swift.fasta\n       percent_n=100\n       touch \\${R1}_bcftools.vcf\n       touch \\${R1}_pre_bcftools.vcf\n    fi\n    \n    cp \\${R1}_summary3.csv \\${R1}_summary.csv\n    printf \",\\$percent_n\" >> \\${R1}_summary.csv\n\n    cat \\${R1}_summary.csv | tr -d \"[:blank:]\" > a.tmp\n    mv a.tmp \\${R1}_summary.csv\n\n    # Correctly calculates %ns and cleans up the summary file.\n    if [[ !{bamsize} > 92 ]]\n    then\n        python3 !{FIX_COVERAGE} \\${R1}\n        mv \\${R1}_summary_fixed.csv \\${R1}_summary.csv\n    fi\n\n    [ -s \\${R1}_swift.fasta ] || echo \"WARNING: \\${R1} produced blank output. Manual review may be needed.\"\n\n    '''\n}"], "list_proc": ["michellejlin/covid_swift_pipeline/GenerateConsensus"], "list_wf_names": ["michellejlin/covid_swift_pipeline"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["microgenlab"], "nb_wf": 1, "list_wf": ["porefile"], "list_contrib": ["iferres"], "nb_contrib": 1, "codes": ["\nprocess Fastq2Fasta {\n\ttag \"$barcode_id\"\n\tlabel \"small_cpus\"\n\tlabel \"small_mem\"\n\n\tinput:\n\ttuple val(barcode_id), path(\"${barcode_id}.fastq\")\n\n\toutput:\n\ttuple val(barcode_id), path(\"${barcode_id}.fasta\")\n\n\tshell:\n\t\"\"\"\n\tseqtk seq -A ${barcode_id}.fastq > ${barcode_id}.fasta\n\t\"\"\"\n}"], "list_proc": ["microgenlab/porefile/Fastq2Fasta"], "list_wf_names": ["microgenlab/porefile"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["microgenlab"], "nb_wf": 1, "list_wf": ["porefile"], "list_contrib": ["iferres"], "nb_contrib": 1, "codes": ["\nprocess Minimap2 {\n\ttag \"$barcode_id\"\n\tlabel \"big_cpus\"\n\n\tinput:\n\ttuple val(barcode_id), path(\"${barcode_id}.fasta\")\n\tpath(\"silva_k${params.minimap2_k}.mmi\")\n\n\toutput:\n\ttuple val(barcode_id), path(\"${barcode_id}.sam\"), path(\"${barcode_id}.fasta\")\n\n\tshell:\n\t\"\"\"\n\tminimap2 \\\n\t\t-K ${params.minimap2_KM}M \\\n\t\t-t ${task.cpus} \\\n\t\t--secondary=no\\\n\t\t-ax ${params.minimap2_x} \\\n\t\tsilva_k${params.minimap2_k}.mmi \\\n\t\t${barcode_id}.fasta > ${barcode_id}.sam\n\t\"\"\"\n}"], "list_proc": ["microgenlab/porefile/Minimap2"], "list_wf_names": ["microgenlab/porefile"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["microgenlab"], "nb_wf": 1, "list_wf": ["porefile"], "list_contrib": ["iferres"], "nb_contrib": 1, "codes": ["\nprocess MegaBlast {\n\ttag \"$barcode_id\"\n\tlabel \"big_cpus\"\n\n\tinput:\n\ttuple val(barcode_id), path(\"${barcode_id}.fasta\")\n\tpath(\"*\")\n\n\toutput:\n\ttuple val(barcode_id), path(\"${barcode_id}.fasta\"), path(\"${barcode_id}.tab\")\n\n\tshell:\n\t\"\"\"\n\tblastn -task \"megablast\" -evalue ${params.megablast_evalue} -num_threads ${task.cpus} -db silva_SSU_tax -query ${barcode_id}.fasta -out ${barcode_id}.tab -outfmt 6\n\t\"\"\"\n}"], "list_proc": ["microgenlab/porefile/MegaBlast"], "list_wf_names": ["microgenlab/porefile"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess readMapping {\n    cpus 8\n    memory '24 GB'\n       \n                                                                           \n                                                                                                          \n             \n              \n      \n\n    tag { sampleName }\n\n    label 'largecpu'\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${sampleName}.sorted.bam\", mode: 'copy'\n\n    input:\n        tuple(path(schemeRepo), sampleName, path(forward), path(reverse))\n\n    output:\n        tuple(sampleName, path(\"ref.fa\"), path(\"${sampleName}.sorted.bam\"))\n\n    script:\n        \"\"\"\n        ln -s ${schemeRepo}/${params.schemeDir}/${params.scheme}/${params.schemeVersion}/*.reference.fasta ref.fa\n        bwa index ref.fa\n        bwa mem -t ${task.cpus} ref.fa ${forward} ${reverse} | samtools view -bS | \\\n        samtools sort -o ${sampleName}.sorted.bam\n        \"\"\"\n}"], "list_proc": ["mikek498/artic-gls/readMapping"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess trimPrimerSequences {\n    cpus 16\n    memory '60 GB'\n    tag { sampleName }\n\n    publishDir \"${params.outdir}/climb_upload/${params.runPrefix}/${sampleName}\", pattern: \"${sampleName}.mapped.primertrimmed.sorted.bam\", mode: 'copy'\n\n    input:\n        tuple(path(bedfile), sampleName, path(ref), path(bam))\n\n    output:\n        tuple(sampleName, path(\"${sampleName}.mapped.primertrimmed.sorted.bam\"))\n\n    script:\n    if (params.allowNoprimer){\n        ivarCmd = \"ivar trim -e\"\n    } else {\n        ivarCmd = \"ivar trim\"\n    }\n        \"\"\"\n        samtools view -F4 -o ivar.bam ${bam}\n        samtools index ivar.bam\n        ${ivarCmd} -i ivar.bam -b ${bedfile} ${params.illuminaKeepLen} -q ${params.illuminaQualThreshold} -p ivar.out\n        samtools sort -o ${sampleName}.mapped.primertrimmed.sorted.bam ivar.out.bam\n        \"\"\"\n}"], "list_proc": ["mikek498/artic-gls/trimPrimerSequences"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["ARTIC"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess articGather {\n    tag params.runPrefix\n\n    label 'largemem'\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${params.runPrefix}_fastq_pass.fastq\", mode: \"copy\"\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${params.runPrefix}_sequencing_summary.txt\", mode: \"copy\"\n\n    input:\n    file(runDirectory)\n\n    output:\n    tuple file(\"${params.runPrefix}_fastq_pass.fastq\"), file(\"${params.runPrefix}_sequencing_summary.txt\"), emit: gathered\n    path \"${params.runPrefix}_fastq_pass.fastq\", emit: fastq\n\n    script:\n    if ( params.barcode ) \n        \"\"\"\n        artic gather \\\n        --min-length ${params.min_length} \\\n        --max-length ${params.max_length} \\\n        --prefix ${params.runPrefix} \\\n        --directory ${runDirectory}\n\n        cat ${params.runPrefix}_barcode*.fastq ${params.runPrefix}_unclassified.fastq > ${params.runPrefix}_fastq_pass.fastq\n        \"\"\"\n    else\n        \"\"\"\n        artic gather \\\n        --min-length ${params.min_length} \\\n        --max-length ${params.max_length} \\\n        --prefix ${params.runPrefix} \\\n        --directory ${runDirectory}\n        \"\"\"\n\n \n}"], "list_proc": ["mikek498/artic-gls/articGather"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["ARTIC"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess articDemultiplex {\n    tag params.runPrefix\n\n    label 'largecpu'\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${params.runPrefix}_fastq_pass-NB*.fastq\", mode: \"copy\"\n\n    input:\n    tuple file(fastqPass), file(sequencingSummary)\n\n    output:\n    file \"${params.runPrefix}_fastq_pass-NB*.fastq\"\n\n    script:\n    \"\"\"\n    artic demultiplex --threads ${task.cpus} ${fastqPass}\n    \"\"\"\n}"], "list_proc": ["mikek498/artic-gls/articDemultiplex"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["ARTIC"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess articMinIONNanopolish {\n    tag { sampleName }\n\n    label 'largecpu'\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${sampleName}.*\", mode: \"copy\"\n    publishDir \"${params.outdir}/climb_upload/${params.runPrefix}/${sampleName}\", pattern: \"${sampleName}.consensus.fasta\", mode: 'copy'\n\n    input:\n    tuple file(\"nanopolish/*\"), file(bcFastqPass), file(\"nanopolish/*\"), file(schemeRepo), file(runDirectory)\n\n    output:\n    file(\"${sampleName}.*\")\n    tuple sampleName, file(\"${sampleName}.primertrimmed.sorted.bam\"), emit: sorted_bam\n    tuple sampleName, file(\"${sampleName}.consensus.fasta\"), emit: consensus_fasta\n\n    script:\n    if ( bcFastqPass =~ /.*NB\\d{2}.fastq$/ ) {\n        sampleName = params.runPrefix + \"-\" + ( bcFastqPass =~ /.*(NB\\d{2}).fastq$/ )[0][1]\n    } else {\n        sampleName = params.runPrefix\n    }\n\n    if ( params.normalise )\n        if ( params.minimap )\n            \"\"\"\n            ln -s ${runDirectory}/fast5_pass .                       \n\n            artic minion --minimap \\\n            --normalise ${params.normalise} \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            --nanopolish-read-file nanopolish/${params.runPrefix}_fastq_pass.fastq \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n        else \n            \"\"\"\n            ln -s ${runDirectory}/fast5_pass .            \n \n            artic minion \\\n            --normalise ${params.normalise} \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            --nanopolish-read-file nanopolish/${params.runPrefix}_fastq_pass.fastq \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n    else\n        if ( params.minimap )\n            \"\"\"\n            ln -s ${runDirectory}/fast5_pass .\n \n            artic minion --minimap \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            --nanopolish-read-file nanopolish/${params.runPrefix}_fastq_pass.fastq \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n        else\n            \"\"\"\n            ln -s ${runDirectory}/fast5_pass .\n\n            artic minion --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            --nanopolish-read-file nanopolish/${params.runPrefix}_fastq_pass.fastq \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n}"], "list_proc": ["mikek498/artic-gls/articMinIONNanopolish"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["ARTIC"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess articMinIONMedaka {\n    tag { sampleName }\n\n    label 'largecpu'\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"${sampleName}.*\", mode: \"copy\"\n    publishDir \"${params.outdir}/climb_upload/${params.runPrefix}/${sampleName}\", pattern: \"${sampleName}.consensus.fasta\", mode: 'copy'\n\n    input:\n    tuple file(bcFastqPass), file(schemeRepo), file(runDirectory)\n\n    output:\n    file(\"${sampleName}.*\")\n    tuple sampleName, file(\"${sampleName}.primertrimmed.sorted.bam\"), emit: sorted_bam\n    tuple sampleName, file(\"${sampleName}.consensus.fasta\"), emit: consensus_fasta\n    \n    script:\n    if ( bcFastqPass =~ /.*NB\\d{2}.fastq$/ ) {\n        sampleName = params.runPrefix + \"-\" + ( bcFastqPass =~ /.*(NB\\d{2}).fastq$/ )[0][1]\n    } else {\n        sampleName = params.runPrefix\n    }\n\n    if ( params.normalise )\n        if ( params.minimap )\n            \"\"\"\n            artic minion --medaka \\\n            --minimap \\\n            --normalise ${params.normalise} \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n        else\n            \"\"\"\n            artic minion --medaka \\\n            --normalise ${params.normalise} \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n    else\n        if ( params.minimap )\n            \"\"\"\n            artic minion --medaka \\\n            --minimap \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n        else\n            \"\"\"\n            artic minion --medaka \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${schemeRepo}/${params.schemeDir} \\\n            --read-file ${bcFastqPass} \\\n            ${params.scheme}/${params.schemeVersion} \\\n            ${sampleName}\n            \"\"\"\n}"], "list_proc": ["mikek498/artic-gls/articMinIONMedaka"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mikek498"], "nb_wf": 1, "list_wf": ["artic-gls"], "list_contrib": ["mikek498", "m-bull"], "nb_contrib": 2, "codes": ["\nprocess articRemoveUnmappedReads {\n    tag { sampleName }\n\n    publishDir \"${params.outdir}/climb_upload/${params.runPrefix}/${sampleName}\", pattern: \"${sampleName}.mapped.primertrimmed.sorted.bam\", mode: 'copy'\n\n    cpus 1\n\n    input:\n    tuple(sampleName, path(bamfile))\n\n    output:\n    tuple( sampleName, file(\"${sampleName}.mapped.primertrimmed.sorted.bam\"))\n\n    script:\n    \"\"\"\n    samtools view -F4 -o ${sampleName}.mapped.primertrimmed.sorted.bam ${bamfile} \n    \"\"\"\n}"], "list_proc": ["mikek498/artic-gls/articRemoveUnmappedReads"], "list_wf_names": ["mikek498/artic-gls"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["milescsmith"], "nb_wf": 1, "list_wf": ["gcls-rnaseq-nf"], "list_contrib": ["milescsmith", "evanfloden", "pditommaso"], "nb_contrib": 3, "codes": ["\nprocess fastqc {\n    tag \"FASTQC on $sample\"\n    publishDir \"${params.outdir}/qc\", mode: \"copy\", overwrite: true\n\n    input:\n    set sample, file(reads) from qc_read_pairs\n\n    output:\n    file \"*_fastqc.{html,zip}\" into fastqc_channel\n\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} -f fastq -q ${reads}\n    \"\"\"\n}"], "list_proc": ["milescsmith/gcls-rnaseq-nf/fastqc"], "list_wf_names": ["milescsmith/gcls-rnaseq-nf"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["milescsmith"], "nb_wf": 1, "list_wf": ["gcls-rnaseq-nf"], "list_contrib": ["milescsmith", "evanfloden", "pditommaso"], "nb_contrib": 3, "codes": ["\nprocess salmon_align {\n    tag \"Aligning $sample\"\n    publishDir \"${params.outdir}/aligned\", mode: 'copy', overwrite: true\n    machineType 'n1-highmem-8'\n    cpus 8\n\n    input:\n    file salmon_index from salmon_index_ch.collect()\n    val sample from sample_name_channel\n    file reads name \"*.R?.trimmed.fq.gz\" from trimmed_reads_channel\n    \n    output:\n    file \"${sample}\" into aligned_channel\n\n    script:\n    \"\"\"\n    salmon quant \\\n        -l A \\\n        -p ${task.cpus} \\\n        -i ${salmon_index} \\\n        --seqBias \\\n        --gcBias \\\n        --validateMappings \\\n        --recoverOrphans \\\n        -1 <(gunzip -c ${reads[0]}) \\\n        -2 <(gunzip -c ${reads[1]}) \\\n        -o ${sample}\n    \"\"\"\n}"], "list_proc": ["milescsmith/gcls-rnaseq-nf/salmon_align"], "list_wf_names": ["milescsmith/gcls-rnaseq-nf"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["milescsmith"], "nb_wf": 1, "list_wf": ["gcls-rnaseq-nf"], "list_contrib": ["milescsmith", "evanfloden", "pditommaso"], "nb_contrib": 3, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    file \"${sample}\" from aligned_channel.collect()\n    file \"*_fastqc.zip\" from fastqc_channel.collect()\n    file \"*.trimmed.fq.gz\" from trimmed_reads_qc_channel.collect()\n    file \"* .waste.fq.gz\" from waste_channel.collect()\n    file \"${sample}.contamination.csv\" from contamination_channel.collect()\n\n    output:\n    file('multiqc_report.html') optional true\n\n    script:\n    \"\"\"multiqc -m bcl2fastq -m fastqc -m bbmap -m salmon -ip -v .\n    \"\"\"\n\n}"], "list_proc": ["milescsmith/gcls-rnaseq-nf/multiqc"], "list_wf_names": ["milescsmith/gcls-rnaseq-nf"]}, {"nb_reuse": 1, "tools": ["USEARCH"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess merge {\n\tpublishDir \"${workingdir}/output_${params.marker}_1.0_merged\", mode: \"copy\", overwrite: true\t\n\n\tinput:\n\tset sample_id, file (rawForward), file (rawReverse) from read_pairs\n\n\toutput:\n\tset sample_id, file (\"${sample_id}_merged.fastq\") into (merged_c1, merged_c2)\n\n\tscript:\n\t\"\"\"\n\tusearch -threads ${params.threads} -fastq_mergepairs $rawForward -fastqout ${sample_id}_merged.fastq\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/merge"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess fastqc_merged {\n\tpublishDir \"${workingdir}/output_${params.marker}_1.1_fastqc_merged\", mode: \"copy\", overwrite: true\n\t\n\tinput:\n\tset sample_id, file (in_merged_c1) from merged_c1\n\n\toutput:\n\tfile (\"${sample_id}_fastqc_merged\") into fastqc_merged\n\t\n\t\"\"\"\n\tmkdir ${sample_id}_fastqc_merged\n\tfastqc -t ${params.threads} -f fastq -q --casava $in_merged_c1 -o ${sample_id}_fastqc_merged\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/fastqc_merged"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess multiqc_merged {\n\tpublishDir \"${workingdir}/output_${params.marker}_1.2_multiqc_merged\", mode: \"copy\", overwrite: true\n\t\n\tinput:\n\tfile (\"*\") from fastqc_merged.collect().ifEmpty([])\n\n\toutput:\n\tfile (\"multiqc_report.html\")\n\tfile \"multiqc_data\"\t\n\n\t\"\"\"\n\tmultiqc .\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/multiqc_merged"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess cutadapters {\n\tpublishDir \"${workingdir}/output_${params.marker}_2.0_cutadatpers\", mode: \"copy\", overwrite: true\t\n\n\tinput:\n\tset sample_id, file (in_merged_c2) from merged_c2\n\n\toutput:\n\tset sample_id, file (\"${sample_id}_adapters_trimmed.fastq\") into (adapters_trimmed_c1, adapters_trimmed_c2)\n\tset sample_id, file (\"${sample_id}_adapters_untrimmed.fastq\")\n\t\n\t\"\"\"\n\tcutadapt -a file:$PWD/adapters_${params.marker}/${sample_id}_adapters.fasta -o ${sample_id}_adapters_trimmed.fastq --untrimmed-o ${sample_id}_adapters_untrimmed.fastq $in_merged_c2\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/cutadapters"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 2, "tools": ["FastQC", "seqtk", "G-BLASTN"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess blastn_16S {\n\tpublishDir \"${workingdir}/output_${params.marker}_10.0_blastn\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tfile (\"zotus_lulu_id.txt\") from zotus_lulu_16S\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep3\n\t\n\toutput:\n\tfile (\"zotus_lulu_blastn_${params.marker}.txt\")\t\n\t\n\twhen:\n\tparams.skip\n\n\t\"\"\"\n\tgrep \"Zotu\" zotus_lulu_id.txt | sed 's/^.*Z/Z/' | sed 's/\\\"//' > lulu_ids.txt\n\tseqtk subseq zotus_${params.marker}.fasta lulu_ids.txt > zotus_lulu.fasta\n\tblastn -db $PWD/database/Coleoptera_COI_db -query zotus_lulu.fasta -evalue 1e-4 -outfmt 5 -max_target_seqs 50 -out zotus_lulu_blastn_${params.marker}.txt -num_threads ${params.threads}\n\t\"\"\"\n}", "\nprocess fastqc_cutadapters {\n        publishDir \"${workingdir}/output_${params.marker}_2.1_fastqc_cutadapters\", mode: \"copy\", overwrite: true\n\n        input:\n        set sample_id, file (in_adapters_trimmed_c1) from adapters_trimmed_c1\n\n        output:\n        file (\"${sample_id}_fastqc_cutadapters\") into fastqc_cutadapters\n\n        \"\"\"\n        mkdir ${sample_id}_fastqc_cutadapters\n        fastqc -t ${params.threads} -f fastq -q --casava $in_adapters_trimmed_c1 -o ${sample_id}_fastqc_cutadapters\n        \"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/blastn_16S", "mingxinliu/nfp4MBC/fastqc_cutadapters"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 3, "tools": ["FastQC", "seqtk", "MultiQC", "G-BLASTN"], "nb_own": 2, "list_own": ["mmcogle", "mingxinliu"], "nb_wf": 2, "list_wf": ["GEMmakerCam", "nfp4MBC"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "mingxinliu", "saioruganti"], "nb_contrib": 7, "codes": ["\nprocess fastqc_1 {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: \"*_fastqc.*\"\n  tag { sample_id }\n  label \"fastqc\"\n\n  input:\n    set val(sample_id), file(pass_files) from COMBINED_SAMPLES_FOR_FASTQC_1\n\n  output:\n    set file(\"${sample_id}_?_fastqc.html\") , file(\"${sample_id}_?_fastqc.zip\") optional true into FASTQC_1_OUTPUT\n    set val(sample_id), val(1) into CLEAN_MERGED_FASTQ_FASTQC_SIGNAL\n\n  script:\n  \"\"\"\n  fastqc $pass_files\n  \"\"\"\n}", "\nprocess commonzotus_CO1 {\n\tpublishDir \"${workingdir}/output_${params.marker}_11.0_commonzotus\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep5\n\tfile (\"zotus_lulu_${params.marker}.txt\") from zotus_lulu_CO1\n\tfile (\"zotus_${params.marker}_codon.fasta\") from zotus_codon\n\n\toutput:\n\tfile (\"zotus_lulu_${params.marker}.fasta\") into zotus_lulu\n\tfile (\"zotus_common_${params.marker}.fasta\") into zotus_common\n\tfile (\"zotus_common_blastn_${params.marker}.txt\")\n\n\twhen:\n\t!params.skip\n\n\t\"\"\"\n\tgrep \"Zotu\" zotus_lulu_${params.marker}.txt | sed 's/^.*Z/Z/' | sed 's/\\\"//' > lulu_ids.txt\n\tseqtk subseq zotus_${params.marker}.fasta lulu_ids.txt > zotus_lulu_${params.marker}.fasta\n\tseqkit common zotus_lulu_${params.marker}.fasta zotus_${params.marker}_codon.fasta -o zotus_common_${params.marker}.fasta\n\tblastn -db $PWD/database/Coleoptera_COI_db -query zotus_common_${params.marker}.fasta -evalue 1e-4 -outfmt 5 -max_target_seqs 50 -out zotus_common_blastn_${params.marker}.txt -num_threads ${params.threads}\n\t\"\"\"\n}", "\nprocess multiqc_cutadapters {\n        publishDir \"${workingdir}/output_${params.marker}_2.2_multiqc_cutadapters\", mode: \"copy\", overwrite: true\n\n        input:\n        file (\"*\") from fastqc_cutadapters.collect().ifEmpty([])\n\n        output:\n        file (\"multiqc_report.html\")\n\t   file \"multiqc_data\"\n\n        \"\"\"\n        multiqc .\n        \"\"\"\n}"], "list_proc": ["mmcogle/GEMmakerCam/fastqc_1", "mingxinliu/nfp4MBC/commonzotus_CO1", "mingxinliu/nfp4MBC/multiqc_cutadapters"], "list_wf_names": ["mingxinliu/nfp4MBC", "mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess cutprimers {\n\tpublishDir \"${workingdir}/output_${params.marker}_3.0_cutprimers\", mode: \"copy\", overwirte: true\n\n\tinput:\n\tset sample_id, file (in_adapters_trimmed_c2) from adapters_trimmed_c2\n\n\toutput:\n\tset sample_id, file (\"${sample_id}_primers_trimmed.fastq\") into (primers_trimmed_c1, primers_trimmed_c2)\n\tset sample_id, file (\"${sample_id}_primers_untrimmed.fastq\")\n\n\tscript:\n\tif (params.marker == \"CO1\")\n\t\t\"\"\"\n\t\tcutadapt --no-indels -M ${params.M} -m ${params.m} -a \"AGATATTGGAACWTTATATTTTATTTTTGG;max_error_rate=0.04;min_overlap=29\"...\"GGAGGATTTGGWAATTGATTAGTW\\$;max_error_rate=0.1;min_overlap=22\" -o ${sample_id}_primers_trimmed.fastq --untrimmed-o ${sample_id}_primers_untrimmed.fastq $in_adapters_trimmed_c2\n\t\t\"\"\"\n\n\telse if ( params.marker == \"16S\")\n\t\t\"\"\"\n\t\tcutadapt --no-indels -M ${params.M} -m ${params.m} -a \"AGACGAGAAGACCCTATAGA;max_error_rate=0.05;min_overlap=19\"...\"TACCTTAGGGATAACAGCGTA\\$;max_error_rate=0.05;min_overlap=20;\" -o ${sample_id}_primers_trimmed.fastq --untrimmed-o ${sample_id}_primers_untrimmed.fastq $in_adapters_trimmed_c2\n\t\t\"\"\"\n\n\telse\n\t\terror \"Invalid marker!\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/cutprimers"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess fastqc_cutprimers {\n        publishDir \"${workingdir}/output_${params.marker}_3.1_fastqc_cutprimers\", mode: \"copy\", overwrite: true\n\n        input:\n        set sample_id, file (in_primers_trimmed_c1) from primers_trimmed_c1\n\n        output:\n        file (\"${sample_id}_fastqc_cutprimers\") into fastqc_cutprimers\n\n        \"\"\"\n        mkdir ${sample_id}_fastqc_cutprimers\n        fastqc -t ${params.threads} -f fastq -q --casava $in_primers_trimmed_c1 -o ${sample_id}_fastqc_cutprimers\n        \"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/fastqc_cutprimers"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 2, "tools": ["SAMtools", "MultiQC"], "nb_own": 2, "list_own": ["mmcogle", "mingxinliu"], "nb_wf": 2, "list_wf": ["nfp4MBC", "GEMmakerCam"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "mingxinliu", "saioruganti"], "nb_contrib": 7, "codes": ["\nprocess multiqc_cutprimers {\n        publishDir \"${workingdir}/output_${params.marker}_3.2_multiqc_cutprimers\", mode: \"copy\", overwrite: true\n\n        input:\n        file (\"*\") from fastqc_cutprimers.collect().ifEmpty([])\n\n        output:\n        file (\"multiqc_report.html\")\n\tfile \"multiqc_data\"\n\n        \"\"\"\n        multiqc .\n        \"\"\"\n}", "\nprocess samtools_index {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: publish_pattern_samtools_index\n  tag { sample_id }\n  label \"samtools\"\n\n  input:\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam\") from SORTED_FOR_INDEX\n\n  output:\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam\") into BAM_INDEXED_FOR_STRINGTIE\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam.bai\") into BAI_INDEXED_FILE\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam.log\") into BAM_INDEXED_LOG\n\n  script:\n    \"\"\"\n    samtools index ${sample_id}_vs_${params.input.reference_name}.bam\n    samtools stats ${sample_id}_vs_${params.input.reference_name}.bam > ${sample_id}_vs_${params.input.reference_name}.bam.log\n    \"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/multiqc_cutprimers", "mmcogle/GEMmakerCam/samtools_index"], "list_wf_names": ["mingxinliu/nfp4MBC", "mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["USEARCH"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess filter {\n\tpublishDir \"${workingdir}/output_${params.marker}_4.0_filtered\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tset sample_id, file (in_primers_trimmed_c2) from primers_trimmed_c2\n\n\toutput:\n\tfile (\"${sample_id}_filtered.fasta\") into (filtered_fasta_c1, filtered_fasta_c2, filtered_fasta_c3)\n\tset sample_id, file (\"${sample_id}_filtered.fastq\") into filtered_fastq_c1\n\t\n\t\"\"\"\n\tusearch -threads ${params.threads} -fastq_filter $in_primers_trimmed_c2 -fastq_maxee 1.0 -fastaout ${sample_id}_filtered.fasta -fastqout ${sample_id}_filtered.fastq -relabel @\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/filter"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess fastqc_filtered {\n        publishDir \"${workingdir}/output_${params.marker}_4.1_fastqc_filtered\", mode: \"copy\", overwrite: true\n\n        input:\n        set sample_id, file (in_filtered_fastq_c1) from filtered_fastq_c1\n\n        output:\n        file (\"${sample_id}_fastqc_filtered\") into fastqc_filtered\n\n        \"\"\"\n        mkdir ${sample_id}_fastqc_filtered\n        fastqc -t ${params.threads} -f fastq -q --casava $in_filtered_fastq_c1 -o ${sample_id}_fastqc_filtered\n        \"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/fastqc_filtered"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess multiqc_filtered {\n        publishDir \"${workingdir}/output_${params.marker}_4.2_multiqc_filtered\", mode: \"copy\", overwrite: true\n\n        input:\n        file (\"*\") from fastqc_filtered.collect().ifEmpty([])\n\n        output:\n        file (\"multiqc_report.html\")\n\tfile \"multiqc_data\"\n\n        \"\"\"\n        multiqc .\n        \"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/multiqc_filtered"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["USEARCH"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess uniques {\n\tpublishDir \"${workingdir}/output_${params.marker}_5.0_uniques\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tfile (\"*.filtered.fasta\") from filtered_fasta_c1.toList()\n\n\toutput:\n\tfile (\"uniques_${params.marker}.fasta\") into (all_uniques)\n\t\n\t\"\"\"\n\tcat *.filtered.fasta > all_filtered.fasta\n\tusearch -fastx_uniques all_filtered.fasta -fastaout uniques_${params.marker}.fasta -relabel Uniq -sizeout -minuniquesize 2 -threads ${params.threads}\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/uniques"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["USEARCH"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess cluster {\n\tpublishDir \"${workingdir}/output_${params.marker}_6.0_clustered\", mode: \"copy\", overwrite: true\n\t\n\tinput:\n\tfile (\"uniques_${params.marker}.fasta\") from all_uniques\n\n\toutput:\n\tfile (\"zotus_${params.marker}.fasta\") into (zotus_rep1, zotus_rep2, zotus_rep3, zotus_rep4, zotus_rep5)\n\n\tscript:\n\tif(params.cluster == 'usearch')\n\t\"\"\"\n\tusearch -unoise3 uniques_${params.marker}.fasta -zotus zotus_${params.marker}.fasta -tabbedout zotus_${params.marker}.txt\n\t\"\"\"\n\n\telse if(params.cluster == 'vsearch')\n\t\"\"\"\n\tvsearch --cluster_unoise uniques_${params.marker}.fasta --centroids zotus_${params.marker}.fasta  --threads ${params.threads}\n\t\"\"\"\n\n\telse\n\terror \"Invalid clustring mode: ${mode}\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/cluster"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 2, "tools": ["BWA", "USEARCH"], "nb_own": 2, "list_own": ["mingxinliu", "mpieva"], "nb_wf": 2, "list_wf": ["nfp4MBC", "quicksand-build"], "list_contrib": ["merszym", "mingxinliu"], "nb_contrib": 2, "codes": ["\nprocess indexFasta{\n    publishDir \"${params.outdir}/genomes/${family}/\", mode: 'copy'\n    tag \"$family:$species\"\n    \n    input:\n        set family, species, \"${species}.fasta\" from for_bwa\n\n    output:\n        file \"${species}.fasta.*\"\n   \n    script:\n        \"\"\"\n        bwa index \"${species}.fasta\"\n        \"\"\"\n}", "\nprocess zotutab_CO1 {\n\tpublishDir \"${workingdir}/output_${params.marker}_7.0_otutab\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tfile (\"*.filtered.fasta\") from filtered_fasta_c2.toList()\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep1\n\n\toutput:\n\tfile (\"zotutab_${params.marker}.txt\") into zotutab_CO1\n\n\twhen:\n\t!params.skip\t\n\n\t\"\"\"\n\tcat *.filtered.fasta > all_filtered.fasta\n\tusearch -otutab all_filtered.fasta -sample_delim . -zotus zotus_${params.marker}.fasta -id 0.97 -otutabout zotutab_${params.marker}.txt -threads ${params.threads}\n\t\"\"\"\n}"], "list_proc": ["mpieva/quicksand-build/indexFasta", "mingxinliu/nfp4MBC/zotutab_CO1"], "list_wf_names": ["mingxinliu/nfp4MBC", "mpieva/quicksand-build"]}, {"nb_reuse": 1, "tools": ["USEARCH"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess zotutab_16S {\n\tpublishDir \"${workingdir}/output_${params.marker}_7.0_otutab\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tfile (\"*.filtered.fasta\") from filtered_fasta_c3.toList()\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep1\n\n\toutput:\n\tfile (\"zotutab_${params.marker}.txt\") into zotutab_16S\n\n\twhen:\n\tparams.skip\t\n\n\t\"\"\"\n\tcat *.filtered.fasta > all_filtered.fasta\n\tusearch -otutab all_filtered.fasta -sample_delim . -zotus zotus_${params.marker}.fasta -id 0.97 -otutabout zotutab_${params.marker}.txt -threads ${params.threads}\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/zotutab_16S"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 2, "tools": ["SAMtools", "G-BLASTN"], "nb_own": 2, "list_own": ["mingxinliu", "mpieva"], "nb_wf": 2, "list_wf": ["quicksand", "nfp4MBC"], "list_contrib": ["merszym", "mingxinliu", "wjv"], "nb_contrib": 3, "codes": ["\nprocess matchlist_CO1 {\n\tpublishDir \"${workingdir}/output_${params.marker}_8.0_matchlist\", mode: \"copy\"\n\n\tinput:\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep2\n\n\toutput:\n\tfile (\"matchlist_${params.marker}.txt\") into matchlist_CO1\n\n\twhen:\n\t!params.skip\n\n\t\"\"\"\n\tmakeblastdb -in zotus_${params.marker}.fasta -parse_seqids -dbtype nucl\t\n\tblastn -db zotus_${params.marker}.fasta -outfmt '6 qseqid sseqid pident' -out matchlist_${params.marker}.txt -qcov_hsp_perc 80 -perc_identity 84 -query zotus_${params.marker}.fasta\n\t\"\"\"\n}", "\nprocess filterLength {\n    tag \"$rg\"\n\n    input:\n    set rg, 'input.bam' from post_filter_unmapped\n\n    output:\n    set rg, 'output.bam', stdout into tofasta_in\n    set rg, 'output.bam' into for_extraction\n    set rg, stdout into filtercounts\n\n    script:\n    \"\"\"\n    bam-lengthfilter -c $params.cutoff -l $params.level -o output.bam input.bam\n    samtools view -c output.bam\n    \"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/matchlist_CO1", "mpieva/quicksand/filterLength"], "list_wf_names": ["mingxinliu/nfp4MBC", "mpieva/quicksand"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess matchlist_16S {\n\tpublishDir \"${workingdir}/output_${params.marker}_8.0_matchlist\", mode: \"copy\"\n\n\tinput:\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep2\n\n\toutput:\n\tfile (\"matchlist_${params.marker}.txt\") into matchlist_16S\n\n\twhen:\n\tparams.skip\n\n\t\"\"\"\n\tmakeblastdb -in zotus_${params.marker}.fasta -parse_seqids -dbtype nucl\t\n\tblastn -db zotus_${params.marker}.fasta -outfmt '6 qseqid sseqid pident' -out matchlist_${params.marker}.txt -qcov_hsp_perc 80 -perc_identity 84 -query zotus_${params.marker}.fasta\n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/matchlist_16S"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["mingxinliu"], "nb_wf": 1, "list_wf": ["nfp4MBC"], "list_contrib": ["mingxinliu"], "nb_contrib": 1, "codes": ["\nprocess nostopcodon_CO1 {\n\tpublishDir \"${workingdir}/output_${params.marker}_10.2_nostopcodon\", mode: \"copy\", overwrite: true\n\n\tinput:\n\tfile (\"zotus_${params.marker}.fasta\") from zotus_rep4\n\tfile (\"*.txt\") from header.toList()\n\t\t\n\toutput:\n\tfile (\"zotus_${params.marker}_codon.fasta\") into zotus_codon\n\n\twhen:\n\t!params.skip\n\n\t\"\"\"\n\tcat *.txt > names.txt\n\tseqtk subseq zotus_${params.marker}.fasta names.txt > subseq.fasta\t\t\n\tseqkit rmdup -n subseq.fasta -o zotus_${params.marker}_codon.fasta \n\t\"\"\"\n}"], "list_proc": ["mingxinliu/nfp4MBC/nostopcodon_CO1"], "list_wf_names": ["mingxinliu/nfp4MBC"]}, {"nb_reuse": 1, "tools": ["Salmon", "HISAT2", "StringTie", "SAMtools", "kallisto", "FastQC"], "nb_own": 1, "list_own": ["mmcogle"], "nb_wf": 1, "list_wf": ["GEMmakerCam"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "saioruganti"], "nb_contrib": 6, "codes": ["\nprocess process_sample {\n  tag { sample_id }\n  label \"gemmaker\"\n  label \"multithreaded\"\n  label \"retry_ignore\"\n  publishDir params.output.sample_dir, mode: params.output.publish_mode\n\n  input:\n    set val(sample_id), val(type), val(remote_ids), val(local_files) from ALL_SAMPLES\n    file fasta_adapter from FASTA_ADAPTER\n    file indexes from INDEXES\n    file gtf_file from GTF_FILE\n\n  output:\n    val(sample_id) into COMPLETED_SAMPLES\n    file(\"*.sra\") optional true into SRA_FILES\n    file(\"*.fastq\") optional true into FASTQ_FILES\n    file(\"*fastqc.*\") optional true into FASTQC_FILES\n    file(\"*.log\") optional true into LOG_FILES\n    file(\"*.sam\") optional true into SAM_FILES\n    file(\"*.bam\") optional true into BAM_FILES\n    file(\"*.bam.bai\") optional true into BAI_FILES\n    file(\"*.ga\") optional true into GA_FILES\n    file(\"*.gtf\") optional true into GTF_FILES\n    file(\"*.raw\") optional true into RAW_FILES\n    file(\"*.fpkm\") optional true into FPKM_FILES\n    file(\"*.tpm\") optional true into TPM_FILES\n\n  script:\n  \"\"\"\n  # for remote samples, prepare FASTQ files from NCBI\n  if [[ \"${type}\" == \"remote\" ]]; then\n    # download SRA files from NCBI\n    SRR_IDS=\"${remote_ids.join(' ')}\"\n\n    for id in \\$SRR_IDS; do\n      ascp_path=`which ascp`\n      prefetch -v --max-size 50G --output-directory . --ascp-path \"\\$ascp_path|\\$ASPERA_KEY\" --ascp-options \"-k 1 -T -l 1000m\" \\$id\n    done\n\n    # extract FASTQ files from SRA files\n    SRA_FILES=\\$(ls *.sra)\n\n    for sra_file in \\$SRA_FILES; do\n      fastq-dump --split-files \\$sra_file\n    done\n\n    # remove SRA files if they will not be published\n    if [[ ${params.output.publish_sra} == false ]]; then\n      rm -f \\$SRA_FILES\n    fi\n\n    # merge the FASTQ files from each run in the experiment\n    DOWNLOADED_FASTQ_FILES=\\$(ls *.fastq)\n\n    if ls *_1.fastq >/dev/null 2>&1; then\n      cat *_1.fastq >> \"${sample_id}_1.fastq\"\n    fi\n\n    if ls *_2.fastq >/dev/null 2>&1; then\n      cat *_2.fastq >> \"${sample_id}_2.fastq\"\n    fi\n\n    # remove downloaded FASTQ files if they will not be published\n    if [[ ${params.output.publish_downloaded_fastq} == false ]]; then\n      rm -f \\$DOWNLOADED_FASTQ_FILES\n    fi\n\n  # for local samples, fetch FASTQ files from filesystem\n  elif [[ \"${type}\" == \"local\" ]]; then\n    cp ${local_files.join(' ')} .\n  fi\n\n  # perform fastqc on raw FASTQ files\n  MERGED_FASTQ_FILES=\\$(ls ${sample_id}_?.fastq)\n\n  fastqc \\$MERGED_FASTQ_FILES\n\n  # use hisat2 for alignment\n  if [[ ${params.input.hisat2.enable} == \"true\" ]]; then\n    # perform trimmomatic on all fastq files\n    # This script calculates average length of fastq files.\n    total=0\n\n    # This if statement checks if the data is single or paired data, and checks length accordingly\n    # This script returns 1 number, which can be used for the minlen in trimmomatic\n    if [ -e ${sample_id}_1.fastq ] && [ -e ${sample_id}_2.fastq ]; then\n      for fastq in ${sample_id}_1.fastq ${sample_id}_2.fastq; do\n        a=`awk 'NR%4 == 2 {lengths[length(\\$0)]++} END {for (l in lengths) {print l, lengths[l]}}' \\$fastq \\\n        | sort \\\n        | awk '{ print \\$0, \\$1*\\$2}' \\\n        | awk '{ SUM += \\$3 } { SUM2 += \\$2 } END { printf(\"%.0f\", SUM / SUM2 * ${params.software.trimmomatic.MINLEN})} '`\n      total=(\\$a + \\$total)\n      done\n      total=( \\$total / 2 )\n      minlen=\\$total\n\n    elif [ -e ${sample_id}_1.fastq ]; then\n      minlen=`awk 'NR%4 == 2 {lengths[length(\\$0)]++} END {for (l in lengths) {print l, lengths[l]}}' ${sample_id}_1.fastq \\\n        | sort \\\n        | awk '{ print \\$0, \\$1*\\$2}' \\\n        | awk '{ SUM += \\$3 } { SUM2 += \\$2 } END { printf(\"%.0f\", SUM / SUM2 * ${params.software.trimmomatic.MINLEN})} '`\n    fi\n\n    if [ -e ${sample_id}_1.fastq ] && [ -e ${sample_id}_2.fastq ]; then\n      java -Xmx512m org.usadellab.trimmomatic.Trimmomatic \\\n        PE \\\n        -threads ${task.cpus} \\\n        ${params.software.trimmomatic.quality} \\\n        ${sample_id}_1.fastq \\\n        ${sample_id}_2.fastq \\\n        ${sample_id}_1p_trim.fastq \\\n        ${sample_id}_1u_trim.fastq \\\n        ${sample_id}_2p_trim.fastq \\\n        ${sample_id}_2u_trim.fastq \\\n        ILLUMINACLIP:${params.software.trimmomatic.clip_path}:2:40:15 \\\n        LEADING:${params.software.trimmomatic.LEADING} \\\n        TRAILING:${params.software.trimmomatic.TRAILING} \\\n        SLIDINGWINDOW:${params.software.trimmomatic.SLIDINGWINDOW} \\\n        MINLEN:\"\\$minlen\" > ${sample_id}.trim.log 2>&1\n    else\n      # For ease of the next steps, rename the reverse file to the forward.\n      # since these are non-paired it really shouldn't matter.\n      if [ -e ${sample_id}_2.fastq ]; then\n        mv ${sample_id}_2.fastq ${sample_id}_1.fastq\n      fi\n      # Now run trimmomatic\n      java -Xmx512m org.usadellab.trimmomatic.Trimmomatic \\\n        SE \\\n        -threads ${task.cpus} \\\n        ${params.software.trimmomatic.quality} \\\n        ${sample_id}_1.fastq \\\n        ${sample_id}_1u_trim.fastq \\\n        ILLUMINACLIP:${fasta_adapter}:2:40:15 \\\n        LEADING:${params.software.trimmomatic.LEADING} \\\n        TRAILING:${params.software.trimmomatic.TRAILING} \\\n        SLIDINGWINDOW:${params.software.trimmomatic.SLIDINGWINDOW} \\\n        MINLEN:\"\\$minlen\" > ${sample_id}.trim.log 2>&1\n    fi\n\n    # remove merged fastq files if they will not be published\n    if [[ ${params.output.publish_downloaded_fastq} == false ]]; then\n      rm -f \\$MERGED_FASTQ_FILES\n    fi\n\n    # perform fastqc on all trimmed fastq files\n    TRIMMED_FASTQ_FILES=\\$(ls ${sample_id}_*trim.fastq)\n\n    fastqc \\$TRIMMED_FASTQ_FILES\n\n    # perform hisat2 alignment of fastq files to a genome reference\n    if [ -e ${sample_id}_2p_trim.fastq ]; then\n      hisat2 \\\n        -x ${params.input.reference_name} \\\n        --no-spliced-alignment \\\n        -q \\\n        -1 ${sample_id}_1p_trim.fastq \\\n        -2 ${sample_id}_2p_trim.fastq \\\n        -U ${sample_id}_1u_trim.fastq,${sample_id}_2u_trim.fastq \\\n        -S ${sample_id}_vs_${params.input.reference_name}.sam \\\n        -t \\\n        -p ${task.cpus} \\\n        --un ${sample_id}_un.fastq \\\n        --dta-cufflinks \\\n        --new-summary \\\n        --summary-file ${sample_id}_vs_${params.input.reference_name}.sam.log\n    else\n      hisat2 \\\n        -x ${params.input.reference_name} \\\n        --no-spliced-alignment \\\n        -q \\\n        -U ${sample_id}_1u_trim.fastq \\\n        -S ${sample_id}_vs_${params.input.reference_name}.sam \\\n        -t \\\n        -p ${task.cpus} \\\n        --un ${sample_id}_un.fastq \\\n        --dta-cufflinks \\\n        --new-summary \\\n        --summary-file ${sample_id}_vs_${params.input.reference_name}.sam.log\n    fi\n\n    rm -f ${sample_id}_un.fastq\n\n    # remove trimmed fastq files if they will not be published\n    if [[ ${params.output.publish_trimmed_fastq} == false ]]; then\n      rm -f \\$TRIMMED_FASTQ_FILES\n    fi\n\n    # sort the SAM alignment file and convert it to BAM\n    samtools sort \\\n      -o ${sample_id}_vs_${params.input.reference_name}.bam \\\n      -O bam \\\n      -T temp \\\n      ${sample_id}_vs_${params.input.reference_name}.sam\n\n    # remove SAM file as it will not be published\n    rm -f *.sam\n\n    # index BAM alignment file\n    samtools index ${sample_id}_vs_${params.input.reference_name}.bam\n    samtools stats ${sample_id}_vs_${params.input.reference_name}.bam > ${sample_id}_vs_${params.input.reference_name}.bam.log\n\n    # generate expression-level transcript abundance\n    stringtie \\\n      -v \\\n      -p ${task.cpus} \\\n      -e \\\n      -o ${sample_id}_vs_${params.input.reference_name}.Hisat2.gtf \\\n      -G ${gtf_file} \\\n      -A ${sample_id}_vs_${params.input.reference_name}.Hisat2.ga \\\n      -l ${sample_id} ${sample_id}_vs_${params.input.reference_name}.bam\n\n    # remove BAM file if it will not be published\n    if [[ ${params.output.publish_bam} == false ]]; then\n      rm -f *.bam\n      rm -f *.bam.bai\n    fi\n\n    # generate raw counts from hisat2/stringtie\n    # Run the prepDE.py script provided by stringtie to get the raw counts.\n    echo \"${sample_id}\\t./${sample_id}_vs_${params.input.reference_name}.Hisat2.gtf\" > gtf_files\n    prepDE.py -i gtf_files -g ${sample_id}_vs_${params.input.reference_name}.raw.pre\n\n    # Reformat the raw file to be the same as the TPM/FKPM files.\n    cat ${sample_id}_vs_${params.input.reference_name}.raw.pre | \\\n      grep -v gene_id | \\\n      perl -pi -e \"s/,/\\\\t/g\" > ${sample_id}_vs_${params.input.reference_name}.Hisat2.raw\n\n    # generate the final FPKM and TPM files\n    if [[ ${params.output.publish_fpkm} == true ]]; then\n      awk -F\"\\t\" '{if (NR!=1) {print \\$1, \\$8}}' OFS='\\t' ${sample_id}_vs_${params.input.reference_name}.Hisat2.ga > ${sample_id}_vs_${params.input.reference_name}.Hisat2.fpkm\n    fi\n\n    if [[ ${params.output.publish_tpm} == true ]]; then\n      awk -F\"\\t\" '{if (NR!=1) {print \\$1, \\$9}}' OFS='\\t' ${sample_id}_vs_${params.input.reference_name}.Hisat2.ga > ${sample_id}_vs_${params.input.reference_name}.Hisat2.tpm\n    fi\n\n    if [[ ${params.output.publish_stringtie_gtf_and_ga} == false ]]; then\n      rm -rf *.ga\n      rm -rf *.gtf\n    fi\n\n  # or use kallisto\n  elif [[ ${params.input.kallisto.enable} == \"true\" ]]; then\n    # perform Kallisto alignment of fastq files\n    if [ -e ${sample_id}_2.fastq ]; then\n      kallisto quant \\\n        -i  ${indexes} \\\n        -o ${sample_id}_vs_${params.input.reference_name}.Kallisto.ga \\\n        ${sample_id}_1.fastq \\\n        ${sample_id}_2.fastq > ${sample_id}.kallisto.log 2>&1\n    else\n      kallisto quant \\\n        --single \\\n        -l 70 \\\n        -s .0000001 \\\n        -i ${indexes} \\\n        -o ${sample_id}_vs_${params.input.reference_name}.Kallisto.ga \\\n        ${sample_id}_1.fastq > ${sample_id}.kallisto.log 2>&1\n    fi\n\n    # generate TPM and raw count files\n    if [[ ${params.output.publish_tpm} == true ]]; then\n      awk -F\"\\t\" '{if (NR!=1) {print \\$1, \\$5}}' OFS='\\t' ${sample_id}_vs_${params.input.reference_name}.Kallisto.ga/abundance.tsv > ${sample_id}_vs_${params.input.reference_name}.Kallisto.tpm\n    fi\n\n    if [[ ${params.output.publish_raw} == true ]]; then\n      awk -F\"\\t\" '{if (NR!=1) {print \\$1, \\$4}}' OFS='\\t' ${sample_id}_vs_${params.input.reference_name}.Kallisto.ga/abundance.tsv > ${sample_id}_vs_${params.input.reference_name}.Kallisto.raw\n    fi\n\n    if [[ ${params.output.publish_gene_abundance} == false ]]; then\n      rm -rf *.ga\n    fi\n\n  # or use salmon\n  elif [[ ${params.input.salmon.enable} == \"true\" ]]; then\n    # perform SALMON alignment of fastq files\n    if [ -e ${sample_id}_2.fastq ]; then\n      salmon quant \\\n        -i . \\\n        -l A \\\n        -1 ${sample_id}_1.fastq \\\n        -2 ${sample_id}_2.fastq \\\n        -p ${task.cpus} \\\n        -o ${sample_id}_vs_${params.input.reference_name}.Salmon.ga \\\n        --minAssignedFrags 1 > ${sample_id}.salmon.log 2>&1\n    else\n      salmon quant \\\n        -i . \\\n        -l A \\\n        -r ${sample_id}_1.fastq \\\n        -p ${task.cpus} \\\n        -o ${sample_id}_vs_${params.input.reference_name}.Salmon.ga \\\n        --minAssignedFrags 1 > ${sample_id}.salmon.log 2>&1\n    fi\n\n    # generate final TPM and raw count files\n    if [[ ${params.output.publish_tpm} == true ]]; then\n      awk -F\"\\t\" '{if (NR!=1) {print \\$1, \\$4}}' OFS='\\t' ${sample_id}_vs_${params.input.reference_name}.Salmon.ga/quant.sf > ${sample_id}_vs_${params.input.reference_name}.Salmon.tpm\n    fi\n\n    if [[ ${params.output.publish_raw} == true ]]; then\n      awk -F\"\\t\" '{if (NR!=1) {print \\$1, \\$5}}' OFS='\\t' ${sample_id}_vs_${params.input.reference_name}.Salmon.ga/quant.sf > ${sample_id}_vs_${params.input.reference_name}.Salmon.raw\n    fi\n\n    if [[ ${params.output.publish_gene_abundance} == false ]]; then\n      rm -rf `find *.ga -type f | egrep -v \"aux_info/meta_info.json|/libParams/flenDist.txt\"`\n    fi\n  fi\n  \"\"\"\n}"], "list_proc": ["mmcogle/GEMmakerCam/process_sample"], "list_wf_names": ["mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mmcogle"], "nb_wf": 1, "list_wf": ["GEMmakerCam"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "saioruganti"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n  label \"multiqc\"\n  publishDir \"${params.output.dir}/reports\", mode: params.output.publish_mode\n\n  input:\n    val signal from MULTIQC_RUN.collect()\n\n  output:\n    file \"multiqc_data\" into MULTIQC_DATA\n    file \"multiqc_report.html\" into MULTIQC_REPORT\n\n  when:\n    params.output.multiqc == true\n\n  script:\n    \"\"\"\n    multiqc \\\n      --ignore ${workflow.launchDir}/${params.output.dir}/GEMs \\\n      --ignore ${workflow.launchDir}/${params.output.dir}/reports \\\n      ${workflow.launchDir}/${params.output.dir}\n    \"\"\"\n}"], "list_proc": ["mmcogle/GEMmakerCam/multiqc"], "list_wf_names": ["mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mmcogle"], "nb_wf": 1, "list_wf": ["GEMmakerCam"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "saioruganti"], "nb_contrib": 6, "codes": ["\nprocess fastqc_2 {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: \"*_fastqc.*\"\n  tag { sample_id }\n  label \"fastqc\"\n\n  input:\n    set val(sample_id), file(pass_files) from TRIMMED_SAMPLES_FOR_FASTQC\n\n  output:\n    set file(\"${sample_id}_??_trim_fastqc.html\"), file(\"${sample_id}_??_trim_fastqc.zip\") optional true into FASTQC_2_OUTPUT\n    set val(sample_id), val(1) into CLEAN_TRIMMED_FASTQ_FASTQC_SIGNAL\n\n  script:\n  \"\"\"\n  fastqc $pass_files\n  \"\"\"\n}"], "list_proc": ["mmcogle/GEMmakerCam/fastqc_2"], "list_wf_names": ["mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mmcogle"], "nb_wf": 1, "list_wf": ["GEMmakerCam"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "saioruganti"], "nb_contrib": 6, "codes": ["\nprocess samtools_sort {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: publish_pattern_samtools_sort\n  tag { sample_id }\n  label \"samtools\"\n\n  input:\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.sam\") from INDEXED_SAMPLES\n\n  output:\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam\") into SORTED_FOR_INDEX\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam\") into BAM_FOR_CLEANING\n    set val(sample_id), val(1) into CLEAN_SAM_SIGNAL\n\n  script:\n    \"\"\"\n    samtools sort \\\n      -o ${sample_id}_vs_${params.input.reference_name}.bam \\\n      -O bam \\\n      -T temp \\\n      ${sample_id}_vs_${params.input.reference_name}.sam\n    \"\"\"\n}"], "list_proc": ["mmcogle/GEMmakerCam/samtools_sort"], "list_wf_names": ["mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["StringTie"], "nb_own": 1, "list_own": ["mmcogle"], "nb_wf": 1, "list_wf": ["GEMmakerCam"], "list_contrib": ["bentsherman", "cwytko", "spficklin", "biggstd", "JohnHadish", "saioruganti"], "nb_contrib": 6, "codes": ["\nprocess stringtie {\n  publishDir params.output.sample_dir, mode: params.output.publish_mode, pattern: publish_pattern_stringtie_gtf_and_ga\n  tag { sample_id }\n  label \"multithreaded\"\n  label \"stringtie\"\n\n  input:\n                                                                \n                                                            \n                                   \n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.bam\") from BAM_INDEXED_FOR_STRINGTIE\n    file gtf_file from GTF_FILE\n\n  output:\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.Hisat2.ga\"), file(\"${sample_id}_vs_${params.input.reference_name}.Hisat2.gtf\") into STRINGTIE_GTF_FOR_FPKM\n    set val(sample_id), file(\"${sample_id}_vs_${params.input.reference_name}.Hisat2.*\") into STRINGTIE_GTF_FOR_CLEANING\n    set val(sample_id), val(1) into CLEAN_BAM_SIGNAL\n\n  script:\n    \"\"\"\n    stringtie \\\n      -v \\\n      -p ${task.cpus} \\\n      -e \\\n      -o ${sample_id}_vs_${params.input.reference_name}.Hisat2.gtf \\\n      -G ${gtf_file} \\\n      -A ${sample_id}_vs_${params.input.reference_name}.Hisat2.ga \\\n      -l ${sample_id} ${sample_id}_vs_${params.input.reference_name}.bam\n    \"\"\"\n}"], "list_proc": ["mmcogle/GEMmakerCam/stringtie"], "list_wf_names": ["mmcogle/GEMmakerCam"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FastQC", "Minimap2", "MultiQC"], "nb_own": 2, "list_own": ["replikation", "monikaBrandt"], "nb_wf": 1, "list_wf": ["nf-core-mynewpipeline", "poreCov"], "list_contrib": ["replikation", "DataSpott", "bwlang", "angelovangel", "MarieLataretu", "RaverJay", "monikaBrandt", "hoelzer"], "nb_contrib": 8, "codes": ["process align_to_reference {\n    label 'minimap2'\n    input:\n        tuple(val(name), path(fastq_file), path(fasta_reference))\n    output:\n        tuple(path(\"${name}.bam\"), path(\"${name}.bam.bai\"))\n    script:\n    \"\"\"\n    set -o pipefail\n\t\n    minimap2 -t ${task.cpus} -ax map-ont ${fasta_reference} ${fastq_file} | samtools view -hbF4 | samtools sort -@ ${task.cpus} > ${name}.bam\n\n    samtools index ${name}.bam\n    \"\"\"\n    stub:\n\t\"\"\"\n\ttouch ${name}.bam ${name}.bam.bai\n\t\"\"\"\n    \n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["replikation/poreCov/align_to_reference"], "list_wf_names": ["replikation/poreCov"]}, {"nb_reuse": 8, "tools": ["Salmon", "DBETH", "SAMtools", "kraken2", "MultiQC", "FastQC", "MSIsensor", "totalVI", "GATK"], "nb_own": 9, "list_own": ["sripaladugu", "replikation", "suzannejin", "supark87", "zamanianlab", "scilus", "wslh-bio", "monikaBrandt", "ralsallaq"], "nb_wf": 8, "list_wf": ["prac_nextflow", "metaGx_nf", "RNAseq-VC-nf", "nf-core-mynewpipeline", "poreCov", "germline_somatic", "nf-proportionality", "tractoflow", "spriggan"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "supark87", "davidmasp", "replikation", "k-florek", "frheault", "pcantalupo", "malinlarsson", "olgabot", "jhlegarreta", "ppoulin91", "skrakau", "hoelzer", "DataSpott", "lescai", "szilvajuhos", "FriederikeHanssen", "arnaudbore", "monikaBrandt", "suzannejin", "jchoude", "mdesco", "RaverJay", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "angelovangel", "wheelern", "bwlang", "GuillaumeTh", "MarieLataretu", "adrlar", "AbigailShockey", "ralsallaq"], "nb_contrib": 39, "codes": ["\nprocess clean_reads {\n  tag \"$name\"\n  publishDir \"${params.outdir}/trimming\", mode: 'copy',pattern:\"*.trim.txt\"\n\n  input:\n  set val(name), file(reads) from read_files_trimming\n\n  output:\n  tuple name, file(\"${name}_clean{_1,_2}.fastq.gz\") into cleaned_reads_shovill, cleaned_reads_fastqc, read_files_kraken\n  file(\"${name}.phix.stats.txt\") into phix_cleanning_stats\n  file(\"${name}.adapters.stats.txt\") into adapter_cleanning_stats\n  file(\"${name}.trim.txt\") into bbduk_files\n  tuple file(\"${name}.phix.stats.txt\"),file(\"${name}.adapters.stats.txt\"),file(\"${name}.trim.txt\") into multiqc_clean_reads\n\n  script:\n  \"\"\"\n  bbduk.sh in1=${reads[0]} in2=${reads[1]} out1=${name}.trimmed_1.fastq.gz out2=${name}.trimmed_2.fastq.gz qtrim=${params.trimdirection} qtrim=${params.qualitytrimscore} minlength=${params.minlength} tbo tbe &> ${name}.out\n  repair.sh in1=${name}.trimmed_1.fastq.gz in2=${name}.trimmed_2.fastq.gz out1=${name}.paired_1.fastq.gz out2=${name}.paired_2.fastq.gz\n  bbduk.sh in1=${name}.paired_1.fastq.gz in2=${name}.paired_2.fastq.gz out1=${name}.rmadpt_1.fastq.gz out2=${name}.rmadpt_2.fastq.gz ref=/bbmap/resources/adapters.fa stats=${name}.adapters.stats.txt ktrim=r k=23 mink=11 hdist=1 tpe tbo\n  bbduk.sh in1=${name}.rmadpt_1.fastq.gz in2=${name}.rmadpt_2.fastq.gz out1=${name}_clean_1.fastq.gz out2=${name}_clean_2.fastq.gz outm=${name}.matched_phix.fq ref=/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 stats=${name}.phix.stats.txt\n  grep -E 'Input:|QTrimmed:|Trimmed by overlap:|Total Removed:|Result:' ${name}.out > ${name}.trim.txt\n  \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess MSIsensor_scan {\n    label 'cpus_1'\n    label 'memory_max'\n\n    tag \"${fasta}\"\n\n    input:\n    file(fasta) from ch_fasta\n    file(fastaFai) from ch_fai\n\n    output:\n    file \"microsatellites.list\" into msi_scan_ch\n\n    when: 'msisensor' in tools\n\n    script:\n    \"\"\"\n    msisensor scan -d ${fasta} -o microsatellites.list\n    \"\"\"\n}", "\nprocess quant {\n    tag \"$pair_id\"\n     \n    input:\n    file index from index_ch\n    set pair_id, file(reads) from read_pairs_ch\n \n    output:\n    file(pair_id) into quant_ch\n \n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess picard_sort_bam {\n\n    cpus large_core\n\n    input:\n        tuple val(id), file(bam) from input_bam\n\n    output:\n        tuple val(id), file(\"${id}_qn_sorted.bam\") into sorted_bams\n\n    when:\n        params.bam\n\n    \"\"\"\n        gatk \\\n          --java-options \\\n          -Xmx6g \\\n          SortSam \\\n          -I ${bam} \\\n          -O ${id}_qn_sorted.bam \\\n          -SO queryname \\\n          -TMP_DIR /home/BIOTECH/zamanian/tmp\n    \"\"\"\n}", " process samTobam {\n        container \"${container_samtools}\"\n        label 'multithread'\n        publishDir \"${params.outD}/assembly-based/readsOntoCDS/\", mode: 'copy'\n        \n        input:\n        set sname, file(samAln), file(cdsFAA) from dmndSam_prokka_ch\n        \n        output:\n        set sname, file(\"${sname}.mapOnTo.CDS.bam\"), file(\"${sname}.mapOnTo.CDS.sorted.bam\") into readsOntoCDS_bam_ch\n        \n        \"\"\"\n        samtools faidx ${cdsFAA}\n        wait\n        faiF=`ls *.fai`\n        samtools view -bh -t \\$faiF -o ${sname}.mapOnTo.CDS.bam  ${samAln}\n        samtools sort -@ ${task.cpus} ${sname}.mapOnTo.CDS.bam -o ${sname}.mapOnTo.CDS.sorted.bam\n        \"\"\"\n    }", "\nprocess Bet_DWI {\n    cpus 2\n    label 'big_mem'\n\n    input:\n    set sid, file(dwi), file(bval), file(bvec) from dwi_gradients_for_bet\n\n    output:\n    set sid, \"${sid}__b0_bet.nii.gz\", \"${sid}__b0_bet_mask.nii.gz\" into\\\n        b0_and_mask_for_crop\n    set sid, \"${sid}__dwi_bet.nii.gz\", \"${sid}__b0_bet.nii.gz\",\n        \"${sid}__b0_bet_mask.nii.gz\" into dwi_b0_b0_mask_for_n4\n    file \"${sid}__b0_no_bet.nii.gz\"\n\n    script:\n    \"\"\"\n    export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=1\n    export OMP_NUM_THREADS=1\n    export OPENBLAS_NUM_THREADS=1\n    scil_extract_b0.py $dwi $bval $bvec ${sid}__b0_no_bet.nii.gz --mean\\\n            --b0_thr $params.b0_thr_extract_b0 --force_b0_threshold\n    bet ${sid}__b0_no_bet.nii.gz ${sid}__b0_bet.nii.gz -m -R -f $params.bet_dwi_final_f\n    scil_image_math.py convert ${sid}__b0_bet_mask.nii.gz ${sid}__b0_bet_mask.nii.gz --data_type uint8 -f\n    mrcalc $dwi ${sid}__b0_bet_mask.nii.gz -mult ${sid}__dwi_bet.nii.gz -quiet -nthreads 1\n    \"\"\"\n}", "process kraken2 {\n        label 'kraken2'\n        publishDir \"${params.output}/${params.readqcdir}/${name}/tax_read_classification\", mode: 'copy'\n    input:\n        tuple val(name), path(reads)\n        path(database)\n  \toutput:\n    \ttuple val(name), path(\"${name}.kraken.out\"), path(\"${name}.kreport\")\n  \tscript:\n    \"\"\"\n    mkdir -p kraken_db && tar xzf ${database} -C kraken_db --strip-components 1\n\n    # mask possible primer regions\n    case \"${reads}\" in\n        *.gz) \n            zcat ${reads} | sed '1b ; s/..............................\\$/NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' |\\\n            sed '1b ; s/^............................../NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' > masked_reads.fastq\n            ;;\n        *.fastq)\n            cat ${reads} | sed '1b ; s/..............................\\$/NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' |\\\n            sed '1b ; s/^............................../NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' > masked_reads.fastq\n            ;;\n        *)\n            echo \"file format not supported...what the ...(.fastq .fastq.gz is supported)\"\n            exit 1\n    esac\n\n    kraken2 --db kraken_db --threads ${task.cpus} --output ${name}.kraken.out --report ${name}.kreport masked_reads.fastq\n\n    # cleanup to reduce footprint\n    rm -rf kraken_db/\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}.kraken.out ${name}.kreport\n    \"\"\"\n  }"], "list_proc": ["wslh-bio/spriggan/clean_reads", "sripaladugu/germline_somatic/MSIsensor_scan", "supark87/prac_nextflow/quant", "suzannejin/nf-proportionality/get_software_versions", "zamanianlab/RNAseq-VC-nf/picard_sort_bam", "ralsallaq/metaGx_nf/samTobam", "scilus/tractoflow/Bet_DWI", "replikation/poreCov/kraken2"], "list_wf_names": ["sripaladugu/germline_somatic", "scilus/tractoflow", "ralsallaq/metaGx_nf", "zamanianlab/RNAseq-VC-nf", "replikation/poreCov", "wslh-bio/spriggan", "supark87/prac_nextflow", "suzannejin/nf-proportionality"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["montilab"], "nb_wf": 1, "list_wf": ["pipeliner-2"], "list_contrib": ["anfederico"], "nb_contrib": 1, "codes": ["process MULTIQC {\n    publishDir \"${params.outdir}/reports\", mode: \"copy\"\n\n    input:\n    path 'data*/*'\n\n    output:\n    path '*.html'\n\n    script:\n    \"\"\"\n    multiqc ${params.outdir} --force \\\n    --cl_config \"extra_fn_clean_exts: ['_val_1', '_val_2', '.read_distribution', '.read_duplication']\"\n    \"\"\"\n}"], "list_proc": ["montilab/pipeliner-2/MULTIQC"], "list_wf_names": ["montilab/pipeliner-2"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["montilab"], "nb_wf": 1, "list_wf": ["pipeliner-2"], "list_contrib": ["anfederico"], "nb_contrib": 1, "codes": ["\nprocess HISAT_MAPPING {\n    tag \"$pid\"\n    publishDir \"${params.outdir}/samples/${pid}/hisat\", mode: \"copy\"\n\n    input:\n    tuple val(pid), path(reads)\n\n    output:\n    tuple val(pid), path('*.bam')\n    tuple val(pid), path('*.log')\n\n    script:\n    def data = params.paired ? \"-1 ${reads[0]} -2 ${reads[1]}\" : \"-U ${reads}\"\n    \"\"\"\n    hisat2 -p $task.cpus \\\n           --summary-file '${pid}.log' \\\n           --new-summary \\\n           -x ${params.index} \\\n           ${data} \\\n           -S '${pid}.sam';\n    samtools view -S -b '${pid}.sam';\n    samtools sort '${pid}.sam' -o '${pid}.bam'\n    \"\"\"\n}"], "list_proc": ["montilab/pipeliner-2/HISAT_MAPPING"], "list_wf_names": ["montilab/pipeliner-2"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["montilab"], "nb_wf": 1, "list_wf": ["pipeliner-2"], "list_contrib": ["anfederico"], "nb_contrib": 1, "codes": ["process FASTQC {\n    tag \"$pid\"\n    publishDir \"${params.outdir}/samples/${pid}/fastqc\", mode: \"copy\"\n\n    input:\n    tuple val(pid), path(reads)\n\n    output:\n    tuple val(pid), path(\"*.zip\")\n    tuple val(pid), path(\"*.html\")\n\n    script:\n    def data = params.paired ? \"${reads[0]} ${reads[1]}\" : \"${reads}\"\n    \"\"\"\n    fastqc -o . -t $task.cpus -q ${data}\n    \"\"\"\n}"], "list_proc": ["montilab/pipeliner-2/FASTQC"], "list_wf_names": ["montilab/pipeliner-2"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["montilab"], "nb_wf": 1, "list_wf": ["pipeliner"], "list_contrib": ["anfederico"], "nb_contrib": 1, "codes": [" process pre_multiqc {\n      cache \"deep\"\n      publishDir \"${params.outdir}/reports/pre_pipeliner\", mode: 'copy'\n\n      input:\n      file ('*') from preqc_results.flatten().toList()\n\n      output:\n      file \"*data\"\n      file \"*report.html\"\n\n      script:\n      \"\"\"\n      multiqc ${params.outdir}/pre_fastqc\n      \"\"\"\n    }"], "list_proc": ["montilab/pipeliner/pre_multiqc"], "list_wf_names": ["montilab/pipeliner"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mpieva"], "nb_wf": 1, "list_wf": ["quicksand"], "list_contrib": ["merszym", "wjv"], "nb_contrib": 2, "codes": ["\nprocess splitStats {\n    tag \"$rg\"\n\n    input:\n    set rg,'input.bam' from splitstats\n\n    output:\n    set rg, stdout into splitcounts\n\n    script:\n    \"\"\"\n    samtools view -c input.bam\n    \"\"\"\n}"], "list_proc": ["mpieva/quicksand/splitStats"], "list_wf_names": ["mpieva/quicksand"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC"], "nb_own": 2, "list_own": ["mvanins", "mpieva"], "nb_wf": 2, "list_wf": ["quicksand", "stress_granule_RNA_manuscript"], "list_contrib": ["merszym", "mvanins", "wjv"], "nb_contrib": 3, "codes": [" process depleted_fastqc {\n    cpus 6\n    time '5h'\n    memory '5G'\n    \n    tag \"FastQC on depleted $library\"\n    publishDir \"${params.outdir}/QC/depleted_fastqc\", mode:'copy', overwrite: true\n    \n    input:\n    set library, file(reads) from depleted_reads_fastqc\n    \n    output:\n    file(\"*_fastqc.{zip,html}\") into multiqc_depleted_fastqc\n    \n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} -f fastq -q ${reads} --nogroup\n    \"\"\"\n  }", "\nprocess filterPaired {\n    tag \"$rg\"\n\n    input:\n    set rg, 'input.bam' from filter_paired_in\n\n    output:\n    set rg, 'output.bam' into filter_paired_out\n\n    script:\n    \"\"\"\n    samtools view -b -u -F 1 -o output.bam input.bam\n    \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/depleted_fastqc", "mpieva/quicksand/filterPaired"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript", "mpieva/quicksand"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mpieva"], "nb_wf": 1, "list_wf": ["quicksand"], "list_contrib": ["merszym", "wjv"], "nb_contrib": 2, "codes": ["\nprocess toFasta {\n    tag \"$rg\"\n\n    input:\n    set rg, 'input.bam', filtered_count from tofasta_in\n\n    output:\n    set rg, 'output.fa' into tofasta_out\n\n    when:\n    filtered_count.toInteger() > 0\n\n    script:\n    \"\"\"\n    samtools fasta input.bam > output.fa\n    \"\"\"\n}"], "list_proc": ["mpieva/quicksand/toFasta"], "list_wf_names": ["mpieva/quicksand"]}, {"nb_reuse": 1, "tools": ["KrakenUniq"], "nb_own": 1, "list_own": ["mpieva"], "nb_wf": 1, "list_wf": ["quicksand"], "list_contrib": ["merszym", "wjv"], "nb_contrib": 2, "codes": ["\nprocess runKrakenUniq {\n\tpublishDir 'kraken', mode: 'copy', pattern:\"*translate\", saveAs: {\"${rg}.translate\"}\n    publishDir 'kraken', mode: 'copy', pattern:\"*report\", saveAs: {\"${rg}.report\"}\n    cpus \"${params.krakenthreads}\"\n    memory '16GB'\n    label 'bigmem'\n    label 'local'\n    tag \"$rg\"\n\n    input:\n    set rg, \"input.fa\", db from pre_kraken\n\n    output:\n    set rg, \"krakenUniq.translate\" into kraken_assignments\n    set rg, \"krakenUniq.report\" into find_best\n\n    script:\n    \"\"\"\n    ${params.kraken_uniq}/krakenuniq --threads ${task.cpus} --db ${db} --fasta-input input.fa --report-file krakenUniq.report > output.krakenUniq\n    ${params.kraken_uniq}/krakenuniq-translate --db ${db} --mpa-format output.krakenUniq > krakenUniq.translate\n    \"\"\"\n}"], "list_proc": ["mpieva/quicksand/runKrakenUniq"], "list_wf_names": ["mpieva/quicksand"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mpieva"], "nb_wf": 1, "list_wf": ["quicksand"], "list_contrib": ["merszym", "wjv"], "nb_contrib": 2, "codes": ["\nprocess dedupBam {\n    publishDir 'out', mode: 'copy', pattern: \"*.bam\", saveAs: {out_bam}\n    tag \"$rg:$family:$species\"\n\n    input:\n    set order, family, rg, species, 'input.bam', taxon from mapped_bam\n\n    output:\n    set order, species, family, rg, \"output.bam\", stdout, \"count.txt\", taxon into deduped_bam\n    set order, family, rg, species, stdout into (coverage_count, coverage_data)\n    set order, family, rg, species, \"count.txt\" into (deduped_count, deduped_data)\n\n    script:\n    if(params.byrg){\n        out_bam = \"${rg}/${taxon}/aligned/${family}.${species}_deduped.bam\"\n    } else {\n        out_bam = \"${taxon}/aligned/${rg}.${family}.${species}_deduped.bam\"\n    }\n    \"\"\"\n    $params.bamrmdup -r -o output.bam input.bam > rmdup.txt\n    samtools coverage -H output.bam | cut -f 5\n    samtools view -c output.bam > count.txt\n    \"\"\"\n}"], "list_proc": ["mpieva/quicksand/dedupBam"], "list_wf_names": ["mpieva/quicksand"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["mpieva"], "nb_wf": 1, "list_wf": ["quicksand"], "list_contrib": ["merszym", "wjv"], "nb_contrib": 2, "codes": ["\nprocess runIntersectBed{\n    tag \"$rg:$family:$species\"\n    publishDir 'out', mode: 'copy', saveAs: {out_bam}\n\n    input:\n    set species, order, family, rg, \"inbam.bam\", \"inbed.bed\", coverage, taxon from to_bed\n\n    output:\n    file \"outbam.bam\"\n    set order,family, rg, species, stdout into (bedfilter_count, bedfilter_data)\n    set order,family, rg, species, \"outbam.bam\", stdout, coverage into deam_stats\n    \n    script:\n    if(params.byrg){\n        out_bam = \"${rg}/${taxon}/bed/${family}.${species}_deduped_bedfiltered.bam\"\n    } else {\n        out_bam = \"${taxon}/bed/${rg}.${family}.${species}_deduped_bedfiltered.bam\"\n    }\n    \"\"\"\n    bedtools intersect -a inbam.bam -b inbed.bed -v > outbam.bam\n    samtools view -c outbam.bam\n    \"\"\"\n    }"], "list_proc": ["mpieva/quicksand/runIntersectBed"], "list_wf_names": ["mpieva/quicksand"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["MGI_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess demux_index {\n  tag \"$sample\"\n  label 'process_high'\n  publishDir \"${cluster_path}/data/03_intermediate/${platform}/${run_id}/${lane}/Index-removal/\", mode: 'copy',\n  saveAs: { filename ->\n    filename.endsWith(\".log\") ? \"logs/$filename\" : filename\n  }\n\n\n  input:\n  set val(row), val(sample), val(index), val(index2), val(barcode), val(run_id), val(lane), val(protocol), val(platform), val(genome), val(user), path(reads) from ch_demux\n\n  output:\n  set val(row), val(sample), path(\"*.fq.gz\"), val(index), val(index2), val(barcode), val(run_id), val(lane), val(protocol), val(platform), val(genome), val(user) into ch_demux_index2\n  path(\"*.log\") optional true\n\n  script:\n                                                                \n  read1 = reads[0]\n  read2 = reads[1]\n  read1_index = \"${sample}_${run_id}_${lane}_${index}_R1.fq.gz\"\n  read2_index = \"${sample}_${run_id}_${lane}_${index}_R2.fq.gz\"\n  errors = index.length() > 6 ? \"-e 0.15\" : \"-e 0.2\"\n\n\n  if (index == \"NNNNNNNN\" | index == \"NNNNNN\") {\n    \"\"\"\n    length=(\\$(echo -e `zcat $read1 | head -2 | tail -1 | awk '{print length(\\$0)}'`))\n    length2=(\\$(echo -e `zcat $read2 | head -2 | tail -1 | awk '{print length(\\$0)}'`))\n    if [ \"\\$length\" = \"\\$length2\" ]\n    then\n    mv $read1 $read1_index\n    mv $read2 $read2_index\n    else\n    cutadapt -l \\$length -o $read1_index $read1 -j 0 > \"${sample}_${run_id}_${lane}_${index}_R1.log\"\n    cutadapt -l \\$length -o $read2_index $read2 -j 0 > \"${sample}_${run_id}_${lane}_${index}_R2.log\"\n    fi\n    \"\"\"\n  } else {\n    \"\"\"\n    cutadapt \\\n    $errors \\\n    --no-indels \\\n    -a $sample=\\\"$index\\$\\\" \\\n    -o $read2_index -p $read1_index \\\n    $read2 $read1 \\\n    -j 0 \\\n    --discard-untrimmed > ${sample}_${run_id}_${lane}_${index}.log\n    \"\"\"\n  }\n\n}"], "list_proc": ["mpozuelo/MGI_demux/demux_index"], "list_wf_names": ["mpozuelo/MGI_demux"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["MGI_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess remove_umi {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${cluster_path}/data/04_pfastq/${platform}/${run_id}/${lane}/${user}/demux_fastq/woUMI\", mode: 'copy',\n  saveAs: { filename ->\n    filename.endsWith(\".csv\") ? null : filename\n  }\n\n  input:\n  set val(row), val(sample), path(reads), val(index), val(barcode), val(run_id), val(lane), val(protocol), val(platform), val(user) from ch_umi_removal\n\n  output:\n  path(\"*woUMI*.fastq.gz\")\n\n  when:\n  protocol == \"RNAseq_3_S\" | protocol == \"RNAseq_3_ULI\"\n\n  script:\n  \"\"\"\n  File_ID_new=\\$(echo \"${sample}\" | rev | cut -c 3- | rev)\n  File_ID_number=\\$(echo \"${sample}\" | rev | cut -c 1 | rev)\n  Lane_ID_number=\\$(echo \"${lane}\" | rev | cut -c 1 | rev)\n  woumi1=\\$(printf \"%s_woUMI_S%s_L00%s_R1_001.fastq.gz\" \"\\${File_ID_new}\" \"\\${File_ID_number}\" \"\\${Lane_ID_number}\")\n  woumi2=\\$(printf \"%s_woUMI_S%s_L00%s_R2_001.fastq.gz\" \"\\${File_ID_new}\" \"\\${File_ID_number}\" \"\\${Lane_ID_number}\")\n  cutadapt -l 10 -j 0 -o $umi ${reads[0]}\n  umi_tools extract -I ${reads[0]} -S \\$woumi1 --read2-in=${reads[1]} --read2-out=\\$woumi2 --bc-pattern=NNNNNNNNNN\n  \"\"\"\n}"], "list_proc": ["mpozuelo/MGI_demux/remove_umi"], "list_wf_names": ["mpozuelo/MGI_demux"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["MGI_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process fastqc {\n     tag \"$sample\"\n     label 'process_medium'\n     publishDir \"${cluster_path}/data/04_pfastq/${platform}/${run_id}/${lane}/${user}/fastqc/${sample}\", mode: 'copy',\n     saveAs: { filename ->\n       filename.endsWith(\".zip\") ? \"zips/$filename\" : filename\n     }\n\n     input:\n     set val(row), val(sample), path(reads), val(index), val(run_id), val(lane), val(protocol), val(platform), val(user) from ch_fastqc\n\n     output:\n     set path(\"*_fastqc.{zip,html}\"), val(run_id), val(lane), val(platform), val(user) into fastqc_results\n\n     script:\n     \"\"\"\n     fastqc --quiet --threads $task.cpus $reads\n     \"\"\"\n   }"], "list_proc": ["mpozuelo/MGI_demux/fastqc"], "list_wf_names": ["mpozuelo/MGI_demux"]}, {"nb_reuse": 2, "tools": ["FastQC", "Picard", "Cutadapt"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 2, "list_wf": ["MGI_demux", "exome_mosdepth"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process get_software_versions {\n     publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n         saveAs: { filename ->\n             if (filename.indexOf(\".csv\") > 0) filename\n             else null\n         }\n\n     output:\n     file 'software_versions_mqc.yaml' into software_versions_yaml\n     file \"software_versions.csv\"\n\n     script:\n     \"\"\"\n     fastqc --version &> v_fastqc.txt\n     cutadapt --version &> v_cutadapt.txt\n     scrape_software_versions.py &> software_versions_mqc.yaml\n     \"\"\"\n }", "\nprocess picard_alignmentmetrics {\n  tag \"$sample\"\n  label 'process_low'\n  publishDir \"${cluster_path}/data/05_QC/${project}/HSmetrics/${sample}\", mode: params.publish_dir_mode\n\n  input:\n  set val(sample), path(bam), path(bai), path(bam_subset), path(bai_subset), path(interval) from ch_picard_alignmentmetrics\n  file(genome) from ch_genome\n  file(index) from ch_genome_index\n\n  output:\n  path(\"${bam.baseName}.alignment_metrics.txt\") into ch_merge_original_Alignmentmetrics\n  path(\"${bam_subset.baseName}.alignment_metrics.txt\") into ch_merge_subset_Alignmentmetrics\n\n  script:\n  outfile = \"${bam.baseName}.alignment_metrics.txt\"\n  outfile_subset = \"${bam_subset.baseName}.alignment_metrics.txt\"\n  java_options = (task.memory.toGiga() > 8) ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2 )+\"g \"+ \"-Xmx\" + (task.memory.toGiga() - 1)+ \"g\\\"\"\n\n  \"\"\"\n  picard ${java_options} CollectAlignmentSummaryMetrics \\\n  INPUT=$bam \\\n  OUTPUT=$outfile \\\n  R=$genome\n\n  picard ${java_options} CollectAlignmentSummaryMetrics \\\n  INPUT=$bam_subset \\\n  OUTPUT=$outfile_subset \\\n  R=$genome\n  \"\"\"\n}"], "list_proc": ["mpozuelo/MGI_demux/get_software_versions", "mpozuelo/exome_mosdepth/picard_alignmentmetrics"], "list_wf_names": ["mpozuelo/MGI_demux", "mpozuelo/exome_mosdepth"]}, {"nb_reuse": 1, "tools": ["Cutadapt", "Picard", "SAMtools", "MultiQC", "FastQC", "STAR"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process get_software_versions {\n     publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n         saveAs: { filename ->\n             if (filename.indexOf(\".csv\") > 0) filename\n             else null\n         }\n\n     output:\n     file 'software_versions_mqc.yaml' into software_versions_yaml\n     file \"software_versions.csv\"\n\n     script:\n     \"\"\"\n     echo $workflow.manifest.version &> v_ngi_QC.txt\n     echo $workflow.nextflow.version &> v_nextflow.txt\n     fastqc --version &> v_fastqc.txt\n     cutadapt --version &> v_cutadapt.txt\n     trim_galore --version &> v_trim_galore.txt\n     STAR --version &> v_star.txt\n     samtools --version &> v_samtools.txt\n     read_duplication.py --version &> v_rseqc.txt\n     picard MarkDuplicates --version &> v_markduplicates.txt  || true\n     multiqc --version &> v_multiqc.txt\n     scrape_software_versions.py &> software_versions_mqc.yaml\n     \"\"\"\n }"], "list_proc": ["mpozuelo/QC_demux/get_software_versions"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process subset_10pc {\n    tag \"$sample\"\n    label 'process_low'\n    publishDir \"${cluster_path}/data/05_QC/${project}/subset_fastq/${sample}\", mode: 'copy',\n    saveAs: { filename ->\n      filename.endsWith(\"QC.fq.gz\") ? filename : null\n    }\n\n    input:\n    set val(sample), path(reads), val(run_id), val(lane), val(date), val(protocol), val(platform), val(source), val(genome), val(user), path(star) from ch_subset\n\n    output:\n    set val(sample), path(\"*QC*.fq.gz\"), val(run_id), val(lane), val(date), val(protocol), val(platform), val(source), val(genome), val(user), path(star) into ch_trimming\n\n    script:\n    read1 = \"${sample}_QC_R1.fq\"\n    read2 = \"${sample}_QC_R2.fq\"\n\n                                                                                                                 \n                                               \n    \"\"\"\n    subset=(\\$(echo \\$((\\$(echo -e `zcat ${reads[0]} | awk 'NR % 4 == 2' - | wc -l`)*10/100))))\n    seqtk sample -s100 ${reads[0]} \\$subset > $read1\n    seqtk sample -s100 ${reads[1]} \\$subset > $read2\n    pigz -p $task.cpus $read1\n    pigz -p $task.cpus $read2\n    \"\"\"\n  }"], "list_proc": ["mpozuelo/QC_demux/subset_10pc"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess trimming {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${cluster_path}/data/05_QC/${project}/trimgalore/${sample}\", mode: 'copy',\n  saveAs: { filename ->\n    if (filename.endsWith(\".log\") || filename.indexOf(\"trimming_report.txt\") > 0) \"logs/$filename\"\n    else if (filename.endsWith(\"_fastqc.html\")) \"fastqc/$filename\"\n    else if (filename.endsWith(\".zip\")) \"fastqc/zips/$filename\"\n    else if (filename.indexOf(\"_fastqc\") > 0) filename\n    else if (filename.endsWith(\"UMI.fq.gz\")) \"UMIs/$filename\"\n    else if (filename.endsWith(\"woUMI_R{1,2}.fq.gz\")) \"fastqWoUMIs/$filename\"\n    else filename\n  }\n\n\n  input:\n  set val(sample), path(subset), val(run_id), val(lane), val(date), val(protocol), val(platform), val(source), val(genome), val(user), path(star) from ch_trimming\n\n  output:\n  set val(sample), path(\"*val_{1,2}.fq.gz\"), val(run_id), val(lane), val(date), val(protocol), val(platform), val(source), val(genome), val(user), path(star) into ch_star\n  file \"*trimming_report.txt\" into trimgalore_trim_mqc\n  file \"*_fastqc.{zip,html}\" into trimgalore_fastqc_mqc\n  file \"*UMI.fq.gz\" optional true\n\n  script:\n                                                                 \n                                                                 \n  umi = \"${sample}_UMI.fq.gz\"\n  woumi1 = \"${sample}_woUMI_R1.fq.gz\"\n  woumi2 = \"${sample}_woUMI_R2.fq.gz\"\n\n  if (protocol == 'RNAseq_3_S' | protocol == 'RNAseq_3_ULI') {\n\n                                                                            \n                                                                                                     \n                                                                                                                                                   \n\n\n                                                                                                      \n                                                                                                                     \n                                    \n                                                                        \n\n                                                           \n                                                \n                                                             \n      \n    \"\"\"\n    cutadapt -l 10 -j 0 -o $umi ${subset[0]}\n\n    umi_tools extract -I ${subset[0]} -S $woumi1 --read2-in=${subset[1]} --read2-out=$woumi2 --bc-pattern=NNNNNNNNNN\n\n    trim_galore \\\\\n    -q 30 \\\\\n    --paired \\\\\n    --length 20 \\\\\n    -j $task.cpus \\\\\n    --fastqc \\\\\n    $woumi1 $woumi2\n    \"\"\"\n\n  } else {\n\n                                                                          \n                                                                            \n                                                                                                                         \n                                                                                                           \n                                    \n                                                                        \n\n                                                           \n                                                                     \n                                                                      \n      \n\n    \"\"\"\n    trim_galore \\\\\n    -q 30 \\\\\n    --paired \\\\\n    --length 20 \\\\\n    -j $task.cpus \\\\\n    --fastqc \\\\\n    ${subset[0]} ${subset[1]}\n    \"\"\"\n  }\n}"], "list_proc": ["mpozuelo/QC_demux/trimming"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess star {\n  tag \"$sample\"\n  label 'process_high'\n  publishDir \"${cluster_path}/data/05_QC/${project}/STAR/${sample}\", mode: 'copy',\n  saveAs: { filename ->\n    if (filename.endsWith(\".out\")) \"logs/$filename\"\n    else if (filename.endsWith(\"*.fq.gz\")) \"unmapped/$filename\"\n                                            \n    else if (filename.endsWith(\".bam\")) \"bam/$filename\"\n    else if (filename.endsWith(\".bai\")) \"bam_index/$filename\"\n  }\n\n  input:\n  set val(sample), path(trimmed), val(run_id), val(lane), val(date), val(protocol), val(platform), val(source), val(genome), val(user), path(star) from ch_star\n\n  output:\n  set val(sample), path(\"*.sortedByCoord.out.bam\"), path(\"*Aligned.sortedByCoord.out.bam.bai\") into ch_bam_samtools,\n                                                                                                    ch_bam_stats\n  set val(sample), path(\"*.sortedByCoord.out.bam\") into ch_markduplicates\n                                                                              \n  set val(sample), path(\"*unmapped*\") optional true\n  path(\"*.out\") into star_logs          \n  set val(sample), path(\"*.tab\") into ch_tab\n  path(\"*.mapped.tsv\") into concatenate_mapped\n\n\n\n  script:\n\n                                                                                                               \n  unaligned = params.complete ? \"--outReadsUnmapped Fastx\" : ''\n\n  def star_mem = task.memory ?: params.star_memory ?: false\n  def avail_mem = star_mem ? \"--limitBAMsortRAM ${star_mem.toBytes() - 100000000}\" : ''\n\n                                                                                         \n  \"\"\"\n  STAR \\\\\n  --genomeDir ${star[1]} \\\\\n  --sjdbGTFfile ${star[0]} \\\\\n  --readFilesIn $trimmed \\\\\n  --runThreadN $task.cpus \\\\\n  --outWigType bedGraph \\\\\n  --readFilesCommand zcat \\\\\n  --outFileNamePrefix $sample \\\\\n  --outSAMunmapped Within \\\\\n  --runDirPerm All_RWX $unaligned \\\\\n  --outSAMtype BAM SortedByCoordinate $avail_mem \\\\\n  --outSAMattributes NH HI AS NM MD \\\\\n  --outSAMattrRGline ID:${sample}.R1.${protocol} SM:${sample} LB:${protocol} PL:${platform} CN:${source} DT:${date}T00:00:00-0400\n\n\n\n  if [ -f ${sample}.Unmapped.out.mate1 ]; then\n    mv ${sample}.Unmapped.out.mate1 ${sample}.unmapped_R1.fq\n    pigz -p $task.cpus ${sample}.unmapped_R1.fq\n  fi\n  if [ -f ${sample}.Unmapped.out.mate2 ]; then\n    mv ${sample}.Unmapped.out.mate2 ${sample}.unmapped_R2.fq\n    pigz -p $task.cpus ${sample}.unmapped_R2.fq\n  fi\n\n  samtools index ${sample}Aligned.sortedByCoord.out.bam\n\n  uniquely=\\$(grep \"Uniquely mapped reads %\" ${sample}Log.final.out | grep -Eo '[0-9.]+%')\n  multiple=\\$(grep \"% of reads mapped to multiple loci\" ${sample}Log.final.out | grep -Eo '[0-9.]+%')\n  many=\\$(grep \"% of reads mapped to too many loci\" ${sample}Log.final.out | grep -Eo '[0-9.]+%')\n  printf \"%s\\t%s\\t%s\\t%s\\n\" \"${sample}\" \"\\$uniquely\" \"\\$multiple\" \"\\$many\" > ${sample}.mapped.tsv\n  \"\"\"\n  }"], "list_proc": ["mpozuelo/QC_demux/star"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess samtools {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${cluster_path}/data/05_QC/${project}/samtools/${sample}\", mode: 'copy'\n                        \n                                                                        \n                                                                        \n                                                                   \n          \n\n  input:\n  set val(sample), path(bam), path(index) from ch_bam_samtools\n\n  output:\n                                              \n                                               \n  path(\"*.stats\") into bam_stats          \n\n                                            \n                                             \n\n  script:\n  \"\"\"\n  samtools stats $bam > ${bam}.stats\n  \"\"\"\n}"], "list_proc": ["mpozuelo/QC_demux/samtools"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess picard {\n  tag \"${bam.baseName - '.sorted'}\"\n  label 'process_medium'\n  publishDir \"${cluster_path}/data/05_QC/${project}/markduplicates/\", mode: params.publish_dir_mode,\n  saveAs: {filename -> filename.indexOf(\"_metrics.txt\") > 0 ? \"metrics/$filename\" : \"$filename\"}\n\n  input:\n  set val(name), file(bam) from ch_markduplicates\n\n  output:\n  file \"${bam.baseName}.markDups_metrics.txt\" into picard_mrkd_results\n  file \"${bam.baseName}.sort.qual_score_dist.txt\" into picard_distribution_results\n\n  script:\n  markdup_java_options = (task.memory.toGiga() > 8) ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2 )+\"g \"+ \"-Xmx\" + (task.memory.toGiga() - 1)+ \"g\\\"\"\n\n  \"\"\"\n  picard ${markdup_java_options} MarkDuplicates \\\\\n  INPUT=$bam \\\\\n  OUTPUT=${bam.baseName}.markDups.bam \\\\\n  METRICS_FILE=${bam.baseName}.markDups_metrics.txt \\\\\n  REMOVE_DUPLICATES=false \\\\\n  ASSUME_SORTED=true \\\\\n  PROGRAM_RECORD_ID='null' \\\\\n  VALIDATION_STRINGENCY=LENIENT\n\n  picard ${markdup_java_options} QualityScoreDistribution \\\\\n  INPUT=$bam \\\\\n  O=${bam.baseName}.sort.qual_score_dist.txt \\\n  CHART=${bam.baseName}.sort.qual_score_dist.pdf\n  \"\"\"\n}"], "list_proc": ["mpozuelo/QC_demux/picard"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process fastqc {\n     tag \"$sample\"\n     label 'process_low'\n     cache true\n     publishDir \"${cluster_path}/data/05_QC/${project}/fastqc/${sample}\", mode: 'copy',\n     saveAs: { filename ->\n       filename.endsWith(\".zip\") ? \"zips/$filename\" : filename\n     }\n\n     input:\n     set val(sample), path(reads), val(run_id), val(lane), val(date), val(protocol), val(platform), val(source), val(genome), val(user), path(star) from ch_fastq\n\n     output:\n     path(\"*_fastqc.{zip,html}\") into fastqc_results          \n     path(\"*.total.reads.tsv\") into total_reads_merge\n           's/Time used: [0-9.]*//g'                                                              \n\n     script:\n     \"\"\"\n     totalReads=\\$(echo \\$(echo -e `zcat ${reads[0]} | awk 'NR % 4 == 2' - | wc -l`))\n     q301=\\$(echo \\$(q30.py ${reads[0]}))\n     q302=\\$(echo \\$(q30.py ${reads[1]}))\n     printf \"%s\\t%s\\t%s\\t%s\\n\" \"${sample}\" \"\\$totalReads\" \"\\$q301\" \"\\$q302\" > \"${sample}.total.reads.tsv\"\n     fastqc --quiet --threads $task.cpus $reads\n     \"\"\"\n   }"], "list_proc": ["mpozuelo/QC_demux/fastqc"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["QC_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process multiqc {\n\n   publishDir \"${cluster_path}/data/05_QC/${project}/multiqc/\", mode: 'copy',\n   saveAs: { filename ->\n     if (filename.endsWith(\".html\")) filename\n   }\n\n   input:\n   path multiqc_config from ch_multiqc_config\n                               \n   path ('software_versions/*') from software_versions_yaml.collect()\n   path workflow_summary from create_workflow_summary(summary)\n   path('fastqc/*') from fastqc_results.collect().ifEmpty([])\n   path('trimgalore/*') from trimgalore_trim_mqc.collect().ifEmpty([])\n   path('trimgalore/fastqc/*') from trimgalore_fastqc_mqc.collect().ifEmpty([])\n   path('star/*') from star_logs.collect().ifEmpty([])\n   path('samtools/stats/*') from bam_stats.collect().ifEmpty([])\n   path('picard/markdups/*') from picard_mrkd_results.collect().ifEmpty([])\n   path('picard/quality_score_distribution/*') from picard_distribution_results.collect().ifEmpty([])\n           'samtools/flagstat/*'                                     \n           'samtools/idxstats/*'                                      \n   path ('rseqc/bam_stat/*') from rseqc_bam.collect().ifEmpty([])\n   path ('rseqc/read_duplication/*') from rseqc_dup.collect().ifEmpty([])\n   path \"*\" from ch_image_docs\n\n   output:\n   path \"*multiqc_report.html\"\n   path \"*_data\"\n                         \n\n   script:\n   rtitle = custom_runName ? \"--title \\\"$project\\\"\" : ''\n   rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : \"$project\"\n   \"\"\"\n   multiqc . -f $rtitle $rfilename --config $multiqc_config\n   \"\"\"\n\n }"], "list_proc": ["mpozuelo/QC_demux/multiqc"], "list_wf_names": ["mpozuelo/QC_demux"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["demultiplexing"], "list_contrib": ["mpozuelo", "mpozuelo-flomics"], "nb_contrib": 2, "codes": ["\nprocess demux_index {\n  tag \"$sample\"\n  label 'process_high'\n  publishDir \"${params.outdir}/${run_id}/${lane}/1-Index-removal/${sample}\", mode: 'copy',\n  saveAs: { filename ->\n    filename.endsWith(\".log\") ? \"logs/$filename\" : filename\n  }\n\n  input:\n  set val(sample), val(index), val(index2), val(barcode), val(run_id), val(lane), path(reads) from ch_demux\n\n  output:\n  set val(sample), path(\"*.fq.gz\"), val(barcode), val(run_id), val(lane) into ch_demux_BC\n                                                                                                                        \n  path(\"*.{fq.gz,log}\")\n\n  script:\n  discard = params.save_untrimmed ? '' : '--discard-untrimmed'\n  read1 = \"${reads[0]}\"\n  read2 = \"${reads[1]}\"\n  read1_index = \"${sample}_${run_id}_${lane}_${index}_R1.fq.gz\"\n  read2_index = \"${sample}_${run_id}_${lane}_${index}_R2.fq.gz\"\n\n  if (index == \"NNNNNNNN\") {\n    \"\"\"\n    cutadapt -l 100 -o $read1_index $read1 > \"${sample}_${run_id}_${lane}_${index}_R1.log\"\n    cutadapt -l 100 -o $read2_index $read2 > \"${sample}_${run_id}_${lane}_${index}_R2.log\"\n    \"\"\"\n  } else {\n    \"\"\"\n    cutadapt \\\n    -e $max_errors \\\n    --no-indels \\\n    -a $sample=\\\"$index\\$\\\" \\\n    -o $read2_index -p $read1_index \\\n    $read2 $read1 \\\n    $discard > ${sample}_${run_id}_${lane}_${index}.log\n    \"\"\"\n  }\n\n}"], "list_proc": ["mpozuelo/demultiplexing/demux_index"], "list_wf_names": ["mpozuelo/demultiplexing"]}, {"nb_reuse": 1, "tools": ["FastQC", "Cutadapt"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["demultiplexing"], "list_contrib": ["mpozuelo", "mpozuelo-flomics"], "nb_contrib": 2, "codes": [" process get_software_versions {\n     publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n         saveAs: { filename ->\n             if (filename.indexOf(\".csv\") > 0) filename\n             else null\n         }\n\n     output:\n     file 'software_versions_mqc.yaml' into software_versions_yaml\n     file \"software_versions.csv\"\n\n     script:\n     \"\"\"\n     fastqc --version &> v_fastqc.txt\n     cutadapt --version &> v_cutadapt.txt\n     scrape_software_versions.py &> software_versions_mqc.yaml\n     \"\"\"\n }"], "list_proc": ["mpozuelo/demultiplexing/get_software_versions"], "list_wf_names": ["mpozuelo/demultiplexing"]}, {"nb_reuse": 2, "tools": ["SAMtools", "STAR", "Sambamba", "mosdepth"], "nb_own": 2, "list_own": ["mpozuelo", "mvanins"], "nb_wf": 2, "list_wf": ["stress_granule_RNA_manuscript", "exome_mosdepth"], "list_contrib": ["mpozuelo", "mvanins"], "nb_contrib": 2, "codes": [" process get_software_versions {\n   publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n   saveAs: { filename ->\n     if (filename.indexOf(\".csv\") > 0) filename\n     else null\n   }\n\n   output:\n   file 'software_versions_mqc.yaml' into software_versions_yaml\n   file \"software_versions.csv\"\n\n   script:\n   \"\"\"\n   echo $workflow.manifest.version &> v_ngi_QC.txt\n   echo $workflow.nextflow.version &> v_nextflow.txt\n   mosdepth --version > v_mosdepth.txt\n   scrape_software_versions.py &> software_versions_mqc.yaml\n   \"\"\"\n }", " process STAR{\n    cpus 24\n    time '4h'\n    memory '75G'\n    cache 'lenient'\n    clusterOptions \"--gres=tmpspace:50G\"\n    \n    tag \"STAR Alignment ${library}\"\n    \n    publishDir \"${params.outdir}/quantification\", pattern: \"*_ReadsPerGene.out.tab\", mode:'copy', overwrite: true\n    publishDir \"${params.outdir}/alignment\", pattern: \"*.ba*\", mode:'copy', overwrite: true\n    \n    input:\n    file starIndex\n    set library, file(reads) from depleted_reads_first \n    \n    output:\n    set file(\"*_Log.final.out\"), file(\"*_ReadsPerGene.out.tab\") into multiqc_star\n    set library, file(\"${library}_Aligned.sortedByCoord.out.bam\"), file(\"${library}_Aligned.sortedByCoord.out.bam.bai\") into aligned_genome\n    set library, file(\"${library}_Aligned.toTranscriptome.out.bam\"), file(\"${library}_Aligned.toTranscriptome.out.bam.bai\") into aligned_transcriptome\n    \t\n    script:\n    \"\"\"\n    STAR \\\n    \t${params.STARmapParams} \\\n    \t--runThreadN ${task.cpus} \\\n    \t--genomeDir ${starIndex} \\\n    \t--readFilesIn ${reads} \\\n    \t--outFileNamePrefix ${library}_ \\\n    \t--outSAMattributes ${params.STARoutSamAttributes}\n    \n    samtools index ${library}_Aligned.sortedByCoord.out.bam\n    \n    sambamba sort \\\n    \t-m 2G \\\n    \t-t ${task.cpus} \\\n    \t${library}_Aligned.toTranscriptome.out.bam\n    \n    rm ${library}_Aligned.toTranscriptome.out.bam\n    \n    mv ${library}_Aligned.toTranscriptome.out.sorted.bam ${library}_Aligned.toTranscriptome.out.bam\n    mv ${library}_Aligned.toTranscriptome.out.sorted.bam.bai ${library}_Aligned.toTranscriptome.out.bam.bai\n    \n    \"\"\"\n  }"], "list_proc": ["mpozuelo/exome_mosdepth/get_software_versions", "mvanins/stress_granule_RNA_manuscript/STAR"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript", "mpozuelo/exome_mosdepth"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["exome_mosdepth"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess count_total {\n  tag \"$sample\"\n  label 'process_low'\n  publishDir \"${cluster_path}/data/05_QC/${project}/numberReadsBAM/\", mode: params.publish_dir_mode\n\n  input:\n  set val(sample), file(bam) from ch_bam_count\n\n  output:\n  path (\"*.tsv\") into ch_total_reads\n\n  script:\n  \"\"\"\n  samtools view -c $bam > \"${sample}.tsv\"\n  printf \"%s\\t%s\\n\" \"$sample\" \"\\$(echo \\$(cat ${sample}.tsv))\" > \"${sample}_total_reads.tsv\"\n  \"\"\"\n}"], "list_proc": ["mpozuelo/exome_mosdepth/count_total"], "list_wf_names": ["mpozuelo/exome_mosdepth"]}, {"nb_reuse": 1, "tools": ["mosdepth"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["exome_mosdepth"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": [" process mosdepth {\n   tag \"$sample\"\n   label 'process_low'\n   publishDir \"${cluster_path}/data/05_QC/${project}/mosdepth/${sample}\", mode: params.publish_dir_mode\n\n\n   input:\n   set val(sample), path(bam), path(bai), path(bam_subset), path(bai_subset), val(experiment), path(bed), path(interval) from ch_mosdepth\n\n   output:\n   set val(sample), path(\"${prefix}.thresholds.bed\"), path(\"${prefix_subset}.thresholds.bed\") into ch_ontarget_coverage\n                                                           \n   path \"*.{txt,gz,csi}\"\n\n   script:\n\n   prefix = \"${bam.baseName}\"\n   prefix_subset = \"${bam_subset.baseName}\"\n   threshold = params.threshold\n\n   \"\"\"\n   mosdepth \\\\\n   --by $bed \\\\\n   --fast-mode \\\\\n   --thresholds 0,1,5,10,25,50,100,200,300,400,500 \\\\\n   -Q 20 \\\\\n   -t $task.cpus \\\\\n   ${prefix} \\\\\n   $bam\n\n   pigz -dk ${prefix}.thresholds.bed.gz\n\n   mosdepth \\\\\n   --by $bed \\\\\n   --fast-mode \\\\\n   --thresholds 0,1,5,10,25,50,100,200,300,400,500 \\\\\n   -Q 20 \\\\\n   -t $task.cpus \\\\\n   ${prefix_subset} \\\\\n   $bam_subset\n\n   pigz -dk ${prefix_subset}.thresholds.bed.gz\n   \"\"\"\n }"], "list_proc": ["mpozuelo/exome_mosdepth/mosdepth"], "list_wf_names": ["mpozuelo/exome_mosdepth"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["exome_mosdepth"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess picard_hsmetrics {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${cluster_path}/data/05_QC/${project}/HSmetrics/${sample}\", mode: params.publish_dir_mode\n\n  input:\n  set val(sample), path(bam), path(bai), path(bam_subset), path(bai_subset), path(interval) from ch_picard_hsmetrics\n  file(genome) from ch_genome\n  file(index) from ch_genome_index\n\n  output:\n  path(\"${bam.baseName}.hybrid_selection_metrics.txt\") into ch_merge_original_HSmetrics\n  path(\"${bam_subset.baseName}.hybrid_selection_metrics.txt\") into ch_merge_subset_HSmetrics\n\n  script:\n  outfile = \"${bam.baseName}.hybrid_selection_metrics.txt\"\n  outfile_subset = \"${bam_subset.baseName}.hybrid_selection_metrics.txt\"\n  java_options = (task.memory.toGiga() > 42) ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2 )+\"g \"+ \"-Xmx\" + (task.memory.toGiga() - 1)+ \"g\\\"\"\n\n  \"\"\"\n  picard ${java_options} CollectHsMetrics \\\n  INPUT=$bam \\\n  OUTPUT=$outfile \\\n  TARGET_INTERVALS=$interval \\\n  BAIT_INTERVALS=$interval \\\n  TMP_DIR=tmp\n\n  picard ${java_options} CollectHsMetrics \\\n  INPUT=$bam_subset \\\n  OUTPUT=$outfile_subset \\\n  TARGET_INTERVALS=$interval \\\n  BAIT_INTERVALS=$interval \\\n  TMP_DIR=tmp\n  \"\"\"\n}"], "list_proc": ["mpozuelo/exome_mosdepth/picard_hsmetrics"], "list_wf_names": ["mpozuelo/exome_mosdepth"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mpozuelo"], "nb_wf": 1, "list_wf": ["illumina_demux"], "list_contrib": ["mpozuelo"], "nb_contrib": 1, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version &> v_ngi_rnaseq.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    fastqc --version &> v_fastqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["mpozuelo/illumina_demux/get_software_versions"], "list_wf_names": ["mpozuelo/illumina_demux"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["msmallegan"], "nb_wf": 1, "list_wf": ["rmghc-workshop-19"], "list_contrib": ["msmallegan", "jrdemasi", "hynesgra"], "nb_contrib": 3, "codes": ["\nprocess make_transcriptome {\n\n  maxForks 1\n  publishDir 'results/genome'\n\n  input:\n  file annotation from annotation_for_transcriptome\n  file genome from genome\n\n  output:\n  file \"transcriptome.fa\" into transcriptome\n  file \"gencode.vM21.annotation.gtf\"\n\n  script:\n  \"\"\"\n  gffread -w transcriptome.fa -g ${genome} ${annotation}\n  \"\"\"\n}"], "list_proc": ["msmallegan/rmghc-workshop-19/make_transcriptome"], "list_wf_names": ["msmallegan/rmghc-workshop-19"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["msmallegan"], "nb_wf": 1, "list_wf": ["rmghc-workshop-19"], "list_contrib": ["msmallegan", "jrdemasi", "hynesgra"], "nb_contrib": 3, "codes": ["\nprocess count {\n\n  publishDir 'results/feature_counts'\n\n  input:\n  set sample_id, file(bam), file(annotation) from mapped_for_count.combine(annotation_for_count)\n\n  output:\n  file '*.fCounts'\n  file '*.fCounts*' into counts\n\n  script:\n  \"\"\"\n  featureCounts  -C \\\n    -p \\\n    -T 1 \\\n    -g gene_id \\\n    -a ${annotation} \\\n    -o ${sample_id}.fCounts \\\n    ${bam}\n  \"\"\"\n}"], "list_proc": ["msmallegan/rmghc-workshop-19/count"], "list_wf_names": ["msmallegan/rmghc-workshop-19"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["msmallegan"], "nb_wf": 1, "list_wf": ["rmghc-workshop-19"], "list_contrib": ["msmallegan", "jrdemasi", "hynesgra"], "nb_contrib": 3, "codes": ["\nprocess sort_bam {\n\n  publishDir 'results/igv'\n\n  input:\n  set sample_id, file(bam_file) from mapped_for_igv\n\n  output:\n  set sample_id, file(\"*.bam\"), file('*.bai') into sorted_bam\n\n  script:\n  \"\"\"\n  samtools sort --threads 1 \\\n    -m 4G \\\n    -o ${sample_id}.bam \\\n    ${bam_file}\n  samtools index ${sample_id}.bam\n  \"\"\"\n}"], "list_proc": ["msmallegan/rmghc-workshop-19/sort_bam"], "list_wf_names": ["msmallegan/rmghc-workshop-19"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["msmallegan"], "nb_wf": 1, "list_wf": ["rmghc-workshop-19"], "list_contrib": ["msmallegan", "jrdemasi", "hynesgra"], "nb_contrib": 3, "codes": ["\nprocess multiqc {\n\n  publishDir \"results/multiqc\"\n\n  input:\n  file fastqc from fastqc_collected\n  file star from star_collected\n  file salmon from salmon_collected\n  file counts from counts_collected\n\n  output:\n  set file('*_multiqc_report.html'), file('*_data/*')\n\n  script:\n  \"\"\"\n  multiqc ${fastqc} \\\n    ${star} \\\n    ${salmon} \\\n    ${counts} \\\n    --title '${params.name}' \\\n    --cl_config \"extra_fn_clean_exts: [ '_1', '_2' ]\"\n  \"\"\"\n}"], "list_proc": ["msmallegan/rmghc-workshop-19/multiqc"], "list_wf_names": ["msmallegan/rmghc-workshop-19"]}, {"nb_reuse": 1, "tools": ["SPAdes"], "nb_own": 1, "list_own": ["multimeric"], "nb_wf": 1, "list_wf": ["AwsNextflowBug"], "list_contrib": ["multimeric"], "nb_contrib": 1, "codes": ["\nprocess spades {\n    container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\"\n    script:\n      \"\"\"\n      spades\n      \"\"\"\n}"], "list_proc": ["multimeric/AwsNextflowBug/spades"], "list_wf_names": ["multimeric/AwsNextflowBug"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process index_genome{\n    cpus 1\n    time '2h'\n    memory '5G'\n\n    publishDir \"${referenceOut}/original/\", mode:'copy', overwrite: true\n\n    tag \"indexing ${params.genomeFasta}\"\n\n    input:\n    file(genomeFasta)\n    file(genomeAnnotations)\n\n    output:\n    set file(genomeFasta), file(\"*.fai\") into genome_euk_tRNA, genome_mit_tRNA, genome_prepare, genome_annotations\n    file(\"${genomeAnnotations}\")\n    file(\"*.fai\") into geome_fai\n    file(\"${genomeFasta}\") into genome_fasta\n\n    script:\n    \"\"\"\n    samtools faidx ${genomeFasta}\n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/index_genome"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["Trnascan-SE"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process mitochondrial_tRNAscan{\n                                                                     \n                                                            \n                                                            \n\n    cpus 36\n    time '24h'\n    memory '50G'\n\n    publishDir \"${referenceOut}/tRNAScan/mitochondrial/\", mode:'copy', overwrite:true\n\n    tag \"Eukaryotic tRNAScan-SE on $params.genomeFasta\"\n\n    input:\n    set file(genome), file(fai) from genome_euk_tRNA\n\n    output:\n    file(\"${genome.baseName}.Mitochondrial_tRNAs_bp.bed\") into mito_tRNAs_bed\n    file(\"*\")\n\n    script:\n    \"\"\"\n    tRNAscan-SE \\\n      -M vert \\\n      -Q \\\n      -o# \\\n      -f# \\\n      -m# \\\n      -b# \\\n      -a# \\\n      -l# \\\n      --brief \\\n      --thread ${task.cpus} \\\n      -p ${genome.baseName}.Mitochondrial_tRNAs \\\n      ${genome}\n\n    cat ${genome.baseName}.Mitochondrial_tRNAs.out | tRNAScanToBed.pl > ${genome.baseName}.Mitochondrial_tRNAs_bp.bed\n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/mitochondrial_tRNAscan"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process create_genomes{\n  \n    time '2h'\n    memory '50G'\n    clusterOptions '--gres=tmpspace:75G'                        \n    \n    tag \"create tRNA-masked genomes for $params.genomeFasta\";\n    \n    publishDir \"${referenceOut}/tRNA-masked/\", mode:'copy', overwrite: true\n    \n    input:\n    file(eukaryotic) from eukar_tRNAs_bed\n    file(mitochondrial) from mito_tRNAs_bed\n    set file(genome), file(genome_index) from genome_prepare\n    file(annotations) from genomeAnnotations\n    \n    output:\n    file(\"${genome.baseName}.tRNA_masked_regions.bed\")\n                                                \n    file(\"${genome.baseName}.pre-tRNAs.bed\")\n    file(\"${genome.baseName}.pre-tRNAs.fa\")\n    file(\"${genome.baseName}.mature-tRNAs.bed\")\n    file(\"${genome.baseName}.mature-tRNAs.fa\")\n    file(\"${genome.baseName}.mature-tRNAs.cluster.fa\")\n    file(\"${genome.baseName}.tRNACluster.gtf\")\n    file(\"${genome.baseName}.tRNA-masked-withmature.fa\")\n    file(\"${genome.baseName}.tRNA-masked-withmature.fa.fai\")\n    file(\"${genome.baseName}.tRNA-masked-withmature.dict\")\n                                                         \n    file(\"${annotations.baseName}.tRNA.gtf\")\n    set file(\"${genome.baseName}.tRNA-masked-withmature.fa\"), file(\"${annotations.baseName}.tRNA.gtf\") into star_genome\n    set file(\"${genome.baseName}.tRNA-masked-withmature.fa\"), file(\"${genome.baseName}.tRNA-masked-withmature.fa.fai\"), file(\"${genome.baseName}.tRNA-masked-withmature.dict\") into gatk_splitCigar, gatk_bqsr1, gatk_bqsr2, gatk_hap1, gatk_hap2, gatk_annot\n    \n    script:\n    \"\"\"\n    #### tRNAs\n    # mask tRNAs in genome\n    cat ${eukaryotic} ${mitochondrial} | sort -k 1,1 -k2,2n  > ${genome.baseName}.tRNA_masked_regions.bed\n    bedtools maskfasta -fi ${genome} -fo ${genome.baseName}.tRNA_masked.fa -bed ${genome.baseName}.tRNA_masked_regions.bed\n    \n    ## pre tRNAs\n    # remove pseudogenes, add 50 nt 5' and 3' flanking regions\n    grep -v \"pseudo\\\\s\" ${eukaryotic} | expandBed12.pl --slop 50 --index ${genome_index} > ${eukaryotic.baseName}.pre-tRNAs.bed\n    \n    # select only MT tRNAs from the MT model, add flaking regions\n    sed '/^MT\\\\|^chrM/!d' ${mitochondrial} > ${mitochondrial.baseName}.only_MT.bed\n    grep -v \"pseudo\\\\s\" ${mitochondrial.baseName}.only_MT.bed | expandBed12.pl --slop 50 --index ${genome_index} > ${mitochondrial.baseName}.pre-tRNAs.bed\n    \n    # combine to pre-tRNAs.bed\n    cat ${eukaryotic.baseName}.pre-tRNAs.bed ${mitochondrial.baseName}.pre-tRNAs.bed | sort -k 1,1 -k2,2n > ${genome.baseName}.pre-tRNAs.bed\n    \n    # extract pre-tRNA sequences\n    bedtools getfasta -name -split -s -fi ${genome} -bed ${genome.baseName}.pre-tRNAs.bed -fo ${genome.baseName}.pre-tRNAs.fa\n    \n    ## mature tRNAs\n    cat ${eukaryotic} ${mitochondrial.baseName}.only_MT.bed | sort -k 1,1 -k2,2n > ${genome.baseName}.mature-tRNAs.bed\n    bedtools getfasta -name -split -s -fi ${genome} -bed ${genome.baseName}.mature-tRNAs.bed | appendFasta.pl --append cca > ${genome.baseName}.mature-tRNAs.fa\n  \n    ## cluster mature tRNAs\n    collapseSequences.pl ${genome.baseName}.mature-tRNAs.fa > ${genome.baseName}.mature-tRNAs.cluster.fa\n    # produces cluster_info.gtf\n    mv cluster_info.gtf ${genome.baseName}.tRNACluster.gtf\n    samtools faidx ${genome.baseName}.mature-tRNAs.cluster.fa\n    \n    ## assemble final genome\n    cat ${genome.baseName}.tRNA_masked.fa ${genome.baseName}.mature-tRNAs.cluster.fa > ${genome.baseName}.tRNA-masked-withmature.fa\n    samtools faidx ${genome.baseName}.tRNA-masked-withmature.fa\n\n    java -jar ${params.GATK} CreateSequenceDictionary \\\n      --REFERENCE ${genome.baseName}.tRNA-masked-withmature.fa \n    \n    #### Annotations\n    mergeGTF.pl ${annotations} ${genome.baseName}.tRNACluster.gtf > ${annotations.baseName}.tRNA.gtf\n    \n    # geneinfo file for later analysis\n    #extractGeneInfo.pl ${annotations.baseName}.tRNA.gtf > ${annotations.baseName}.tRNA.geneinfo\n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/create_genomes"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process star_reference{\n  \n    cpus 24\n    time '12h'\n    memory '100G'\n    \n    tag \"STAR reference\"\n    \n    publishDir \"${referenceOut}/\", mode:'copy', overwrite: true\n    \n    input:\n    set file(masked_genome), file(annotations) from star_genome\n    \n    output:\n    file(\"star_tRNAmasked_${params.starOverhang}\")\n    file(\"star_tRNAmasked_${params.starOverhang}\") into starIndex\n    \n    script:\n    \"\"\"\n    mkdir ./star_tRNAmasked_${params.starOverhang}\n    \n    STAR \\\n    \t--runMode genomeGenerate \\\n    \t--runThreadN ${task.cpus} \\\n    \t--genomeDir ./star_tRNAmasked_${params.starOverhang} \\\n    \t--genomeFastaFiles ${masked_genome} \\\n    \t--limitGenomeGenerateRAM 75161927680 \\\n    \t--sjdbGTFfile ${annotations} \\\n    \t--sjdbOverhang ${params.starOverhang}\n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/star_reference"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process depletion_reference{\n\n    cpus 12\n    time '12h'\n    memory '50G'\n\n    tag 'Reference for depletion'\n\n    publishDir \"${referenceOut}/depletion/\", mode:'copy', overwrite: true\n\n    input:\n    file(depleteFasta)\n\n    output:\n    file(\"bwa_depletion\")\n    file(\"bwa_depletion\") into depletionIndex\n\n\n    script:\n    \"\"\"\n    mkdir ./bwa_depletion\n    bwa index -p bwa_depletion/depletion $depleteFasta\n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/depletion_reference"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": ["\nprocess raw_fastqc {\n  cpus 6\n  time '5h'\n  memory '5G'\n  \n  tag \"FastQC on raw $library\"\n  publishDir \"${params.outdir}/QC/raw_fastqc\", mode:'copy', overwrite: true\n  \n  input:\n  set name, file(reads) from raw_reads_fastqc\n  \n  output:\n  file(\"*_fastqc.{zip,html}\") into multiqc_fastqc\n  \n  script:\n  library = name.toString().tokenize('_').get(0)\n  \"\"\"\n  fastqc -t ${task.cpus} -f fastq -q ${reads} --nogroup\n  \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/raw_fastqc"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 2, "tools": ["FastQC", "BCFtools"], "nb_own": 2, "list_own": ["mvanins", "nebfield"], "nb_wf": 2, "list_wf": ["stress_granule_RNA_manuscript", "snpQT"], "list_contrib": ["rcappa1", "mvanins", "nebfield", "ChristinaVasil"], "nb_contrib": 4, "codes": [" process trimmed_fastqc {\n    cpus 6\n    time '5h'\n    memory '5G'\n    \n    tag \"FastQC on trimmed $library\"\n    publishDir \"${params.outdir}/QC/trimmed_fastqc\", mode:'copy', overwrite: true\n    \n    input:\n    set library, file(reads) from trimmed_reads_fastqc\n    \n    output:\n    file(\"*_fastqc.{zip,html}\") into multiqc_trimmed_fastqc\n    \n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} -f fastq -q ${reads} --nogroup\n    \"\"\"\n  }", "\nprocess bcf_to_vcf {\n    label 'bcftools'\n    publishDir \"${params.results}/preImputation/files\", mode: 'copy'\n\n    input:\n    path(bcf)\n    \n    output:\n    path \"F7.vcf.gz\", emit: vcf\n    path \"F7.vcf.gz.csi\", emit: idx\n\t\n    shell:\n    '''\n    bcftools sort !{bcf} | bcftools convert -Oz --threads !{task.cpus} > F7.vcf.gz\n    bcftools index F7.vcf.gz \n    '''\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/trimmed_fastqc", "nebfield/snpQT/bcf_to_vcf"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript", "nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process STARsolo_first {\n    cpus 24\n    time '4h'\n    memory '85G'\n    cache 'lenient'\n    clusterOptions \"--gres=tmpspace:50G\"\n    \n    tag \"STARSolo first pass $library\"\n    \n    beforeScript \"source /hpc/hub_oudenaarden/mvanins/local/virtualEnvironments/venv38/bin/activate\"\n    afterScript \"deactivate\"\n    \n    input:\n    file whitelist\n    file starIndex\n    set library, file(reads) from depleted_reads_first\n    \n    output:\n    file(\"*_SJ.out.tab\") into sjtab_first\n\n    script:\n    if(params.cDNARead == \"R2\"){\n      readOrder = \"${reads[1]} ${reads[0]}\"\n    }else if(params.cDNARead == \"R1\"){\n      readOrder = \"${reads[0]} ${reads[1]}\"\n    }\n    \"\"\"\n    STAR \\\n      ${params.STARmapParams} \\\n      --runThreadN ${task.cpus} \\\n      --genomeDir ${starIndex} \\\n      --readFilesIn ${readOrder} \\\n      --outFileNamePrefix ${library}_ \\\n      --outSAMattributes ${params.STARoutSamAttributes} ${params.STARsoloSamAttributes} \\\n      --soloType CB_UMI_Simple \\\n      --soloCBwhitelist ${whitelist} \\\n      --soloCBstart ${params.CBstart} \\\n      --soloCBlen ${params.CBlen} \\\n      --soloUMIstart ${params.UMIstart} \\\n      --soloUMIlen ${params.UMIlen} \\\n      --soloBarcodeReadLength 0 \\\n      --soloStrand ${params.readStrand} \\\n      --soloFeatures Gene GeneFull SJ Velocyto \\\n      --soloUMIdedup 1MM_Directional \\\n      --soloCellFilter None\n  \n    rm -rf *.bam \n    rm -rf *.bai\n    rm -rf *.out\n    rm -rf *STARtmp\n    \n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/STARsolo_first"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["STAR", "Sambamba"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": [" process STARsolo_second {\n    cpus 24\n    time '4h'\n    memory '85G'\n    cache 'lenient'\n    clusterOptions \"--gres=tmpspace:50G\"\n    \n    tag \"STARSolo second pass $library\"\n    \n    beforeScript \"source /hpc/hub_oudenaarden/mvanins/local/virtualEnvironments/venv38/bin/activate\"\n    afterScript \"deactivate\"\n    \n    publishDir \"${params.outdir}/alignment\", pattern: \"*.ba*\", mode:'copy', overwrite: true\n    publishDir \"${params.outdir}/quantification\", pattern: \"*_Solo.out\", mode:'copy', overwrite: true\n    publishDir \"${params.outdir}/quantification\", pattern: \"*_ReadsPerGene.out.tab\", mode:'copy' , overwrite: true\n    \n    input:\n    file whitelist\n    file starIndex\n    set library, file(reads) from depleted_reads_second\n    file sjtabs from sjtab_first.collect()\n    \n    output:\n    file(\"*_Solo.out\")\n    set file(\"*_Log.final.out\"), file(\"*_ReadsPerGene.out.tab\") into multiqc_star\n    set library, file(\"${library}_Aligned.sortedByCoord.out.bam\"), file(\"${library}_Aligned.sortedByCoord.out.bam.bai\") into aligned_genome\n    set library, file(\"${library}_Aligned.toTranscriptome.out.bam\"), file(\"${library}_Aligned.toTranscriptome.out.bam.bai\") into aligned_transcriptome\n    \t\n    script:\n    if(params.cDNARead == \"R2\"){\n      readOrder = \"${reads[1]} ${reads[0]}\"\n    }else if(params.cDNARead == \"R1\"){\n      readOrder = \"${reads[0]} ${reads[1]}\"\n    }\n    \"\"\"\n    # filter from https://groups.google.com/g/rna-star/c/Cpsf-_rLK9I?pli=1\n    cat ${sjtabs} | awk '(\\$5 > 0 && \\$7 > 2 && \\$6==0)' | cut -f1-6 | sort | uniq > merged_SJ.tab\n\n    STAR \\\n      ${params.STARmapParams} \\\n      --sjdbFileChrStartEnd merged_SJ.tab \\\n      --runThreadN ${task.cpus} \\\n      --genomeDir ${starIndex} \\\n      --readFilesIn ${readOrder} \\\n      --outFileNamePrefix ${library}_ \\\n      --outSAMattributes ${params.STARoutSamAttributes} ${params.STARsoloSamAttributes} \\\n      --soloType CB_UMI_Simple \\\n      --soloCBwhitelist ${whitelist} \\\n      --soloCBstart ${params.CBstart} \\\n      --soloCBlen ${params.CBlen} \\\n      --soloUMIstart ${params.UMIstart} \\\n      --soloUMIlen ${params.UMIlen} \\\n      --soloBarcodeReadLength 0 \\\n      --soloStrand ${params.readStrand} \\\n      --soloFeatures Gene GeneFull SJ Velocyto \\\n      --soloUMIdedup 1MM_Directional \\\n      --soloCellFilter None\n\n    #addRG.pl --bam ${library}_Aligned.sortedByCoord.out.bam\n    #rm ${library}_Aligned.sortedByCoord.out.bam\n    #mv ${library}_Aligned.sortedByCoord.out.rg.bam ${library}_Aligned.sortedByCoord.out.bam\n    \n    sambamba index -t ${task.cpus} ${library}_Aligned.sortedByCoord.out.bam\n    \n    addCB.py \\\n      -b ${library}_Aligned.toTranscriptome.out.bam \\\n      -t ${task.cpus} \\\n      -c ${whitelist} \n    \n    sambamba sort \\\n      -m 2G \\\n      -t ${task.cpus} \\\n      ${library}_Aligned.toTranscriptome.out_CB.bam\n    \n    rm ${library}_Aligned.toTranscriptome.out.bam\n    rm ${library}_Aligned.toTranscriptome.out_CB.bam\n    \n    mv ${library}_Aligned.toTranscriptome.out_CB.sorted.bam ${library}_Aligned.toTranscriptome.out.bam\n    mv ${library}_Aligned.toTranscriptome.out_CB.sorted.bam.bai ${library}_Aligned.toTranscriptome.out.bam.bai\n    \n    find . -type f \\\\( -name '*.tsv' -o -name '*.mtx' \\\\) -exec gzip \"{}\" \\\\;\n    \"\"\"\n  }"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/STARsolo_second"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": ["\nprocess sort_dedup_genome {\n  memory '50G'\n  cpus 12\n  time '2h'\n  clusterOptions \"--gres=tmpspace:50G\"\n  \n  tag \"sorting deduped $library\"\n\n  publishDir \"${params.outdir}/deduplicate\", mode:'copy', overwrite: true\n\n  input:\n  set library, file(bam) from deduplicated_genome\n\n  output:\n  set file(\"${bam.baseName}.sorted.bam\"), file(\"${bam.baseName}.sorted.bam.bai\") into sorted_deduplicated_genome\n  file(\"${bam.baseName}.sorted.bam\")\n  file(\"${bam.baseName}.sorted.bam.bai\")\n  \n  script:\n  \"\"\"\n  sambamba sort \\\n    -m 4G \\\n    -t ${task.cpus} \\\n    ${bam}\n\n  \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/sort_dedup_genome"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": ["\nprocess merge_genome{\n  memory '50G'\n  cpus 12\n  time '2h'\n  clusterOptions \"--gres=tmpspace:50G\"\n  \n  tag \"merging genome alignments\"\n\n  input:\n  file(\"*\") from sorted_deduplicated_genome.collect()\n\n  output:\n  set file(\"merged.bam\"), file(\"merged.bam.bai\") into merged_genome\n  \n  script:\n  \"\"\"\n  sambamba merge \\\n    -t ${task.cpus} \\\n    merged.bam \\\n    *.bam\n  \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/merge_genome"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": ["\nprocess gather_splitCigar{\n  memory '50G'\n  cpus 12\n  time '2h'\n  clusterOptions \"--gres=tmpspace:50G\"\n  \n  tag \"merging genome alignments\"\n  publishDir \"${params.outdir}/gatk/splitCigar\", mode:'copy', overwrite: true\n\n  input:\n  file(\"*\") from splitCigar_gather.collect()\n\n  output:\n  set file(\"merged_split.bam\"), file(\"merged_split.bam.bai\") into gathered_splitCigar1, gathered_splitCigar2\n  file(\"merged_split.bam\")\n  file(\"merged_split.bam.bai\")\n  \n  script:\n  \"\"\"\n  sambamba merge \\\n    -t ${task.cpus} \\\n    merged_split.bam \\\n    *.bam.split\n  \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/gather_splitCigar"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": ["\nprocess select_annotate_variants{\n  cpus 1\n  memory '20G'\n  time '10h'\n  clusterOptions \"--gres=tmpspace:5G\"\n\n  tag \"Selecting variants\"\n\n  publishDir \"${params.outdir}/gatk/haplotype\", mode:'copy', overwrite: true\n\n  input:\n  set file(variants), file(variantsIndex) from hap_second_merged\n  file(exonsIntervals) from exonsIntervals\n  file(intronsIntervals) from intronsIntervals\n  set file(genomeFasta), file(genomeFai), file(genomeDict) from gatk_annot\n  file(exonsBed) from exonsBed\n  set file(knownVariants), file(knownVarIndex) from knownVar_annot\n\n  output:\n  file(\"haplotype_merged_known_filtered_introns.vcf.gz\")\n  file(\"haplotype_merged_filtered_exons_transcriptome_annot.vcf.gz\")\n  file(\"haplotype_merged_filtered_exons_transcriptome_annot_SB.txt.gz\")\n\n\n  script:\n  \"\"\"\n  ## select exonic and intronic variants\n  java -jar ${params.GATK} SelectVariants \\\n    -V ${variants} \\\n    --intervals ${exonsIntervals} \\\n    -O temp_filtered_exons.vcf.gz \n\n  java -jar ${params.GATK} SelectVariants \\\n    -V ${variants} \\\n    --intervals ${intronsIntervals} \\\n    -O temp_filtered_introns.vcf.gz\n\n  java -jar ${params.GATK} SelectVariants \\\n    -V temp_filtered_exons.vcf.gz \\\n    --discordance ${knownVariants} \\\n    -O temp_known_filtered_exons.vcf.gz\n  \n  java -jar ${params.GATK} SelectVariants \\\n    -V temp_filtered_introns.vcf.gz \\\n    --discordance ${knownVariants} \\\n    -O haplotype_merged_known_filtered_introns.vcf.gz\n\n  ## annotate\n  echo '##INFO=<ID=STRAND,Number=1,Type=String,Description=\"Strand from bed/gtf file\">' > annot.hdr\n  echo '##INFO=<ID=GENE,Number=1,Type=String,Description=\"ENSG__Gene__Biotype from bed/gtf file\">' >> annot.hdr\n\n  tabix -p bed ${exonsBed}\n  bcftools annotate \\\n    -a  ${exonsBed}\\\n    -h annot.hdr \\\n    -c CHROM,FROM,TO,STRAND,GENE temp_known_filtered_exons.vcf.gz | \\\n  gzip -c > temp_transannotated.vcf.gz\n\n  annotateSequenceContext.pl \\\n    -r ${genomeFasta} \\\n    -v temp_transannotated.vcf.gz \\\n    -o haplotype_merged_filtered_exons_transcriptome_annot.vcf.gz\n\n  java -jar ${params.GATK} VariantsToTable \\\n    -V haplotype_merged_filtered_exons_transcriptome_annot.vcf.gz \\\n    -F CHROM -F POS -F REF -F ALT -F TYPE -F FILTER -F QUAL -F STRAND -F GENE -F CONTEXT -GF SB \\\n    -O haplotype_merged_filtered_exons_transcriptome_annot_SB.txt \\\n    --moltenize true\n\n  sed -i '/\\\\tNA\\$/d' haplotype_merged_filtered_exons_transcriptome_annot_SB.txt\n\n  gzip haplotype_merged_filtered_exons_transcriptome_annot_SB.txt\n\n  rm temp_*\n\n  \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/select_annotate_variants"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["mvanins"], "nb_wf": 1, "list_wf": ["stress_granule_RNA_manuscript"], "list_contrib": ["mvanins"], "nb_contrib": 1, "codes": ["\nprocess multiQC {\n  time '1h'\n  memory '5G'\n\n  publishDir \"${params.outdir}/QC\", mode:'copy', overwrite: true\n\n  beforeScript \"source /hpc/hub_oudenaarden/mvanins/local/virtualEnvironments/mqcdev/bin/activate\"\n  afterScript \"deactivate\"\n  \n  input:\n  file('*') from multiqc_fastqc.collect().ifEmpty([])\n  file('*') from multiqc_trimmed_fastqc.collect().ifEmpty([])\n        file('*') from multiqc_depleted_fastqc.collect().ifEmpty([]) \n  file('*') from multiqc_cutadapt.collect().ifEmpty([])\n  file('*') from multiqc_star.collect().ifEmpty([])\n  file('*') from multiqc_deduplicate_genome.collect().ifEmpty([])\n\n  output:\n  file \"multiqc_report.html\"\n  file \"multiqc_data\"\n\n  script:\n  \"\"\"\n  multiqc .\n  \"\"\"\n}"], "list_proc": ["mvanins/stress_granule_RNA_manuscript/multiQC"], "list_wf_names": ["mvanins/stress_granule_RNA_manuscript"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nanjalaruth"], "nb_wf": 1, "list_wf": ["HLA-typing"], "list_contrib": ["nanjalaruth"], "nb_contrib": 1, "codes": ["process extract_hla_reads {\n    tag \"Extracting reads aligned to the HLA loci\"\n    publishDir \"${params.outDir}/hla_reads\", mode: 'copy', overwrite: false\n    \n    input:\n        tuple val(dataset), path(sorted_bam)\n    output:\n        tuple val(dataset), path(partial_bam)\n    script:\n        partial_bam = \"${dataset}_partial.bam\"\n        \"\"\"\n        samtools index ${sorted_bam}\n        samtools view -b ${sorted_bam} \"6:29000000-34000000\" > ${partial_bam}\n        \"\"\"\n}"], "list_proc": ["nanjalaruth/HLA-typing/extract_hla_reads"], "list_wf_names": ["nanjalaruth/HLA-typing"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nanjalaruth"], "nb_wf": 1, "list_wf": ["HLA-typing"], "list_contrib": ["nanjalaruth"], "nb_contrib": 1, "codes": ["\nprocess read_pairs_search {\n    tag \"Convert bam to fastq\"\n    publishDir \"${params.outDir}/typing\", mode: 'copy', overwrite: false\n    \n    input:\n        tuple val(dataset), path(partial_bam)\n    output:\n        tuple val(dataset), path(\"unzipped_{1,2}.fastq\")\n    script:\n        if( !params.single_end)\n            \"\"\"\n            samtools fastq -1 unzipped_1.fastq -2 unzipped_2.fastq ${partial_bam}\n            \"\"\"\n        else\n            \"\"\"\n            samtools fastq ${partial_bam} > unzipped_1.fastq \n            \"\"\"\n}"], "list_proc": ["nanjalaruth/HLA-typing/read_pairs_search"], "list_wf_names": ["nanjalaruth/HLA-typing"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nanjalaruth"], "nb_wf": 1, "list_wf": ["HLA-typing"], "list_contrib": ["nanjalaruth"], "nb_contrib": 1, "codes": ["\nprocess unmapped_reads {\n    tag \"Extract unmapped reads\"\n    publishDir \"${params.outDir}/typing\", mode: 'copy', overwrite: false\n    \n    input:\n        tuple val(dataset), path(sorted_bam)\n    output:\n        tuple val(dataset), path(\"unmapped_{1,2}.fastq\")\n    script:\n        if( !params.single_end)\n            \"\"\"\n            samtools view -bh -f 12 ${sorted_bam} > ${dataset}.sorted_unmapped.bam\n            java -jar /usr/local/bin/SamToFastq.jar I=${dataset}.sorted_unmapped.bam F=unmapped_1.fastq F2=unmapped_2.fastq\n            \"\"\"\n        else\n            \"\"\"\n            samtools view -bh -f 12 ${sorted_bam} > ${dataset}.sorted_unmapped.bam\n            java -jar /usr/local/bin/SamToFastq.jar I=${dataset}.sorted_unmapped.bam F=unmapped_1.fastq\n            \"\"\"\n}"], "list_proc": ["nanjalaruth/HLA-typing/unmapped_reads"], "list_wf_names": ["nanjalaruth/HLA-typing"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["nanjalaruth"], "nb_wf": 1, "list_wf": ["HLA-typing"], "list_contrib": ["nanjalaruth"], "nb_contrib": 1, "codes": ["\nprocess map_to_hla_loci {\n    tag \"Searching read pairs and their sequences on HLA loci\"\n    publishDir \"${params.outDir}/typing\", mode: 'copy', overwrite: false\n    \n    input:\n        tuple val(dataset), path(fastq)\n        path(ref)\n    output:\n        tuple val(dataset), path(sam)\n    script:\n        sam = \"${dataset}_part.sam\"\n        if( !params.single_end)\n            \"\"\"\n            #mapping using -p to map paired end as single-end\n            #index reference\n            bwa index ${ref}\n            #sort read pairs\n            awk '{printf substr(\\$0,1,length-2);getline;printf \"\\\\t\"\\$0;getline;getline;print \"\\\\t\"\\$0}' ${fastq[0]} | sort -S 8G -T. > read1.txt\n            awk '{printf substr(\\$0,1,length-2);getline;printf \"\\\\t\"\\$0;getline;getline;print \"\\\\t\"\\$0}' ${fastq[1]} | sort -S 8G -T. > read2.txt\n            join read1.txt read2.txt | awk '{print \\$1\"\\\\n\"\\$2\"\\\\n+\\\\n\"\\$3 > \"r1.fq\";print \\$1\"\\\\n\"\\$4\"\\\\n+\\\\n\"\\$5 > \"r2.fq\"}'\n            #mapping\n            bwa mem -t 8 -P -L 10000 -a ${ref} r1.fq r2.fq -p > ${sam}\n            #bwa mem -t 8 -P -L 10000 -a ${ref} r1.fq r2.fq > ${sam}\n            \"\"\"\n        else\n            \"\"\"\n            #mapping using -p to map paired end as single-end\n            #index reference\n            bwa index ${ref}\n\n            #mapping\n            bwa mem -t 8 -P -L 10000 -a ${ref} ${fastq[0]} > ${sam}\n            \"\"\"\n\n}"], "list_proc": ["nanjalaruth/HLA-typing/map_to_hla_loci"], "list_wf_names": ["nanjalaruth/HLA-typing"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["nanjalaruth"], "nb_wf": 1, "list_wf": ["HLA-typing"], "list_contrib": ["nanjalaruth"], "nb_contrib": 1, "codes": ["\nprocess mapping {\n    tag \"Mapping ${dataset} to the reference genome\"\n    publishDir \"${params.outDir}/fastq_preprocessing\", mode: 'copy', overwrite: false\n    label \"bigmem\"\n    \n    input:\n        tuple val(dataset), path(reads), path(ref)\n    output:\n        tuple val(dataset), path(mapping_out)\n    script:\n        mapping_out = \"${dataset}.sam\"\n        if( !params.single_end)\n            \"\"\"\n            minimap2 -t 32 -ax sr ${ref} ${reads[0]} ${reads[1]} > ${mapping_out}\n            \"\"\"\n        else\n            \"\"\"\n            minimap2 -ax sr ${ref} ${reads[0]} > ${mapping_out}  \n            \"\"\"\n}"], "list_proc": ["nanjalaruth/HLA-typing/mapping"], "list_wf_names": ["nanjalaruth/HLA-typing"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nanjalaruth"], "nb_wf": 1, "list_wf": ["HLA-typing"], "list_contrib": ["nanjalaruth"], "nb_contrib": 1, "codes": ["\nprocess sam_bam {\n    tag \"Converting sam to bam and sorting the bam file\"\n    publishDir \"${params.outDir}/fastq_preprocessing\", mode: 'copy', overwrite: false\n    \n    input:\n        tuple val(dataset), path(sam_file)\n    output:\n        tuple val(dataset), path(sorted_bam)\n    script:\n        bam_file = \"${dataset}.bam\"\n        sorted_bam = \"${dataset}.sorted.bam\"\n        \"\"\"\n        samtools view -@ 20 -Sb ${sam_file} > ${bam_file}\n        samtools sort -@ 20 -O bam -o ${sorted_bam} ${bam_file}\n        \"\"\"\n}"], "list_proc": ["nanjalaruth/HLA-typing/sam_bam"], "list_wf_names": ["nanjalaruth/HLA-typing"]}, {"nb_reuse": 1, "tools": ["project"], "nb_own": 1, "list_own": ["nanozoo"], "nb_wf": 1, "list_wf": ["wf_prepare_submission_dammit"], "list_contrib": ["replikation", "hoelzer"], "nb_contrib": 2, "codes": ["\nprocess xml_get_study {\n  label 'label'  \n  publishDir \"${params.output}/\", mode: 'copy', pattern: \"project.xml\"\n  \n  input:\n    file(yml)\n    file(script)\n  \n  output:\n    file(\"project.xml\")\n  \n  script:\n    \"\"\"\n    #!/usr/bin/env bash\n\n    source ${script}\n    eval \\$(parse_yml ${yml} \"CONF_\")\n\n    cat >project.xml <<EOL\n<PROJECT_SET>\n   <PROJECT alias=\"\\${CONF_study_alias}\">\n      <TITLE>\"\\${CONF_study_title}\"</TITLE>\n      <DESCRIPTION>\"\\${CONF_study_description}\"</DESCRIPTION>\n      <SUBMISSION_PROJECT>\n         <SEQUENCING_PROJECT/>\n      </SUBMISSION_PROJECT>\n   </PROJECT>\n</PROJECT_SET>\n\n    \"\"\"\n}"], "list_proc": ["nanozoo/wf_prepare_submission_dammit/xml_get_study"], "list_wf_names": ["nanozoo/wf_prepare_submission_dammit"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runFastqc {\n    tag { filename + ' - Fastqc' }\n\n    publishDir \"${params.outDir}/${filename}/fastqc\", mode: 'copy'\n\n    input:\n    set group,\n        sample,\n        filename,\n        copy_path,\n        file(r1),\n        file(r2) from ch_fastqc\n\n    output:\n    file \"*.{html,zip}\" into results_fastqc\n\n    script:\n    \"\"\"\n    fastqc \\\n        -t ${task.cpus} \\\n        -o . \\\n        ${r1} ${r2}\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runFastqc"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 1, "tools": ["Bismark"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runBismarkPE {\n    tag { filename + ' - BismarkPE' }\n\n    publishDir \"${params.outDir}/${filename}/bismark\", mode: 'copy'\n\n    input:\n    set filename,\n        file(val_1),\n        file(val_2) from ch_BismarkPE\n\n    output:\n    file \"${filename}*.{bam,gz,txt,log}\" into results_BismarkPE\n    set filename,\n        file(\"${filename}_R1_001_val_1.fq.gz_unmapped_reads_1.fq.gz\") optional true into bismarkR1_channel\n    set filename,\n        file(\"${filename}_R2_001_val_2.fq.gz_unmapped_reads_2.fq.gz\") optional true into bismarkR2_channel\n    set filename,\n        file(\"${filename}_R1_001_val_1_bismark_bt2_pe.bam\") optional true into bamPE_channel\n\n    script:\n    \"\"\"\n    echo -e \"Part 1 - Running bismark_alignment for PE directional on both R1 and R2:\\n\"\n\n    /homes/nader.aryamanesh/apps_nader/Bismark-master/bismark \\\n        -N 1 -L 20 --un \\\n        --parallel 2 \\\n        -p ${task.cpus} \\\n        /homes/nader.aryamanesh/projects/191111_pig_embryo_wgbs/reference \\\n        -1 ${val_1} -2 ${val_2}\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runBismarkPE"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 2, "tools": ["Bismark", "PLINK"], "nb_own": 2, "list_own": ["nebfield", "naryamanesh"], "nb_wf": 2, "list_wf": ["snpQT", "nf-bismarkAlignment"], "list_contrib": ["rcappa1", "ChristinaVasil", "nebfield", "naryamanesh"], "nb_contrib": 4, "codes": ["\nprocess individual_missingness {\n  label 'plink1'\n  \n  input:\n  path(C1_bed)\n  path(C1_bim)\n  path(C1_fam)\n\n  output:\n  path \"C2.bed\", emit: bed\n  path \"C2.bim\", emit: bim\n  path \"C2.fam\", emit: fam\n  path \"C2.log\", emit: log\n  path \"before.imiss\", emit: imiss_before\n  path \"after.imiss\", emit: imiss_after\n  \n  \n  shell:\n  '''\n  plink --bfile !{C1_bed.baseName} \\\n    --missing \\\n    --out before\n  plink --bfile !{C1_bed.baseName} \\\n    --make-bed \\\n    --mind !{params.mind} \\\n    --out C2 \n  plink --bfile C2 \\\n    --missing \\\n    --out after\n  '''\n}", "\nprocess runBismarkR1 {\n    tag { filename + ' - BismarkR1' }\n\n    publishDir \"${params.outDir}/${filename}/bismark\", mode: 'copy'\n\n    input:\n    set filename,\n        file(un_fq_1) from bismarkR1_channel\n\n    output:\n    file \"${filename}*.{bam,gz,txt,log}\" into results_BismarkR1\n    set filename,\n        file(\"${filename}_R1_001_val_1.fq.gz_unmapped_reads_1_bismark_bt2.bam\") optional true into bamR1_channel\n\n    script:\n    \"\"\"\n    echo -e \"Part 2 - Running bismark_alignment for SE directional on unmapped R1:\\n\"\n\n    /homes/nader.aryamanesh/apps_nader/Bismark-master/bismark \\\n    -N 1 -L 20 \\\n    --parallel 2 \\\n    -p ${task.cpus} \\\n    /homes/nader.aryamanesh/projects/191111_pig_embryo_wgbs/reference \\\n    ${un_fq_1}\n    \"\"\"\n}"], "list_proc": ["nebfield/snpQT/individual_missingness", "naryamanesh/nf-bismarkAlignment/runBismarkR1"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment", "nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["Bismark"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runBismarkR2 {\n    tag { filename + ' - BismarkR2' }\n\n    publishDir \"${params.outDir}/${filename}/bismark\", mode: 'copy'\n\n    input:\n    set filename,\n        file(un_fq_2) from bismarkR2_channel\n\n    output:\n    file \"${filename}*.{bam,gz,txt,log}\" into results_BismarkR2\n    set filename,\n        file(\"${filename}_R2_001_val_2.fq.gz_unmapped_reads_2_bismark_bt2.bam\") optional true into bamR2_channel\n\n    script:\n    \"\"\"\n    echo -e \"Part 3 - Running bismark_alignment for SE directional pbat on unmapped R2:\\n\"\n\n    /homes/nader.aryamanesh/apps_nader/Bismark-master/bismark \\\n        -N 1 -L 20 --pbat \\\n        --parallel 2 \\\n        -p ${task.cpus} \\\n        /homes/nader.aryamanesh/projects/191111_pig_embryo_wgbs/reference \\\n        ${un_fq_2}\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runBismarkR2"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runSamtools_view {\n    tag { filename + ' - samtools_view' }\n\n    publishDir \"${params.outDir}/${filename}/samtools\", mode: 'copy'\n\n    input:\n    set filename,\n        file(bamPE) from ch_Samtools_bamPE\n    set filename,\n        file(bamR1) from ch_Samtools_bamR1\n    set filename,\n        file(bamR2) from ch_Samtools_bamR2\n\n    output:\n    set filename,\n        file(\"${filename}_bismark_PE.bam\"),\n        file(\"${filename}_unmapped_bismark_SE1.bam\"),\n        file(\"${filename}_unmapped_bismark_SE2.bam\") optional true into merge_channel\n\n    script:\n    \"\"\"\n    echo -e \"Running samtools view\"\n\t\tsamtools view -@ ${task.cpus} -b -h ${bamPE} > ${filename}_bismark_PE.bam\n\t\tsamtools view -@ ${task.cpus} -b -h ${bamR1} > ${filename}_unmapped_bismark_SE1.bam\n\t\tsamtools view -@ ${task.cpus} -b -h ${bamR2} > ${filename}_unmapped_bismark_SE2.bam\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runSamtools_view"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runSamtools_merge {\n    tag { filename + ' - samtools_merge' }\n\n    publishDir \"${params.outDir}/${filename}/samtools\", mode: 'copy'\n\n    input:\n    set filename,\n        file(bamPE),\n        file(bamR1),\n        file(bamR2) from merge_channel\n\n    output:\n    set filename,\n        file(\"${filename}_bismark_combined.bam\") optional true into sam_merge_channel\n\n    script:\n    \"\"\"\n    echo -e \"combining bam files\"\n\t\tsamtools merge -@ ${task.cpus} -h ${bamPE} ${filename}_bismark_combined.bam ${bamPE} ${bamR1} ${bamR2}\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runSamtools_merge"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runSamtools_merge_sort {\n    tag { filename + ' - samtools_sort' }\n\n    publishDir \"${params.outDir}/${filename}/samtools\", mode: 'copy'\n\n    input:\n    set filename,\n        file(bam_merge) from ch_Samtools_merge_sort\n\n    output:\n    set filename,\n        file(\"${filename}_bismark_combined_sort.bam\") optional true into sam_sort_channel\n\n    script:\n    \"\"\"\n    samtools sort -@ ${task.cpus} -o ${filename}_bismark_combined_sort.bam ${bam_merge}\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runSamtools_merge_sort"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["naryamanesh"], "nb_wf": 1, "list_wf": ["nf-bismarkAlignment"], "list_contrib": ["naryamanesh"], "nb_contrib": 1, "codes": ["\nprocess runSamtools_merge_index {\n    tag { filename + ' - samtools_index' }\n\n    publishDir \"${params.outDir}/${filename}/samtools\", mode: 'copy'\n\n    input:\n    set filename,\n        file(bam_sort) from ch_Samtools_merge_index\n\n    output:\n    file \"${filename}*.{bai}\" into results_Samtools_index\n\n    script:\n    \"\"\"\n    samtools index ${bam_sort}\n    \"\"\"\n}"], "list_proc": ["naryamanesh/nf-bismarkAlignment/runSamtools_merge_index"], "list_wf_names": ["naryamanesh/nf-bismarkAlignment"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR", "GATK"], "nb_own": 1, "list_own": ["naveen584"], "nb_wf": 1, "list_wf": ["nextflow-variantcalling"], "list_contrib": ["naveen584", "andrewmlynn"], "nb_contrib": 2, "codes": ["\nprocess genomePreparation {\n  input:\n  file genome from g\n\n  output:\n  file \"genomeDir\" into genome_dir\n\n  script:\n  genomeSAindexNbases = \"--genomeSAindexNbases ${params.sa}\"\n  runThreadN = \"--runThreadN ${params.threads}\"\n  \"\"\"\n  mkdir genomeDir\n  cp $genome genomeDir/genome.fa\n  samtools faidx genomeDir/genome.fa\n  gatk CreateSequenceDictionary --REFERENCE genomeDir/genome.fa --OUTPUT=genomeDir/genome.dict\n  STAR --runMode genomeGenerate --genomeDir genomeDir --genomeFastaFiles genomeDir/genome.fa $genomeSAindexNbases $runThreadN\n  \"\"\"\n}"], "list_proc": ["naveen584/nextflow-variantcalling/genomePreparation"], "list_wf_names": ["naveen584/nextflow-variantcalling"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["naveen584"], "nb_wf": 1, "list_wf": ["nextflow-variantcalling"], "list_contrib": ["naveen584", "andrewmlynn"], "nb_contrib": 2, "codes": ["\nprocess runStar {\n  input:\n  file genome from genome_dir_1.collect()\n  file reads1 from r1\n  file reads2 from r2\n\n  output:\n  file \"star_rundir\" into star_rundir\n\n  script:\n  runThreadN = \"--runThreadN ${params.threads}\"\n  \"\"\"\n  mkdir star_rundir\n  cd star_rundir\n  STAR --genomeDir ../$genome/ --readFilesIn ../$reads1 ../$reads2 $runThreadN\n  \"\"\"\n}"], "list_proc": ["naveen584/nextflow-variantcalling/runStar"], "list_wf_names": ["naveen584/nextflow-variantcalling"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["naveen584"], "nb_wf": 1, "list_wf": ["nextflow-variantcalling"], "list_contrib": ["naveen584", "andrewmlynn"], "nb_contrib": 2, "codes": ["\nprocess runGatk {\n  publishDir \"${params.outdir}\", mode: 'copy'\n\n  input:\n  file genome from genome_dir_2.collect()\n  file star_rundir from star_rundir.collect()\n\n  output:\n  file \"Aligned_sorted_RG_index.marked_split.bam\" into bam\n  file \"Aligned_sorted_RG_dup_metrics\" into metrics\n  file \"variants.vcf\" into variants\n  file \"indels.vcf\" into indels\n\n  script:\n  rgid = \"--RGID=${params.rgid}\"\n  rglb = \"--RGLB=${params.rglb}\"\n  rgpl = \"--RGPL=${params.rgpl}\"\n  rgsm = \"--RGSM=${params.rgsm}\"\n  rgpu = \"--RGPU=${params.rgpu}\"\n  \"\"\"\n  mkdir gatk_rundir\n  cd gatk_rundir\n  gatk SamFormatConverter -I ../$star_rundir/Aligned.out.sam -O Aligned.out.bam\n  gatk SortSam -I Aligned.out.bam -O Aligned_sorted.bam -SO coordinate\n  gatk AddOrReplaceReadGroups -I Aligned_sorted.bam -O Aligned_sorted_RG.bam --SORT_ORDER=coordinate $rgid $rglb $rgpl $rgsm $rgpu\n  gatk BuildBamIndex -I Aligned_sorted_RG.bam\n  gatk MarkDuplicates -I Aligned_sorted_RG.bam -O Aligned_sorted_RG_index.marked.bam --METRICS_FILE=Aligned_sorted_RG_dup_metrics --VALIDATION_STRINGENCY=LENIENT --CREATE_INDEX=true --REMOVE_DUPLICATES=true\n  gatk SplitNCigarReads -R ../$genome/genome.fa -I Aligned_sorted_RG_index.marked.bam -O Aligned_sorted_RG_index.marked_split.bam\n  gatk HaplotypeCaller -R ../$genome/genome.fa -I Aligned_sorted_RG_index.marked_split.bam -O variants.vcf\n  gatk SelectVariants -R ../$genome/genome.fa -V variants.vcf -O indels.vcf --select-type-to-include INDEL\n  mv Aligned_sorted_RG_index.marked_split.bam Aligned_sorted_RG_dup_metrics variants.vcf indels.vcf ../\n  \"\"\"\n}"], "list_proc": ["naveen584/nextflow-variantcalling/runGatk"], "list_wf_names": ["naveen584/nextflow-variantcalling"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess variant_missingness {\n  label 'plink1'\n  \n  input:\n  path(in_bed)\n  path(in_bim)\n  path(in_fam)\n\n  output:\n  path \"C1.bed\", emit: bed\n  path \"C1.bim\", emit: bim\n  path \"C1.fam\", emit: fam\n  path \"C1.log\", emit: log\n\n  shell:\n  '''\n  plink --bfile !{in_bed.baseName} \\\n      --fam !{in_fam} \\\n      --make-bed \\\n      --out data \n  plink --bfile data \\\n      --geno 0.1 \\\n      --make-bed  \\\n      --out C1 \n  '''\n}"], "list_proc": ["nebfield/snpQT/variant_missingness"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess check_sex {\n    label 'plink1'\n\t\n    input:\n    path(C2_bed)\n    path(C2_bim)\n    path(C2_fam)\n    \n    output:\n    path \"C3.bed\", emit: bed \n    path \"C3.bim\", emit: bim\n    path \"C3.fam\", emit: fam\n    path \"C3.log\", emit: log\n    path \"before.sexcheck\", emit: sexcheck_before\n    path \"after.sexcheck\", emit: sexcheck_after\n \n    \n    shell:\n    '''\n    plink --bfile !{C2_bed.baseName} \\\n        --check-sex \\\n        --out before\n    # Identify the samples with sex discrepancies \n    grep \"PROBLEM\" before.sexcheck | awk '{print $1,$2}'> \\\n        problematic_samples.txt\n    # Delete all problematic samples\n    plink --bfile !{C2_bed.baseName} \\\n        --remove problematic_samples.txt \\\n        --make-bed \\\n        --out C3\n    plink --bfile C3 \\\n        --check-sex \\\n\t--out after \n    '''\n}"], "list_proc": ["nebfield/snpQT/check_sex"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess heterozygosity_rate {\n    label 'plink1'\n\t\n    input:\n    path(C4_bed)\n    path(C4_bim)\n    path(C4_fam)\n    \n    output:\n    path \"C5_het.het\", emit: het\n\t\n    shell:\n    '''    \n    plink --bfile !{C4_bed.baseName} \\\n        --het \\\n        --out C5_het\n    '''\n}"], "list_proc": ["nebfield/snpQT/heterozygosity_rate"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess heterozygosity_prune {\n    label 'plink1'\n\t\n    input:\n    path(C4_bed)\n    path(C4_bim)\n    path(C4_fam)\n    path(het_failed)\n\n    output:\n    path \"C5.bed\", emit: bed\n    path \"C5.bim\", emit: bim\n    path \"C5.fam\", emit: fam\n    path \"C5.log\", emit: log\n    path \"C5_after.het\", emit: het\n\n    shell:\n    '''\n\tif !{params.heterozygosity};\n\tthen\n\t\tcut -f 1,2 !{het_failed} > het_failed_plink.txt\n\t\tplink --bfile !{C4_bim.baseName} \\\n\t\t  --make-bed \\\n\t\t  --remove het_failed_plink.txt \\\n\t\t  --out C5\n\t\tplink --bfile C5 \\\n\t\t  --het \\\n\t\t  --out C5_after\n\telse \n\t\tplink --bfile !{C4_bim.baseName} \\\n\t\t  --make-bed \\\n\t\t  --out C5\n\t\tplink --bfile C5 \\\n\t\t  --het \\\n\t\t  --out C5_after\n\tfi\n    '''\n}"], "list_proc": ["nebfield/snpQT/heterozygosity_prune"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess missing_phenotype {\n    label 'plink1'\n \n    input:\n    path(C6_bed)\n    path(C6_bim)\n    path(C6_fam)\n\n    output:\n    path \"C7.bed\", emit: bed\n    path \"C7.bim\", emit: bim\n    path \"C7.fam\", emit: fam\n    path \"C7.log\", emit: log\n\n    shell:\n    '''\n    plink --bfile !{C6_bed.baseName} \\\n        --prune \\\n        --make-bed \\\n        --out C7\n    '''\n}"], "list_proc": ["nebfield/snpQT/missing_phenotype"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess mpv {\n    label 'plink1'\n\t\n    input:\n    path(E7_bed)\n    path(E7_bim)\n    path(E7_fam)\n\n    output:\n    path \"E8.bed\", emit: bed\n    path \"E8.bim\", emit: bim\n    path \"E8.fam\", emit: fam\n    path \"E8.log\", emit: log\n    path \"E8_before.lmiss\", emit: lmiss_before\n    path \"E8_after.lmiss\", emit: lmiss_after\n\t\n    shell:\n    '''\n    plink --bfile !{E7_bed.baseName} \\\n        --missing \\\n\t--out E8_before\n    plink --bfile !{E7_bed.baseName} \\\n        --geno !{params.variant_geno} \\\n    \t--make-bed \\\n\t--out E8 \n    plink --bfile E8 \\\n\t--missing \\\n\t--out E8_after\n    '''\n}"], "list_proc": ["nebfield/snpQT/mpv"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess hardy {\n    label 'plink1'\n\t\n    input:\n    path(E8_bed)\n    path(E8_bim)\n    path(E8_fam)\n\n    output:\n    path \"E9.bed\", emit: bed\n    path \"E9.bim\", emit: bim\n    path \"E9.fam\", emit: fam\n    path \"E9.log\", emit: log\n    path \"plink_sub_before.hwe\", emit: sub_before\n    path \"plinkzoomhwe_before.hwe\", emit: zoom_before\n    path \"plink_sub_after.hwe\", emit: sub_after\n    path \"plinkzoomhwe_after.hwe\", emit: zoom_after\n\n    shell: \n    '''\n    plink --bfile !{E8_bed.baseName} \\\n\t--hardy \\\n\t--out plink_before\n    # sample 1% of SNPs\n    head -n1 plink_before.hwe > plink_sub_before.hwe\n    awk 'BEGIN {srand()} !/^$/ { if (rand() <= .01) print $0}' < plink_before.hwe >> plink_sub_before.hwe\n    awk '{ if ($3==\"TEST\" || $3==\"UNAFF\" && $9 <0.001) print $0 }' \\\n\tplink_before.hwe > plinkzoomhwe_before.hwe\n    plink --bfile !{E8_bed.baseName} \\\n        --hwe !{params.hwe} \\\n        --make-bed \\\n        --out E9 \n    plink --bfile E9 \\\n        --hardy \\\n        --out plink_after\n    # sample 1% of SNPs\n    head -n1 plink_after.hwe > plink_sub_after.hwe\n    awk 'BEGIN {srand()} !/^$/ { if (rand() <= .01) print $0}' < plink_after.hwe >> plink_sub_after.hwe\n    awk '{ if ($3==\"TEST\" || $3==\"UNAFF\" && $9 <0.001) print $0 }' \\\n        plink_after.hwe > plinkzoomhwe_after.hwe\n    '''\n}"], "list_proc": ["nebfield/snpQT/hardy"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess maf {  \n    label 'plink1'\n    if (params.linear == true){\n       publishDir \"${params.results}/qc/bfiles/\", pattern: \"E10.*\",  mode: 'copy'\n    }\n\n    input:\n    path(E9_bed)\n    path(E9_bim)\n    path(E9_fam)\n\n    output:\n    path \"E10.bed\", emit: bed\n    path \"E10.bim\", emit: bim\n    path \"E10.fam\", emit: fam\n    path \"MAF_check_before.frq\", emit: before\n    path \"MAF_check_after.frq\", emit: after\n    path \"E10.log\", emit: log\n\n    shell:\n    '''\n    plink --bfile !{E9_bed.baseName} \\\n        --freq \\\n        --out MAF_check_before\n    plink --bfile !{E9_bed.baseName} \\\n        --maf !{params.maf} \\\n        --make-bed \\\n        --out E10\n    plink --bfile E10 \\\n        --freq \\\n        --out MAF_check_after\n    '''\n}"], "list_proc": ["nebfield/snpQT/maf"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess test_missing {\n    label 'plink1'\n    publishDir \"${params.results}/qc/bfiles/\", pattern: \"E11.*\",  mode: 'copy'\n\n    input:\n    path(E10_bed)\n    path(E10_bim)\n    path(E10_fam)\n\n    output:\n    path \"E11.bed\", emit: bed\n    path \"E11.bim\", emit: bim\n    path \"E11.fam\", emit: fam\n    path \"before.missing\", emit: before\n    path \"after.missing\", emit: after\n    path \"E11.log\", emit: log\n\n    shell:\n    '''\n    plink --bfile !{E10_bed.baseName} \\\n        --test-missing \\\n        --out before\n    awk '{ if ($5 < !{params.missingness}) print $2 }' before.missing > fail_missingness.txt\n    plink --bfile !{E10_bed.baseName} \\\n        --exclude fail_missingness.txt \\\n\t--make-bed \\\n\t--out E11\n    plink --bfile E11 \\\n        --test-missing \\\n        --out after\n    '''\n}"], "list_proc": ["nebfield/snpQT/test_missing"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess pca {\n    label 'plink1'\n\t\n    input:\n    path(bed)\n    path(bim)\n    path(fam)\n    path(exclude_regions)\n\n    output:\n    path \"E12_indep.log\", emit: log \n    path \"E12_pca.eigenvec\", emit: eigenvec_user \n    path \"E12_indep.fam\", emit: fam\n    \n    shell:\n    '''\n    plink --bfile !{bed.baseName} \\\n\t--exclude !{exclude_regions} \\\n\t--indep-pairwise !{params.indep_pairwise} \\\n\t--out indepSNPs_1k_1\n    plink --bfile !{bed.baseName} \\\n\t--extract indepSNPs_1k_1.prune.in \\\n\t--make-bed \\\n\t--out E12_indep\n    # Perform a PCA on user's data   \n    plink --bfile E12_indep \\\n        --pca header \\\n        --out E12_pca\n    '''\n}"], "list_proc": ["nebfield/snpQT/pca"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess to_bcf {\n    label 'bcftools'\n\t\n    input:\n    path(vcf)\n    \n    output:\n    path \"F5.bcf\", emit: bcf\n\n    shell:\n    '''\n    bcftools index !{vcf}\n    bcftools convert !{vcf} -Ou -o F5.bcf --threads !{task.cpus}\n    '''\n}"], "list_proc": ["nebfield/snpQT/to_bcf"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess check_ref_allele {\n    label 'bcftools'\n\t\n    input:\n    path(bcf)\n    path(dbsnp)\n    path(dbsnp_idx)\n    path(g37)\n\n    output:\n    path \"F6.bcf\", emit: bcf\n\n    shell:\n    '''\n    bcftools +fixref !{bcf} \\\n        -Ob -o F6.bcf -- \\\n        -d -f !{g37} \\\n        -i !{dbsnp}\n    '''\n}"], "list_proc": ["nebfield/snpQT/check_ref_allele"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess split_user_chrom {\n    label 'bcftools'\n\t\n    input:\n    path(vcf)\n    path(idx)\n    each chr\n    \n    output:\n    tuple val(chr), file('G1.vcf.gz'), file('G1.vcf.gz.csi'), emit: chrom \n\n    shell:\n    '''\n    bcftools view -r !{chr} !{vcf} -Oz -o G1.vcf.gz --threads !{task.cpus}\n    bcftools index G1.vcf.gz --threads !{task.cpus}\n    '''\n}"], "list_proc": ["nebfield/snpQT/split_user_chrom"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess bcftools_index_chr {\n    label 'bcftools'\n\t\n    input:\n    tuple val(chr), path('chr.vcf.gz')\n\n    output:\n    tuple val(chr), path('chr.vcf.gz'), path('chr.vcf.gz.csi'), emit: chrom_idx\n\n    shell:\n    '''\n    bcftools index chr.vcf.gz \n    '''\n}"], "list_proc": ["nebfield/snpQT/bcftools_index_chr"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess merge_imp {\n    label 'bcftools'\n\t\n    input:\n    path(imp)\n    \n    output:\n    path 'merged_imputed.vcf.gz', emit: vcf\n    \n    shell:\n    '''\n    # file order is important so use command substition\n    bcftools concat -n $(ls *.vcf.gz | sort -t . -k 1n) -Oz -o merged_imputed.vcf.gz --threads !{task.cpus}\n    '''\n}"], "list_proc": ["nebfield/snpQT/merge_imp"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess prep_ref_data {\n  label 'samtools'\n\t\n  input:\n  path(h37)\n\n  output:\n  path \"h37_squeezed.fasta\", emit: h37\n  path \"h37_squeezed.fasta.fai\", emit: h37_idx\n  \n  shell:\n  '''\n  # -q to suppress warnings, which make nextflow unhappy\n  # readlink to fix symlink gzip problem\n  # || true to force exit code 0 \n  gunzip --quiet -dc $(readlink !{h37}) > human_g1k_v37.fasta || true\n  \n  # Fix formatting in the file or plink explodes\n  tr -s ':' < human_g1k_v37.fasta | tr -s '\\n' > h37_squeezed.fasta\n  samtools faidx h37_squeezed.fasta\n  '''\n}"], "list_proc": ["nebfield/snpQT/prep_ref_data"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess qc {\n  label 'bcftools'\n\t\n  input:\n  tuple val(chr), path(vcf), path(g37)\n\n  output:\n  tuple val(chr), path(\"*.vcf.gz\"), emit: vcf\n  \n  shell:\n  '''\n  bcftools norm -m-any --check-ref w -f !{g37} !{vcf} | bcftools norm -Oz --rm-dup none -o !{chr}_norm.vcf.gz\n  '''\n}"], "list_proc": ["nebfield/snpQT/qc"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess annotate_ids {\n  label 'bcftools'\n  \n  input:\n  tuple val(chr), path(vcf)\n\n  output:\n  path \"*.vcf.gz\", emit: vcf\n \n  shell:\n  '''\n  bcftools annotate --set-id '%CHROM\\\\_%POS\\\\_%REF\\\\_%FIRST_ALT'  !{vcf} -Oz -o !{chr}.vcf.gz\n  '''\n}"], "list_proc": ["nebfield/snpQT/annotate_ids"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess flip_snps {  \n    label 'plink1'\n\t\n    input:\n    path(bed)\n    path(bim)\n    path(fam)\n    path(snpflip_rev)\n    path(snpflip_ambig)\n\n    output:\n    path \"D4.bed\", emit: bed\n    path \"D4.bim\", emit: bim\n    path \"D4.fam\", emit: fam\n    path \"D4.log\", emit: log\n\n    shell:\n    '''\n    # Flip all reversed SNPs\n    plink --bfile !{bed.baseName} \\\n        --flip !{snpflip_rev} \\\n        --make-bed \\\n        --out flipped\n\n    # Remove ambiguous SNPs\n    plink --bfile flipped \\\n        --exclude !{snpflip_ambig} \\\n        --make-bed \\\n        --out D4\n    '''\n}"], "list_proc": ["nebfield/snpQT/flip_snps"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess align {\n    label 'plink1'\n\t\n    input:\n    path(bed)\n    path(bim)\n    path(fam)\n    path(ref_bed)\n    path(ref_bim)\n    path(ref_fam)\n\n    output:\n    path \"D5.bed\", emit: bed\n    path \"D5.bim\", emit: bim\n    path \"D5.fam\", emit: fam\n    path \"D5.log\", emit: log\n\n    shell:\n    '''\n    # set 1k genome as reference to user's data \n    awk '{print$2,$5}' !{ref_bim} > 1kg_ref-list.txt\n\n    plink --bfile !{bed.baseName} \\\n        --reference-allele 1kg_ref-list.txt \\\n        --make-bed \\\n        --out D5\n    '''\n}"], "list_proc": ["nebfield/snpQT/align"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess merge {\n    label 'plink1'\n\t\n    input:\n    path(bed)\n    path(bim)\n    path(fam)\n    path(ref_bed)\n    path(ref_bim)\n    path(ref_fam)\n    \n    output:\n    path \"D6.bed\", emit: bed\n    path \"D6.bim\", emit: bim\n    path \"D6.fam\", emit: fam\n    path \"D6.log\", emit: log\n    \n    shell:\n    '''\n    # Extract the variants present in user's dataset from the 1000 genomes dataset\n    awk '{print $2}' !{bim} > user_snps.txt\n    plink --bfile !{ref_bed.baseName} \\\n        --extract user_snps.txt \\\n        --make-bed \\\n        --out 1kG_subset\n\n    # Extract the variants present in 1000 Genomes dataset from the user's dataset\n    awk '{print $2}' 1kG_subset.bim > 1kG_PCA6_SNPs.txt\n    plink --bfile !{bim.baseName} \\\n        --extract 1kG_PCA6_SNPs.txt \\\n        --make-bed \\\n        --out D6_subset\n\n    # Find differences between the two files that still appear after flipping \n    # and removing ambiguous SNPs\n    awk '{print $2,$5,$6}' D6_subset.bim > user_data_corrected_tmp\n    awk '{print $2,$5,$6}' 1kG_subset.bim > 1k_corrected_tmp\n    sort user_data_corrected_tmp 1k_corrected_tmp | uniq -u > uncorresponding_SNPs.txt\n\n    # Keep only the unique SNP ids \n    awk '{print $1}' uncorresponding_SNPs.txt | sort -u > SNPs_for_exclusion.txt\n\n    # Remove the problematic SNPs from both datasets\n    plink --bfile D6_subset \\\n        --exclude SNPs_for_exclusion.txt \\\n        --make-bed \\\n        --out D6_subset_exclude\n          \n    plink --bfile 1kG_subset \\\n        --exclude SNPs_for_exclusion.txt \\\n        --make-bed \\\n        --out 1kG_subset_exclude\n\n    # Merge user's dataset with 1000 Genomes Data\n    plink --bfile D6_subset_exclude \\\n        --bmerge 1kG_subset_exclude.bed 1kG_subset_exclude.bim 1kG_subset_exclude.fam \\\n        --allow-no-sex \\\n        --make-bed \\\n        --out D6 \n    '''\n}"], "list_proc": ["nebfield/snpQT/merge"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess pca_plink {\n    label 'plink1'\n\t\n    input:\n    path bed\n    path bim\n    path fam\n    path eigenvec\n\n    output:\n    path 'before.eigenvec', emit: before\n    path 'after.eigenvec', emit: after\n    \n    shell:\n    '''\n    plink --bfile !{bed.baseName} \\\n\t--pca header \\\n\t--out before\n    # Keep only a homogenous cohort\n    awk '{print $1}' !{eigenvec} | tail -n +2 | awk -F \":\" '{print $1,$2}' > keep_sample_list.txt\n    plink --bfile !{bed.baseName} \\\n        --keep keep_sample_list.txt \\\n        --make-bed \\\n        --out keep\n    plink --bfile keep \\\n        --pca header \\\n        --out after\n    '''\n}"], "list_proc": ["nebfield/snpQT/pca_plink"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess dictionary {\n    label 'small'\n    label 'picard'\n  \n    input:\n    path(fa)\n\n    output:\n    path \"*.dict\", emit: dict\n\n    shell:\n    '''\n    # make it so!\n    picard \\\n\tCreateSequenceDictionary \\\n\t-R !{fa} \\\n\t-O !{fa.baseName}.fa.dict\n    '''\n}"], "list_proc": ["nebfield/snpQT/dictionary"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess num_to_chr {\n    label 'bcftools'\n  \n    input:\n    path(in_vcf)\n    path(chr_map)\n\n    output:\n    path \"out.vcf.gz\", emit: vcf\n\n    shell:\n    '''\n    bcftools annotate --rename-chr !{chr_map} !{in_vcf} \\\n\t-Oz -o out.vcf.gz --threads !{task.cpus}\n    '''\n}"], "list_proc": ["nebfield/snpQT/num_to_chr"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess liftover {\n    label 'small'\n    label 'picard'\n    memory { (params.mem * 1.2) + 'G' }\n\n    input:\n    path(vcf)\n    path(hg)\n    path(chain)\n    path(dict)\n\n    output:\n    path \"out.vcf\", emit: vcf\n    path \"rejected_variants.vcf\", emit: rejected_vcf\n\n    shell:\n    '''\n    # !{dict} unused but needed to stage in file\n    picard \"-Xmx!{params.mem}G\" LiftoverVcf \\\n\t-I !{vcf} \\\n\t-O out.vcf \\\n\t-CHAIN !{chain} \\\n\t-REJECT rejected_variants.vcf \\\n\t-R !{hg}\n    '''\n}"], "list_proc": ["nebfield/snpQT/liftover"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess chr_to_num {  \n    label 'bcftools'\n    publishDir \"${params.results}/convertBuild/files/\", mode: 'copy'\n\n    input:\n    path(vcf)\n    path(chr_map)\n\n    output:\n    path \"converted.vcf.gz\", emit: vcf\n\n    shell:\n    '''\n    awk '{print $2 \"\\t\" $1}' !{chr_map} > Chr1To1.txt\n\n    # Change the chromosome ids again\n    bcftools annotate --rename-chr Chr1To1.txt !{vcf} -Oz -o converted.vcf.gz --threads !{task.cpus}\n    '''\n}"], "list_proc": ["nebfield/snpQT/chr_to_num"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["nebfield"], "nb_wf": 1, "list_wf": ["snpQT"], "list_contrib": ["rcappa1", "nebfield", "ChristinaVasil"], "nb_contrib": 3, "codes": ["\nprocess duplicates_cat3 {\n    label 'plink1'\n \n    input:\n    path(bed)\n    path(bim)\n    path(fam)\n\n    output:\n    path \"H5.bed\", emit: bed\n    path \"H5.bim\", emit: bim\n    path \"H5.fam\", emit: fam\n    path \"H5.log\", emit: log \n    \n    shell:\n    '''\n    cut -f 2 !{bim} | sort | uniq -d > merged_variants.txt\n\n    if [[ $(wc -l < merged_variants.txt) -gt 0 ]]\n    then\n      plink --bfile !{bim.baseName} \\\n          --extract merged_variants.txt \\\n          --make-bed \\\n          --out merged_snps\n      plink --bfile !{bim.baseName} \\\n          --exclude merged_variants.txt \\\n          --make-bed \\\n          --out excluded_snps\n      plink --bfile merged_snps \\\n          --set-all-var-ids @:#:\\\\$r:\\\\$a \\\n          --new-id-max-allele-len 300 missing\\\n          --make-bed \\\n          --out annotated\n      plink --bfile excluded_snps \\\n          --bmerge annotated \\\n          --make-bed \\\n          --out H5   \n    else\n      plink -bfile !{bim.baseName} \\\n          --make-bed \\\n          --out H5\n    fi\n    '''\n}"], "list_proc": ["nebfield/snpQT/duplicates_cat3"], "list_wf_names": ["nebfield/snpQT"]}, {"nb_reuse": 1, "tools": ["bedGraphToBigWig"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess make_bigwig {\n    cpus 1\n    conda \"bioconda::ucsc-bedgraphtobigwig\"\n    publishDir \"/mnt/home/mcampbell/20200317_new_emseq_figure\", mode: \"copy\"\n    errorStrategy 'finish'\n\n    input:\n        file bed from for_bigwig\n\n    output:\n        file ('*.bw') into _bigwig\n\n    shell:\n    '''\n    lib=$(basename !{bed} .bed)\n    bedGraphToBigWig !{bed} !{ref_len} ${lib}.bw\n    '''\n\n}"], "list_proc": ["nebiolabs/EM-seq/make_bigwig"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["seqtk", "Sambamba", "fastPHASE"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess mapping {\n    cpus fastq_mode == 'tile-fastq' ? 4 : 16\n    errorStrategy 'retry'\n    tag { [flowcell, fq_set.library] }\n    conda \"bwameth=0.2.2 seqtk=1.3 sambamba=0.7.0 fastp=0.20.1 mark-nonconverted-reads=1.1\"\n\n    input:\n        val fq_set from fq_set_channel\n\n    output:\n        set val(fq_set.library), file(\"*.aln.bam\") into aligned_files\n        set val(fq_set.library), file(\"*.nonconverted.tsv\") into nonconverted_counts\n        set val(fq_set.library), file(\"*_fastp.json\") into fastp_log_files\n\n    shell:\n    '''\n    inst_name=$(zcat -f '!{fq_set.insert_read1}' | head -n 1 | cut -f 1 -d ':' | sed 's/^@//')\n    fastq_barcode=$(zcat -f '!{fq_set.insert_read1}' | head -n 1 | sed -r 's/.*://')\n\n    if [[ \"${inst_name:0:2}\" == 'A0' ]] || [[ \"${inst_name:0:2}\" == 'NS' ]] || \\\n       [[ \"${inst_name:0:2}\" == 'NB' ]] || [[ \"${inst_name:0:2}\" == 'VH' ]] ; then\n       trim_polyg='--trim_poly_g'\n       echo '2-color instrument: poly-g trim mode on'\n    else\n       trim_polyg=''\n    fi\n    seqtk mergepe <(zcat -f \"!{fq_set.insert_read1}\") <(zcat -f \"!{fq_set.insert_read2}\") \\\n    | fastp --stdin --stdout -l 2 -Q ${trim_polyg} --interleaved_in --overrepresentation_analysis \\\n            -j \"!{fq_set.library}_fastp.json\" 2> fastp.stderr \\\n    | bwameth.py -p -t !{task.cpus} --read-group \"@RG\\\\tID:${fastq_barcode}\\\\tSM:!{fq_set.library}\" --reference !{genome} /dev/stdin \\\n                 2>  \"!{fq_set.library}_${fastq_barcode}!{fq_set.flowcell}_!{fq_set.lane}_!{fq_set.tile}.log.bwamem\" \\\n    | mark-nonconverted-reads.py 2> \"!{fq_set.library}_${fastq_barcode}_!{fq_set.flowcell}_!{fq_set.lane}_!{fq_set.tile}.nonconverted.tsv\" \\\n    | sambamba view -t 2 -S -f bam -o \"!{fq_set.library}_${fastq_barcode}_!{fq_set.flowcell}_!{fq_set.lane}_!{fq_set.tile}.aln.bam\" /dev/stdin 2> sambamba.stderr;\n    '''\n\n}"], "list_proc": ["nebiolabs/EM-seq/mapping"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Sambamba", "SAMBLASTER"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess mergeAndMarkDuplicates {\n    cpus 8\n    errorStrategy 'retry'\n    tag { library }\n    publishDir \"${outputPath}\", mode: 'copy', pattern: '*.{md.bam}*'\n    conda \"samtools=1.9 samblaster=0.1.24 sambamba=0.7.0\"\n\n    input:\n        set val(library), file(libraryBam) from aligned_files.groupTuple()\n\n    output:\n        set val(library), file('*.md.bam'), file('*.md.bam.bai') into md_bams\n        file('*.samblaster') into samblaster_logs\n\n    shell:\n    '''\n    samtools cat  -b <( find . -name '*.aln.bam' ) \\\n    | samtools view -h /dev/stdin \\\n    | samblaster 2> !{library}.log.samblaster \\\n    | sambamba view -t 2 -l 0 -S -f bam /dev/stdin \\\n    | sambamba sort --tmpdir=!{params.tmp_dir} -t !{task.cpus} -m 20GB -o !{library}.md.bam /dev/stdin\n\n    '''\n}"], "list_proc": ["nebiolabs/EM-seq/mergeAndMarkDuplicates"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process select_human_reads {\n        cpus 8\n        tag {library}\n        conda \"sambamba=0.7.1 bedtools=2.29.2\"\n\n        input:\n            tuple library, file(md_file), file(md_bai) from md_files_for_human_reads.groupTuple()\n\n        output:\n            tuple library, file('*.human.bam') into human_bams_gc\n            tuple library, file('*.human.bam') into human_bams_inserts\n\n        shell:\n        '''\n        sambamba view -t 8 -l 0 -f bam !{md_file} chr1 chr2 chr3 chr4 chr5 chr6 \\\n                                                  chr7 chr8 chr9 chr10 chr11 chr12 \\\n                                                  chr13 chr14 chr15 chr16 chr17 chr18 \\\n                                                  chr19 chr20 chr21 chr22 chrX chrY \\\n        > !{md_file}.human.bam\n        '''\n\n    }"], "list_proc": ["nebiolabs/EM-seq/select_human_reads"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process runFastQC {\n        cpus 1\n        errorStrategy 'retry'\n        tag { library }\n        conda \"fastqc=0.11.8\"\n\n        input:\n            tuple library, file(md_file), file(md_bai) from md_files_for_fastqc.groupTuple()\n\n        output:\n            file('*_fastqc.zip') into fastqc_results\n            tuple library, file('*_fastqc.zip') into fastqc_results_for_aggregate\n\n        shell:\n        '''\n        fastqc -f bam !{md_file}\n        '''\n\n    }"], "list_proc": ["nebiolabs/EM-seq/runFastQC"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process samtools_flagstats {\n        cpus 2\n        errorStrategy 'retry'\n        tag { library }\n        conda \"samtools=1.9\"\n\n        input:\n            tuple library, file(md_file), file(md_bai) from md_files_for_samflagstats.groupTuple(by: 0)\n\n        output:\n            file('*.flagstat') into flagstats\n            file('*.idxstat') into idxstats\n            tuple library, file('*.flagstat') into flagstats_for_aggregate\n            tuple library, file('*.idxstat') into idxstats_for_aggregate\n            \n\n        shell:\n        '''\n        samtools flagstat -@!{task.cpus} !{md_file} > !{md_file}.flagstat\n        samtools idxstats !{md_file} > !{md_file}.idxstat\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/samtools_flagstats"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process samtools_stats {\n        cpus 2\n        errorStrategy 'retry'\n        tag { library }\n        conda \"samtools=1.9\"\n\n        input:\n            tuple library, file(md_file),file(md_bai) from md_files_for_samstats.groupTuple()\n\n        output:\n\n            file('*.samstat') into samstats\n\n        shell:\n        '''\n        samtools stats -@!{task.cpus} !{md_file} > !{md_file}.samstat\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/samtools_stats"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 2, "tools": ["Picard", "FeatureCounts"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess epd_promoter_counts{\n    conda \"subread=2.0.0\"\n    cpus 16\n\n    input:\n        file gtf from epd_promoters_gtf\n        path('*') from bams_for_epd.map{ [it[1][0],it[1][1]] }.flatten().toList()\n\n    output:\n        file 'epd_promoter_counts.tsv' into epd_promoter_counts\n\n    shell:\n    '''\n    featureCounts --primary --ignoreDup -Q 10 -M -f -o -O --fraction -p -P -B -C \\\n        -a !{gtf} \\\n        --tmpDir !{params.tmp_dir} \\\n        -T !{task.cpus} \\\n        -o epd_promoter_counts.tsv *.bam\n    '''\n}", " process picard_gc_bias {\n        cpus 1\n        errorStrategy 'retry'\n        tag { library }\n        conda \"picard=2.20.7\"\n\n        input:\n            tuple library, file(md_file), file(md_bai) from md_files_for_picard_gc.groupTuple()\n\n        output:\n            file('*gc_metrics') into picard_gc_stats\n\n        shell:\n        '''\n        picard -Xmx4g CollectGcBiasMetrics IS_BISULFITE_SEQUENCED=true VALIDATION_STRINGENCY=LENIENT I=!{md_file} O=!{md_file}.gc_metrics S=!{md_file}.gc_summary_metrics CHART=!{md_file}.gc.pdf R=!{genome}\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/epd_promoter_counts", "nebiolabs/EM-seq/picard_gc_bias"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process picard_stats {\n\n        cpus 4\n        errorStrategy 'retry'\n        tag { library }\n        conda \"picard=2.20.7\"\n\n        input:\n            tuple library, file(md_file), file(md_bai) from md_files_for_picard.groupTuple()\n\n        output:\n            file('*_metrics') into picard_stats\n\n        shell:\n        '''\n        picard -Xmx16g CollectInsertSizeMetrics VALIDATION_STRINGENCY=LENIENT  I=!{md_file} O=!{md_file}.insertsize_metrics MINIMUM_PCT=0 HISTOGRAM_FILE=/dev/null\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/picard_stats"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process human_gc_bias {\n        cpus 1\n        errorStrategy 'retry'\n        tag { library }\n        conda \"picard=2.20.7\"\n\n        input:\n            tuple library, file(md_file) from human_bams_gc.groupTuple()\n\n        output:\n            tuple library, file('*gc_metrics') into human_gc_stats_for_aggregate\n\n        shell:\n        '''\n        picard -Xmx4g CollectGcBiasMetrics IS_BISULFITE_SEQUENCED=true VALIDATION_STRINGENCY=LENIENT I=!{md_file} O=!{md_file}.gc_metrics S=!{md_file}.gc_summary_metrics CHART=!{md_file}.gc.pdf R=!{genome}\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/human_gc_bias"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process human_insert_size {\n\n        cpus 4\n        errorStrategy 'retry'\n        tag { library }\n        conda \"picard=2.20.7\"\n\n        input:\n            tuple library, file(md_file) from human_bams_inserts.groupTuple()\n\n        output:\n            tuple library, file('*insertsize_metrics') into human_stats_for_aggregate\n\n        shell:\n        '''\n        picard -Xmx16g CollectInsertSizeMetrics VALIDATION_STRINGENCY=LENIENT  I=!{md_file} O=!{md_file}.insertsize_metrics MINIMUM_PCT=0.0001 HISTOGRAM_FILE=/dev/null\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/human_insert_size"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "MultiQC"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": [" process multiqc {\n        cpus 1\n        publishDir \"${outputPath}\", mode: 'copy'\n        conda \"multiqc=1.7\"\n\n        input:\n            file('*') from fastqc_results.flatten().toList()\n            file('*') from flagstats.flatten().toList()\n            file('*') from idxstats.flatten().toList()\n            file('*') from samstats.flatten().toList()\n            file('*') from picard_stats.flatten().toList()\n            file('*') from picard_gc_stats.flatten().toList()\n            file('*') from goleft_ped.flatten().toList()\n            file('*') from goleft_roc.flatten().toList()\n            file('*') from samblaster_logs.flatten().toList()\n            file('*') from fastp_log_files.flatten().toList()\n\n        output:\n            file \"*report.html\"\n\n        shell:\n        '''\n        for file in $(cat input.* | sed -e 's/\\\\[//g' | sed -e 's/, \\\\|\\\\]/\\\\n/g'); do ln -s ${file} ./; done\n        cat <<CONFIG > multiqc_config.yaml \n    title: Bwameth Alignment Summary - !{flowcell}\n    extra_fn_clean_exts:\n        - '.md'\n        - '_fastp'\n    custom_plot_config:\n        picard_insert_size:\n            xmax: 1000\n    table_columns_placement:\n        Samtools Stats:\n            raw_total_sequences: 10\n            reads_mapped_percent: 20\n            reads_properly_paired_percent: 30\n            reads_MQ0_percent: 35\n        Samblaster:\n            pct_dups: 40\n        Picard:\n            summed_median: 50\n    table_columns_visible:\n        Picard:\n            PCT_PF_READS_ALIGNED: False\n            summed_mean: False\n        Samtools Stats:\n            reads_mapped: False\n            mapped_passed: False\n            non-primary_alignments: False\n            reads_MQ0_percent: True\n        Samtools Flagstat:\n            mapped_passed: False\n        samtools_idxstats_always:\n            - plasmid_puc19c\n            - phage_lambda\n        FastQC:\n            percent_duplicates: False\n            total_sequences: False\n            avg_sequence_length: False\n            percent_fails: False\n            total_sequences: False\n    CONFIG\n\n        multiqc -ip  .\n        '''\n    }"], "list_proc": ["nebiolabs/EM-seq/multiqc"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 2, "tools": ["FastQC", "BEDTools"], "nb_own": 2, "list_own": ["nebiolabs", "nextflow-io"], "nb_wf": 2, "list_wf": ["elixir-workshop-21", "EM-seq"], "list_contrib": ["mattsoup", "bwlang", "lucacozzuto", "pditommaso"], "nb_contrib": 4, "codes": ["\nprocess epd_methylation {\n    conda \"bedtools=2.29.2 htslib=1.9\"\n    publishDir \"$params.output_dir\", mode: 'copy'\n\n    input:\n        file gtf from epd_promoters_gtf\n        file bed from hq_meth_bed\n\n    output:\n        file 'epd_promoter_methylation.tsv' into epd_promoter_meth\n\n    shell:\n    '''\n    bedtools intersect -nonamecheck \\\n    -wa -wb -loj \\\n    -a !{gtf} -b <(bgzip -d < !{bed} ) \\\n    | awk -v FS='\\\\t' -v OFS='\\\\t' '$14>0 {print $10,$11,$12,$1\":\"$4-1\"-\"$5,($15*1.0)/$14 }' \\\n    | bedtools groupby -g 4 -o mean -c 5 \\\n    > epd_promoter_methylation.tsv \n    '''\n}", "\nprocess fastqc {\n    \n                                                  \n    publishDir(params.OUTPUT, mode: 'copy') \n\n                                                                         \n    container params.CONTAINER\n\n                                                   \n    tag \"${reads}\" \n    \n    input:\n    path(reads)\n\n    output:\n    path(\"*_fastqc*\")\n\n    script:\n    \"\"\"\n        fastqc ${reads}\n    \"\"\"\n}"], "list_proc": ["nebiolabs/EM-seq/epd_methylation", "nextflow-io/elixir-workshop-21/fastqc"], "list_wf_names": ["nextflow-io/elixir-workshop-21", "nebiolabs/EM-seq"]}, {"nb_reuse": 2, "tools": ["Salmon", "BEDTools"], "nb_own": 2, "list_own": ["nebiolabs", "nextflow-io"], "nb_wf": 2, "list_wf": ["nf-hack17-tutorial", "EM-seq"], "list_contrib": ["mattsoup", "bwlang", "pditommaso", "KevinSayers", "evanfloden"], "nb_contrib": 5, "codes": ["\nprocess cpg_island_methylation {\n    conda \"bedtools=2.29.2 htslib=1.9\"\n    publishDir \"$params.output_dir\", mode: 'copy'\n\n    input:\n        file gtf from cpg_islands_gtf\n        file bed from hq_meth_bed\n\n    output:\n        file 'cpg_island_methylation.tsv' into cpg_island_meth\n\n    shell:\n    '''\n    bedtools intersect -nonamecheck \\\n    -wa -wb -loj \\\n    -a !{gtf} -b <(bgzip -d < !{bed} ) \\\n    | awk -v FS='\\\\t' -v OFS='\\\\t' '$14>0 {print $10,$11,$12,$1\":\"$4-1\"-\"$5,($15*1.0)/$14 }' \\\n    | bedtools groupby -g 4 -o mean -c 5 \\\n    > cpg_island_methylation.tsv \n    '''\n}", "\nprocess index {\n    \n    input:\n    file transcriptome from transcriptome_file\n     \n    output:\n    file 'index' into index_ch\n\n    script:       \n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["nebiolabs/EM-seq/cpg_island_methylation", "nextflow-io/nf-hack17-tutorial/index"], "list_wf_names": ["nextflow-io/nf-hack17-tutorial", "nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess cpg_island_counts{\n    conda \"subread=2.0.0\"\n    cpus 16\n\n    input:\n        file gtf from cpg_islands_gtf\n        path('*') from bams_for_cpgs.map{ [it[1][0],it[1][1]] }.flatten().toList()\n\n    output:\n        file 'cpg_island_counts.tsv' into cpg_island_counts\n\n    shell:\n    '''\n    featureCounts --primary --ignoreDup -Q 10 -M -f -o -O --fraction -p -P -B -C \\\n        -a !{gtf} \\\n        --tmpDir !{params.tmp_dir} \\\n        -T !{task.cpus} \\\n        -o cpg_island_counts.tsv *.bam\n    '''\n}"], "list_proc": ["nebiolabs/EM-seq/cpg_island_counts"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess refseq_feature_gtfs {\n    tag {feature}\n    conda \"subread=2.0.0 bedtools=2.29.2\"\n\n    input:\n        file(gtf) from refseq_gtf\n        file(assembly_report) from ncbi_assembly_report\n        val feature from refseq_feature_types_for_gtf\n\n    output:\n        tuple feature, file('*.gtf') into feature_gtf_for_meth\n        tuple feature, file('*_flat.saf') into feature_saf_for_counts\n\n    shell:\n    '''\n    # uses awk to create a hash lookup from the first file (NCBI assembly report) \n    # translating chr name in the second file\n    awk -v OFS='\\\\t' -v FS='\\\\t' 'NR==FNR {dict[$1]=$2; next} {$1=dict[$1]; print}' \\\n    <(grep -v '^#' !{assembly_report} | cut -f 7,10 | tr -d '\\\\r')  \\\n    <(zcat !{gtf} | grep -v '^#') \\\n    | grep \"GeneID:\" \\\n    | grep -P -v \"_alt\\\\t\" \\\n    | grep -P -v \"^na\\\\t\" \\\n    | sed -r 's/;Dbxref(=[^;]*)GeneID:([^,;]+)([;,])/;gene_id=\\\\2;Dbxref\\\\1GeneID:\\\\2\\\\3/' \\\n    | awk  -v OFS='\\\\t' -v FS='\\\\t' \\\n        '($3==\"exon\") && (index($9,\"gbkey=mRNA\") > 0) && (index($9,\"-1;Parent\") > 0) \\\n           { print($1,$2,\"mRNAexon1\",$4,$5,$6,$7,$8,$9); next }\n         ($3==\"exon\") && (index($9,\"gbkey=mRNA\") > 0)  \\\n           { print($1,$2,\"mRNAexon\",$4,$5,$6,$7,$8,$9); next }\n         { print }  \n        ' \\\n    > name_converted.gff\n\n    #exons overlap, we want only the longest to avoid 0 cov exons from featureCounts\n    flattenGTF -a name_converted.gff -o flat_name_converted.saf -t !{feature}\n\n    #need to switch to bed for intersection later\n    tail -n +2 flat_name_converted.saf \\\n      | awk -v OFS='\\\\t' -v FS='\\\\t' '{print $2,$3-1,$4,$1,\"-\",$5}' \\\n      | bedtools sort -faidx !{params.genome}.fai -i /dev/stdin > !{feature}_flat.bed\n    \n    #filters by feature type\n    awk -v type=!{feature} -v OFS='\\\\t' -v FS='\\\\t' '($3==type) { print}' name_converted.gff \\\n    > !{feature}.gtf\n\n    #only include those entries that intersect with the desired feature type, back to SAF format\n    echo \"GeneID\\tChr\\tStart\\tEnd\\tStrand\" > !{feature}_flat.saf\n    bedtools intersect -a !{feature}_flat.bed -b !{feature}.gtf  -u \\\n      | awk -v OFS='\\\\t' -v FS='\\\\t' '{print $4,$1,$2+1,$3,$6}' >> !{feature}_flat.saf\n    '''\n}"], "list_proc": ["nebiolabs/EM-seq/refseq_feature_gtfs"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 2, "tools": ["BEDTools", "MultiQC"], "nb_own": 2, "list_own": ["nebiolabs", "nextflow-io"], "nb_wf": 2, "list_wf": ["nf-hack17-tutorial", "EM-seq"], "list_contrib": ["mattsoup", "bwlang", "pditommaso", "KevinSayers", "evanfloden"], "nb_contrib": 5, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n       \n    input:\n    file('*') from quant_ch.mix(fastqc_ch).collect()\n    \n    output:\n    file('multiqc_report.html')  \n     \n    script:\n    \"\"\"\n    multiqc . \n    \"\"\"\n}", "\nprocess refseq_feature_methylation {\n    tag {feature}\n    conda \"bedtools=2.29.2 htslib=1.9\"\n    publishDir \"$params.output_dir\", mode: 'copy'\n\n    input:\n        file bed from hq_meth_bed\n        tuple feature, file(feature_gtf) from feature_gtf_for_meth \n\n    output:\n        file '*_methylation.tsv' into feature_methylation\n        \n    shell:\n    '''\n    bedtools intersect -nonamecheck \\\n    -wa -wb -loj \\\n    -a !{feature_gtf} -b <(bgzip -d < !{bed} ) \\\n    | awk -v FS='\\\\t' -v OFS='\\\\t' '$14>0 {print $10,$11,$12,$1\":\"$4-1\"-\"$5,($15*1.0)/$14 }' \\\n    | bedtools groupby -g 4 -o mean -c 5 \\\n    > !{feature}_methylation.tsv \n    '''\n}"], "list_proc": ["nextflow-io/nf-hack17-tutorial/multiqc", "nebiolabs/EM-seq/refseq_feature_methylation"], "list_wf_names": ["nextflow-io/nf-hack17-tutorial", "nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess refseq_feature_counts {\n\n    conda \"subread=2.0.0\"\n    publishDir \"$params.output_dir\", mode: 'copy'\n    cpus 16\n\n    input:\n        tuple (feature, path(feature_saf), path('*'), path('*') ) from feature_bams_for_refseq\n\n    output:\n        file '*_counts.tsv' into feature_counts\n\n    shell:\n    '''\n    featureCounts --primary --ignoreDup -Q 10 -M -f -O --fraction -p -P -B -C \\\n    -a !{feature_saf} -F SAF\\\n    -t !{feature} \\\n    -g 'ID' \\\n    --tmpDir !{params.tmp_dir} \\\n    -T !{task.cpus} \\\n    -o !{feature}_counts.tsv *.bam \n    '''\n}"], "list_proc": ["nebiolabs/EM-seq/refseq_feature_counts"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess dfam_feature_methylation {\n    conda \"bedtools=2.29.2 htslib=1.9\"\n    publishDir \"$params.output_dir\", mode: 'copy'\n\n    input:\n        file bed from hq_meth_bed\n        file(gtf) from dfam_gtf_for_meth\n\n    output:\n        file '*_methylation.tsv' into dfam_methylation\n        \n    shell:\n    '''\n    bedtools intersect -nonamecheck \\\n    -wa -wb -loj \\\n    -a !{gtf} -b <(bgzip -d < !{bed} ) \\\n    | awk -v FS='\\\\t' -v OFS='\\\\t' '$14>0 {print $10,$11,$12,$1\":\"$4-1\"-\"$5,($15*1.0)/$14 }' \\\n    | bedtools groupby -g 4 -o mean -c 5 \\\n    > dfam_methylation.tsv \n    '''\n}"], "list_proc": ["nebiolabs/EM-seq/dfam_feature_methylation"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["nebiolabs"], "nb_wf": 1, "list_wf": ["EM-seq"], "list_contrib": ["mattsoup", "bwlang"], "nb_contrib": 2, "codes": ["\nprocess dfam_feature_counts {\n\n    conda \"subread=2.0.0\"\n    publishDir \"$params.output_dir\", mode: 'copy'\n    cpus 16\n\n    input:\n        file(gtf) from dfam_gtf_for_counts\n        path('*') from bams_for_dfam.map{ [it[1][0],it[1][1]] }.flatten().toList()\n\n    output:\n        file '*_counts.tsv' into dfam_feature_counts\n\n    shell:\n    '''\n        featureCounts --primary --ignoreDup -Q 10 -M -f -o -O --fraction -p -P -B -C \\\n        -a !{gtf} \\\n        -t transcript \\\n        -g 'transcript_id' \\\n        --tmpDir !{params.tmp_dir} \\\n        -T !{task.cpus} \\\n        -o dfam_counts.tsv *.bam \n    '''\n}"], "list_proc": ["nebiolabs/EM-seq/dfam_feature_counts"], "list_wf_names": ["nebiolabs/EM-seq"]}, {"nb_reuse": 1, "tools": ["BLASTP-ACC"], "nb_own": 1, "list_own": ["nextflow-io"], "nb_wf": 1, "list_wf": ["blast-example"], "list_contrib": ["pditommaso"], "nb_contrib": 1, "codes": ["\nprocess blast {\n    input:\n    path 'query.fa' from fasta_ch\n    path db from db_dir\n\n    output:\n    file 'top_hits' into hits_ch\n\n    \"\"\"\n    blastp -db $db/$db_name -query query.fa -outfmt 6 > blast_result\n    cat blast_result | head -n 10 | cut -f 2 > top_hits\n    \"\"\"\n}"], "list_proc": ["nextflow-io/blast-example/blast"], "list_wf_names": ["nextflow-io/blast-example"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["nextflow-io"], "nb_wf": 1, "list_wf": ["elixir-workshop-21"], "list_contrib": ["lucacozzuto", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n\n                                                  \n    publishDir(params.OUTPUT, mode: 'copy')\n\n                                                                           \n    container params.CONTAINER\n\n    input:\n    path (inputfiles)\n\n    output:\n    path \"multiqc_report.html\"\n\n    script:\n    \"\"\"\n         multiqc .\n    \"\"\"\n}"], "list_proc": ["nextflow-io/elixir-workshop-21/multiqc"], "list_wf_names": ["nextflow-io/elixir-workshop-21"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nextflow-io"], "nb_wf": 1, "list_wf": ["elixir-workshop-21"], "list_contrib": ["lucacozzuto", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess Index {\n\n                                                                           \n    container params.CONTAINER\n\n                                                   \n    tag \"${ref}\"\n    \t\t\t\t\t\t\t\n    input:\n    path ref   \t\t\t\t\t\t\t\n\n    output:\t\t\t\t\t\t\t\t\t\n    tuple val(\"${ref}\"), path (\"${ref}*.ebwt\")\n\n    script:\t\t\t\t\t\t\t\t\t\n    \"\"\"\n        gunzip -c ${ref} > reference.fa\n        bowtie-build reference.fa ${ref}\n        rm reference.fa\n    \"\"\"\n}"], "list_proc": ["nextflow-io/elixir-workshop-21/Index"], "list_wf_names": ["nextflow-io/elixir-workshop-21"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["nextflow-io"], "nb_wf": 1, "list_wf": ["elixir-workshop-21"], "list_contrib": ["lucacozzuto", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess Align {\n\n                                                  \n    publishDir(params.OUTPUT, pattern: '*.sam')\n\n                                                                           \n    container params.CONTAINER\n\n                                                                       \n    label (params.LABEL)\n\n\n    tag \"${reads}\" \t\t\t\t\t\t\t\n\n    input:\n    tuple val(refname), path (ref_files)\n    path reads  \t\t\t\t\t\t\t\n\n    output:\t\t\t\t\t\t\t\t\t\n    path \"${reads}.sam\", emit: samples_sam\n    path \"${reads}.log\", emit: samples_log\n\n    script:\t\t\t\t\t\t\t\t\t\n    \"\"\"\n        bowtie -p ${task.cpus} ${refname} -q ${reads} -S > ${reads}.sam 2> ${reads}.log\n    \"\"\"\n}"], "list_proc": ["nextflow-io/elixir-workshop-21/Align"], "list_wf_names": ["nextflow-io/elixir-workshop-21"]}, {"nb_reuse": 3, "tools": ["Salmon", "MultiQC", "snippy"], "nb_own": 3, "list_own": ["trev-f", "nextflow-io", "peterk87"], "nb_wf": 3, "list_wf": ["nf-hack18", "SRAlign", "nf-illmap"], "list_contrib": ["t-f-freeman", "trev-f", "pditommaso", "KevinSayers", "evanfloden", "emi80", "peterk87", "damjanaram"], "nb_contrib": 8, "codes": ["\nprocess ContaminantStatsQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}/align\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        file sFS\n        val runName\n        val toolIDs\n\n    output:\n        path '*'\n\n    script:\n                                       \n        toolIDs += 'mqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        \"\"\"\n        multiqc \\\n            -n ${runName}${suffix} \\\n            --module samtools \\\n            ${sFS}\n        \"\"\"\n}", "\nprocess SNIPPY {\n  tag \"$sample VS $ref_name\"\n  publishDir \"${params.outdir}/snippy/\", pattern: \"${sample}-VS-${ref_name}\", mode: 'copy'\n\n  input:\n    tuple sample,\n          path(reads),\n          path(ref)\n  output:\n    tuple sample,\n          path(ref),\n          path(\"${sample}-VS-${ref_name}/\")\n\n  script:\n  ref_name = file(ref).getBaseName()\n  \"\"\"\n  snippy \\\\\n    --cpus ${task.cpus} \\\\\n    --outdir ${sample}-VS-${ref_name} \\\\\n    --prefix $sample \\\\\n    --ref $ref \\\\\n    --R1 ${reads[0]} \\\\\n    --R2 ${reads[1]} \n  \"\"\"\n}", "\nprocess index {\n    \n    input:\n    file transcriptome from transcriptome_file\n     \n    output:\n    file 'index' into index_ch\n\n    script:       \n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["trev-f/SRAlign/ContaminantStatsQC", "peterk87/nf-illmap/SNIPPY", "nextflow-io/nf-hack18/index"], "list_wf_names": ["peterk87/nf-illmap", "nextflow-io/nf-hack18", "trev-f/SRAlign"]}, {"nb_reuse": 5, "tools": ["SAMtools", "fastPHASE", "MultiQC", "ThermoRawFileParser"], "nb_own": 5, "list_own": ["proteomicsunitcrg", "trev-f", "peterk87", "telatin", "nextflow-io"], "nb_wf": 5, "list_wf": ["nf-ionampliseq", "qcloud2-pipeline", "nextflow-example", "nf-hack18", "SRAlign"], "list_contrib": ["t-f-freeman", "toniher", "lucacozzuto", "trev-f", "pditommaso", "vmikk", "KevinSayers", "evanfloden", "emi80", "peterk87", "damjanaram", "telatin", "rolivella"], "nb_contrib": 13, "codes": ["\nprocess fastp {\n       \n                                                                 \n      \n    tag \"filter $sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) \n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}_filt_R*.fastq.gz\"), emit: reads\n    path(\"${sample_id}.fastp.json\"), emit: json\n\n \n    script:\n    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} \\\\\n      -o ${sample_id}_filt_R1.fastq.gz -O ${sample_id}_filt_R2.fastq.gz \\\\\n      --detect_adapter_for_pe -w ${task.cpus} -j ${sample_id}.fastp.json\n \n    \"\"\"  \n}", "\nprocess FullMultiQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}\",   mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        val  runName\n        path config\n        path multiqcFiles\n\n    output:\n        path \"*\"\n\n    script:\n        \"\"\"\n        multiqc \\\n            -n ${runName} -i ${runName} \\\n            -c ${config} \\\n            ${multiqcFiles}\n        \"\"\"\n}", "\nprocess BAM_TO_FASTQ {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/reads/fastq\",\n             pattern: \"*.fastq.gz\",\n             mode: params.publish_dir_mode\n\n  input:\n  tuple val(sample), path(bam)\n\n  output:\n  tuple val(sample), path(\"*.fastq.gz\")\n\n  script:\n  \"\"\"\n  samtools fastq $bam | pigz -c - > ${sample}.fastq.gz\n  \"\"\"\n}", "\nprocess thermofilerawparser {\n    label 'thermoconvert'  \n    tag { \"${labsys}_${qcode}_${checksum}\" }\n\n    input:\n    set orifile, labsys, qcode, checksum, file(zipfile) from zipfiles\n\n    output:\n    set val(\"${labsys}_${qcode}_${checksum}\"), qcode, checksum, file(\"${labsys}_${qcode}_${checksum}.mzML\") into mzmlfiles_for_correction\n    \n    script:\n    def filename = zipfile.getBaseName()\n\tdef extens = filename.split('.')\n    if (extens.length == 0) {\n\t\tfilename = filename + \".raw\"\n\t} else if (extens[-1] != \"raw\" ) {\n\t\tfilename = filename + \".raw\"\n\t}\n    \"\"\"\n    unzip ${zipfile}\n    ThermoRawFileParser -i=${filename} -f=1 -m=0 -o ./\n    mv *.mzML ${labsys}_${qcode}_${checksum}.mzML\n    rm *.raw\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n       \n    input:\n    file('*') from quant_ch.mix(fastqc_ch).collect()\n    \n    output:\n    file('multiqc_report.html')  \n     \n    script:\n    \"\"\"\n    multiqc . \n    \"\"\"\n}"], "list_proc": ["telatin/nextflow-example/fastp", "trev-f/SRAlign/FullMultiQC", "peterk87/nf-ionampliseq/BAM_TO_FASTQ", "proteomicsunitcrg/qcloud2-pipeline/thermofilerawparser", "nextflow-io/nf-hack18/multiqc"], "list_wf_names": ["peterk87/nf-ionampliseq", "trev-f/SRAlign", "telatin/nextflow-example", "nextflow-io/nf-hack18", "proteomicsunitcrg/qcloud2-pipeline"]}, {"nb_reuse": 5, "tools": ["ABRicate", "Salmon", "BWA", "SAMtools", "FastQC"], "nb_own": 5, "list_own": ["tolkit", "propan2one", "telatin", "nextflow-io", "shaunchuah"], "nb_wf": 5, "list_wf": ["variantcallerbench", "cfdna_nextflow", "nfcamp-tutorial", "nextflow-example", "nemADSQ"], "list_contrib": ["pgonzale60", "lstevens17", "pditommaso", "vmikk", "evanfloden", "propan2one", "telatin", "shaunchuah"], "nb_contrib": 8, "codes": ["\nprocess bwa_mem {\n    tag \"${assemName}_${read_ID}\"\n\n    input:\n      tuple val(strain), val(assemName), path(indexBase), val(strain2), val(read_ID), path(readFile)\n      \n\n    output:\n      tuple val(strain), val(assemName), path(\"${assemName}_${read_ID}.bam\")\n\n    script:\n      \"\"\"\n      INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n      \n      bwa mem -A1 -B4 -E50 -L0 \\\n        -t ${task.cpus} \\\n        \\$INDEX \\\n        $readFile \\\n        | samtools view -@ 2 -bhS -o ${assemName}_${read_ID}.bam -\n      \"\"\"\n}", "process ABRICATE {\n    tag { sample_id }\n    \n    publishDir \"$params.outdir/abricate/\", \n        mode: 'copy'\n    \n    input:\n    tuple val(sample_id), path(assembly)  \n    \n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}.tab\")\n\n    script:\n    \"\"\"\n    abricate --threads ${task.cpus}  ${assembly} > ${sample_id}.tab\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess index {\n    tag \"$transcriptome.simpleName\"\n\n    input:\n    path transcriptome from params.transcriptome\n\n    output:\n    path 'index' into index_ch\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}", "\nprocess fastqc_run {\n    publishDir \"$params.outdir/fastqc/$sample_id/\", mode: 'copy'\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    tag \"$sample_id - FastQC\"\n    cpus \"$params.cpus\".toInteger()\n\n    input:\n    tuple val(sample_id), file(reads_file) from fastqc_reads\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc $reads_file -o . --threads ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["tolkit/nemADSQ/bwa_mem", "telatin/nextflow-example/ABRICATE", "propan2one/variantcallerbench/fastqc", "nextflow-io/nfcamp-tutorial/index", "shaunchuah/cfdna_nextflow/fastqc_run"], "list_wf_names": ["nextflow-io/nfcamp-tutorial", "telatin/nextflow-example", "propan2one/variantcallerbench", "shaunchuah/cfdna_nextflow", "tolkit/nemADSQ"]}, {"nb_reuse": 5, "tools": ["Flye", "Salmon", "BWA", "SAMtools", "fastPHASE"], "nb_own": 5, "list_own": ["sheenamt", "peterk87", "tolkit", "telatin", "nextflow-io"], "nb_wf": 5, "list_wf": ["nf-ionampliseq", "nfcamp-tutorial", "nextflow-example", "nemADSQ", "hrd_pipeline"], "list_contrib": ["pgonzale60", "lstevens17", "sheenamt", "pditommaso", "vmikk", "evanfloden", "peterk87", "telatin"], "nb_contrib": 8, "codes": ["\nprocess flye {\n    tag \"${strain}\"\n    publishDir \"$params.outdir\", mode: 'copy'\n    label 'btk'\n\n    input:\n      tuple val(strain), path(reads)\n\n    output:\n      tuple val(strain), path(\"${strain}.flye.fasta.gz\")\n\n    script:\n      \"\"\"\n      /software/team301/Flye-2.8.2/Flye/bin/flye --threads ${task.cpus} \\\n       --pacbio-hifi $reads --meta -o flyemeta\n       cat flyemeta/assembly.fasta | bgzip -c > ${strain}.flye.fasta.gz\n      \"\"\"\n}", "\nprocess fastp {\n       \n                                                                 \n      \n    tag \"filter $sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) \n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}_filt_R*.fastq.gz\"), emit: reads\n    path(\"${sample_id}.fastp.json\"), emit: json\n\n \n    script:\n    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} \\\\\n      -o ${sample_id}_filt_R1.fastq.gz -O ${sample_id}_filt_R2.fastq.gz \\\\\n      --detect_adapter_for_pe -w ${task.cpus} -j ${sample_id}.fastp.json\n \n    \"\"\"  \n}", "\nprocess BAM_TO_FASTQ {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/reads/fastq\",\n             pattern: \"*.fastq.gz\",\n             mode: params.publish_dir_mode\n\n  input:\n  tuple val(sample), path(bam)\n\n  output:\n  tuple val(sample), path(\"*.fastq.gz\")\n\n  script:\n  \"\"\"\n  samtools fastq $bam | pigz -c - > ${sample}.fastq.gz\n  \"\"\"\n}", "\nprocess quant {\n    tag \"$pair_id\"\n\n    input:\n    path index from index_ch\n    tuple pair_id, path(reads) from read_pairs_ch\n\n    output:\n    path pair_id into quant_ch\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}", "\nprocess alignment {\n                                   \n    label 'alignment'\n\n    tag \"${sample_id}-${sample_type}\"\n\n    input:\n        path ref_fasta\n        path ref_index\n        tuple val(sample_id), val(sample_type), file(fastq1), file(fastq2) from fastqs\n\n    output:\n        tuple val(sample_id), val(sample_type), file(\"${sample_id}.${sample_type}.raw.bam\") into raw_bams\n\n                                                              \n\n                                                                                            \n    script:\n    \"\"\" \n    bwa mem \\\n       -R \"@RG\\\\tID:${sample_id}\\\\tPL:ILLUMINA\\\\tPU:NA\\\\tSM:${sample_id}\\\\t\" \\\n       -K 100000000 \\\n       -t ${task.cpus}  \\\n       ${ref_fasta} ${fastq1} ${fastq2} 2> log.txt \\\n     | samtools sort -t${task.cpus} -m4G - -o ${sample_id}.${sample_type}.raw.bam\n     \"\"\"\n}"], "list_proc": ["tolkit/nemADSQ/flye", "telatin/nextflow-example/fastp", "peterk87/nf-ionampliseq/BAM_TO_FASTQ", "nextflow-io/nfcamp-tutorial/quant", "sheenamt/hrd_pipeline/alignment"], "list_wf_names": ["nextflow-io/nfcamp-tutorial", "peterk87/nf-ionampliseq", "telatin/nextflow-example", "sheenamt/hrd_pipeline", "tolkit/nemADSQ"]}, {"nb_reuse": 5, "tools": ["SAMtools", "FastQC", "preseq", "GATK"], "nb_own": 5, "list_own": ["sheenamt", "trev-f", "peterk87", "qbic-pipelines", "nextflow-io"], "nb_wf": 5, "list_wf": ["nf-ionampliseq", "nfcamp-tutorial", "cellranger", "SRAlign", "hrd_pipeline"], "list_contrib": ["t-f-freeman", "ggabernet", "trev-f", "sheenamt", "pditommaso", "evanfloden", "peterk87"], "nb_contrib": 7, "codes": ["\nprocess gatk_bqsr {\n    label 'gatk'\n\n    tag \"${sample_id}-${sample_type}\"\n\n    input:\n        path ref_fasta\n        path ref_index\n        path gatk_mills\n        path gatk_mills_index\n        path gatk_1kg\n        path gatk_1kg_index\n        tuple val(sample_id), val(sample_type), file(bam_file), file(bam_bai) from rmdup_bams\n\n    output:\n        tuple val(sample_id), val(sample_type), file(\"${sample_id}.${sample_type}.bqsr.bam\") into bqsr_bams\n\n                                                             \n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n    BaseRecalibrator \\\n    --reference ${ref_fasta} \\\n    --input ${bam_file} \\\n    --known-sites ${gatk_mills} \\\n    --known-sites ${gatk_1kg} \\\n    --output ${sample_id}.${sample_type}.recal_table\n\n    gatk ApplyBQSR \\\n    --reference ${ref_fasta} \\\n    --input ${bam_file} \\\n    --bqsr-recal-file ${sample_id}.${sample_type}.recal_table \\\n    --output ${sample_id}.${sample_type}.bqsr.bam\n    \"\"\"\n}", "\nprocess Preseq {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/preseq:3.1.2--h2c25361_3'\n\n    publishDir \"${params.baseDirData}/align/preseq\", mode: 'copy', pattern: '*.txt'\n\n    input:\n        tuple val(metadata), path(bam), val(toolIDs)\n\n    output:\n        path '*_psL.txt', emit: psL\n\n    script:\n                                        \n        toolIDspsL = toolIDs\n        toolIDspsL += 'psL'\n        suffixpsL = toolIDspsL ? \"__${toolIDspsL.join('_')}\" : ''\n\n        toolIDs += ['psL']\n\n                              \n        readTypeArg = metadata.readType == 'single' ? '' : '-pe'\n\n        \"\"\"\n        preseq lc_extrap \\\n            -o ${metadata.sampleName}${suffixpsL}.txt \\\n            ${readTypeArg} \\\n            -bam ${bam}\n        \"\"\"\n}", "\nprocess SAMTOOLS_DEPTH {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/samtools/depth\",\n             pattern: \"*-depths.tsv\",\n             mode: params.publish_dir_mode\n\n  input:\n  tuple val(sample), path(bam), path(ref_fasta)\n\n  output:\n  tuple val(sample), path('*-depths.tsv'), path(ref_fasta)\n\n  script:\n  \"\"\"\n  samtools depth -aa -d 0 ${bam[0]} > ${sample}-depths.tsv\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                    filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    tuple val(GEM), val(sample), val(lane), file(R1), file(R2) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus ${R1} ${R2} \n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    tuple sample_id, path(reads) from read_pairs2_ch\n\n    output:\n    path \"fastqc_${sample_id}_logs\" into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}"], "list_proc": ["sheenamt/hrd_pipeline/gatk_bqsr", "trev-f/SRAlign/Preseq", "peterk87/nf-ionampliseq/SAMTOOLS_DEPTH", "qbic-pipelines/cellranger/fastqc", "nextflow-io/nfcamp-tutorial/fastqc"], "list_wf_names": ["nextflow-io/nfcamp-tutorial", "peterk87/nf-ionampliseq", "trev-f/SRAlign", "sheenamt/hrd_pipeline", "qbic-pipelines/cellranger"]}, {"nb_reuse": 3, "tools": ["SAMtools", "MultiQC"], "nb_own": 3, "list_own": ["torchij", "sheenamt", "nextflow-io"], "nb_wf": 3, "list_wf": ["nfcamp-tutorial", "hrd_pipeline", "nextflow"], "list_contrib": ["evanfloden", "sheenamt", "pditommaso", "torchij"], "nb_contrib": 4, "codes": ["\nprocess samtools_mpileup {\n    label 'samtools'\n\n    tag \"${sample_id}-${sample_type}\"\n\n\n    input:\n        path ref_fasta\n        tuple val(sample_id), val(sample_type), file(final_bam) from final_bams\n\n    output:\n        tuple val(sample_id), file(\"${sample_id}.${sample_type}.mpileup\") into mpileups\n\n    publishDir params.output, mode: 'copy', overwrite: true\n\n                     \n                  \n                    \n    script:\n    \"\"\"\n    samtools mpileup -f ${ref_fasta} -d 1000000 -A -B ${final_bam} > ${sample_id}.${sample_type}.mpileup\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n    \n    input:\n    path 'data*/*' from quant_ch.mix(fastqc_ch).collect()\n    path config from params.multiqc\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    cp $config/* .\n    echo \"custom_logo: \\$PWD/logo.png\" >> multiqc_config.yaml\n    multiqc -v .\n    \"\"\"\n}", "\nprocess index {\n    label 'index'\n    tag '_${id}'\n    cpus 4\n    memory '16 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    path(bam) from bam_ch\n\n    output:\n    tuple id, path(bam), path(\"*.bam.bai\") into bamNidx_ch\n    \n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools index -@${task.cpus} ${bam}\n    \"\"\"\n    \n\n}"], "list_proc": ["sheenamt/hrd_pipeline/samtools_mpileup", "nextflow-io/nfcamp-tutorial/multiqc", "torchij/nextflow/index"], "list_wf_names": ["nextflow-io/nfcamp-tutorial", "sheenamt/hrd_pipeline", "torchij/nextflow"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["nextflow-io"], "nb_wf": 1, "list_wf": ["rnaseq-nf"], "list_contrib": ["molecules", "evanfloden", "pditommaso"], "nb_contrib": 3, "codes": ["\nprocess QUANT {\n    tag \"$pair_id\"\n\n    input:\n    path index \n    tuple val(pair_id), path(reads) \n\n    output:\n    path pair_id \n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}"], "list_proc": ["nextflow-io/rnaseq-nf/QUANT"], "list_wf_names": ["nextflow-io/rnaseq-nf"]}, {"nb_reuse": 2, "tools": ["MMseqs", "MultiQC"], "nb_own": 2, "list_own": ["phiweger", "nextflow-io"], "nb_wf": 2, "list_wf": ["rnaseq-nf", "isolate_pe"], "list_contrib": ["molecules", "pditommaso", "evanfloden", "hoelzer", "phiweger"], "nb_contrib": 5, "codes": ["\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    path('*') \n    path(config) \n\n    output:\n    path('multiqc_report.html')\n\n    script:\n    \"\"\"\n    cp $config/* .\n    echo \"custom_logo: \\$PWD/logo.png\" >> multiqc_config.yaml\n    multiqc .\n    \"\"\"\n}", "\nprocess concern {\n    container 'nanozoo/mmseqs2:11.e1a1c--55acb62'\n    publishDir \"${params.results}\", mode: 'copy', overwrite: true\n                                 \n    \n    input:\n        tuple(val(name), path(proteins), path(db))\n\n    output:\n        tuple(val(name), path('aln.m8'))\n\n    \"\"\"\n    mmseqs easy-search --max-accept 1 --min-seq-id 0.8 -c 0.5 ${proteins} ${db} aln.m8 tmp\n    \"\"\"\n}"], "list_proc": ["nextflow-io/rnaseq-nf/MULTIQC", "phiweger/isolate_pe/concern"], "list_wf_names": ["nextflow-io/rnaseq-nf", "phiweger/isolate_pe"]}, {"nb_reuse": 2, "tools": ["Salmon", "fastPHASE"], "nb_own": 2, "list_own": ["samlhao", "nextflow-io"], "nb_wf": 2, "list_wf": ["rnaseq-nf", "nextflow-spid"], "list_contrib": ["molecules", "samlhao", "evanfloden", "pditommaso"], "nb_contrib": 4, "codes": ["\nprocess INDEX {\n    tag \"$transcriptome.simpleName\"\n\n    input:\n    path transcriptome \n\n    output:\n    path 'index' \n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}", "\nprocess se_fastp {\n    tag \"$sample_id\"\n    label 'process_medium'\n    label 'fastp'\n    publishDir \"${params.outdir}/samples/${sample_id}\", mode: 'copy'\n\n    input:\n    tuple (sample_id, path (reads)) from se_read_files_trimming\n\n    output:\n    tuple (sample_id, path ('*_trimmed.fq.gz')) into (se_aln_ch, se_mlst_ch, se_amr_ch)\n    path (\"*.json\") into se_fastp_results\n    path (\"*.html\")\n\n    when:\n    !params.skip_trimming && params.se_reads\n\n    script:\n    \"\"\"\n    fastp -i ${reads[0]} -o ${reads[0].getSimpleName()}_trimmed.fq.gz -w ${task.cpus} --json ${reads[0].getSimpleName()}_fastp.json --html ${reads[0].getSimpleName()}_fastp.html\n    \"\"\"\n}"], "list_proc": ["nextflow-io/rnaseq-nf/INDEX", "samlhao/nextflow-spid/se_fastp"], "list_wf_names": ["nextflow-io/rnaseq-nf", "samlhao/nextflow-spid"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["nf-modules"], "nb_wf": 1, "list_wf": ["picard"], "list_contrib": ["abhi18av"], "nb_contrib": 1, "codes": ["\nprocess CollectAlignmentSummaryMetrics {\n    publishDir params.collectAlignmentSummaryMetricsResultsDir, mode: params.saveMode\n    container \"quay.io/biocontainers/picard:2.23.4--0\"\n\n    when:\n    params.collectAlignmentSummaryMetrics\n\n    input:\n    path refFasta from ch_refFasta\n    file(dedupedSortedBamFile) from ch_in_collectAlignmentSummaryMetrics\n\n    output:\n    file \"*.txt\" into ch_out_collectAlignmentSummaryMetrics\n\n\n    script:\n    refFastaName = refFasta.toString().split(\"\\\\.\")[0]\n\n    \"\"\"\n    picard CollectAlignmentSummaryMetrics R=${refFasta} I=${dedupedSortedBamFile}  O=${refFastaName}_alignment_metrics.txt\n    \"\"\"\n}"], "list_proc": ["nf-modules/picard/CollectAlignmentSummaryMetrics"], "list_wf_names": ["nf-modules/picard"]}, {"nb_reuse": 7, "tools": ["gffread", "HISAT2", "SAMtools", "BEDTools", "mosdepth", "GATK"], "nb_own": 6, "list_own": ["ssun1116", "rmoran7", "ryanlayerlab", "sickle-in-africa", "nibscbioinformatics", "robinfchan"], "nb_wf": 6, "list_wf": ["meripseqpipe", "layer_lab_chco", "citeseq-nf", "dx_sarek", "saw.snv-indel", "humgen"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "lescai", "kingzhuky", "javaidm", "MSBradshaw", "bleazard", "robinfchan", "juneb4869"], "nb_contrib": 10, "codes": ["\nprocess extract_transcriptome {\n    if (params.custom_container) container \"${params.custom_container}\"\n    \n    tag \"${genome_fasta}\"\n    label 'midi_memory'\n    publishDir \"${params.outdir}/reference_data/extract_transcriptome\", mode: 'copy'\n\n    input:\n    file genome_fasta from genome_fasta_extract_transcriptome\n    file gtf from gtf_extract_transcriptome_trimmed\n\n    output:\n    file \"${genome_fasta}.transcriptome.fa\" into transcriptome_fasta_alevin_extracted\n\n    when:\n    !params.transcriptome_fasta && !params.skip_rna \n    \n    script:\n                                                        \n    \"\"\"\n    gffread -F $gtf -w \"${genome_fasta}.transcriptome.fa\" -g $genome_fasta\n    \"\"\"\n}", "\nprocess combine_samples{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/VariantCalling/CombineCNV/\", mode: params.publish_dir_mode\n\n    input:\n    file(all_files)\n    file(all_logs)\n    file(example_vcf)\n    file(fasta)\n    file(fastaFai)\n    file(dict)\n\n    output:\n    file(\"aggregated_multi_sample_multi_caller.bed\")\n    file(\"*.vcf\")\n    path \"cnv_all_samples.vcf\", emit: cnv_all_samples_vcf\n    path \"cnv_all_samples.log\", emit: cnv_all_samples_log\n\n    script:\n    \"\"\"\n    for file in ${all_files}; do\n        cat \\${file} >> temp.bed\n    done\n\n    for file in ${all_logs}; do\n        cat \\${file} >> cnv_all_samples.log\n    done\n\n    #sort it and cluster it\n    bedtools sort -i temp.bed > sorted_temp.bed\n    cat sorted_temp.bed | sort -k1,1V -k2,2n -k3,3n > tripple_sorted.bed\n    bedtools cluster -i tripple_sorted.bed > clustered_test.bed\n\n    multi_sample_agg_cluster.py clustered_test.bed > aggregated_multi_sample_multi_caller.bed\n    multi_caller_single_sample_bed_to_vcf.py --bed aggregated_multi_sample_multi_caller.bed --example_vcf $example_vcf --ref $fasta > cnv_all_samples.vcf\n    \"\"\"\n}", "\nprocess addTagInfo {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.gatk4Image\n\n    input:\n    tuple val(name), path(bamFile)\n\n    output:\n    tuple val(name), path(\"${name}.fixed.bam\")\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        SetNmMdAndUqTags \\\n        -R ${params.referenceSequence['path']} \\\n        -I ${bamFile} \\\n        -O \"${name}.fixed.bam\"\n    \"\"\"\n\n}", "\nprocess BuildDict {\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publishDirMode,\n        saveAs: {params.saveGenomeIndex ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess mosdepth {\n                                                       \n                            \n    label 'cpus_1'\n\n                     \n    publishDir \"${params.outdir}/CNV_Plotting/${idSample}/Mosdepth\", mode: params.publish_dir_mode\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.per-base.bed.gz\"), file(\"${idSample}.per-base.bed.gz.tbi\")\n\n\n                                       \n\n    script:\n    \"\"\"\n    mosdepth $idSample $bam\n    tabix -p bed ${idSample}.per-base.bed.gz\n    \"\"\"\n}", " process FilterrRNA {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/rRNA_dup\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from rRNA_reads\n    file index from rRNA_index.collect()\n\n    output:\n    set val(sample_name), file(\"*.fastq.gz\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into tophat2_reads, hisat2_reads, bwa_reads, star_reads\n    file \"*_summary.txt\" into rRNA_log\n\n    when:\n    params.rRNA_fasta && !params.skip_filterrRNA\n\n    script:\n    gzip = true\n    index_base = index[0].toString() - ~/(\\.exon)?(\\.\\d)?(\\.fa)?(\\.gtf)?(\\.ht2)?$/\n    if (reads_single_end) {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-gz ${sample_id}.fastq.gz \\\n            -x $index_base \\\n            -U $reads | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        \"\"\"\n    } else {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-conc-gz ${sample_name}_fastq.gz \\\n            -x $index_base \\\n            -1 ${reads[0]} -2 ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        mv ${sample_name}_fastq.1.gz ${sample_name}_1.fastq.gz\n        mv ${sample_name}_fastq.2.gz ${sample_name}_2.fastq.gz\n        \"\"\"\n    }\n    }", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}"], "list_proc": ["robinfchan/citeseq-nf/extract_transcriptome", "ryanlayerlab/layer_lab_chco/combine_samples", "sickle-in-africa/saw.snv-indel/addTagInfo", "nibscbioinformatics/humgen/BuildDict", "ryanlayerlab/layer_lab_chco/mosdepth", "ssun1116/meripseqpipe/FilterrRNA", "rmoran7/dx_sarek/MergeBamMapped"], "list_wf_names": ["ssun1116/meripseqpipe", "ryanlayerlab/layer_lab_chco", "nibscbioinformatics/humgen", "robinfchan/citeseq-nf", "rmoran7/dx_sarek", "sickle-in-africa/saw.snv-indel"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nibscbioinformatics"], "nb_wf": 1, "list_wf": ["humgen"], "list_contrib": ["bleazard", "lescai"], "nb_contrib": 2, "codes": ["\nprocess baserecalibrationtable {\n    tag \"$name\"\n    label 'process_medium'\n\n  input:\n  set ( sampleprefix, file(markedbamfile) ) from markedbamfortable\n  file(dbsnp) from ch_dbsnp\n  file(dbsnpIndex) from ch_dbsnpIndex\n  file(fasta) from ch_fasta\n  file(fastaFai) from ch_fastaFai\n  file(knownIndels) from ch_knownIndels\n  file(knownIndelsIndex) from ch_knownIndelsIndex\n  \n  output:\n  set ( sampleprefix, file(\"${sampleprefix}.recal_data.table\") ) into recaltable\n\n  \"\"\"\n  gatk BaseRecalibrator -I $markedbamfile --known-sites $dbsnp --known-sites $knownIndels -O ${sampleprefix}.recal_data.table -R $fasta\n  \"\"\"\n}"], "list_proc": ["nibscbioinformatics/humgen/baserecalibrationtable"], "list_wf_names": ["nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nibscbioinformatics"], "nb_wf": 1, "list_wf": ["humgen"], "list_contrib": ["bleazard", "lescai"], "nb_contrib": 2, "codes": ["\nprocess applybaserecalibration {\n  publishDir \"$params.outdir/alignments\", mode: \"copy\"\n    tag \"$name\"\n    label 'process_medium'\n\n  input:\n  set ( sampleprefix, file(recalibrationtable), file(markedbamfile) ) from forrecal\n\n  output:\n  set ( sampleprefix, file(\"${sampleprefix}.bqsr.bam\") ) into (recalibratedforindex, recalibratedforcaller)\n\n  \"\"\"\n  gatk ApplyBQSR -I $markedbamfile -bqsr $recalibrationtable -O ${sampleprefix}.bqsr.bam\n  \"\"\"\n}"], "list_proc": ["nibscbioinformatics/humgen/applybaserecalibration"], "list_wf_names": ["nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["nibscbioinformatics"], "nb_wf": 1, "list_wf": ["humgen"], "list_contrib": ["bleazard", "lescai"], "nb_contrib": 2, "codes": ["\nprocess variantevaluation {\n  publishDir \"$params.outdir/analysis\", mode: \"copy\"\n    tag \"$name\"\n    label 'process_medium'\n\n  input:\n  set ( sampleprefix, file(germline), file(germlineindex), file(somatic), file(somaticindex) ) from germsomvars1\n  file(dbsnp) from ch_dbsnp\n  file(dbsnpIndex) from ch_dbsnpIndex\n  file(fasta) from ch_fasta\n  file(fastaFai) from ch_fastaFai\n\n  output:\n  set ( sampleprefix, file(\"${sampleprefix}.germline.eval.grp\"), file(\"${sampleprefix}.somatic.eval.grp\") ) into variantevaluations\n\n  \"\"\"\n  gatk VariantEval -eval $germline -O ${sampleprefix}.germline.eval.grp -R $fasta -D $dbsnp\n  gatk VariantEval -eval $somatic -O ${sampleprefix}.somatic.eval.grp -R $fasta -D $dbsnp\n  \"\"\"\n}"], "list_proc": ["nibscbioinformatics/humgen/variantevaluation"], "list_wf_names": ["nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["nibscbioinformatics"], "nb_wf": 1, "list_wf": ["humgen"], "list_contrib": ["bleazard", "lescai"], "nb_contrib": 2, "codes": ["\nprocess effectprediction {\n  publishDir \"$params.outdir/analysis\", mode: \"copy\"\n    tag \"$name\"\n    label 'process_medium'\n\n  input:\n  set ( sampleprefix, file(germline), file(germlineindex), file(somatic), file(somaticindex) ) from germsomvars2\n\n  output:\n  set ( sampleprefix, file(\"${sampleprefix}.germline.annotated.vcf\"), file(\"${sampleprefix}.somatic.annotated.vcf\") ) into annotatedvars\n\n  \"\"\"\n  snpEff -Xmx8g hg19 $germline > ${sampleprefix}.germline.annotated.vcf\n  snpEff -Xmx8g hg19 $somatic > ${sampleprefix}.somatic.annotated.vcf\n  \"\"\"\n}"], "list_proc": ["nibscbioinformatics/humgen/effectprediction"], "list_wf_names": ["nibscbioinformatics/humgen"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["nibscbioinformatics"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["lescai"], "nb_contrib": 1, "codes": ["\nprocess MAFFT {\n                                                                       \n                            \n    label 'process_low'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename ->\n          saveFiles(filename:filename, options:options, publish_dir:getSoftwareName(task.process), publish_id:meta.sampleID)\n        }\n\n    container \"quay.io/biocontainers/mafft:7.471--h516909a_0\"\n\n    conda (params.conda ? \"${moduleDir}/environment.yml\" : null)\n\n\n  input:\n                                                                                        \n                                                       \n                                                                             \n                                                                                          \n  tuple val(meta), path(fasta)\n\n  val options\n\n  output:\n  tuple val(meta), path(\"${meta.sampleID}_mafft.fasta\"), emit: fasta\n  tuple val(meta), path(\"*.tree\"), emit: tree\n  path \"*.version.txt\", emit: version\n\n  script:\n  \"\"\"\n  mafft \\\n  ${options.args} \\\n  ${fasta} \\\n  > ${meta.sampleID}_mafft.fasta\n\n  mafft --version >mafft.version.txt\n  \"\"\"\n}"], "list_proc": ["nibscbioinformatics/modules/MAFFT"], "list_wf_names": ["nibscbioinformatics/modules"]}, {"nb_reuse": 0, "tools": ["QIIME", "BCFtools"], "nb_own": 1, "list_own": ["nibscbioinformatics"], "nb_wf": 0, "list_wf": ["nf-core-buggybarcodes", "nf-core-viralevo"], "list_contrib": ["MGordon09", "kaurravneet4123"], "nb_contrib": 2, "codes": ["\nprocess QIIME2_FEATURETABLE_SUMMARIZE {\n                      \n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? { exit 1 \"QIIME2 has no conda package\" } : null)\n    container \"quay.io/qiime2/core:2021.2\"\n\n    input:\n    path table\n\n    output:\n    path \"table.qzv\"    , emit: qzv\n    path \"*.version.txt\", emit: version\n\n\n    script:\n    def software      = getSoftwareName(task.process)\n    \"\"\"\n    qiime feature-table summarize \\\\\n        $options.args \\\\\n        --i-table  $table \\\\\n        --o-visualization table.qzv \\\\\n\n    echo \\$(qiime --version | sed -e \"s/q2cli version //g\" | tr -d '`' | sed -e \"s/Run qiime info for more version details.//g\") > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_VIEW {\n    tag \"$vcf\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    path vcf\n\n    output:\n    path \"*.vcf.gz\", emit: vcf\n    path \"*.version.txt\"     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def filename = \"$vcf\".tokenize('_')[0]\n    def caller   = (\"$vcf\".contains(\"_ivar\")) ? \"ivar\" :  (\"$vcf\".contains(\"lofreq\")) ? \"lofreq\" : ''\n    \"\"\"\n    bcftools view \\\\\n        $vcf \\\\\n        $options.args \\\\\n        -o ${filename}_${caller}.vcf.gz\n\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 11, "tools": ["Bracken", "Arriba", "SAMtools", "MultiQC", "Filtlong", "AIVAR", "MEGAHIT", "FastQC"], "nb_own": 9, "list_own": ["replikation", "xiaoli-dong", "thanhleviet", "raygozag", "nibscbioinformatics", "sickle-in-africa", "peterk87", "vincenthhu", "ralsallaq"], "nb_wf": 9, "list_wf": ["nf-core-westest", "metaGx_nf", "nf-core-buggybarcodes", "scoop", "nf-virontus", "saw.sarek", "magph", "nf-nanopore-assembly", "docker_pipelines", "viralevo", "rnaseq"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "thanhleviet", "raygozag", "davidmasp", "replikation", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "peterk87", "skrakau", "mult1fractal", "MGordon09", "xiaoli-dong", "lescai", "szilvajuhos", "FriederikeHanssen", "vincenthhu", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "bleazard", "adrlar", "ralsallaq"], "nb_contrib": 29, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                       \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess bracken {\n  publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", mode: \"copy\"\n  \n  tag {sample_id}\n\n                    \n  \n  input:\n  tuple val(sample_id), path(\"${sample_id}_kraken2.report\"), path(bracken_db)\n  \n  output:\n  tuple val(sample_id), path(\"${sample_id}_bracken.tsv\")\n  \n  script:\n  \"\"\"\n  bracken \\\n    -d ${bracken_db} \\\n    -r ${params.bracken_read_length} \\\n    -i ${sample_id}_kraken2.report \\\n    -l ${params.taxlevel} \\\n    -o ${sample_id}_bracken.tsv \\\n    -w ${sample_id}_bracken.kraken2_report\n  \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess IVAR_TRIM {\n  publishDir \"${params.outdir}/mapping/$sample/bamfiles\", pattern: \"*.trim.bam\"\n  input:\n    path(bedfile)\n    tuple sample,\n          path(ref_fasta),\n          path(bam)\n  output:\n    tuple sample,\n          path(ref_fasta),\n          path(trimmed_bam)\n\n  script:\n  ref_name = ref_fasta.getBaseName()\n  trimmed_bam = \"${sample}-${ref_name}.trim.bam\"\n  \"\"\"\n  ivar trim \\\\\n    -i $bam \\\\\n    -b $bedfile \\\\\n    -p trim -q 1 -m 20 -s 4 -e\n  samtools sort -o $trimmed_bam trim.bam\n  rm trim.bam\n  \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    export LC_ALL=C.UTF-8\n    export LANG=C.UTF-8\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess AlignReadsToReferenceSequence {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), file(inputFile1), file(inputFile2)\n        file(bwaIndex)\n        file(fasta)\n        file(fastaFai) \n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_${idRun}.bam\")\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n                                                                                          \n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m ${params.samtoolsSortMemory} - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "process filtlong {\n                                                                                         \n    label 'filtlong'\n  input:\n    tuple val(name), file(reads) \n  output:\n\t  tuple val(name), file(\"${name}_filtered.fastq\") \n  script:\n    \"\"\"\n  \tfiltlong --min_length 2000 ${reads} > ${name}_filtered.fastq\n    \"\"\"\n}", "process ARRIBA {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::arriba=2.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/arriba:2.1.0--h3198e80_1' :\n        'quay.io/biocontainers/arriba:2.1.0--h3198e80_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path fasta\n    path gtf\n\n    output:\n    tuple val(meta), path(\"*.fusions.tsv\")          , emit: fusions\n    tuple val(meta), path(\"*.fusions.discarded.tsv\"), emit: fusions_fail\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def blacklist = (args.contains('-b')) ? '' : '-f blacklist'\n    \"\"\"\n    arriba \\\\\n        -x $bam \\\\\n        -a $fasta \\\\\n        -g $gtf \\\\\n        -o ${prefix}.fusions.tsv \\\\\n        -O ${prefix}.fusions.discarded.tsv \\\\\n        $blacklist \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        arriba: \\$(arriba -h | grep 'Version:' 2>&1 |  sed 's/Version:\\s//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MEGAHIT {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::megahit=1.2.9 conda-forge::pigz=2.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-0f92c152b180c7cd39d9b0e6822f8c89ccb59c99:8ec213d21e5d03f9db54898a2baeaf8ec729b447-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-0f92c152b180c7cd39d9b0e6822f8c89ccb59c99:8ec213d21e5d03f9db54898a2baeaf8ec729b447-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n                            \"megahit_out/*.contigs.fa.gz\"                                            \n    tuple val(meta), path(\"megahit_out/*.contigs.fa\")                            , emit: contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.contigs.fa.gz\")      , emit: k_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.addi.fa.gz\")         , emit: addi_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.local.fa.gz\")        , emit: local_contigs\n    tuple val(meta), path(\"megahit_out/intermediate_contigs/k*.final.contigs.fa.gz\"), emit: kfinal_contigs\n    path \"versions.yml\"                                                             , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        megahit \\\\\n            -r ${reads} \\\\\n            -t $task.cpus \\\\\n            $options.args \\\\\n            --out-prefix $prefix\n\n        pigz \\\\\n            --no-name \\\\\n            -p $task.cpus \\\\\n            $options.args2 \\\\\n            #megahit_out/*.fa \\\\\n            megahit_out/intermediate_contigs/*.fa\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(echo \\$(megahit -v 2>&1) | sed 's/MEGAHIT v//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        megahit \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            -t $task.cpus \\\\\n            $options.args \\\\\n            --out-prefix $prefix\n\n        pigz \\\\\n            --no-name \\\\\n            -p $task.cpus \\\\\n            megahit_out/intermediate_contigs/*.fa\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$(echo \\$(megahit -v 2>&1) | sed 's/MEGAHIT v//')\n        END_VERSIONS\n        \"\"\"\n    }\n}", " process samTobam {\n        container \"${container_samtools}\"\n        label 'multithread'\n        publishDir \"${params.outD}/assembly-based/readsOntoCDS/\", mode: 'copy'\n        \n        input:\n        set sname, file(samAln), file(cdsFAA) from dmndSam_prokka_ch\n        \n        output:\n        set sname, file(\"${sname}.mapOnTo.CDS.bam\"), file(\"${sname}.mapOnTo.CDS.sorted.bam\") into readsOntoCDS_bam_ch\n        \n        \"\"\"\n        samtools faidx ${cdsFAA}\n        wait\n        faiF=`ls *.fai`\n        samtools view -bh -t \\$faiF -o ${sname}.mapOnTo.CDS.bam  ${samAln}\n        samtools sort -@ ${task.cpus} ${sname}.mapOnTo.CDS.bam -o ${sname}.mapOnTo.CDS.sorted.bam\n        \"\"\"\n    }", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["nibscbioinformatics/scoop/get_software_versions", "thanhleviet/nf-nanopore-assembly/bracken", "sickle-in-africa/saw.sarek/FastQCFQ", "peterk87/nf-virontus/IVAR_TRIM", "nibscbioinformatics/viralevo/multiqc", "sickle-in-africa/saw.sarek/AlignReadsToReferenceSequence", "raygozag/rnaseq/FASTQC", "replikation/docker_pipelines/filtlong", "xiaoli-dong/magph/ARRIBA", "xiaoli-dong/magph/MEGAHIT", "ralsallaq/metaGx_nf/samTobam"], "list_wf_names": ["replikation/docker_pipelines", "nibscbioinformatics/viralevo", "thanhleviet/nf-nanopore-assembly", "ralsallaq/metaGx_nf", "xiaoli-dong/magph", "raygozag/rnaseq", "nibscbioinformatics/scoop", "sickle-in-africa/saw.sarek", "peterk87/nf-virontus"]}, {"nb_reuse": 7, "tools": ["SAMtools", "BCFtools"], "nb_own": 5, "list_own": ["ray1919", "remiolsen", "nibscbioinformatics", "vibbits", "tamara-hodgetts"], "nb_wf": 4, "list_wf": ["hicscaff", "nf-core-viralevo", "rnaseq-editing", "nf-core-conva", "nf-atac-seq", "lRNA-Seq"], "list_contrib": ["ray1919", "alex-botzki", "kaurravneet4123", "remiolsen", "abotzki", "tamara-hodgetts"], "nb_contrib": 6, "codes": ["\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n                                                    \n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_MPILEUP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.mpileup\"), emit: mpileup\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools mpileup \\\\\n        --fasta-ref $fasta \\\\\n        --output ${prefix}.mpileup \\\\\n        $options.args \\\\\n        $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), emit: bai\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bcftools=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bcftools:1.11--h7c999a4_0\"\n    } else {\n        container \"quay.io/biocontainers/bcftools:1.11--h7c999a4_0\"\n    }\n\n    input:\n    tuple val(meta), path(vcf)\n\n    output:\n    tuple val(meta), path(\"*stats.txt\"), emit: stats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bcftools stats $options.args $vcf > ${prefix}.bcftools_stats.txt\n    echo \\$(bcftools --version 2>&1) | sed 's/^.*bcftools //; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["tamara-hodgetts/nf-atac-seq/SAMTOOLS_SORT", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_VIEW", "remiolsen/hicscaff/SAMTOOLS_SORT", "remiolsen/hicscaff/SAMTOOLS_VIEW", "remiolsen/hicscaff/SAMTOOLS_INDEX", "vibbits/rnaseq-editing/SAMTOOLS_SORT", "ray1919/lRNA-Seq/SAMTOOLS_SORT"], "list_wf_names": ["vibbits/rnaseq-editing", "remiolsen/hicscaff", "ray1919/lRNA-Seq", "tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 7, "tools": ["BCFtools", "SAMtools", "MultiQC", "fastPHASE", "FastQC", "snpEff", "GATK"], "nb_own": 7, "list_own": ["pblaney", "noelnamai", "oisinmccaffrey", "nibscbioinformatics", "vibbits", "zamanianlab", "telatin"], "nb_wf": 7, "list_wf": ["differential-expression-analysis", "scranger", "RNAseq-VC-nf", "clipseq.nextflow", "nextflow-example", "mgp1000", "nextflow-jnj"], "list_contrib": ["pblaney", "lescai", "tmuylder", "noelnamai", "wheelern", "oisinmccaffrey", "vmikk", "telatin"], "nb_contrib": 8, "codes": ["\nprocess convert_sam_to_bam {\n\n    cpus = 2\n    tag \"$state_replicate\"\n    container \"noelnamai/asimov:1.0\"\n\n    input:\n    set state_replicate, file(sam_file) from aligned_sam_ch\n\n    output:\n    set state_replicate, file(\"${sam_file.baseName}.bam\") into aligned_bam_ch\n\n    script:\n    \"\"\"\n    samtools view -bS ${sam_file} > ${sam_file.baseName}.bam \n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc/${sampleID}\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(sampleID), file(reads) from fastqc_files_ch\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess gatherNormalPileupSummariesForMutect2Contamination_gatk {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(per_chromosome_normal_pileup), path(reference_genome_fasta_dict) from per_chromosome_normal_pileups_forMutectPileupGather.groupTuple().combine(reference_genome_fasta_dict_forMutectPileupGatherNormal)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(normal_pileup) into normal_pileups_forMutectContamination\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\tnormal_id = \"${tumor_normal_sample_id}\".replaceFirst(/.*\\_vs\\_/, \"\")\n\tnormal_pileup = \"${normal_id}.pileup\"\n\t\"\"\"\n\tgatk GatherPileupSummaries \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--sequence-dictionary \"${reference_genome_fasta_dict}\" \\\n\t${per_chromosome_normal_pileup.collect { \"--I $it \" }.join()} \\\n\t--O \"${normal_pileup}\"\n\t\"\"\"\n}", "\nprocess variant_annotate {\n\n      publishDir \"${output}/annotations\", mode: 'copy', pattern: '*_ann.vcf'\n      publishDir \"${output}/annotations\", mode: 'copy', pattern: '*.html'\n      publishDir \"${output}/annotations\", mode: 'copy', pattern: '*.txt'\n\n      input:\n          tuple val(id), file(vcf) from input_vcf\n\n      when:\n          params.vcf && params.filter\n\n      \"\"\"\n         bcftools view --threads ${large_core} -Ov ${vcf} | snpEff -v Aedes_aegypti_VB - > ${id}_ann.vcf\n      \"\"\"\n}", "\nprocess multiqc {\n\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from fastqc_ch.collect().ifEmpty([])\n    file ('premap/*') from ch_premap_mqc.collect().ifEmpty([])\n    file ('mapped/*') from ch_align_mqc.collect().ifEmpty([])\n    path ('preseq/*') from ch_preseq_mqc.collect().ifEmpty([])\n    file ('clipqc/*') from ch_clipqc_mqc.collect().ifEmpty([])\n\n    output:\n    file \"*html\" into ch_multiqc_report\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}", "\nprocess fastp {\n       \n                                                                 \n      \n    tag \"filter $sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) \n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}_filt_R*.fastq.gz\"), emit: reads\n    path(\"${sample_id}.fastp.json\"), emit: json\n\n \n    script:\n    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} \\\\\n      -o ${sample_id}_filt_R1.fastq.gz -O ${sample_id}_filt_R2.fastq.gz \\\\\n      --detect_adapter_for_pe -w ${task.cpus} -j ${sample_id}.fastp.json\n \n    \"\"\"  \n}", "\nprocess fastqc_raw_reads {\n\n    input:\n    path read from reads_ch \n    \n    script:\n    \"\"\"\n    fastqc ${read}\n    \"\"\"\n}"], "list_proc": ["noelnamai/differential-expression-analysis/convert_sam_to_bam", "nibscbioinformatics/scranger/fastqc", "pblaney/mgp1000/gatherNormalPileupSummariesForMutect2Contamination_gatk", "zamanianlab/RNAseq-VC-nf/variant_annotate", "oisinmccaffrey/clipseq.nextflow/multiqc", "telatin/nextflow-example/fastp", "vibbits/nextflow-jnj/fastqc_raw_reads"], "list_wf_names": ["vibbits/nextflow-jnj", "noelnamai/differential-expression-analysis", "zamanianlab/RNAseq-VC-nf", "pblaney/mgp1000", "telatin/nextflow-example", "nibscbioinformatics/scranger", "oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["noamteyssier"], "nb_wf": 1, "list_wf": ["nextflow_pipelines"], "list_contrib": ["noamteyssier"], "nb_contrib": 1, "codes": ["\nprocess call_gvcf {\n\t\n\tpublishDir \"$params.outdir/variants/intervals/${region}\"\n\n\tclusterOptions = clusterOptions_multi.concat(\" -o call_gvcf.log\")\n\n\tinput:\n\tset pair_id, file(recal_bam) from recal_for_gvcf\n\teach region from regions.readLines()\n\n\toutput:\n\tfile \"${region.replace(':', '-')}.${pair_id}.g.vcf\" into gvcf_list\n\tfile \"${region.replace(':', '-')}.${pair_id}.g.vcf.idx\" into gvcf_index_list\n\n\tscript:\n\t\"\"\"\n\t#!/usr/bin/env bash\n\n\t# call gvcf for sample\n\tgatk HaplotypeCaller \\\n\t\t-R ${ref} \\\n\t\t-I ${recal_bam} \\\n\t\t--ERC GVCF \\\n\t\t-L ${region} \\\n\t\t-O ${region.replace(':', '-')}.${pair_id}.g.vcf\n\n\t# index gvcf\n\tgatk IndexFeatureFile -F ${region.replace(':', '-')}.${pair_id}.g.vcf\n\n\t\"\"\"\n}"], "list_proc": ["noamteyssier/nextflow_pipelines/call_gvcf"], "list_wf_names": ["noamteyssier/nextflow_pipelines"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["noamteyssier"], "nb_wf": 1, "list_wf": ["nextflow_pipelines"], "list_contrib": ["noamteyssier"], "nb_contrib": 1, "codes": ["\nprocess joint_genotype {\n\n\tpublishDir \"$params.outdir/variants/merged/${region}\"\n\n\tclusterOptions = clusterOptions_single.concat(\" -o joint_geno.log\")\n\n\tinput:\n\tset region, file(gvcf) from grouped_vcfs\n\tset region2, file(idx) from grouped_idx\n\n\toutput:\n\tfile \"${region}.merged.vcf\" into merged_regions\n\tfile \"${region}.merged.vcf.idx\" into merged_regions_idx\n\tfile \"${region}_gendb\"\n\n\tscript:\n\t\"\"\"\n\t#!/usr/bin/env bash\n\n\t# Load gvcfs into a gendb \n\tgatk GenomicsDBImport \\\n\t\t--genomicsdb-workspace-path ${region}_gendb \\\n\t\t-L ${region.replaceFirst('-' , ':')} \\\n\t\t\\$(for v in ${gvcf}; do echo \"-V \\$v\"; done)\n\n\n\t# genotype\n\tgatk GenotypeGVCFs \\\n\t    -R ${ref} \\\n\t    -V gendb://${region}_gendb \\\n\t    -G StandardAnnotation \\\n\t    --new-qual true \\\n\t    -L ${region.replaceFirst('-' , ':')} \\\n\t    -O ${region}.merged.vcf \n\n\t\"\"\"\n}"], "list_proc": ["noamteyssier/nextflow_pipelines/joint_genotype"], "list_wf_names": ["noamteyssier/nextflow_pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BCFtools", "Clair", "pysamstats"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess ClairEvaluation {\n\n    label \"clair\"\n    tag { \"$id - $reference\" }\n    \n    memory { params.clair_mem * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..143 ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    publishDir \"${params.outdir}/${reference.simpleName}/evaluation/${eval_set}/clair\", mode: \"copy\", pattern: \"${id}.vcf\"\n    publishDir \"${params.outdir}/${reference.simpleName}/evaluation/${eval_set}/clair\", mode: \"copy\", pattern: \"${id}.txt\"\n\n    input:\n    tuple val(eval_set), val(id), file(reference), file(bam), file(bai)\n\n    output:\n    tuple val(eval_set), val(id), val(\"${reference.simpleName}\"), file(\"${id}.vcf\"), file(\"${id}.txt\")\n\n    \n    \"\"\"\n    samtools faidx $reference\n    np phybeast utils print-header --fasta $reference | while read -r contig ; do\n    echo \"Processing contig: \\$contig\"\n    clair callVarBam --chkpnt_fn ${params.clair_model} \\\n                     --ref_fn $reference \\\n                     --bam_fn $bam \\\n                     --sampleName $id \\\n                     --minCoverage 1 \\\n                     --threads $task.cpus \\\n                     --call_fn ${id}.\\${contig}.clair.vcf \\\n                     --ctgName \\$contig \\\n                     ${params.clair_haploid}\n    done\n\n    vcfcat ${id}.*.clair.vcf | bcftools sort -m 8G -o ${id}.vcf \n\n    pysamstats -t variation_strand $bam -f $reference > ${id}.txt\n    \n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/ClairEvaluation"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 2, "tools": ["BEAST", "snippy"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process BeastCPU {\n\n                 \n    \n    label \"beast\"\n    tag { \"BEAST2\" }\n\n    publishDir \"${params.outdir}/beast\", mode: \"copy\"\n\n    input:\n    tuple val(id), file(xml)\n    val(beagle_params)\n\n    output:\n    tuple val(id), file(\"${id}.*\")\n\n    \"\"\"\n    beast -threads $task.cpus ${beagle_params} ${params.beast_params} $xml\n    \"\"\"\n\n}", " process SnippyTraining {\n        \n                                                                                             \n\n        label \"snippy\"\n        tag { id }\n\n        publishDir \"${params.outdir}/${ref}/polishers/snippy\", mode: \"symlink\", pattern: \"${id}.vcf\"\n\n        input:\n        tuple val(model), val(id), file(forward), file(reverse), file(ont)\n        each file(reference)\n\n        output:\n        tuple val(model), val(id), val(ref), file(reference), file(ont), file(\"${id}.vcf\") \n\n        script:\n\n        ref = reference.simpleName\n\n        \"\"\"\n        snippy --cpus $task.cpus --outdir ${id}_snippy --prefix $id --reference $reference --R1 $forward --R2 $reverse $params.snippy_params\n        mv ${id}_snippy/${id}.vcf . \n        \"\"\"\n\n    }"], "list_proc": ["np-core/modules/BeastCPU", "np-core/modules/SnippyTraining"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["kraken2", "Bracken"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process Kraken {\n\n    tag { id }\n    label \"kraken2\"\n\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.kraken\"\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.bracken\"\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.report\"\n    publishDir \"$params.outdir/fastq\", pattern: \"$fq\"                                                                                                                                     \n\n    input:\n    tuple val(id), file(fq)\n    each file(db)\n\n    output:\n    tuple val(id), file(\"${id}.kraken\"), file(\"${id}.kraken.report\"), file(\"${id}.bracken\"), file(\"${id}.bracken.report\")\n\n    \"\"\"\n    kraken2 --db $db --threads $task.cpus --output ${id}.kraken --report ${id}.kraken.report $fq\n    bracken -d $db -i ${id}.kraken.report -o ${id}.bracken -w ${id}.bracken.report -r $params.bracken_length -l $params.bracken_level -t $params.bracken_threshold\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/Kraken"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 2, "tools": ["Bracken", "SAMtools", "kraken2", "Minimap2", "Picard"], "nb_own": 2, "list_own": ["np-core", "npacc"], "nb_wf": 2, "list_wf": ["HIV-Drug-Resistance", "modules"], "list_contrib": ["esteinig", "npacc"], "nb_contrib": 2, "codes": ["\nprocess CleanHIVReads {\n    input:\n    tuple dataset_id, file(forward), file(reverse), file(ref) from TrimmedReads.combine(Ch_HIVComp)\n\n    output:\n    tuple dataset_id, file(\"${dataset_id}.clean_1.fq.gz\"), file(\"${dataset_id}.clean_2.fq.gz\") into HIVCleanReadsAssembly, HIVCleanReadsPolishing, HIVCleanReadsVariantCalling\n\n    script:\n    \"\"\"\n    minimap2 -x sr -a $ref $forward $reverse | samtools view -F 4 -b > clean.bam\n    picard SamToFastq VALIDATION_STRINGENCY=LENIENT I=clean.bam F=${dataset_id}.clean_1.fq.gz F2=${dataset_id}.clean_2.fq.gz\n    \"\"\"\n}", "\nprocess KrakenOnline {\n\n    tag { \"Batch $batch - $db\" }\n    label \"kraken2\"\n\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.kraken\"\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.bracken\"\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.report\"\n\n    input:\n    tuple val(id), file(fq), val(batch)\n    each file(db)\n\n    output:\n    tuple val(id), file(\"${id}.${batch}.kraken\"), file(\"${id}.${batch}.kraken.report\"), file(\"${id}.${batch}.bracken\"), file(\"${id}.${batch}.bracken.report\")\n\n    \"\"\"\n    kraken2 --db $db --threads $task.cpus --output ${id}.${batch}.kraken --report ${id}.${batch}.kraken.report $fq\n    bracken -d $db -i ${id}.${batch}.kraken.report -o ${id}.${batch}.bracken -w ${id}.${batch}.bracken.report -r $params.bracken_length -l $params.bracken_level -t $params.bracken_threshold\n    \"\"\"\n\n}"], "list_proc": ["npacc/HIV-Drug-Resistance/CleanHIVReads", "np-core/modules/KrakenOnline"], "list_wf_names": ["np-core/modules", "npacc/HIV-Drug-Resistance"]}, {"nb_reuse": 1, "tools": ["kraken2", "Bracken"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess KrakenAssemblyOnline {\n\n    tag { \"Batch $batch - $db\" }\n    label \"kraken2\"\n\n    publishDir \"$params.outdir/assembly/kraken/$db\", mode: \"copy\", pattern: \"*.kraken\"\n    publishDir \"$params.outdir/assembly/kraken/$db\", mode: \"copy\", pattern: \"*.bracken\"\n    publishDir \"$params.outdir/assembly/kraken/$db\", mode: \"copy\", pattern: \"*.report\"\n\n    input:\n    tuple val(id), file(fq), val(batch)\n    each file(db)\n\n    output:\n    tuple val(id), file(\"${id}.${batch}.kraken\"), file(\"${id}.${batch}.kraken.report\"), file(\"${id}.${batch}.bracken\"), file(\"${id}.${batch}.bracken.report\")\n\n    \"\"\"\n    kraken2 --db $db --threads $task.cpus --output ${id}.${batch}.kraken --report ${id}.${batch}.kraken.report $fq\n    bracken -d $db -i ${id}.${batch}.kraken.report -o ${id}.${batch}.bracken -w ${id}.${batch}.bracken.report -r $params.bracken_length -l $params.bracken_level -t $params.bracken_threshold\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/KrakenAssemblyOnline"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["kraken2", "Bracken"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess KrakenIllumina {\n\n    tag { id }\n    label \"kraken2\"\n\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.kraken\"\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.bracken\"\n    publishDir \"$params.outdir/kraken/$db\", mode: \"copy\", pattern: \"*.report\"\n\n    input:\n    tuple val(id), file(forward), file(reverse)\n    each file(db)\n\n    output:\n    tuple val(id), file(\"${id}.kraken\"), file(\"${id}.kraken.report\"), file(\"${id}.bracken\"), file(\"${id}.bracken.report\")\n\n    \"\"\"\n    kraken2 --db $db --threads $task.cpus --output ${id}.kraken --report ${id}.kraken.report --paired $forward $reverse\n    bracken -d $db -i ${id}.kraken.report -o ${id}.bracken -w ${id}.bracken.report -r $params.bracken_length -l $params.bracken_level -t $params.bracken_threshold\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/KrakenIllumina"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process Fastp {\n\n    label \"fastp\"\n    tag { id }\n\n    input:\n    tuple val(id), file(forward), file(reverse)\n\n    output:\n    tuple val(id), file(\"${id}_1_qc.fq.gz\"), file(\"${id}_2_qc.fq.gz\")\n\n    \"\"\"\n    fastp -i $forward -I $reverse -o ${id}_1_qc.fq.gz -O ${id}_2_qc.fq.gz --thread $task.cpus\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/Fastp"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 2, "tools": ["fastPHASE", "MultiQC"], "nb_own": 2, "list_own": ["ntanh1999", "np-core"], "nb_wf": 2, "list_wf": ["nextflow", "modules"], "list_contrib": ["ntanh1999", "esteinig"], "nb_contrib": 2, "codes": ["\nprocess MULTIQC_READS{\n    publishDir \"$params.result\", mode: 'copy'\n    \n    input:\n        path fastqc\n    \n    output:\n        path \"multiqc\"\n\n    script:\n    \"\"\"\n    multiqc -o multiqc $fastqc\n    \"\"\"\n}", "\nprocess FastpTraining {\n\n    label \"fastp\"\n    tag { id }\n\n    input:\n    tuple val(model), val(id), file(forward), file(reverse), file(ont)\n\n    output:\n    tuple val(model), val(id), file(\"${id}_1_qc.fq.gz\"), file(\"${id}_2_qc.fq.gz\"), file(ont)\n\n    \"\"\"\n    fastp --in1 $forward --in2 $reverse --out1 ${id}_1_qc.fq.gz --out2 ${id}_2_qc.fq.gz --thread $task.cpus\n    \"\"\"\n\n}"], "list_proc": ["ntanh1999/nextflow/MULTIQC_READS", "np-core/modules/FastpTraining"], "list_wf_names": ["np-core/modules", "ntanh1999/nextflow"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess FastpEvaluation {\n\n    label \"fastp\"\n    tag { id }\n\n    input:\n    tuple val(eval_set), val(id), file(forward), file(reverse)\n\n    output:\n    tuple val(eval_set), val(id), file(\"${id}_1_qc.fq.gz\"), file(\"${id}_2_qc.fq.gz\")\n\n    \"\"\"\n    fastp --in1 $forward --in2 $reverse --out1 ${id}_1_qc.fq.gz --out2 ${id}_2_qc.fq.gz --thread $task.cpus\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/FastpEvaluation"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["pysamstats"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess MedakaVariants {\n\n    label \"medaka\"\n    tag { \"$id\" }\n\n    memory { params.medaka_mem * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..143 ? 'retry' : 'ignore' }\n    maxRetries 5\n\n    publishDir \"${params.outdir}/medaka\", mode: \"copy\", pattern: \"${id}.vcf\"\n    publishDir \"${params.outdir}/medaka\", mode: \"copy\", pattern: \"${id}.txt\"\n\n    input:\n    tuple val(id), file(fq)\n    file(reference)\n\n    output:\n    tuple val(id), file(\"${id}.vcf\"), file(\"${id}.txt\")\n    tuple val(id), file(bam), file(bai)\n\n    \"\"\"\n    medaka_haploid_variant --model $params.medaka_model --threads $task.cpus --output_dir vars $fq $reference\n    mv vars/consensus_to_ref.vcf ${id}_${coverage}.vcf\n    pysamstats -t variation_strand vars/calls_to_draft.bam -f $reference > ${id}_${coverage}.txt\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MedakaVariants"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["pysamstats"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess MedakaTraining {\n\n    label \"medaka\"\n    tag { \"$model_name - $id - $reference\" }\n\n    memory { params.medaka_mem * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..143 ? 'retry' : 'ignore' }\n    maxRetries 5\n\n    publishDir \"${params.outdir}/${ref}/polishers/variants\", mode: \"copy\", pattern: \"${id}_${coverage}.vcf\"\n    publishDir \"${params.outdir}/${ref}/polishers/variants\", mode: \"copy\", pattern: \"${id}_${coverage}.txt\"\n\n    input:\n    tuple val(model_name), val(id), val(ref), val(coverage), file(reference), file(fq), file(snippy_vcf)\n\n    output:\n    tuple val(model_name), val(ref), file(\"${id}_${coverage}.vcf\"), file(\"${id}_${coverage}.txt\"), file(snippy_vcf)\n\n\n    \"\"\"\n    medaka_haploid_variant --model $params.medaka_model --threads $task.cpus --output_dir vars $fq $reference\n    mv vars/consensus_to_ref.vcf ${id}_${coverage}.vcf\n    pysamstats -t variation_strand vars/calls_to_draft.bam -f $reference > ${id}_${coverage}.txt\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MedakaTraining"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["pysamstats"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess MedakaEvaluation {\n\n    label \"medaka\"\n    tag { \"$id - $reference\" }\n\n    memory { params.medaka_mem * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..143 ? 'retry' : 'ignore' }\n    maxRetries 5\n\n    publishDir \"${params.outdir}/${reference.simpleName}/evaluation/${eval_set}/medaka\", mode: \"copy\", pattern: \"${id}.vcf\"\n    publishDir \"${params.outdir}/${reference.simpleName}/evaluation/${eval_set}/medaka\", mode: \"copy\", pattern: \"${id}.txt\"\n\n    input:\n    tuple val(eval_set), val(id), file(fq)\n    each file(reference)\n\n    output:\n    tuple val(eval_set), val(id), val(\"${reference.simpleName}\"), file(\"${id}.vcf\"), file(\"${id}.txt\")\n\n\n    \"\"\"\n    medaka_haploid_variant --model $params.medaka_model --threads $task.cpus --output_dir vars $fq $reference\n    mv vars/consensus_to_ref.vcf ${id}.vcf\n    pysamstats -t variation_strand vars/calls_to_draft.bam -f $reference > ${id}.txt\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MedakaEvaluation"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["Minimap2", "Racon"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess Racon {\n\n    tag { id }\n    label \"racon\"\n\n    input:\n    tuple val(id), file(assembly), file(fastq)\n\n    output:\n    tuple val(id), file(\"${id}.racon.fasta\"), file(fastq)\n\n    script:\n    \"\"\"\n    minimap2 -x map-ont -t $task.cpus $assembly $fastq > assembly_1.paf\n    racon -m 8 -x -6 -g -8 -w 500 -t $task.cpus $fastq assembly_1.paf $assembly > assembly_consensus_1.fasta\n    minimap2 -x map-ont -t $task.cpus assembly_consensus_1.fasta $fastq > assembly_2.paf\n    racon -m 8 -x -6 -g -8 -w 500 -t $task.cpus $fastq assembly_2.paf assembly_consensus_1.fasta > assembly_consensus_2.fasta\n    minimap2 -x map-ont -t $task.cpus assembly_consensus_2.fasta $fastq > assembly_3.paf\n    racon -m 8 -x -6 -g -8 -w 500 -t $task.cpus $fastq assembly_3.paf assembly_consensus_2.fasta > assembly_consensus_3.fasta\n    minimap2 -x map-ont -t $task.cpus assembly_consensus_3.fasta $fastq > assembly_4.paf\n    racon -m 8 -x -6 -g -8 -w 500 -t $task.cpus $fastq assembly_4.paf assembly_consensus_3.fasta > ${id}.racon.fasta\n    \"\"\"\n}"], "list_proc": ["np-core/modules/Racon"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process MinimapONT {\n\n    label \"minimap2\"\n    tag { \"$id\" }\n\n    publishDir \"${params.outdir}/minimap2\", mode: \"symlink\"\n\n    input:\n    tuple val(id), file(fastq)\n    file(reference)\n\n    output:\n    tuple val(id), file(\"${id}.bam\"), file(\"${id}.bam.bai\")\n\n    \"\"\"\n    minimap2 -t $task.cpus -ax map-ont $reference $fastq | samtools sort | samtools view -Sb > ${id}.bam\n    samtools index ${id}.bam\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MinimapONT"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess MinimapTraining {\n\n    label \"minimap2\"\n    tag { \"$model_name - $id - $reference\" }\n\n    publishDir \"${params.outdir}/${ref}/polishers/alignments\", mode: \"symlink\"\n \n    input:\n    tuple val(model_name), val(id), val(ref), val(coverage), file(reference), file(fq_cov), file(snippy_vcf)\n\n    output:\n    tuple val(model_name), val(id), val(ref), val(coverage), file(reference), file(\"${id}_${coverage}.bam\"), file(\"${id}_${coverage}.bam.bai\"), file(snippy_vcf)\n\n    \"\"\"\n    minimap2 -t $task.cpus -ax map-ont $reference $fq_cov | samtools sort | samtools view -Sb > ${id}_${coverage}.bam\n    samtools index ${id}_${coverage}.bam\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MinimapTraining"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess MinimapEvaluation {\n\n    label \"minimap2\"\n    tag { \"$id - $reference\" }\n\n    memory { params.minimap_mem * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..140 ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    publishDir \"${params.outdir}/${reference.simpleName}/evaluation/${eval_set}\", mode: \"symlink\"\n \n    input:\n    tuple val(eval_set), val(id), file(fq)\n    each file(reference)\n\n    output:\n    tuple val(eval_set), val(id), file(reference), file(\"${id}.bam\"), file(\"${id}.bam.bai\")\n\n                                                                                      \n                                           \n\n    \"\"\"\n    minimap2 -t $task.cpus -ax map-ont $reference $fq > tmp.sam \n    samtools sort tmp.sam | samtools view -Sb > ${id}.bam\n    samtools index ${id}.bam\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MinimapEvaluation"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 2, "tools": ["SAMtools", "GraftM", "Bowtie"], "nb_own": 2, "list_own": ["np-core", "oisinmccaffrey"], "nb_wf": 2, "list_wf": ["clipseq.nextflow", "modules"], "list_contrib": ["esteinig", "oisinmccaffrey"], "nb_contrib": 2, "codes": ["process GraftM {\n\n    label \"graftm\"\n    tag { \"$id\" }\n\n    publishDir \"${params.outdir}/graftm/$pkg\", mode: \"copy\", pattern: \"*\"\n\n    input:\n    tuple val(id), file(fwd), file(rev), val(pkg), file(graftm)\n\n    output:\n    file(\"${id}\")\n\n    \"\"\"\n    graftM graft --forward $fwd --reverse $rev --graftm_package $graftm --output_directory $id\n    \"\"\"\n\n}", "\nprocess premap {\n\n    publishDir \"${params.outdir}/premap\", mode: 'copy'\n\n    tag \"$key\"\n\n    input:\n\n    tuple val(key), file(reads) from mapping_reads\n    path(index) from ch_bt2_index.collect()\n\n    output:\n    tuple val(key), path(\"${key}.unmapped.fq.gz\") into ch_unmapped\n    tuple val(key), path(\"${key}.premapped.bam\"), path(\"${key}.premapped.bam.bai\")\n    path \"*.log\" into ch_premap_mqc, ch_premap_qc\n\n    script:\n    \"\"\"\n    bowtie2 -p $task.cpus -x ${index[0].simpleName} --un-gz ${key}.unmapped.fq.gz -U $reads 2> ${key}.premap.log | \\\n    samtools sort -@ $task.cpus /dev/stdin > ${key}.premapped.bam && \\\n    samtools index -@ $task.cpus ${key}.premapped.bam\n    \"\"\"\n}"], "list_proc": ["np-core/modules/GraftM", "oisinmccaffrey/clipseq.nextflow/premap"], "list_wf_names": ["np-core/modules", "oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["GraftM"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess GraftMAG {\n\n    label \"graftm\"\n    tag { \"$id\" }\n\n    publishDir \"${params.outdir}/graftm/$pkg\", mode: \"copy\", pattern: \"*\"\n\n    input:\n    tuple val(id), file(fa), val(pkg), file(graftm)\n\n    output:\n    file(\"${id}\")\n\n    \"\"\"\n    graftM graft --forward $fa --graftm_package $graftm --output_directory $id\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/GraftMAG"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process ProkkaBacteria {\n\n    tag { id }\n    label \"prokka\"\n\n    publishDir \"$params.outdir/prokka\", mode: \"copy\"\n\n    input:\n    tuple val(id), file(fasta)\n\n    output:\n    tuple val(id), file(\"${id}_prokka\")\n\n    \"\"\"\n    prokka --compliant --outdir ${id}_prokka \\\n        --locustag $locus_tag --prefix $locus_tag --kingdom Bacteria \\\n        --genus $params.genus --species $params.species --usegenus $fasta\n\n    \n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/ProkkaBacteria"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 2, "tools": ["BEDTools", "snippy"], "nb_own": 2, "list_own": ["np-core", "oisinmccaffrey"], "nb_wf": 2, "list_wf": ["clipseq.nextflow", "modules"], "list_contrib": ["esteinig", "oisinmccaffrey"], "nb_contrib": 2, "codes": ["\nprocess piranha_motif_dreme {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/piranha_motif\", mode: 'copy'\n\n        input:\n        tuple val(name), path(peaks) from ch_peaks_piranha\n        path(fasta) from ch_fasta_dreme_piranha.collect()\n        path(fai) from ch_fai_piranha_motif.collect()\n\n        output:\n         tuple val(name), path(\"${name}_dreme/*\") into ch_motif_dreme_piranha\n\n        script:\n        motif_sample = params.motif_sample\n        \"\"\"\n        pigz -d -c $peaks | awk '{OFS=\"\\t\"}{if(\\$6 == \"+\") print \\$1, \\$2, \\$2+1, \\$4, \\$5, \\$6; else print \\$1, \\$3-1, \\$3, \\$4, \\$5, \\$6}' | \\\\\n        bedtools slop -s -l 20 -r 20 -i /dev/stdin -g $fai | \\\\\n        shuf -n $motif_sample > resized_peaks.bed\n        bedtools getfasta -fi $fasta -bed resized_peaks.bed -fo resized_peaks.fasta\n        dreme -norc -o ${name}_dreme -p resized_peaks.fasta\n        \"\"\"\n }", " process SnippyFastq {\n\n        label \"snippy\"\n        tag { id }\n\n        publishDir \"${params.outdir}/snippy\", mode: \"symlink\", pattern: \"$id\"\n\n        input:\n        tuple val(id), file(forward), file(reverse)\n        file(reference)\n\n        output:\n        file(\"$id\")                       \n\n        \"\"\"\n        snippy --cpus $task.cpus --outdir $id --prefix $id --reference $reference --R1 $forward --R2 $reverse $params.snippy_params\n        \"\"\"\n\n    }"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/piranha_motif_dreme", "np-core/modules/SnippyFastq"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow", "np-core/modules"]}, {"nb_reuse": 1, "tools": ["snippy"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": [" process SnippyFasta {\n\n        label \"snippy\"\n        tag { id }\n\n        publishDir \"${params.outdir}/snippy\", mode: \"symlink\", pattern: \"$id\"\n\n        input:\n        tuple val(id), file(fasta)\n        file(reference)\n\n        output:\n        file(\"$id\")                       \n\n        \"\"\"\n        snippy --cpus $task.cpus --outdir $id --prefix $id --reference $reference --ctgs $fasta $params.snippy_params\n        \"\"\"\n\n    }"], "list_proc": ["np-core/modules/SnippyFasta"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["snippy"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": [" process SnippyEvaluation {\n        \n                                                                                             \n\n        label \"snippy\"\n        tag { id }\n\n        publishDir \"${params.outdir}/${reference.simpleName}/evaluation/$eval_set\", mode: \"copy\", pattern: \"${id}.ref.vcf\"\n\n        input:\n        tuple val(eval_set), val(id), file(forward), file(reverse)\n        each file(reference)\n\n        output:\n        tuple val(eval_set), val(id), val(\"${reference.simpleName}\"), file(\"${id}.ref.vcf\") \n\n\n        \"\"\"\n        snippy --cpus $task.cpus --outdir ${id}_snippy --prefix $id --reference $reference --R1 $forward --R2 $reverse $params.snippy_params\n        mv ${id}_snippy/${id}.vcf ${id}.ref.vcf\n        \"\"\"\n\n    }"], "list_proc": ["np-core/modules/SnippyEvaluation"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["RAxML-NG"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process RAxML {\n\n    label \"raxml\"\n    tag { \"$params.raxml_model\" }\n\n    publishDir \"${params.outdir}/phylogeny\", mode: \"copy\"\n\n    input:\n    tuple val(id), file(alignment)\n\n    output:\n    tuple val(id), file(\"${id}.newick\")\n\n    \"\"\"\n    raxml-ng --msa $alignment --model $params.raxml_model $params.raxml_params --threads $task.cpus --prefix rax --force\n    mv rax.raxml.bestTree ${id}.newick\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/RAxML"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["process Flye {\n    \n    tag { id }\n    label \"flye\"\n\n    memory { params.flye_mem * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..140 ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    publishDir \"$params.outdir/ont/assembly\", mode: \"copy\", pattern: \"*.fasta\"\n    publishDir \"$params.outdir/ont/assembly\", mode: \"copy\", pattern: \"*.gfa\"\n    publishDir \"$params.outdir/ont/assembly\", mode: \"copy\", pattern: \"*.txt\"\n\n    input:\n    tuple val(id), file(fq)\n\n    output:\n    tuple val(id), file(\"${id}.fasta\")\n    tuple file(\"${id}.txt\"), file(\"${id}.gfa\")\n\n    \"\"\"\n    flye --nano-raw $fq $params.assembly_options -t $task.cpus -o assembly\n    mv assembly/assembly_info.txt ${id}.txt\n    mv assembly/assembly.fasta ${id}.fasta\n    mv assembly/assembly_graph.gfa ${id}.gfa\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/Flye"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["np-core"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["esteinig"], "nb_contrib": 1, "codes": ["\nprocess MetaFlye {\n    \n    tag { \"$id\" }\n    label \"flye\"\n\n    publishDir \"$params.outdir/assembly/metaflye\", mode: \"copy\", pattern: \"*.fasta\"\n    publishDir \"$params.outdir/assembly/metaflye\", mode: \"copy\", pattern: \"*.gfa\"\n    publishDir \"$params.outdir/assembly/metaflye\", mode: \"copy\", pattern: \"*.txt\"\n\n    input:\n    tuple val(id), file(fq)\n\n    output:\n    tuple val(id), file(\"${id}.fasta\") optional true                                                                    \n    tuple file(\"${id}.gfa\"), file(\"${id}.txt\") optional true\n\n    \"\"\"\n    flye --nano-raw $fq --meta $params.assembly_options -t $task.cpus -o assembly\n    \n    if [ -f assembly/assembly.fasta ]; then\n        mv assembly/assembly_info.txt ${id}.txt\n        mv assembly/assembly.fasta ${id}.fasta\n        mv assembly/assembly_graph.gfa ${id}.gfa\n    fi\n    \"\"\"\n\n}"], "list_proc": ["np-core/modules/MetaFlye"], "list_wf_names": ["np-core/modules"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["npacc"], "nb_wf": 1, "list_wf": ["HIV-Drug-Resistance"], "list_contrib": ["npacc"], "nb_contrib": 1, "codes": ["\nprocess MultiQC {\n                                                                                    \n    container 'multiqc_docker:v.01'\n\n    publishDir \"${FastqDir}/TrimmedGalore/MultiQC\"\n\n    input:\n    file(\"*\") from txtfiles.collect().combine(htmlfiles.collect())\n\n    output:\n    file \"multiqc_report.html\"\n\n    script:\n     \"\"\"\n      multiqc -m cutadapt -m fastqc -n multiqc_report.html .\n     \"\"\"\n}"], "list_proc": ["npacc/HIV-Drug-Resistance/MultiQC"], "list_wf_names": ["npacc/HIV-Drug-Resistance"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["npacc"], "nb_wf": 1, "list_wf": ["HIV-Drug-Resistance"], "list_contrib": ["npacc"], "nb_contrib": 1, "codes": ["\nprocess HIVShiver {\n    publishDir \"${FastqDir}/Shiver\", pattern: \"${dataset_id}.shiver.fa\", mode: 'copy'\n    publishDir \"${FastqDir}/Shiver\", pattern: \"${dataset_id}.shiverlog.txt\", mode: 'copy'   \n\n    input:\n    tuple dataset_id, file(assembly), file(forward), file(reverse), file(shiverconf), file(shiverinit) from HIVIVAAssembly.join(HIVCleanReadsPolishing, by: [0]).combine(ShiverConf).combine(ShiverInit)\n\n    output:\n    tuple dataset_id, file(\"${dataset_id}.shiver.fa\") optional true into HIVAssemblyBAM, HIVAssemblyVariants\n    file(\"${dataset_id}.shiver.txt\") optional true\n    file(\"${dataset_id}.shiverlog.txt\") optional true\n\n    script:\n     \"\"\"\n    if shiver_align_contigs.sh ${shiverinit} ${shiverconf} ${assembly} ${dataset_id}; then\n        if [ -f ${dataset_id}_cut_wRefs.fasta ]; then\n            shiver_map_reads.sh ${shiverinit} ${shiverconf} ${assembly} ${dataset_id} ${dataset_id}.blast ${dataset_id}_cut_wRefs.fasta ${forward} ${reverse}\n        else\n            shiver_map_reads.sh ${shiverinit} ${shiverconf} ${assembly} ${dataset_id} ${dataset_id}.blast ${dataset_id}_raw_wRefs.fasta ${forward} ${reverse}\n        fi\n        seqtk seq -l0 ${dataset_id}_remap_consensus_MinCov_15_30.fasta | head -n2 | sed '/>/!s/-//g' | sed 's/\\\\?/N/g' | sed 's/_remap_consensus//g' | seqtk seq -l80 > ${dataset_id}.shiver.fa\n    else\n        echo \"No HIV contigs found. This sample is likely to be purely contamination\" > ${dataset_id}.shiverlog.txt\n    fi\n    \"\"\"\n}"], "list_proc": ["npacc/HIV-Drug-Resistance/HIVShiver"], "list_wf_names": ["npacc/HIV-Drug-Resistance"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["npacc"], "nb_wf": 1, "list_wf": ["HIV-Drug-Resistance"], "list_contrib": ["npacc"], "nb_contrib": 1, "codes": ["\nprocess HIVMappingVariantCalling {\n    input:\n    tuple dataset_id, file(forward), file(reverse), file(assembly) from HIVCleanReadsVariantCalling.join(HIVAssemblyBAM, by: [0])\n\n    output:\n    tuple dataset_id, file(\"${dataset_id}.variants.bam\") into HIVMappingNoDupsBAM\n\n    script:\n    \"\"\"\n    samtools faidx $assembly\n    minimap2 -x sr -a $assembly $forward $reverse | samtools view -@ 2 -b | samtools sort -@ 2 -o ${dataset_id}.variants.bam\n    \"\"\"\n}"], "list_proc": ["npacc/HIV-Drug-Resistance/HIVMappingVariantCalling"], "list_wf_names": ["npacc/HIV-Drug-Resistance"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BCFtools"], "nb_own": 1, "list_own": ["npacc"], "nb_wf": 1, "list_wf": ["HIV-Drug-Resistance"], "list_contrib": ["npacc"], "nb_contrib": 1, "codes": ["\nprocess HIVVariantCallingVarScan {\n    publishDir \"${FastqDir}/CallVariants/fasta/minor_variants\", pattern: \"${dataset_id}.${minvarfreq}.minor.fa\", mode: 'copy'\n    publishDir \"${FastqDir}/CallVariants/fasta/IUPAC\", pattern: \"${dataset_id}.${minvarfreq}.iupac.consensus.fa\", mode: 'copy'\n    publishDir \"${FastqDir}/CallVariants/vcf\", pattern: \"${dataset_id}.${minvarfreq}.consensus.vcf\", mode: 'copy'\n\n    input:\n    tuple dataset_id, file(bam), file(assembly), minvarfreq from HIVMappingNoDupsBAM.join(HIVAssemblyVariants, by: [0]).combine(MinVarFreq)\n\n    output:\n    tuple dataset_id, minvarfreq, file(\"*.${minvarfreq}.iupac.consensus.fa\") into HIVAssemblyWithVariants\n    file \"${dataset_id}.${minvarfreq}.minor.fa\"\n    file \"${dataset_id}.${minvarfreq}.consensus.vcf\"\n\n    script:\n    \"\"\"\n    samtools mpileup --max-depth 10000000 --redo-BAQ --min-MQ 17 --min-BQ 20 --output ${dataset_id}.mpileup --fasta-ref ${assembly} ${bam}\n    java -Xmx17G -jar /home/centos/miniconda3/envs/trialrun/share/varscan-2.4.4-0/VarScan.jar mpileup2cns ${dataset_id}.mpileup --min-var-freq ${minvarfreq} --p-value 95e-02 --min-coverage 100 --output-vcf 1 > ${dataset_id}.varscan.cns.vcf\n    bgzip ${dataset_id}.varscan.cns.vcf\n    tabix -p vcf ${dataset_id}.varscan.cns.vcf.gz\n    bcftools view -i'FILTER=\"PASS\"' -Oz -o ${dataset_id}.varscan.cns.filtered.vcf.gz ${dataset_id}.varscan.cns.vcf.gz\n    zcat ${dataset_id}.varscan.cns.filtered.vcf.gz > ${dataset_id}.${minvarfreq}.consensus.vcf\n    tabix -p vcf ${dataset_id}.varscan.cns.filtered.vcf.gz\n    bcftools consensus -f $assembly ${dataset_id}.varscan.cns.filtered.vcf.gz --output ${dataset_id}.${minvarfreq}.minor.fa\n    bcftools consensus -I -f $assembly ${dataset_id}.varscan.cns.filtered.vcf.gz --output ${dataset_id}.${minvarfreq}.iupac.consensus.fa\n    sed -i 's/polished/consensus-minor/g' ${dataset_id}.${minvarfreq}.minor.fa\n    sed -i 's/polished/consensus-iupac/g' ${dataset_id}.${minvarfreq}.iupac.consensus.fa\n    sed -i '/^>/ s/\\$/ [Variant caller: ${params.variantstrategy}] [Minor variant bases IUPAC] [Variant frequency: ${minvarfreq}] /' ${dataset_id}.${minvarfreq}.iupac.consensus.fa\n    sed -i '/^>/ s/\\$/ [Variant caller: ${params.variantstrategy}] [Minor variants bases ONLY] [Variant frequency: ${minvarfreq}] /' ${dataset_id}.${minvarfreq}.minor.fa\n    \"\"\"\n}"], "list_proc": ["npacc/HIV-Drug-Resistance/HIVVariantCallingVarScan"], "list_wf_names": ["npacc/HIV-Drug-Resistance"]}, {"nb_reuse": 6, "tools": ["FastQC", "Bowtie", "MultiQC"], "nb_own": 10, "list_own": ["samlhao", "yassineS", "suzannejin", "supark87", "nriddiford", "paulstretenowich", "steepale", "propan2one", "vladsaveliev", "veitveit"], "nb_wf": 6, "list_wf": ["variantcallerbench", "wgsfastqtobam", "umcaw", "nf-core-lohcator", "nf-core-abricate", "nf-demux", "minion_hrp2", "nf-proportionality", "nf-core-wombatp", "nf-core-mutenrich", "nf-core-viralrecon"], "list_contrib": ["veitveit", "supark87", "propan2one", "nriddiford", "steepale", "yassineS", "suzannejin", "sjsabin", "vladsaveliev", "drpatelh"], "nb_contrib": 10, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess maptoreference {\n    container 'supark87/minion'\n\n    publishDir \"$params.output.folder/samfiles/\", pattern: \"*.sam\", mode : \"copy\"\n\n      input:\n  \n        path(ref_dir) from ref_dir\n        file(\"*\") from trim_out1.collect()\n\n\n       \n    output:\n       \n       file(\"bowtie.sam\") into sam\n\n    script:\n        \"\"\"\n     cat *.fq >> all.fq\n     bowtie2-build-s $ref_dir/XM_002808697.2.fasta myIndex\n     bowtie2-align-s -I 0 -X 800 -p 16 --fast --dovetail --met-file bmet.txt -x myIndex -U all.fq -S bowtie.sam\n\n\n        \"\"\"\n\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\" }\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    when:\n    params.reads\n\n    script:\n    \"\"\"\n    fastqc --threads $task.cpus $reads\n    \"\"\"\n}"], "list_proc": ["vladsaveliev/umcaw/fastqc", "supark87/minion_hrp2/maptoreference", "steepale/wgsfastqtobam/fastqc", "propan2one/variantcallerbench/fastqc", "yassineS/nf-demux/fastqc", "suzannejin/nf-proportionality/fastqc"], "list_wf_names": ["suzannejin/nf-proportionality", "vladsaveliev/umcaw", "supark87/minion_hrp2", "steepale/wgsfastqtobam", "propan2one/variantcallerbench", "yassineS/nf-demux"]}, {"nb_reuse": 4, "tools": ["GmT", "fastPHASE", "FastQC", "MiXCR", "GATK"], "nb_own": 5, "list_own": ["yonghah", "pblaney", "nriddiford", "szabogtamas", "vib-singlecell-nf"], "nb_wf": 4, "list_wf": ["vsn-pipelines", "repertoireseq_container", "geodata-pipeline", "nf-core-lohcator", "mgp1000"], "list_contrib": ["dependabot[bot]", "yonghah", "pblaney", "KrisDavie", "nriddiford", "ghuls", "dweemx", "szabogtamas", "cflerin"], "nb_contrib": 9, "codes": ["\nprocess FASTP__CLEAN_AND_FASTQC {\n\n    container params.tools.fastp.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        set val(sample), path(reads)\n    \n    output:\n        tuple file('*_R{1,2}.clean.fastq.gz'), emit: fastq\n        tuple file('*_fastp.{json,html}'), emit: report\n    \n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.fastp)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        fastp --thread ${processParams.thread} \\\n            -i ${reads[0]} \\\n            -I ${reads[1]} \\\n            -o ${sample}_R1.clean.fastq.gz \\\n            -O ${sample}_R2.clean.fastq.gz \\\n            --length_required ${processParams.clean_and_fastqc.length_required} \\\n            --adapter_fasta ${processParams.clean_and_fastqc.adapter_fasta} \\\n            -j ${sample}_fastp.json \\\n            -h ${sample}_fastp.html\n        \"\"\"\n}", "\nprocess splitIntervalList_gatk {\n\t\n\tinput:\n\ttuple path(reference_genome_fasta_forSplitIntervals), path(reference_genome_fasta_index_forSplitIntervals), path(reference_genome_fasta_dict_forSplitIntervals) from reference_genome_bundle_forSplitIntervals\n\tpath gatk_bundle_wgs_interval_list\n\n\toutput:\n\tpath \"splitIntervals/*-split.interval_list\" into split_intervals mode flatten\n\n\tscript:\n\t\"\"\"\n\tgatk SplitIntervals \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--reference \"${reference_genome_fasta_forSplitIntervals}\" \\\n\t--intervals \"${gatk_bundle_wgs_interval_list}\" \\\n\t--subdivision-mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION \\\n\t--extension -split.interval_list \\\n\t--scatter-count 20 \\\n\t--output splitIntervals\n\t\"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess rangeplot {\n    publishDir 'output', mode: 'copy'\n    input:\n        path \"lnglat.csv\"\n    output:\n        path \"range.pdf\"\n    script:\n        diameter = [\n            0,          \n            *range.collect{it*2}]\n        \"\"\"\n        gmt begin\n            gmt figure range pdf\n            gmt pscoast -Rd -JE125.75/39.02/${maprange}/20c -Gburlywood -Slightblue -A1000 \n            gmt plot lnglat.csv -Sa.2c -Wthicker,blue\n            for r in ${diameter}\n            do\n                gmt plot lnglat.csv -SE-\\$r -Wthin,firebrick\n            done\n        gmt end\n        \"\"\"\n}", "\nprocess reconstructBCRepertoireMiXCR {\n\n    publishDir params.clonotype_dir, pattern: '*.txt', mode: 'copy'\n\n    input:\n        val species_alias from params.species_alias\n        tuple sample, \"${sample}_trim_1.fastq\", \"${sample}_trim_2.fastq\" from trimmed_fastqs\n\n    output:\n        file \"${sample}_repertoire.txt\" into repertoire_reports\n        tuple sample, \"${sample}.clns\" into repertoire_clns\n\n    \"\"\"\n    /usr/share/mixcr/mixcr-3.0.13/mixcr analyze shotgun\\\n    --only-productive\\\n    --starting-material rna\\\n    --receptor-type bcr\\\n    --species $species_alias\\\n    --report ${sample}_repertoire.txt\\\n    ${sample}_trim_1.fastq ${sample}_trim_2.fastq\\\n    $sample\n    \"\"\"\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/FASTP__CLEAN_AND_FASTQC", "pblaney/mgp1000/splitIntervalList_gatk", "yonghah/geodata-pipeline/rangeplot", "szabogtamas/repertoireseq_container/reconstructBCRepertoireMiXCR"], "list_wf_names": ["yonghah/geodata-pipeline", "pblaney/mgp1000", "szabogtamas/repertoireseq_container", "vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK", "Salmon", "MultiQC"], "nb_own": 4, "list_own": ["vib-singlecell-nf", "pblaney", "nriddiford", "t-neumann"], "nb_wf": 3, "list_wf": ["vsn-pipelines", "mgp1000", "nf-core-lohcator", "salmon-nf"], "list_contrib": ["dependabot[bot]", "pblaney", "KrisDavie", "nriddiford", "ghuls", "dweemx", "cflerin", "t-neumann"], "nb_contrib": 8, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess mergeAndSortGvcfs_gatk {\n\ttag \"${sample_id}\"\n\t\n\tinput:\n\ttuple val(sample_id), path(gvcf_per_interval_raw), path(gvcf_per_interval_raw_index) from raw_gvcfs.groupTuple()\n\n\toutput:\n\tpath gvcf_merged_raw into merged_raw_gcvfs\n\tpath gvcf_merged_raw_index into merged_raw_gcvfs_indicies\n\n\tscript:\n\tgvcf_merged_raw = \"${sample_id}.g.vcf.gz\"\n\tgvcf_merged_raw_index = \"${gvcf_merged_raw}.tbi\"\n\t\"\"\"\n\tgatk SortVcf \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--TMP_DIR . \\\n\t--MAX_RECORDS_IN_RAM 4000000 \\\n\t${gvcf_per_interval_raw.collect { \"--INPUT $it \" }.join()} \\\n\t--OUTPUT \"${gvcf_merged_raw}\"\n\t\"\"\"\n}", "\nprocess PICARD__MERGE_SAM_FILES_AND_SORT {\n\n    container toolParams.container\n    label 'compute_resources__default','compute_resources__24hqueue'\n\n    input:\n        tuple val(sampleId),\n              path(bams)\n\n    output:\n        tuple val(sampleId),\n              path(\"${sampleId}.bwa.out.fixmate.merged.bam\")\n\n    script:\n                                                                                    \n                                            \n        \"\"\"\n        gatk MergeSamFiles \\\n            ${\"-I \"+bams.join(\" -I \")} \\\n            -O /dev/stdout \\\n        | gatk SortSam \\\n            -I /dev/stdin \\\n            -O ${sampleId}.bwa.out.fixmate.merged.bam \\\n            --SORT_ORDER queryname\n        \"\"\"\n}", "\nprocess salmon {\n\n\ttag { lane }\n\n    input:\n    set val(lane), file(reads) from fastqChannel\n    file index from indexChannel.first()\n\n    output:\n    file (\"${lane}_salmon/quant.sf\") into salmonChannel\n    file (\"${lane}_pseudo.bam\") into pseudoBamChannel\n\n    shell:\n\n    def single = reads instanceof Path\n\n    if (!single)\n\n      '''\n      salmon quant -i !{index} -l A -1 !{reads[0]} -2 !{reads[1]} -o !{lane}_salmon -p !{task.cpus} --validateMappings --no-version-check -z | samtools view -Sb -F 256 - > !{lane}_pseudo.bam\n\t    '''\n    else\n      '''\n      salmon quant -i !{index} -l A -r !{reads} -o !{lane}_salmon -p !{task.cpus} --validateMappings --no-version-check -z | samtools view -Sb -F 256 - > !{lane}_pseudo.bam\n\t    '''\n\n}"], "list_proc": ["pblaney/mgp1000/mergeAndSortGvcfs_gatk", "vib-singlecell-nf/vsn-pipelines/PICARD__MERGE_SAM_FILES_AND_SORT", "t-neumann/salmon-nf/salmon"], "list_wf_names": ["t-neumann/salmon-nf", "vib-singlecell-nf/vsn-pipelines", "pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["ntanh1999"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": ["ntanh1999"], "nb_contrib": 1, "codes": ["\nprocess TRIM_PE_TRIMMOMATIC{\n    tag \"$sample_id\"\n    publishDir \"$params.result/$sample_id/trimmomatic\", mode: 'copy'\n\n    input:\n        tuple val(sample_id), path(reads)\n        path adapter\n    \n    output:\n        tuple \\\n            val(sample_id), \\\n            path(\"${sample_id}_R1.fastq.gz\"), \\\n            path(\"${sample_id}_R2.fastq.gz\")\n        \n        tuple \\\n            val(sample_id), \\\n            path(\"${sample_id}_S1.fastq.gz\"), \\\n            path(\"${sample_id}_S2.fastq.gz\")\n\n    script:\n    \"\"\"\n    trimmomatic PE -threads $task.cpus \\\n        $reads \\\n        ${sample_id}_R1.fastq.gz ${sample_id}_S1.fastq.gz \\\n        ${sample_id}_R2.fastq.gz ${sample_id}_S2.fastq.gz \\\n        ILLUMINACLIP:$adapter:2:30:10:2:true \\\n        SLIDINGWINDOW:4:15 LEADING:3 TRAILING:3 MINLEN:36\n    \"\"\"\n}"], "list_proc": ["ntanh1999/nextflow/TRIM_PE_TRIMMOMATIC"], "list_wf_names": ["ntanh1999/nextflow"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["ntanh1999"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": ["ntanh1999"], "nb_contrib": 1, "codes": ["\nprocess ANNOTATE_PROKKA{\n    tag \"$sample_id\"\n    publishDir \"$params.result/$sample_id\", mode: 'copy'\n    input:\n        tuple val(sample_id), path(assembly)\n\n    output:\n        path \"prokka/${sample_id}.gff\", emit: gff\n        path 'prokka'\n    \n    script:\n    \"\"\"\n    prokka --force --cpus $task.cpus --addgenes --mincontiglen 200 \\\n    --prefix $sample_id --locus $sample_id --outdir prokka $assembly\n    \"\"\"\n}"], "list_proc": ["ntanh1999/nextflow/ANNOTATE_PROKKA"], "list_wf_names": ["ntanh1999/nextflow"]}, {"nb_reuse": 1, "tools": ["Roary"], "nb_own": 1, "list_own": ["ntanh1999"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": ["ntanh1999"], "nb_contrib": 1, "codes": ["\nprocess PANGENOME_ANALYSIS_ROARY{\n    publishDir \"$params.result\", mode: 'copy'\n    \n    input:\n        path gff\n    \n    output:\n        path \"roary\"\n\n    script:\n    \"\"\"\n    roary -v -z -p $task.cpus -f roary $gff\n    \"\"\"\n}"], "list_proc": ["ntanh1999/nextflow/PANGENOME_ANALYSIS_ROARY"], "list_wf_names": ["ntanh1999/nextflow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess TRIMMING {\n    publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"*/*.html\"\n    if( params.output_trimmed ){\n        publishDir \"${params.outdir}/trimmed_reads\", mode: 'copy', pattern: \"*val*.fq.gz\"}\n\n    input:\n        tuple val(pair_id), path(reads)\n        val trim_args\n\n    output:\n        tuple val(pair_id), path('*.fq.gz'),     emit: trimread\n        path \"*/*fastqc.html\" ,                  emit: fastqc\n\n    script:\n    def read_files = params.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    def read_pairing = params.single_end ? \"\" : \"--paired\"\n    \"\"\"\n    mkdir FASTQC_raw_reads FASTQC_trimmed_reads\n    fastqc -o FASTQC_raw_reads -t $task.cpus $read_files\n    trim_galore --cores $task.cpus --fastqc --gzip $trim_args $read_pairing $read_files\n    fastqc -o FASTQC_trimmed_reads -t $task.cpus *val*.fq.gz\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/TRIMMING"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["Tally"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess DEDUPE {\n    input:\n        tuple val(pair_id), path(reads)\n\n    output:\n        tuple val(pair_id), path('reads.unique.*.fq.gz'),     emit: dedupreads\n\n    script:\n    def read_in = params.single_end ? \"-i $reads \" : \"-i ${reads[0]} -j ${reads[1]}\"\n    def read_out = params.single_end ? \"-o reads.unique.1.fq.gz \" : \"-o reads.unique.1.fq.gz -p reads.unique.2.fq.gz\"\n    \"\"\"\n    tally $read_in $read_out --pair-by-offset --with-quality\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/DEDUPE"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["NGMLR"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess NGMALIGN {\n    input:\n        tuple val(pair_id), path(reads)\n        path reference\n\n    output:\n        tuple val(pair_id), path('align.ngm.bam'),                  emit: ngmbam\n\n    script:\n    def read_in = params.single_end ? \"-q $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    ngm -b -r $reference $read_in -o align.ngm.bam -t $task.cpus > ngm.log 2> ngm.err\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/NGMALIGN"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess COMPLEXITYFILTER {\n    input:\n        tuple val(pair_id), path(reads)\n        val trimargs\n\n    output:\n        tuple val(pair_id), path('good*fq.gz'),    emit: filterread\n\n    script:\n    def read_in = params.single_end ? \"-i $reads\" : \"-i ${reads[0]} -I ${reads[1]}\"\n    def read_out = params.single_end ? \"-o good.fq.gz\" : \"-o good.1.fq.gz -O good.2.fq.gz\"\n    \"\"\"\n    fastp -G -A -L -Q --low_complexity_filter $read_in $read_out\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/COMPLEXITYFILTER"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["SAMtools", "fastPHASE", "NGMLR"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess CYCLEASSEM {\n\n    input:\n        tuple val(pair_id), path(initial_contigs)\n        tuple val(pair_id), path(reads)\n        path reference\n        val fasta_command\n        val maxit\n\n    output:\n        tuple val(pair_id), path(\"${pair_id}.scaffolds.fa\"),           emit: cyclecontigs\n\n    script:\n    def ngm_in = params.single_end ? \"-q $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    filter_in = params.single_end ? \"-i reads/output.fq.gz\" : \"-i reads/output.1.fq.gz -I reads/output.2.fq.gz\"\n    filter_out = params.single_end ? \"-o reads/good.fq.gz\" : \"-o reads/good.1.fq.gz -O reads/good.2.fq.gz\"\n    def spades_in = params.single_end ? \"-s reads/good.fq.gz\" : \"-1 reads/good.1.fq.gz -2 reads/good.2.fq.gz\"\n    def bam_extract = params.single_end ? \"extract_bam_reads_se\" : \"extract_bam_reads\"\n    \"\"\"\n    source $projectDir/bin/functions.sh\n    i=0\n    contcount=10 \n    cycle_genome=$initial_contigs\n    mkdir -p reads\n\n    while ( [ \\\"\\$i\\\" -lt \\\"$maxit\\\" ] && [ \\\"\\$contcount\\\" -gt \\\"1\\\" ] ); do\n        let i=i+1 \n        mkdir run_\\$i/\n\n        #### Map reads against seed contigs to get mapped reads ####\n        ngm -b -i 0.99 -r \\$cycle_genome $ngm_in -o run_\\$i/output.bam -t $task.cpus > run_\\$i/ngm.log 2> run_\\$i/ngm.err\n        command_success=0\n        grep '(0 reads mapped' run_\\$i/ngm.err > /dev/null 2>1 || command_success=1\n        if [ \\\"\\$command_success\\\" -eq 0 ]; then\n          if [ \\\"\\$i\\\" -eq 1 ]; then\n            echo 'Mapping to cycle assembly failed. Using inital contigs.'\n            cp $initial_contigs ${pair_id}.final_scaffolds.fa\n          else\n            echo 'Subsequent cycle mapping failed. Using previous cycle.'\n            let i=i-1\n            cp run_\\$i/scaffolds.verified.fasta ${pair_id}.final_scaffolds.fa\n            break\n          fi\n        fi\n        $bam_extract run_\\$i/output $task.cpus\n        mv run_\\$i/*.fq.gz reads/\n        rm -f run_\\$i/*.bam run_\\$i/*.ngm run_\\$i/*.bt2\n\n        #### Do de novo assembly of plastid reads ####\n        fastp -G -A -L -Q --low_complexity_filter $filter_in $filter_out\n        spades.py --cov-cutoff 1 $spades_in -t $task.cpus -o run_\\$i/spades_assembly\n        if [ -s run_\\$i/spades_assembly/scaffolds.fasta ]; then\n          cp run_\\$i/spades_assembly/scaffolds.fasta run_\\$i/scaff.fa\n        else\n          if [ -s run_\\$i/spades_assembly/contigs.fasta ]; then\n            echo 'Warning: only contigs produced in spades assembly.'\n            cp run_\\$i/spades_assembly/contigs.fasta run_\\$i/scaff.fa\n          else\n            echo 'Error in spades cycle assembly. Low quality data likely.'\n            exit 1\n          fi\n        fi\n\n        #### Only keep scaffolds that blast back to reference genome to discard junk ####\n        $fasta_command -E 1e-10 -T $task.cpus -m 8 run_\\$i/scaff.fa $reference | \\\n            sed 's/_/ /g' |sed 's/ /_/g' |awk '{print \\$1}' |sort |uniq > run_\\$i/spades_assembly/scaffolds.blast.list\n        if [ ! -s run_\\$i/spades_assembly/scaffolds.blast.list ]; then\n          if [ \\\"\\$i\\\" -eq 1 ]; then\n            echo 'No hits found. Using inital contigs.'\n            cp $initial_contigs run_\\$i/scaff.fa\n          else\n            echo 'No hits found. Using previous cycle.'\n            let i=i-1\n            cp run_\\$i/scaffolds.verified.fasta run_\\$i/scaff.fa\n          fi\n          $fasta_command -E 1e-10 -T $task.cpus -m 8 run_\\$i/scaff.fa $reference | \\\n            sed 's/_/ /g' |sed 's/ /_/g' |awk '{print \\$1}' |sort |uniq > run_\\$i/spades_assembly/scaffolds.blast.list\n          extractBlastedScaff run_\\$i/spades_assembly/scaffolds.blast.list run_\\$i/scaff.fa run_\\$i/scaffolds.verified.fasta T\n          samtools faidx run_\\$i/scaffolds.verified.fasta\n          break\n        fi\n        extractBlastedScaff run_\\$i/spades_assembly/scaffolds.blast.list run_\\$i/scaff.fa run_\\$i/scaffolds.verified.fasta T\n\n        #### Check summary statistics of new plastid assembly ####\n        contcount=\\$(grep -c \\\">\\\" run_\\$i/scaffolds.verified.fasta)\n        samtools faidx run_\\$i/scaffolds.verified.fasta\n        cycle_genome=run_\\$i/scaffolds.verified.fasta\n        rm -f run_\\$i/*fq* run_\\$i/*fa run_\\$i/*ngm reads/*fq.gz\n    done\n    if [ \\\"\\$i\\\" -eq \\\"$maxit\\\" ] || [ \\\"\\$contcount\\\" -eq \\\"1\\\" ]; then\n        cp run_\\$i/scaffolds.verified.fasta ${pair_id}.scaffolds.fa\n    fi\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/CYCLEASSEM"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["SAMtools", "seqtk"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess ORIENT {\n\n    input:\n        tuple val(pair_id), path(contigs)\n        path(reference)\n\n    output:\n        tuple val(pair_id), path(\"${pair_id}.oriented.fa\"),           emit: oriented\n\n    script:\n    \"\"\"\n    source $projectDir/bin/functions.sh\n    cp $reference reference.fa\n    sed -i 's/_length.*//g' $contigs\n    # Coordinates start from the end of the contig if they align on negative strand so avoid that by adding revcomp of all contigs\n    seqtk seq -r $contigs > scaffolds.rev.fa\n    sed -i 's/>/>R/g' scaffolds.rev.fa\n    cat scaffolds.rev.fa >> $contigs\n    fasta36 -E 1E-10 -T $task.cpus -m 8 $reference $contigs > gene_search.txt 2> /dev/null\n    awk '(\\$8>\\$7) && (\\$10>\\$9) {print \\$2}' gene_search.txt |sort |uniq > gene_search.stranded.txt\n    samtools faidx $contigs\n    extract_seq gene_search.stranded.txt $contigs ${pair_id}.oriented.fa F    \n    sed -i 's/-/__/g' ${pair_id}.oriented.fa\n    sed -i 's/:/___/g' ${pair_id}.oriented.fa\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/ORIENT"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["SAMtools", "seqtk"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess EXTRACTEXONS {\n    publishDir \"$params.outdir/exons/sequences/\", mode: 'copy'\n\n    input:\n        tuple val(pair_id), path(contigs)\n        path(exons)\n\n    output:\n        tuple val(pair_id), path(\"${pair_id}.fa\"),           emit: exonseqs\n\n    script:\n    \"\"\"\n    source $projectDir/bin/functions.sh\n    cp $exons exons.fa\n    # Extract sequences that are hits for exons\n    sed -i 's/_length.*//g' $contigs\n    # Coordinates start from the end of the contig if they align on negative strand so avoid that by adding revcomp of all contigs\n    seqtk seq -r $contigs > scaffolds.rev.fa\n    sed -i 's/>/>R/g' scaffolds.rev.fa\n    cat scaffolds.rev.fa >> $contigs\n    fasta36 -E 1E-10 -T $task.cpus -m 8 $exons $contigs > gene_search.txt 2> /dev/null\n    awk '(\\$8>\\$7) && (\\$10>\\$9) {print \\$0}' gene_search.txt > gene_search.stranded.txt\n    # Only except hits that span >80% of the exon\n    samtools faidx exons.fa\n    $projectDir/bin/mergeBlastHits.py gene_search.stranded.txt gene_search.stranded_merge.txt flanking_positive 50\n    join <(awk '{printf \\\"%s %s:%s-%s %s\\\\n\\\",\\$1,\\$2,\\$9,\\$10,\\$8-\\$7}' gene_search.stranded_merge.txt |sort -k1,1 |uniq) <(awk '{print \\$1,\\$2}' exons.fa.fai |sort -k1,1) \\\n        |awk '((0.8*\\$4) < \\$3) {print \\$2}' |uniq > gene_search.filtered.txt\n    extract_seq gene_search.filtered.txt $contigs ${pair_id}.fa F    \n    sed -i 's/-/__/g' ${pair_id}.fa\n    sed -i 's/:/___/g' ${pair_id}.fa\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/EXTRACTEXONS"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ntoda03"], "nb_wf": 1, "list_wf": ["cycleassembler"], "list_contrib": ["ntoda03"], "nb_contrib": 1, "codes": ["\nprocess FINDEXONS {\n    publishDir \"$params.outdir/exons/identified/\", mode: 'copy'\n\n    input:\n        tuple val(pair_id), path(seqs)\n        path(exons)\n\n    output:\n        tuple val(pair_id), path(\"$pair_id/*.fa\"),           emit: exonsbra\n\n    script:\n    \"\"\"\n    source $projectDir/bin/functions.sh\n    # Get the best reciprical alignment between exons and extracted sequences to only have 1 per exons\n    getBRA $seqs $exons dna_dna\n    mkdir -p $pair_id/\n    while IFS=' ' read col1 col2\n    do\n        echo \\$col2\n      samtools faidx $seqs \\$col1 > $pair_id/\\${col2}.fa\n      sed -i 's/___/:/g' $pair_id/\\${col2}.fa\n      sed -i 's/__/-/g' $pair_id/\\${col2}.fa\n    done < ${seqs}.BRA.dna_dna.txt\n    \"\"\"\n}"], "list_proc": ["ntoda03/cycleassembler/FINDEXONS"], "list_wf_names": ["ntoda03/cycleassembler"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess generate_premap_index{\n\n\ttag \"$smrna_fasta\"\n\n    \tinput:\n    \tpath(smrna_fasta) from ch_smrna_fasta\n\n    \toutput:\n\n    \tpath(\"${smrna_fasta.simpleName}.*.bt2\") into ch_bt2_index\n\n    \tscript:\n    \t\"\"\"\n    \tbowtie2-build --threads $task.cpus $smrna_fasta ${smrna_fasta.simpleName}\n    \t\"\"\"\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/generate_premap_index"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess star_index{\n\t\t\t\t\n\ttag \"$fasta\"\n    \n   \tpublishDir path: \"${params.outdir}/STAR_index\", mode: 'copy'\n\n\n        input:\n        path(fasta) from ch_fasta_star\n        path(gtf) from ch_gtf_star\n\n        output:\n        \n        path(\"index_star_ch20\") into ch_star_index\n\n\tscript:\n    \t\"\"\"\n        mkdir -p index_star_ch20/\n    \t\t\n    \t\tSTAR \\\\\n        --runMode genomeGenerate \\\\\n        --genomeDir index_star_ch20 \\\\\n        --genomeFastaFiles $fasta \\\\\n        --sjdbGTFfile $gtf \\\\\n        --genomeSAindexNbases 10 \\\\\n        --outFileNamePrefix Hsapiens_chr20 \\\\\n        --runThreadN 4\n        \"\"\"\n\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/star_index"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": [" process cutadapt {\n\n        tag \"$key\"\n        publishDir \"${params.outdir}/cutadapt\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(key), file(reads) from trimming_reads\n\n        output:\n        tuple val(key), path(\"${key}.trimmed.fq\") into mapping_reads\n        path \"*.log\" into ch_cutadapt_mqc\n\n        script:\n        \"\"\"\n        ln -s $reads ${key}.fq\n        cutadapt -j $task.cpus -a ${params.adapter} -m 12 -o ${key}.trimmed.fq ${key}.fq > ${key}_cutadapt.log\n        \"\"\"\n    }"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/cutadapt"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess align {\n    tag \"$name\"\n    publishDir \"${params.outdir}/mapped\", mode: 'copy'\n\n    input:\n    tuple val(name), path(reads) from ch_unmapped\n    path(index) from ch_star_index.collect()\n\n    output:\n    tuple val(name), path(\"${name}.Aligned.sortedByCoord.out.bam\"), path(\"${name}.Aligned.sortedByCoord.out.bam.bai\") into ch_aligned, ch_aligned_preseq\n    path \"*.Log.final.out\" into ch_align_mqc, ch_align_qc\n\n    script:\n    clip_args = \"--outFilterMultimapNmax 1 \\\n                --outFilterMultimapScoreRange 1 \\\n                --outSAMattributes All \\\n                --alignSJoverhangMin 8 \\\n                --alignSJDBoverhangMin 1 \\\n                --outFilterType BySJout \\\n                --alignIntronMin 20 \\\n                --alignIntronMax 1000000 \\\n                --outFilterScoreMin 10  \\\n                --alignEndsType Extend5pOfRead1 \\\n                --twopassMode Basic \\\n                --outSAMtype BAM Unsorted\"\n    \"\"\"\n    STAR \\\\\n        --runThreadN $task.cpus \\\\\n        --runMode alignReads \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads --readFilesCommand gunzip -c \\\\\n        --outFileNamePrefix ${name}. $clip_args\n    samtools sort -@ $task.cpus -o ${name}.Aligned.sortedByCoord.out.bam ${name}.Aligned.out.bam\n    samtools index -@ $task.cpus ${name}.Aligned.sortedByCoord.out.bam\n    \"\"\"\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/align"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["preseq"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess preseq {\n    tag \"$name\"\n    publishDir \"${params.outdir}/preseq\", mode: 'copy'\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_aligned_preseq\n\n    output:\n    path '*.ccurve.txt' into ch_preseq_mqc\n    path '*.log'\n\n    script:\n    \"\"\"\n    preseq lc_extrap \\\\\n        -output ${name}.ccurve.txt \\\\\n        -verbose \\\\\n        -bam \\\\\n        -seed 42 \\\\\n        $bam\n    cp .command.err ${name}.command.log\n    \"\"\"\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/preseq"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess dedup {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/dedup\", mode: 'copy'\n\n        input:\n        tuple val(name), path(bam), path(bai) from ch_aligned\n\n        output:\n        tuple val(name), path(\"${name}.dedup.bam\"), path(\"${name}.dedup.bam.bai\") into ch_dedup, ch_dedup_pureclip, ch_dedup_rseqc\n                                                     \n\n        script:\n        \"\"\"\n        samtools rmdup -S $bam ${name}.dedup.bam\n        samtools index -@ $task.cpus ${name}.dedup.bam\n        \"\"\"\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/dedup"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess get_crosslinks {\n    tag \"$name\"\n    publishDir \"${params.outdir}/xlinks\", mode: 'copy'\n\n    input:\n    tuple val(name), path(bam), path(bai) from ch_dedup\n    path(fai) from ch_fai_crosslinks.collect()\n\n    output:\n    tuple val(name), path(\"${name}.xl.bed.gz\") into ch_xlinks_icount, ch_xlinks_paraclu, ch_xlinks_piranha\n    tuple val(name), path(\"${name}.xl.bedgraph.gz\") into ch_xlinks_bedgraphs\n    path \"*.xl.bed.gz\" into ch_xlinks_qc\n\n    script:\n    \"\"\"\n    bedtools bamtobed -i $bam > dedup.bed\n    bedtools shift -m 1 -p -1 -i dedup.bed -g $fai > shifted.bed\n    bedtools genomecov -dz -strand + -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"+\"}' > pos.bed\n    bedtools genomecov -dz -strand - -5 -i shifted.bed -g $fai | awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$2+1, \".\", \\$3, \"-\"}' > neg.bed\n    cat pos.bed neg.bed | sort -k1,1 -k2,2n | pigz > ${name}.xl.bed.gz\n    zcat ${name}.xl.bed.gz | awk '{OFS = \"\\t\"}{if (\\$6 == \"+\") {print \\$1, \\$2, \\$3, \\$5} else {print \\$1, \\$2, \\$3, -\\$5}}' | pigz > ${name}.xl.bedgraph.gz\n    \"\"\"\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/get_crosslinks"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 1, "tools": ["PiRaNhA"], "nb_own": 1, "list_own": ["oisinmccaffrey"], "nb_wf": 1, "list_wf": ["clipseq.nextflow"], "list_contrib": ["oisinmccaffrey"], "nb_contrib": 1, "codes": ["\nprocess piranha_peak_call {\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/piranha\", mode: 'copy'\n\n        input:\n        tuple val(name), path(xlinks) from ch_xlinks_piranha\n\n        output:\n        tuple val(name), path(\"${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\") into ch_peaks_piranha\n        path \"*.peaks.bed.gz\" into ch_piranha_qc\n\n        script:\n        bin_size_both = 3\n        cluster_dist = 3\n        \"\"\"\n        pigz -d -c $xlinks | \\\\\n        awk '{OFS=\"\\t\"}{for(i=0;i<\\$5;i++) print }' \\\\\n        > expanded.bed\n        Piranha \\\\\n            expanded.bed \\\\\n            -s \\\\\n            -b $bin_size_both \\\\\n            -u $cluster_dist \\\\\n            -o paraclu.bed\n        awk '{OFS=\"\\t\"}{print \\$1, \\$2, \\$3, \".\", \\$5, \\$6}' paraclu.bed | \\\\\n        pigz > ${name}.${bin_size_both}nt_${cluster_dist}nt.peaks.bed.gz\n        \"\"\"\n}"], "list_proc": ["oisinmccaffrey/clipseq.nextflow/piranha_peak_call"], "list_wf_names": ["oisinmccaffrey/clipseq.nextflow"]}, {"nb_reuse": 2, "tools": ["TagDust", "fastPHASE"], "nb_own": 2, "list_own": ["oist", "palfalvi"], "nb_wf": 2, "list_wf": ["nanoporeseq", "plessy_CAGEscan_Nextflow"], "list_contrib": ["charles-plessy", "palfalvi"], "nb_contrib": 2, "codes": ["process fastp {\ntag \"$sample_id\"\n\nlabel 'small_plus'\n\npublishDir \"${params.out}/fastp_qc\", mode: 'copy', pattern: '*.json'\n\nconda \"$baseDir/conda-envs/fastp-env.yaml\"\n                                         \n\ninput:\n  tuple val(sample_id), file(reads)\n\noutput:\n  tuple val(sample_id), file(\"trim_*\"), optional: true, emit: trimmed\n  path \"*.json\", emit: json\n\nscript:\n\n  def readfiles  = params.single_end    ? \"-i $reads\"      : \"-i ${reads[0]} -I ${reads[1]}\"\n  def outfiles   = params.single_end    ? \"-o trim_$reads\" : \"-o trim_${reads[0]} -O trim_${reads[1]}\"\n  def outff      = params.skip_trim     ? \"\"               : \"$outfiles\"\n  def adapter    = params.single_end    ? \"\"               : \"--detect_adapter_for_pe\"\n\n  \"\"\"\n  fastp \\\n  -w ${task.cpus} \\\n  $readfiles \\\n  $outff \\\n  $adapter \\\n  --overrepresentation_analysis \\\n  --json ${sample_id}_fastp.json\n  \"\"\"\n}", "\nprocess TagDust2 {\n\n    container = 'cagescan/tagdust2:2020071401'\n\n    publishDir \"${params.outdir}/tagdust2\",\n        mode: \"copy\", overwrite: true\n\n    input:\n        tuple val(sampleName), path(reads1), path(reads2)\n        path(tagdust_arch)\n        path(tagdust_ref)\n\n    output:\n                                                                                               \n        path \"*_BC_*_READ1.fq\", emit: demultiplexedFastqFilesR1\n        path \"*_BC_*_READ2.fq\", emit: demultiplexedFastqFilesR2\n        path \"*_logfile.txt\",   emit: TagDust2LogFile\n\n    script:\n\n                                                                           \n    command = \"\"\"\n        tagdust                            \\\n            -show_finger_seq               \\\n            -ref $tagdust_ref              \\\n            -arch $tagdust_arch            \\\n            -o ${sampleName}               \\\n            ${reads1}                      \\\n            ${reads2}                      \\\n         \"\"\"\n\n    if (params.verbose){\n        println (\"[MODULE] TagDust2 command: \" + command)\n    }\n\n    \"\"\"\n    ${command}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/fastp", "oist/plessy_CAGEscan_Nextflow/TagDust2"], "list_wf_names": ["palfalvi/nanoporeseq", "oist/plessy_CAGEscan_Nextflow"]}, {"nb_reuse": 2, "tools": ["Minimap2", "TagDust"], "nb_own": 2, "list_own": ["oist", "palfalvi"], "nb_wf": 2, "list_wf": ["nanoporeseq", "plessy_CAGEscan_Nextflow"], "list_contrib": ["charles-plessy", "palfalvi"], "nb_contrib": 2, "codes": ["process minimap2 {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/minimap-env.yaml\"\n\n                                                    \n\n  input:\n    path fastq\n    path assembly\n\n  output:\n    path \"*.paf\", emit: map\n\n  script:\n    \"\"\"\n    minimap2 -x map-ont -t ${task.cpus} $assembly $fastq > ${assembly.simpleName}-${fastq.simpleName}.paf\n    \"\"\"\n}", "\nprocess TagDust2_cutAdapt {\n    container = 'cagescan/tagdust2:2020071401'\n\n    publishDir \"${params.outdir}/tagdust2\",\n        mode: \"copy\", overwrite: true\n\n    input:\n        tuple val(sampleName), path(reads1), path(reads2)\n        val(tagdust_arch)\n\n    output:\n        path \"*_cut_*READ1.fq\", emit: fastqR1\n        path \"*_cut_*READ2.fq\", emit: fastqR2\n        path \"*_logfile.txt\", emit: logFile\n\n    script:\n\n    command = \"\"\"\n        tagdust                      \\\n            -arch $tagdust_arch      \\\n            -o ${sampleName}_cut_    \\\n            ${reads1}                \\\n            ${reads2}                \\\n         \"\"\"\n\n    if (params.verbose){\n        println (\"[MODULE] TagDust2_cutAdapt command: \" + command)\n    }\n\n    \"\"\"\n    ${command}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/minimap2", "oist/plessy_CAGEscan_Nextflow/TagDust2_cutAdapt"], "list_wf_names": ["palfalvi/nanoporeseq", "oist/plessy_CAGEscan_Nextflow"]}, {"nb_reuse": 1, "tools": ["TagDust"], "nb_own": 1, "list_own": ["oist"], "nb_wf": 1, "list_wf": ["plessy_Nextflow_SpliceLeaderCAGE"], "list_contrib": ["charles-plessy"], "nb_contrib": 1, "codes": ["\nprocess TagDust2 {\n\n    container = 'cagescan/tagdust2:2020071401'\n\n    publishDir \"${params.outdir}/tagdust2\",\n        mode: \"copy\", overwrite: true\n\n    input:\n        tuple val(sampleName), path(reads1), path(reads2)\n        path(tagdust_arch)\n        path(tagdust_ref)\n\n    output:\n                                                                                               \n        path \"*_BC_*_READ1.fq\", emit: demultiplexedFastqFilesR1\n        path \"*_BC_*_READ2.fq\", emit: demultiplexedFastqFilesR2\n        path \"*_logfile.txt\",   emit: TagDust2LogFile\n\n    script:\n\n                                                                           \n    command = \"\"\"\n        tagdust                            \\\n            -show_finger_seq               \\\n            -ref $tagdust_ref              \\\n            -arch $tagdust_arch            \\\n            -o ${sampleName}               \\\n            ${reads1}                      \\\n            ${reads2}                      \\\n         \"\"\"\n\n    if (params.verbose){\n        println (\"[MODULE] TagDust2 command: \" + command)\n    }\n\n    \"\"\"\n    ${command}\n    \"\"\"\n}"], "list_proc": ["oist/plessy_Nextflow_SpliceLeaderCAGE/TagDust2"], "list_wf_names": ["oist/plessy_Nextflow_SpliceLeaderCAGE"]}, {"nb_reuse": 1, "tools": ["TagDust"], "nb_own": 1, "list_own": ["oist"], "nb_wf": 1, "list_wf": ["plessy_Nextflow_SpliceLeaderCAGE"], "list_contrib": ["charles-plessy"], "nb_contrib": 1, "codes": ["\nprocess TagDust2_cutAdapt {\n    container = 'cagescan/tagdust2:2020071401'\n\n    publishDir \"${params.outdir}/tagdust2\",\n        mode: \"copy\", overwrite: true\n\n    input:\n        tuple val(sampleName), path(reads1), path(reads2)\n        val(tagdust_arch)\n\n    output:\n        path \"*_cut_*READ1.fq\", emit: fastqR1\n        path \"*_cut_*READ2.fq\", emit: fastqR2\n        path \"*_logfile.txt\", emit: logFile\n\n    script:\n\n    command = \"\"\"\n        tagdust                      \\\n            -arch $tagdust_arch      \\\n            -o ${sampleName}_cut_    \\\n            ${reads1}                \\\n            ${reads2}                \\\n         \"\"\"\n\n    if (params.verbose){\n        println (\"[MODULE] TagDust2_cutAdapt command: \" + command)\n    }\n\n    \"\"\"\n    ${command}\n    \"\"\"\n}"], "list_proc": ["oist/plessy_Nextflow_SpliceLeaderCAGE/TagDust2_cutAdapt"], "list_wf_names": ["oist/plessy_Nextflow_SpliceLeaderCAGE"]}, {"nb_reuse": 1, "tools": ["TagDust"], "nb_own": 1, "list_own": ["oist"], "nb_wf": 1, "list_wf": ["plessy_Nextflow_SpliceLeaderCAGE"], "list_contrib": ["charles-plessy"], "nb_contrib": 1, "codes": ["\nprocess TagDust2_filter_ref_SE {\n                                              \n                                                         \n    container = 'cagescan/tagdust2:2020071401'\n\n    publishDir \"${params.outdir}/tagdust2\",\n        mode: \"copy\", overwrite: true\n\n    input:\n        path(reads)\n        path(tagdust_ref)\n\n    output:\n        path \"*.fq\", emit: demultiplexedFastqFile\n        path \"*_logfile.txt\",   emit: TagDust2LogFile\n\n    script:\n\n    command = \"\"\"\n        tagdust                            \\\n            -ref $tagdust_ref              \\\n            -arch trivial.arch             \\\n            -o ${reads}                    \\\n            ${reads}                       \\\n         \"\"\"\n\n    if (params.verbose){\n        println (\"[MODULE] TagDust2_filter_ref_SE command: \" + command)\n    }\n\n    \"\"\"\n    printf 'tagdust -1 R:N\\n' > trivial.arch\n    ${command}\n    \"\"\"\n}"], "list_proc": ["oist/plessy_Nextflow_SpliceLeaderCAGE/TagDust2_filter_ref_SE"], "list_wf_names": ["oist/plessy_Nextflow_SpliceLeaderCAGE"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["oist"], "nb_wf": 1, "list_wf": ["plessy_draft_nf-busco"], "list_contrib": ["charles-plessy"], "nb_contrib": 1, "codes": ["\nprocess BUSCO {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::busco=5.2.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/busco:5.2.1--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/busco:5.2.1--pyhdfd78af_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta), path(augustus_config), val(species)\n    val(lineage)\n\n    output:\n    tuple val(meta), path(\"${meta.id}/run_*/full_table.tsv\"),    emit: tsv\n    tuple val(meta), path(\"${meta.id}/run_*/short_summary.txt\"), emit: txt\n    path \"*.version.txt\",                                        emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (lineage) options.args += \" --lineage_dataset $lineage\"\n    \"\"\"\n    if [ -e $augustus_config ]\n    then\n        # Copy the AUGUSTUS config directory as BUSO needs write access\n        cp -rL $augustus_config ${augustus_config}_copy && export AUGUSTUS_CONFIG_PATH=${augustus_config}_copy\n    else\n        # Copy the image's AUGUSTUS config directory if it was not provided to the module\n        cp -a /usr/local/config augustus_config && export AUGUSTUS_CONFIG_PATH='augustus_config'\n    fi\n    busco \\\\\n        $options.args \\\\\n        --augustus \\\\\n        --augustus_species $species \\\\\n        --mode genome \\\\\n        --cpu $task.cpus \\\\\n        --in  $fasta \\\\\n        --out $meta.id\n\n    echo \\$(busco --version 2>&1) | sed 's/^BUSCO //' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["oist/plessy_draft_nf-busco/BUSCO"], "list_wf_names": ["oist/plessy_draft_nf-busco"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["olavurmortensen"], "nb_wf": 1, "list_wf": ["demuxlink"], "list_contrib": ["olavurmortensen"], "nb_contrib": 1, "codes": ["\nprocess fastqc_analysis {\n    memory = \"1 GB\"\n    cpus = 4\n\n    publishDir \"$outdir/fastqc/$sample\", mode: 'copy', pattern: '{*.zip,*.html}',\n        saveAs: {filename -> filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"}\n    publishDir \"$outdir/fastqc/$sample\", mode: 'copy', pattern: '.command.log',\n        saveAs: {filename -> 'fastqc.log'}\n\n    input:\n    set sample, file(fastqs) from fastq_qc_ch\n\n    output:\n    set sample, file('*.{zip,html}') into fastqc_report_ch\n    set sample, file('.command.log') into fastqc_stdout_ch\n\n    script:\n    fastq_list = (fastqs as List).join(' ')\n    \"\"\"\n    # We unset the DISPLAY variable to avoid having FastQC try to open the GUI.\n    unset DISPLAY\n    mkdir tmp\n    fastqc -q --dir tmp --threads ${task.cpus} --outdir . $fastq_list\n    \"\"\"\n}"], "list_proc": ["olavurmortensen/demuxlink/fastqc_analysis"], "list_wf_names": ["olavurmortensen/demuxlink"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["olavurmortensen"], "nb_wf": 1, "list_wf": ["demuxlink"], "list_contrib": ["olavurmortensen"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir \"$outdir/multiqc\", mode: 'copy', overwrite: true\n\n    input:\n    val status from status_ch\n\n    output:\n    file \"multiqc_report.html\" into multiqc_report_ch\n    file \"multiqc_data\" into multiqc_data_ch\n\n    script:\n    \"\"\"\n    multiqc -f $outdir --config ${params.multiqc_config}\n    \"\"\"\n}"], "list_proc": ["olavurmortensen/demuxlink/multiqc"], "list_wf_names": ["olavurmortensen/demuxlink"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["oskarvid"], "nb_wf": 1, "list_wf": ["nextflow-GermlineVarCall"], "list_contrib": ["oskarvid"], "nb_contrib": 1, "codes": ["\nprocess BwaMem {\n\ttag { sample_name }\n\tmaxForks = 1\n    \n    input:\n\tfile reads from tsv_ch1\n    file fasta_ref\n    file fasta_ref_fai\n    file fasta_ref_sa\n    file fasta_ref_bwt\n    file fasta_ref_ann\n    file fasta_ref_amb\n    file fasta_ref_pac\n\n    output:\n    file(\"bwamem.sam\") into BwaMem_output\n\n    script:\n    \"\"\"\n    bwa mem -t 2 $fasta_ref \\\n      -R '@RG\\\\tID:${reads[1]}\\\\tSM:${reads[0]}\\\\tLB:${reads[5]}\\\\tPL:${reads[6]}\\\\tPU:NotDefined' \\\n      -M ${reads[3]} ${reads[4]} > bwamem.sam\n    \"\"\"\n}"], "list_proc": ["oskarvid/nextflow-GermlineVarCall/BwaMem"], "list_wf_names": ["oskarvid/nextflow-GermlineVarCall"]}, {"nb_reuse": 1, "tools": ["snippy"], "nb_own": 1, "list_own": ["oxfordfun"], "nb_wf": 1, "list_wf": ["FunSnippy"], "list_contrib": ["oxfordfun"], "nb_contrib": 1, "codes": ["\nprocess snippy{\n    echo true\n    scratch true\n\n    publishDir \"${params.output_dir}/\", mode: \"copy\"\n\n    tag {dataset_id}\n\n    input:\n    set dataset_id, file(forward), file(reverse) from snippy_read_pairs\n    file ref\n\n    output:\n    file(\"${dataset_id}/snps.vcf\")\n    file(\"${dataset_id}/snps.consensus.fa\")\n    file(\"${dataset_id}/snps.log\")\n    file(\"${dataset_id}/snps.aligned.fa\")\n    file(\"${dataset_id}/snps.bam\")\n    file(\"${dataset_id}/snps.raw.vcf\")\n    file(\"${dataset_id}/snps.filt.vcf\")\n\n\n    \"\"\"\n    snippy --cpus ${params.cpus } \\\n           --outdir ${dataset_id} \\\n           --ref ${ref} \\\n           --R1 ${forward} \\\n           --R2 ${reverse} \n    \"\"\"\n}"], "list_proc": ["oxfordfun/FunSnippy/snippy"], "list_wf_names": ["oxfordfun/FunSnippy"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["SARS-CoV2_workflows"], "list_contrib": ["JezSw", "nick297", "bede"], "nb_contrib": 3, "codes": ["\nprocess FN4_upload {\n    tag { sampleName }\n    label 'fn4'\n\n    input:\n    tuple(val(sampleName),  file('consensus.fasta'), path(variant_definitions), path(reffasta), path(\"*\"))\n\n    script:\n    \"\"\"\n    mafft --auto \\\n        --thread 1 \\\n        --addfull 'consensus.fasta' \\\n        --keeplength $reffasta \\\n        > ${sampleName}_wuhan.fa\n\n    FN4ormater.py -i ${sampleName}_wuhan.fa -r MN908947.3 -s ${sampleName} -o ${sampleName}.fasta\n\n    oci os object put \\\n\t-bn ${params.bucketNameFN4} \\\n\t--force \\\n    --auth instance_principal \\\n\t--file ${sampleName}.fasta \\\n\t--metadata \"{\\\\\"sampleID\\\\\":\\\\\"$sampleName\\\\\"}\"\n\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/SARS-CoV2_workflows/FN4_upload"], "list_wf_names": ["oxfordmmm/SARS-CoV2_workflows"]}, {"nb_reuse": 1, "tools": ["eFetch Pmc", "QResearch"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["SARS-CoV2_workflows"], "list_contrib": ["JezSw", "nick297", "bede"], "nb_contrib": 3, "codes": ["\nprocess getGFF3 {\n    label 'bcftools'\n\n    output:\n    path \"MN908947.3.gff3\"\n\n    script:\n    \"\"\"\n    esearch -db nucleotide -query \"MN908947.3\" | efetch -format gb > MN908947.3.gb\n\n    cat MN908947.3.gb | gbk2gff3.py > MN908947.3.gff3\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/SARS-CoV2_workflows/getGFF3"], "list_wf_names": ["oxfordmmm/SARS-CoV2_workflows"]}, {"nb_reuse": 1, "tools": ["vcfR", "BCFtools"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["SARS-CoV2_workflows"], "list_contrib": ["JezSw", "nick297", "bede"], "nb_contrib": 3, "codes": ["\nprocess bcftools_csq {\n    tag { sampleName }\n    label 'bcftools'\n\n    publishDir \"${params.outdir}/analysis/bcftools/${params.prefix}\", mode: 'copy'\n\n    input:\n    tuple(val(sampleName), path('vcf'), path('reffasta'), path('GFF3'))\n\n    output:\n    tuple(val(sampleName), path(\"${sampleName}.vcf\")) optional true\n\n    script:\n    \"\"\"\n    bcftools csq \\\n\t-f reffasta \\\n\t-g GFF3 \\\n\tvcf \\\n\t-Ot -o ${sampleName}.vcf \\\n\t--force\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/SARS-CoV2_workflows/bcftools_csq"], "list_wf_names": ["oxfordmmm/SARS-CoV2_workflows"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess bam2fastq {\n       \n                        \n      \n\n    tag { bam_file.getBaseName() }\n\n    memory '5 GB'\n    \n    when:\n    is_ok == 'OK'    \n\n    input:\n    path(bam_file)\n    \n    output:\n    tuple val(\"${bam_file.getBaseName()}\"), path(\"${bam_file.getBaseName()}_1.fq.gz\"), path(\"${bam_file.getBaseName()}_2.fq.gz\"), emit: bam2fastq_fqs\n\n    script:\n    \"\"\"\n    samtools sort -n $bam_file -o ${bam_file.getBaseName()}.sorted.bam\n\n    bedtools bamtofastq -i ${bam_file.getBaseName()}.sorted.bam -fq ${bam_file.getBaseName()}_1.fq -fq2 ${bam_file.getBaseName()}_2.fq\n\n    rm ${bam_file.getBaseName()}.sorted.bam\n\n    gzip ${bam_file.getBaseName()}_1.fq || true\n    gzip ${bam_file.getBaseName()}_2.fq || true\n\n    printf 'OK'\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/bam2fastq"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess fastp {\n       \n                                                                             \n      \n     \n    tag { sample_name }\n \n    publishDir \"${params.output_dir}/$sample_name/raw_read_QC_reports\", mode: 'copy', pattern: '*.json'\n    publishDir \"${params.output_dir}/$sample_name/output_reads\", mode: 'copy', pattern: '*.fq.gz'                                         \n    publishDir \"${params.output_dir}/$sample_name\", mode: 'copy', pattern: '*.log'\n\n    memory '5 GB'\n\n    when:\n    run_fastp == 'pass'\n\n    input:\n    tuple val(sample_name), path(fq1), path(fq2), val(run_fastp)\n\t\t\n    output:\n    tuple val(sample_name), path(\"${sample_name}_cleaned_1.fq.gz\"), path(\"${sample_name}_cleaned_2.fq.gz\"), stdout, emit: fastp_fqs\n    path(\"${sample_name}_fastp.json\", emit: fastp_json)\n    path(\"${sample_name}.log\", emit: fastp_log)\n   \n    script:\n    clean_fq1  = \"${sample_name}_cleaned_1.fq.gz\"\n    clean_fq2  = \"${sample_name}_cleaned_2.fq.gz\"\n    fastp_json = \"${sample_name}_fastp.json\"\n    fastp_html = \"${sample_name}_fastp.html\"\n    error_log  = \"${sample_name}.log\"\n\t\n    \"\"\"\n    fastp -i $fq1 -I $fq2 -o ${clean_fq1} -O ${clean_fq2} -j ${fastp_json} -h ${fastp_html} --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20\n    \n    rm -rf ${fastp_html}\n\n    num_reads=\\$(jq '.summary.after_filtering.total_reads' ${fastp_json} | awk '{sum+=\\$0} END{print sum}')\n\n    if (( \\$num_reads > 100000 )); then printf \"\" >> ${error_log} && printf \"pass\"; else echo \"error: after fastp, sample did not have > 100k reads (it only contained \\$num_reads)\" >> ${error_log} && printf \"fail\"; fi\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/fastp"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess fastQC {\n       \n                        \n      \n\t\n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/raw_read_QC_reports\", mode: 'copy'\n\n    memory '5 GB'\n\n    input:\n    tuple val(sample_name), path(fq1), path(fq2), val(enough_reads)\n\t\n    output:\n    path(\"*\", emit: fastQC_all)\n\t\n    script:\n    \"\"\"\n    cat $fq1 $fq2 > ${sample_name}.fq.gz\n    fastqc ${sample_name}.fq.gz\n    rm ${sample_name}.fq.gz\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/fastQC"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess kraken2 {\n       \n                                                                                                  \n      \n\n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/speciation_reports_cleanedReads\", mode: 'copy', pattern: '*_kraken_report.*'\n    publishDir \"${params.output_dir}/$sample_name\", mode: 'copy', pattern: '*.log'\n       \n    cpus 8\n\n    memory '10 GB'\n\n    when:\n    enough_reads == 'pass'\n\n    input:\n    tuple val(sample_name), path(fq1), path(fq2), val(enough_reads)\n    path(database)\n\t\t\n    output:\n    tuple path(\"${sample_name}_kraken_report.txt\"), path(\"${sample_name}_kraken_report.json\"), emit: kraken2_report\n    tuple val(sample_name), path(\"${sample_name}_clean_exclNonBacteria_1.fq.gz\"), path(\"${sample_name}_clean_exclNonBacteria_2.fq.gz\"), stdout, emit: kraken2_fqs\n    path(\"${sample_name}.log\", emit: kraken2_log)\n\t\t\t\n    script:\n    kraken2_report = \"${sample_name}_kraken_report.txt\"\n    kraken2_json = \"${sample_name}_kraken_report.json\"\n    kraken2_read_classification = \"${sample_name}_read_classifications.txt\"\n    nonBac_depleted_reads_1 = \"${sample_name}_clean_exclNonBacteria_1.fq\"\n    nonBac_depleted_reads_2 = \"${sample_name}_clean_exclNonBacteria_2.fq\"\n    error_log = \"${sample_name}.log\"\n\t\n    \"\"\"\n    kraken2 --threads ${task.cpus} --db . --output ${kraken2_read_classification} --report ${kraken2_report} --paired $fq1 $fq2\n\n    perl ${baseDir}/bin/parse_kraken_report2.pl ${kraken2_report} ${kraken2_json} 0.5 5000\n\n    ${baseDir}/bin/extract_kraken_reads.py -k ${kraken2_read_classification} -r ${kraken2_report} -s $fq1 -s2 $fq2 -o ${nonBac_depleted_reads_1} -o2 ${nonBac_depleted_reads_2} --taxid 2 --include-children --fastq-output >/dev/null\n\n    gzip ${nonBac_depleted_reads_1}\n    gzip ${nonBac_depleted_reads_2}\n\n    rm -rf ${sample_name}_read_classifications.txt\n\n    run_mykrobe=\\$(jq '.Mykrobe' ${kraken2_json})\n\n    if [ \\$run_mykrobe == '\\\"true\\\"' ]; then printf \"\" >> ${error_log} && printf \"yes\"; else echo \"error: Kraken's top family hit either wasn't Mycobacteriaceae, or represented < 5000 reads in absolute terms or < 0.5% of the total\" >> ${error_log} && printf \"no\"; fi\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/kraken2"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Mykrobe", "BWA"], "nb_own": 2, "list_own": ["oxfordmmm", "palfalvi"], "nb_wf": 2, "list_wf": ["nanoporeseq", "preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun", "palfalvi"], "nb_contrib": 4, "codes": ["process bwa_mem_hic {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/bwa-samtools-env.yaml\"\n\n                                                                \n\n  input:\n    tuple val(sample_id), file(reads)\n    val num\n    path assembly\n    path index\n\n  output:\n    path \"*.bam\", emit: bam\n                                   \n\n  script:\n\n    def read = num == 0 ? \"${reads[0]}\" : \"${reads[1]}\"\n\n    \"\"\"\n    bwa mem -t ${task.cpus} ${assembly} $read | samtools view -@ ${task.cpus} -Sb - > ${assembly.simpleName}_${num}.bam\n\n    #samtools index ${assembly.simpleName}_${num}.bam\n    \"\"\"\n}", "\nprocess mykrobe {\n       \n                        \n      \n\n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/speciation_reports_cleanedReads\", mode: 'copy', pattern: '*_mykrobe_report.json'\n\n    cpus 8\n\n    memory '5 GB'\n\n    when:\n    run_mykrobe == 'yes'\n\t\n    input:\n    tuple val(sample_name), path(fq1), path(fq2), val(run_mykrobe)\n\t\t\n    output:\n    tuple val(sample_name), path(\"${sample_name}_mykrobe_report.json\"), stdout, emit: mykrobe_report\n\n    script:\n    mykrobe_report = \"${sample_name}_mykrobe_report.json\"\n\t\n    \"\"\"\n    mykrobe predict ${sample_name} tb --threads ${task.cpus} --format json --output ${mykrobe_report} -1 $fq1 $fq2\n    printf 'yes'\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/bwa_mem_hic", "oxfordmmm/preprocessing/mykrobe"], "list_wf_names": ["palfalvi/nanoporeseq", "oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess bowtie2 {\n       \n                        \n      \n\n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/output_reads\", mode: 'copy', pattern: '*.fq.gz', overwrite: 'true'\n\n    cpus 8\n\n    memory '5 GB'\n\n    when:\n    enough_myco_reads == 'yes'\n\n    input:\n    tuple val(sample_name), path(fq1), path(fq2), val(enough_myco_reads)\n    path(index)\n\n    output:\n    tuple val(sample_name), path(\"${sample_name}_cleaned_1.fq.gz\"), path(\"${sample_name}_cleaned_2.fq.gz\"), emit: bowtie2_fqs\n\n    script:\n    bam = \"${sample_name}.bam\"\n    humanfree_fq1 = \"${sample_name}_cleaned_1.fq\"\n    humanfree_fq2 = \"${sample_name}_cleaned_2.fq\"\n\t\n    \"\"\"\n    bowtie2 --very-sensitive -p ${task.cpus} -x ${index}/${params.bowtie_index_name} -1 $fq1 -2 $fq2 | samtools view -f 4 -Shb - > ${bam}\n    samtools fastq -1 ${humanfree_fq1} -2 ${humanfree_fq2} -s singleton.fq ${bam}\n\n    rm -rf ${bam}\n    rm -rf singleton.fq\n\n    gzip -f ${humanfree_fq1}\n    gzip -f ${humanfree_fq2}\n    \"\"\"\t\n}"], "list_proc": ["oxfordmmm/preprocessing/bowtie2"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess mapToContamFa {\n       \n                        \n      \n\n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/output_reads\", mode: 'copy', pattern: '*.fq.gz', overwrite: 'true'\n\n    cpus 8\n\n    memory '10 GB'\n\n    when:\n    does_fa_pass == 'pass'\n\n    input:\n    tuple val(sample_name), path(fq1), path(fq2)\n    tuple path(contam_fa), val(does_fa_pass)\n\t\t\t\n    output:\n    tuple val(sample_name), path(\"${sample_name}_cleaned_1.fq.gz\"), path(\"${sample_name}_cleaned_2.fq.gz\"), emit: reClassification_fqs\n\n    script:\n    bam = \"${sample_name}.bam\"\n    decontam_fq1 = \"${sample_name}_cleaned_1.fq\"\n    decontam_fq2 = \"${sample_name}_cleaned_2.fq\"\n\t\n    \"\"\"\n    bwa index ${contam_fa}\n    bwa mem -t ${task.cpus} -M ${contam_fa} ${fq1} ${fq2} | samtools view -f 4 -f 8 -Shb - > ${bam}\n\n    samtools fastq -1 ${decontam_fq1} -2 ${decontam_fq2} ${bam}\n    rm -rf ${bam}\n\n    gzip -f ${decontam_fq1}\n    gzip -f ${decontam_fq2}\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/mapToContamFa"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess reKraken {\n       \n                        \n      \n\n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/speciation_reports_cleanedAndUnmixedReads\", mode: 'copy', pattern: '*_kraken_report.*'\n\n    cpus 8\n\n    memory '10 GB'\n    \n    input:\n    tuple val(sample_name), path(fq1), path(fq2)\n    path(database)\n\t\t\n    output:\n    tuple path(\"${sample_name}_kraken_report.txt\"), path(\"${sample_name}_kraken_report.json\"), emit: reKraken_report\n\n    script:\n    kraken2_report = \"${sample_name}_kraken_report.txt\"\n    kraken2_json = \"${sample_name}_kraken_report.json\"\n    kraken2_read_classification = \"${sample_name}_read_classifications.txt\"\n    \n    \"\"\"\n    kraken2 --threads ${task.cpus} --db . --output ${kraken2_read_classification} --report ${kraken2_report} --paired $fq1 $fq2\n\n    perl ${baseDir}/bin/parse_kraken_report2.pl ${kraken2_report} ${kraken2_json} 0.5 5000\n    rm -rf ${sample_name}_read_classifications.txt\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/reKraken"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["Mykrobe"], "nb_own": 1, "list_own": ["oxfordmmm"], "nb_wf": 1, "list_wf": ["preprocessing"], "list_contrib": ["annacprice", "martinghunt", "oxfordfun"], "nb_contrib": 3, "codes": ["\nprocess reMykrobe {\n       \n                        \n      \n     \n    tag { sample_name }\n\n    publishDir \"${params.output_dir}/$sample_name/speciation_reports_cleanedAndUnmixedReads\", mode: 'copy', pattern: '*_mykrobe_report.json'\n\t\n    cpus 8\n\n    memory '5 GB'\n\n    input:\n    tuple val(sample_name), path(fq1), path(fq2)\n\t\t\n    output:\n    tuple val(sample_name), path(\"${sample_name}_mykrobe_report.json\"), emit: reMykrobe_report\n\n    script:\n    mykrobe_report = \"${sample_name}_mykrobe_report.json\"\n\t\n    \"\"\"\n    mykrobe predict ${sample_name} tb --threads ${task.cpus} --format json --output ${mykrobe_report} -1 $fq1 $fq2\n    \"\"\"\n}"], "list_proc": ["oxfordmmm/preprocessing/reMykrobe"], "list_wf_names": ["oxfordmmm/preprocessing"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process bam_merge {\n\n  conda \"$baseDir/conda-envs/samtools-env.yaml\"\n  label 'small_plus'\n                                                 \n\n                                                                \n\n  input:\n    path bams\n    val name\n\n  output:\n    path \"*.merged.bam\", emit: bam\n    path \"*.merged.bam.bai\", emit: baidx\n\n  script:\n    \"\"\"\n    samtools merge ${name}.merged.bam $bams\n\n    samtools index ${name}.merged.bam\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/bam_merge"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BCFtools", "FreeBayes"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process freebayes_call {\n  tag \"$contig_index\"\n  label 'large_mem'\n\n  conda \"$baseDir/conda-envs/freebayes-env.yaml\"\n\n                                                                \n\n  input:\n    path contig\n    val avg_depth\n    path bam\n    path baidx\n    val contig_index\n\n  output:\n    path \"*.bcf\"\n\n  script:\n    \"\"\"\n    samtools faidx ${contig}\n\n    coverage=`printf \"%.0f\" ${avg_depth}`\n\n    LEN=`wc -l ${contig}.fai | awk '{print \\$1}'`\n\n    for j in \\$(seq ${contig_index} 100 \\$LEN )\n    do\n      contig=`sed -n \\${j}p ${contig}.fai | awk '{print \\$1}'`\n      contig_no_pipe=`echo \\$contig | sed 's/|/_/g'`\n      end=`sed -n \\${j}p ${contig}.fai | awk '{print \\$2}'`\n\n      freebayes --bam $bam --region=\\$contig:1-\\$end --skip-coverage \\$((\\$coverage*12)) -f ${contig} | bcftools view --no-version -Ob > \\${contig_no_pipe}.bcf\n    done\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/freebayes_call"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process hisat2_align {\n\ttag \"$sample_id\"\n\tlabel 'small_plus'\n\tpublishDir \"${params.outdir}/hisat2\", mode: 'copy'\n\n  conda \"$baseDir/conda-envs/hisat-env.yaml\"\n\n  input:\n    path genome_idx\n    tuple val(sample_id), file(reads)\n  output:\n    path \"*.hisat.bam\", emit: bam\n\n  script:\n\t\tdef strandedness = params.orientation  ? \"--fr\" : \"--rf\"\n    \"\"\"\n    hisat2 \\\n\t\t--dta-cufflinks \\\n\t\t-p $task.cpus \\\n\t\t-x $genome_idx \\\n\t\t-1 ${reads[0]} -2 ${reads[1]} |\n\t\tsamtools sort -@ $task.cpus -O BAM - > ${sample_id}.hisat.bam\n\n\n\t\t\"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/hisat2_align"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process minimap2_sr {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/minimap-env.yaml\"\n\n                                                    \n\n  input:\n    tuple val(sample_id), file(reads)\n    path assembly\n\n  output:\n    path \"*.bam\", emit: bam\n    path \"*.bam.bai\", emit: baidx\n\n  script:\n    \"\"\"\n    minimap2 --secondary=no --MD -L -t $task.cpus -ax map-ont $assembly $reads | samtools sort -@ $task.cpus -O BAM - > ${assembly.simpleName}-${sample_id}.bam\n    samtools index ${assembly.simpleName}-${sample_id}.bam\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/minimap2_sr"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process arima_add_read_group {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/picard-env.yaml\"\n\n  input:\n    path bam\n\n  output:\n    path \"*.readgrouped.bam\", emit: bam\n\n  script:\n\n    \"\"\"\n    picard AddOrReplaceReadGroups \\\n      --INPUT $bam \\\n      --OUTPUT ${bam.simpleName}.readgrouped.bam \\\n      -ID ${bam.simpleName} \\\n      -LB ${bam.simpleName} \\\n      -SM ${bam.simpleName} \\\n      -PL ILLUMINA \\\n      -PU none\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/arima_add_read_group"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["CANU"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process canu {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/canu-env.yaml\"\n\n  publishDir \"${params.outdir}/canu\", mode: 'copy'\n\n  input:\n    path fastq\n    val genome_size\n\n  output:\n    path \"*.contigs.fasta\", emit: assembly\n    path \"*.unitigs.gfa\", emit: gfa\n\n  script:\n    \"\"\"\n    canu \\\n    -p  canu_assembly \\\n    -d canu_out \\\n    genomeSize=$genome_size \\\n    useGrid='false' \\\n    -nanopore $fastq\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/canu"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process arima_filter {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/samtools-env.yaml\"\n\n  input:\n    path bam\n\n  output:\n    path \"*filtered.bam\", emit: bam\n\n  script:\n\n    \"\"\"\n    samtools view -h $bam | perl $baseDir/scripts/filter_five_end.pl | samtools view -Sb - > ${bam.simpleName}.filtered.bam \n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/arima_filter"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["NextPolish"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process nextpolish {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/nextpolish-env.yaml\"\n\n                                                           \n\n  input:\n    path nextpolish\n    path assembly\n    path reads\n\n  output:\n    path \"*nextpolish.fasta\", emit: assembly\n\n  script:\n    \"\"\"\n    ls  $reads > sgs.fofn\n\n    echo -e \"task = best\\ngenome = ${assembly}\\nsgs_fofn = sgs.fofn\" > run.cfg\n\n    nextPolish run.cfg\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/nextpolish"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process picard_mark_duplicates {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/picard-env.yaml\"\n\n  input:\n    path bam\n\n  output:\n    path \"*.deduplicated.bam\", emit: bam\n    path \"*.metrics.txt\", emit: metrics\n\n  script:\n\n    \"\"\"\n    picard MarkDuplicates \\\n      --INPUT $bam \\\n      --OUTPUT ${bam.simpleName}.deduplicated.bam \\\n      --METRICS_FILE ${bam.simpleName}.metrics.txt \\\n      --ASSUME_SORTED true \\\n      --VALIDATION_STRINGENCY LENIENT \\\n      --REMOVE_DUPLICATES true\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/picard_mark_duplicates"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Mikado"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process mikado_prepare {\n  label \"long_job\"\n\n  conda \"$baseDir/conda-envs/mikado-env.yaml\"\n                                          \n\n                                                                            \n\n  input:\n    path genome\n    path('*')\n    path scoring\n    path junction\n\n  output:\n    path \"mikado_prepared.fasta\", emit: fasta\n    path \"mikado_prepared.gtf\", emit: gtf\n    path ('*'), emit: mikado\n\n  script:\n    def sh  = params.short_reads    ? \"${projectDir}/scripts/short_gtf.txt\"   : \"\"\n    def ont = params.ont_reads      ? \"${projectDir}/scripts/ont_gtf.txt\"     : \"\"\n    def pb  = params.pb_reads       ? \"${projectDir}/scripts/pb_gtf.txt\"      : \"\"\n    def pr  = !params.skip_abinitio ? \"${projectDir}/scripts/prot_gtf.txt\"    : \"\"\n\n    def protein  =   params.protein  ? \"-bt ${params.protein}\" : \"\"\n    def junc     =   params.short_reads  ? \"--junction ${junction}\": \"\"\n    \"\"\"\n    cat $sh $ont $pb $pr > file.txt\n    sed -e 's/ /\\t/g' file.txt > gtf_list.txt\n\n    mikado configure \\\n    --list gtf_list.txt \\\n    --reference $genome \\\n    --mode permissive \\\n    --scoring $scoring  \\\n    $junc \\\n    $protein \\\n    --threads $task.cpus \\\n    configuration.yaml\n\n    mikado prepare \\\n    --json-conf configuration.yaml\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/mikado_prepare"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Racon"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process racon {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/racon-env.yaml\"\n\n  publishDir \"${params.outdir}/racon\", mode: 'copy'\n\n  input:\n    path fastq\n    path overlap\n    path assembly\n\n  output:\n    path \"${fastq.simpleName}_racon.fasta\", emit: assembly\n\n  script:\n    \"\"\"\n    racon \\\n    --threads ${task.cpus} \\\n    -u \\\n    $fastq \\\n    $overlap \\\n    $assembly > ${fastq.simpleName}_racon.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/racon"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["LoReAn"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process lorean {\n\n  container = 'docker://lfaino/lorean'\n  publishDir \"${params.outdir}/lorean\", mode: 'copy'\n\n  input:\n    path genome\n    path protein_ref\n\n  output:\n    path \"*\", emit: annotation\n\n  script:\n\n    def prefix      = params.lorean_prefix   ? \"--prefix_gene ${params.prefix}\"       : \"\"\n    def stranded    = params.lorean_stranded ? \"--stranded\"                           : \"\"\n    def iproscan    = params.lorean_iproscan ? \"--interproscan\"                       : \"\"\n    def adapters    = params.lorean_adapters ? \"--adapter ${params.lorean_adapters}\"  : \"\"\n    def long_reads  = params.lorean_long     ? \"--long_reads ${params.lorean_long}\"   : \"\"\n    def short_reads = params.lorean_short    ? \"--short_reads ${params.lorean_short}\" : \"\"\n    def species     = params.lorean_species  ? \"--species ${params.lorean_species}\"   : \"--species Xx\"\n\n    \"\"\"\n    wget https://github.com/lfaino/LoReAn/raw/master/third_party/software/config.augustus.tar.gz && tar -zxvf config.augustus.tar.gz\n\n    wget https://github.com/lfaino/LoReAn/raw/master/third_party/software/RepeatMasker.Libraries.tar.gz && tar -xvzf RepeatMasker.Libraries.tar.gz\n\n    lorean \\\n    --threads $task.cpus \\\n    --minimap2 \\\n    --max_intron_length 10000 \\\n    -pr $protein_ref \\\n    $species \\\n    $long_reads \\\n    $short_reads \\\n    $prefix \\\n    $stranded \\\n    $iproscan \\\n    $adapters \\\n    $genome\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/lorean"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process flye {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/flye-env.yaml\"\n\n  publishDir \"${params.outdir}/flye\", mode: 'copy'\n\n  input:\n    path fastq\n    val genome_size\n\n  output:\n    path \"flye_out/assembly.fasta\", emit: assembly\n    path \"flye_out/assembly_graph.gfa\", emit: gfa\n\n  script:\n    \"\"\"\n    flye \\\n    --nano-raw ${fastq} \\\n    --threads ${task.cpus} \\\n    --genome_size ${genome_size} \\\n    --asm_coverage 50 \\\n    --out_dir flye_out\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/flye"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["StringTie"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process stringtie2 {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/stringtie2-env.yaml\"\n\n  publishDir \"${params.outdir}/transcript_predictions/\", mode: 'copy', pattern: '*.gtf'\n\n  input:\n    path genome\n    path bam\n    val extra\n\n  output:\n    path \"*.gtf\", emit: gtf\n\n  script:\n\n    \"\"\"\n    stringtie \\\n    -p $task.cpus \\\n    -o ${bam.simpleName}_stringtie2.gtf \\\n    $extra \\\n    $bam\n\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/stringtie2"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process minimap_rna {\n  tag \"$sample_id\"\n  label 'long_job'\n\n  conda \"$baseDir/conda-envs/minimap-env.yaml\"\n\n                                                    \n\n  input:\n    path genome\n    tuple val(sample_id), file(reads)\n\n  output:\n    path \"*.bam\", emit: bam\n\n  script:\n    \"\"\"\n    minimap2 -ax splice -uf -k14 -t ${task.cpus} $genome $reads | samtools sort -@ $task.cpus -O BAM - > ${sample_id}_minimap2.bam\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/minimap_rna"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["wtdbg2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process wtdbg {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/wtdbg-env.yaml\"\n\n  publishDir \"${params.outdir}/wtdbg\", mode: 'copy'\n\n  input:\n    path fastq\n    val genome_size\n\n  output:\n    path \"wtdbg2_assembly.ctg.fasta\", emit: assembly\n\n  script:\n    \"\"\"\n    wtdbg2 \\\n    -x ont \\\n    -g ${genome_size} \\\n    -t ${task.cpus} \\\n    -i $fastq \\\n    -fo wtdbg2_assembly\n\n    wtpoa-cns -t ${task.cpus} \\\n    -i wtdbg2_assembly.ctg.lay.gz \\\n    -fo wtdbg2_assembly.ctg.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/wtdbg"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Trinity", "Minimap2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process trinity_gg {\n\n  label \"long_job\"\n\n                                                \n  container \"peegee/nanoporeseq:latest\"\n\n  publishDir \"${params.outdir}/transcript_predictions/\", mode: 'copy', pattern: '*.gtf'\n\n  input:\n    path genome\n    path bam\n\n  output:\n    path \"*trinity.fasta\", emit: fasta\n    path \"*.gtf\", emit: gtf\n\n  script:\n\n    def strand = params.orientation ? \"--SS_lib_type FR\" : \"\"\n                                         \n    \"\"\"\n    Trinity \\\n    --genome_guided_bam $bam \\\n    --min_contig_length 100 \\\n    --genome_guided_max_intron 10000 \\\n    --max_memory ${task.memory.toGiga()}G \\\n    --CPU $task.cpus \\\n    --output trinity_gg \\\n    --full_cleanup \\\n    $strand\n\n    mv ./trinity_gg/Trinity-GG.fasta ./${bam.simpleName}_trinity.fasta\n\n    minimap2 -ax splice:hq --cs -uf $genome ${bam.simpleName}_trinity.fasta > ${bam.simpleName}_trinity.bam\n\n    bam2gtf.py ${bam.simpleName}_trinity.bam ${bam.simpleName}_trinity.gtf\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/trinity_gg"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process star_idx {\n  tag \"$genome\"\n  label 'small_plus'\n  cpus \"$params.cpus\"\n\n  conda \"$baseDir/conda-envs/star-env.yaml\"\n\n  input:\n    path genome\n  output:\n    path \"${genome.simpleName}_idx\"\n  script:\n    \"\"\"\n\t\tmkdir ${genome.simpleName}_idx\n    STAR --runMode genomeGenerate \\\n    --runThreadN ${task.cpus} \\\n    --genomeDir ${genome.simpleName}_idx \\\n    --genomeFastaFiles $genome\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/star_idx"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process softmasking {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/bedtools-env.yaml\"\n\n  publishDir \"${params.outdir}/soft_masking\", mode: 'copy'\n\n  input:\n    path genome\n    path tes\n\n  output:\n    path \"*_softmasked.fasta\", emit: masked\n\n  script:\n    \"\"\"\n    bedtools maskfasta -soft -fi $genome -bed $tes -fo ${genome.simpleName}_softmasked.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/softmasking"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Minimap2", "purge_dups"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process purge_dups {\n\n  label 'long_job'\n\n  container \"https://depot.galaxyproject.org/singularity/purge_dups:1.2.5--h5bf99c6_1\"\n\n  publishDir \"${params.outdir}/purge_dups\", mode: 'copy'\n\n  input:\n  path genome\n  path long_reads\n  val platform                                                 \n\n  output:\n    path \"purged.fa \", emit: purged\n    path \"hap.fa \", emit: haplo\n\n  script:\n    \"\"\"\n    minimap2 -t $task.cpus -x $platform -d ${genome.simpleName}.idx $genome\n\n    minimap2 -x $platform -t $task.cpus ${genome.simpleName}.idx $long_reads | gzip -c - > ${genome.simpleName}_${long_reads.simpleName}.paf.gz\n\n    minimap2 -xasm5 -DP -t $task.cpus $genome $genome | gzip -c - > ${genome.simpleName}_self.paf.gz\n\n    pbcstat ${genome.simpleName}_${long_reads.simpleName}.paf.gz\n\n    calcuts PB.stat > cutoffs 2>calcults.log\n\n    purge_dups -2 -T cutoffs -c PB.base.cov ${genome.simpleName}_self.paf.gz > dups.bed 2> purge_dups.log\n\n    get_seqs dups.bed $genome > purged.fa 2> hap.fa\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/purge_dups"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process bwa_index {\n\n  label 'small_job'\n\n  conda \"$baseDir/conda-envs/bwa-samtools-env.yaml\"\n\n                                                                \n\n  input:\n    path assembly\n    val options\n\n  output:\n    path \"${assembly}.*\", emit: index\n\n  script:\n    \"\"\"\n    bwa index $options ${assembly}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/bwa_index"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["KAT"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process kat {\n  label \"small_plus\"\n\n  conda \"$baseDir/conda-envs/kat-env.yaml\"\n\n  publishDir \"${params.outdir}/kat_plot\", mode: 'copy'\n\n  input:\n    path genome\n    tuple val(sample_id), file(reads)\n\n  output:\n    path \"*dist_analysis.json\", emit: kat_json\n    path \"*.png\", emit: kat_plot\n\n  script:\n    \"\"\"\n    kat comp -t ${task.cpus} -o ${genome.simpleName}_kat ${reads} ${genome}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/kat"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process samtools_index {\n\n  label 'small_job'\n\n  conda \"$baseDir/conda-envs/samtools-env.yaml\"\n\n                                                                \n\n  input:\n    path bam\n    val options\n\n  output:\n    path \"*.bai\", emit: baidx\n\n  script:\n    \"\"\"\n    samtools index $options ${bam}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/samtools_index"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Mikado"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process mikado_serialise {\n  label \"long_job\"\n\n  conda \"$baseDir/conda-envs/mikado-env.yaml\"\n                                          \n\n                                                                            \n\n  input:\n    path genome\n    path blastp\n    path transdecoder\n    path scoring\n    path mikado\n    path junction\n\n  output:\n    path('*'), emit: mikado\n\n  script:\n    def prot     =   params.protein ? \"--xml mikado_prepared.blast.tsv --blast_targets ${params.protein}\" : \"\"\n    def junc     =   params.short_reads  ? \"--junction ${junction}\": \"\"\n\n    \"\"\"\n    mikado serialise --json-conf configuration.yaml $prot --orfs $transdecoder $junc\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/mikado_serialise"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process miniasm {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/miniasm-env.yaml\"\n\n  publishDir \"${params.outdir}/miniasm\", mode: 'copy'\n\n  input:\n    path fastq\n\n  output:\n    path \"miniasm_assembly.fasta\", emit: assembly\n    path \"miniasm_assembly.gfa\", emit: gfa\n\n  script:\n    \"\"\"\n    minimap2 -x ava-ont -t $task.cpus $fastq $fastq | gzip -1 > reads.paf.gz\n\n    miniasm -f $fastq reads.paf.gz > miniasm_assembly.gfa\n\n    any2fasta miniasm_assembly.gfa > miniasm_assembly.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/miniasm"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Mikado"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process mikado_pick {\n  label \"long_job\"\n\n  conda \"$baseDir/conda-envs/mikado-env.yaml\"\n                                          \n\n  publishDir \"${params.outdir}/mikado/\", mode: 'copy', pattern: 'mikado*'\n\n  input:\n    path genome\n    path prepare\n    path serialise\n    path scoring\n    path junction\n\n  output:\n    path \"mikado.loci.gff3\", emit: loci\n    path \"mikado.subloci.gff3\", emit: subloci\n    path \"mikado.loci.metrics.tsv\", emit: metrics\n    path \"mikado.loci.scores.tsv\", emit: scores\n    path \"*pick.log\", emit: log\n\n  script:\n    def protein  =   params.protein  ? \"-bt ${params.protein}\" : \"\"\n    def prot     =   params.protein ? \"--xml mikado_prepared.blast.tsv --blast_targets ${params.protein}\" : \"\"\n    def junc     =   params.short_reads  ? \"--junction ${junction}\": \"\"\n\n    \"\"\"\n    mikado pick --json-conf configuration.yaml --subloci-out mikado.subloci\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/mikado_pick"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["Cufflinks"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process cufflinks {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/cufflinks-env.yaml\"\n\n  publishDir \"${params.outdir}/transcript_predictions/\", mode: 'copy', pattern: '*.gtf'\n\n  input:\n    path genome\n    tuple file(bam), file(baidx)\n\n  output:\n    path \"*.gtf\", emit: gtf\n\n  script:\n\n    \"\"\"\n    cufflinks \\\n    --num-threads $task.cpus \\\n    --output-dir /path/to/outputDirectory ${bam.simpleName} \\\n    $bam\n\n    mv ./${bam.simpleName}/transcripts.gtf ./${bam.simpleName}_cufflinks.gtf\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/cufflinks"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process hypo {\n  label \"assembly\"\n\n  conda \"$baseDir/conda-envs/hypo-env.yaml\"\n\n  publishDir \"${params.outdir}/short_polished\", mode: 'copy'\n\n  input:\n    path genome\n    tuple val(sample_id), file(short_reads)\n    path long_reads\n    val genome_size\n    path short_bam\n    path short_baidx\n    val coverage\n\n  output:\n    path \"*hypo.fasta\", emit: assembly\n\n  script:\n\n    \"\"\"\n    # short read\n    # minimap2 --secondary=no --MD -L -t ${task.cpus} -ax sr $genome ${short_reads[0]} ${short_reads[1]} | samtools sort -@ $task.cpus -O BAM - > short.bam\n    # samtools index short.bam\n\n    #long (ONT)\n    minimap2 --secondary=no --MD -L -t ${task.cpus} -ax map-ont $genome ${long_reads} | samtools sort -@ $task.cpus -O BAM - > long.bam\n    samtools index long.bam\n\n    # avg_depth=`samtools depth short.bam  |  awk '{sum+=\\$3} END { print sum/NR}'`\n\n    echo -e \"${short_reads[0]}\\n${short_reads[1]}\" > names.txt\n\n    hypo \\\n    --draft $genome \\\n    --reads-short @names.txt \\\n    --size-ref $genome_size \\\n    --coverage-short $coverage \\\n    --processing-size 96 \\\n    --bam-sr $short_bam \\\n    --bam-lr long.bam \\\n    --threads ${task.cpus} \\\n    --output ${genome.simpleName}_hypo.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/hypo"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process salsa {\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/salsa-env.yaml\"\n\n  publishDir \"${params.outdir}/HiC\", mode: 'copy'\n\n  input:\n    path assembly\n    path bam\n    path baidx\n\n  output:\n    path 'scaffolds/scaffolds_FINAL.fasta', emit: assembly\n\n  script:\n    \"\"\"\n      bamToBed -i $bam > ${bam.simpleName}.bed\n      sort -k 4 ${bam.simpleName}.bed > tmp && mv tmp ${bam.simpleName}.bed\n\n      samtools faidx $assembly\n\n      run_pipeline.py \\\n        -a ${assembly} \\\n        -l ${assembly}.fai \\\n        -b ${bam.simpleName}.bed \\\n        -e GATC,GANTC \\\n        -o scaffolds \\\n        -m yes\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/salsa"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process freebayes_consensus {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/freebayes-env.yaml\"\n\n  publishDir \"${params.outdir}/short_polished\", mode: 'copy'\n\n  input:\n    path assembly\n    path bcf_list\n\n  output:\n    path \"*freebayes.fasta\", emit: assembly\n\n  script:\n    \"\"\"\n\n    for file in ./tarseq_*.bcf; do bcftools index \\$file  -f; done\n\n    ls *.bcf > bcf_files.txt\n\n    bcftools concat -af bcf_files.txt | bcftools view -Ou -e'type=\"ref\"' --threads ${task.cpus} | bcftools norm --threads ${task.cpus} -Ob -f ${assembly} -o ${assembly.simpleName}.bcf\n    bcftools index ${assembly.simpleName}.bcf\n\n    bcftools consensus -i'QUAL>1 && (GT=\"AA\" || GT=\"Aa\")' -Hla -f ${assembly} ${assembly.simpleName}.bcf > ${assembly.simpleName}_freebayes.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/freebayes_consensus"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process freebayes_bwa {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/freebayes-env.yaml\"\n\n                                                                \n\n  input:\n    path assembly\n    tuple val(sample_id), file(reads)\n\n  output:\n    path \"*.bam\", emit: bam\n    path \"*.bam.bai\", emit: baidx\n    val avg_depth, emit: avg_depth\n\n  script:\n    \"\"\"\n    # bwa index\n    bwa index ${assembly}\n\n    # bwa map\n    bwa mem -t ${task.cpus} ${assembly} ${reads} | samtools sort -@ ${task.cpus} -O BAM - > ${assembly.simpleName}.bam\n\n    samtools index ${assembly.simpleName}.bam\n\n    avg_depth=`samtools depth ${assembly.simpleName}.bam  |  awk '{sum+=\\$3} END { print sum/NR}'`\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/freebayes_bwa"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["pilon"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process pilon {\n  label \"assembly\"\n\n  conda \"$baseDir/conda-envs/pilon-env.yaml\"\n\n  publishDir \"${params.outdir}/short_polished\", mode: 'copy'\n\n  input:\n    path genome\n    path bam\n    path baidx\n\n  output:\n    path \"pilon*.fasta\", emit: assembly\n\n  script:\n\n    \"\"\"\n    pilon \\\n    -Xmx${task.memory.toGiga()}g \\\n    --genome $genome \\\n    --bam $bam \\\n    --diploid \\\n    --output pilon \\\n    --threads ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/pilon"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process bwa_mem {\n\n  label 'assembly'\n\n  conda \"$baseDir/conda-envs/bwa-samtools-env.yaml\"\n\n                                                                \n\n  input:\n    tuple val(sample_id), file(reads)\n    path assembly\n    path index\n\n  output:\n    path \"*.bam\", emit: bam\n    path \"*.bam.bai\", emit: baidx\n\n  script:\n    \"\"\"\n    bwa mem -t ${task.cpus} ${assembly} ${reads} | samtools sort -@ ${task.cpus} -O BAM - > ${assembly.simpleName}.bam\n\n    samtools index ${assembly.simpleName}.bam\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/bwa_mem"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process arima_qc {\n\n  label \"small_job\"\n\n  conda \"$baseDir/conda-envs/samtools-env.yaml\"\n\n  input:\n    path bam1\n    path bam2\n    path genome\n\n  output:\n    path \"*.merged.bam\", emit: bam\n\n  script:\n\n    \"\"\"\n    samtools faidx $genome\n\n    perl $baseDir/scripts/two_read_bam_combiner.pl $bam1 $bam2 samtools $params.mapq_filter | samtools view -bS -t ${genome}.faidx - | samtools sort -@ $task.cpus -o ${bam1.simpleName}.merged.bam -\n\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/arima_qc"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process multiqc {\n\n  conda \"$baseDir/conda-envs/multiqc-env.yaml\"\n\n  publishDir \"${params.outdir}\", mode: 'copy'\n\n  input:\n    path('*')\n    path config\n\n  output:\n    path \"*.html\"\n\n  script:\n    \"\"\"\n    export LC_ALL=en_US.utf8\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/multiqc"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["MaSuRCA"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process masurca {\n\n  label 'assembly'\n\n  container \"https://depot.galaxyproject.org/singularity/masurca:4.0.7--pl5262h86ccdc5_0\"\n\n  publishDir \"${params.outdir}/masurca\", mode: 'copy'\n\n  input:\n    path masurca_file\n\n  output:\n    path \"final.genome.scf.fasta\", emit: assembly\n\n  script:\n    \"\"\"\n    masurca ${masurca_file}\n\n    ./assemble.sh\n\n    cp CA.mr*/final.genome.scf.fasta final.genome.scf.fasta\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/masurca"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process star_align {\n\ttag \"$sample_id\"\n\tlabel 'small_plus'\n\tpublishDir \"${params.outdir}/star\", mode: 'copy'\n\n  conda \"$baseDir/conda-envs/star-env.yaml\"\n\n  input:\n    path genome_idx\n    tuple val(sample_id), file(reads)\n  output:\n    path \"*.bam\", emit: bam\n\n  script:\n    \"\"\"\n    mkdir $sample_id\n    STAR \\\n\t\t --twopassMode Basic \\\n\t\t--runThreadN $task.cpus \\\n\t\t--genomeDir $genome_idx \\\n\t\t--readFilesIn $reads \\\n\t\t--readFilesCommand zcat \\\n\t\t--outFileNamePrefix ${sample_id}.star \\\n\t\t--outSAMattributes XS \\\n\t\t--outSAMtype BAM SortedByCoordinate \\\n\t\t--outSAMstrandField intronMotif\n\t\t\"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/star_align"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["nanoporeseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process busco {\n  tag \"$genome-$lineage\"\n  label \"long_job\"\n\n  conda \"$baseDir/conda-envs/busco-env.yaml\"\n\n  publishDir \"${params.outdir}/busco\", mode: 'copy'\n\n  input:\n    path genome\n    val lineage\n    val mode\n\n  output:\n    path \"busco_${genome.simpleName}_${lineage}/short_summary*\", emit: summary\n\n  script:\n    def busco_long = params.busco_long ? \"--long\" : \"\"\n\n    \"\"\"\n    busco \\\n    --in $genome \\\n    --lineage_dataset $lineage \\\n    --out busco_${genome.simpleName}_${lineage} \\\n    --mode $mode \\\n    $busco_long \\\n    --cpu ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["palfalvi/nanoporeseq/busco"], "list_wf_names": ["palfalvi/nanoporeseq"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process kallisto_quant {\n\ttag \"${sample_id}\"\n\tlabel 'small'\n\tpublishDir \"${params.out}/kallisto\", mode: 'copy'\n\tcpus \"$params.cpus\"\n  conda \"$baseDir/conda-envs/kallisto-env.yaml\"\n\n  input:\n  \tpath transcriptome_idx\n    tuple val(sample_id), file(reads)\n  output:\n    path sample_id\n  script:\n  \t\"\"\"\n    kallisto quant \\\n\t\t-t ${task.cpus} \\\n\t\t-i $transcriptome_idx \\\n\t\t-o $sample_id ${reads[0]} ${reads[1]} \\\n\t\t&> ${sample_id}.log\n\n    mv ${sample_id}.log ${sample_id}/\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/kallisto_quant"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process salmon_quant {\n\ttag \"${sample_id}\"\n\tlabel 'small'\n\tpublishDir \"${params.out}/salmon\", mode: 'copy'\n\tcpus \"$params.cpus\"\n  conda \"$baseDir/conda-envs/salmon-env.yaml\"\n\n  input:\n  \tpath transcriptome_idx\n    tuple val(sample_id), file(reads)\n  output:\n    path sample_id\n  script:\n    \"\"\"\n    salmon quant \\\n\t\t--libType A \\\n\t\t--validateMappings \\\n\t\t-p ${task.cpus} \\\n\t\t-i $transcriptome_idx \\\n\t\t-1 ${reads[0]} \\\n\t\t-2 ${reads[1]} \\\n\t\t-o $sample_id\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/salmon_quant"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_fastpSE {\ntag \"$sample_id\"\nlabel 'small_plus'\ncpus \"$params.cpus\"\npublishDir \"${params.out}/fastp_qc\", mode: 'copy', pattern: '*.json'\npublishDir path: { params.save_trimmed ? \"${params.out}/trimmed\" : params.out },\n            mode: 'copy', saveAs: { params.save_index ? it : null }, pattern: '*.fastq.gz'\nconda \"$baseDir/conda-envs/trim-env.yaml\"\n\nwhen:\n  !skip_trim\ninput:\n  file reads\noutput:\n  path \"trim_*\", emit: trimmed\n  path \"*.json\", emit: json\nscript:\n  \"\"\"\n  fastp \\\n  -w ${task.cpus} \\\n  -i ${reads} \\\n  -o trim_${reads} \\\n  --overrepresentation_analysis \\\n  --json ${reads.simpleName}_fastp.json\n  \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_fastpSE"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process star_idx {\n  tag \"$genome\"\n  label 'small_plus'\n  cpus \"$params.cpus\"\n  publishDir path: { params.save_index ? \"${params.out}/star_index\" : params.out },\n             mode: 'copy', saveAs: { params.save_index ? it : null }\n  conda \"$baseDir/conda-envs/star-env.yaml\"\n\n  input:\n    path genome\n    path gtf\n  output:\n    path \"${genome.simpleName}_idx\"\n  script:\n    \"\"\"\n\t\tmkdir ${genome.simpleName}_idx\n    STAR --runMode genomeGenerate \\\n    --runThreadN ${task.cpus} \\\n    --genomeDir ${genome.simpleName}_idx \\\n    --genomeFastaFiles $genome \\\n    --sjdbGTFfile $gtf \\\n    --sjdbGTFfeatureExon \"${params.sjdbGTFfeatureExon}\"\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/star_idx"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_fastqcSE {\n        tag \"$reads.simpleName\"\n        cpus \"$params.cpus\"\n\n\tpublishDir \"${params.out}/fastqc\", mode: 'copy'\n\n        conda \"$baseDir/conda-envs/fastqc-env.yaml\"\n\n        when:\n\t\t!params.skip_qc\n\tinput:\n                path reads\n        output:\n                path 'fastqc_*'\n        script:\n                \"\"\"\n                mkdir fastqc_${reads.simpleName}\n\t\tfastqc -t ${task.cpus} -o fastqc_${reads.simpleName} $reads\n                \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_fastqcSE"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_fastp {\ntag \"$sample_id\"\nlabel 'small_plus'\ncpus \"$params.cpus\"\npublishDir \"${params.out}/fastp_qc\", mode: 'copy', pattern: '*.json'\npublishDir path: { params.save_trimmed ? \"${params.out}/trimmed\" : params.out },\n            mode: 'copy', saveAs: { params.save_index ? it : null }, pattern: '*.fastq.gz'\nconda \"$baseDir/conda-envs/trim-env.yaml\"\n\nwhen:\n  !skip_trim\ninput:\n  tuple val(sample_id), file(reads)\noutput:\n  tuple val(sample_id), file(\"trim_*\"), emit: trimmed\n  path \"*.json\", emit: json\nscript:\n\n  \"\"\"\n  fastp \\\n  -w ${task.cpus} \\\n  -i ${reads[0]} \\\n  -I ${reads[1]} \\\n  -o trim_${reads[0]} \\\n  -O trim_${reads[1]} \\\n  --detect_adapter_for_pe \\\n  --overrepresentation_analysis \\\n  --json ${sample_id}_fastp.json\n  \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_fastp"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 2, "tools": ["Salmon", "fastPHASE"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_fastpSE_qc {\ntag \"$sample_id\"\nlabel 'small_plus'\ncpus \"$params.cpus\"\npublishDir \"${params.out}/fastp_qc\", mode: 'copy', pattern: '*.json'\nconda \"$baseDir/conda-envs/trim-env.yaml\"\n\nwhen:\n  !skip_qc\ninput:\n  file reads\noutput:\n  path \"*.json\", emit: json\nscript:\n  \"\"\"\n  fastp \\\n  -w ${task.cpus} \\\n  -i ${reads} \\\n  --overrepresentation_analysis \\\n  --json ${reads.simpleName}_fastp.json\n  \"\"\"\n}", "process salmon_quantSE {\n\ttag \"${reads.simpleName}\"\n\tlabel 'small'\n\tpublishDir \"${params.out}/salmon\", mode: 'copy'\n\tcpus \"$params.cpus\"\n  conda \"$baseDir/conda-envs/salmon-env.yaml\"\n\n  input:\n  \tpath transcriptome_idx\n    path reads\n  output:\n    path \"$reads.simpleName\"\n  script:\n    \"\"\"\n    salmon quant \\\n\t\t--libType A \\\n\t\t--validateMappings \\\n\t\t-p ${task.cpus} \\\n\t\t-i $transcriptome_idx \\\n\t\t-r ${reads} \\\n\t\t-o $reads.simpleName\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_fastpSE_qc", "palfalvi/rnaseq/salmon_quantSE"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_multiqc {\n  conda \"$baseDir/conda-envs/multiqc-env.yaml\"\n \tpublishDir \"${params.out}\", mode: 'move'\n  when:\n    !params.skip_multiqc\n\tinput:\n\t\tpath('*')\n\t\tpath config\n  output:\n    path 'multiqc*html'\n  script:\n    \"\"\"\n    export LC_ALL=en_US.utf8\n    multiqc $launchDir/$config/\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_multiqc"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 2, "tools": ["FeatureCounts", "GATK"], "nb_own": 2, "list_own": ["pblaney", "palfalvi"], "nb_wf": 2, "list_wf": ["mgp1000", "rnaseq"], "list_contrib": ["pblaney", "palfalvi"], "nb_contrib": 2, "codes": ["\nprocess splitIntervalList_gatk {\n\t\n\tinput:\n\ttuple path(reference_genome_fasta_forSplitIntervals), path(reference_genome_fasta_index_forSplitIntervals), path(reference_genome_fasta_dict_forSplitIntervals) from reference_genome_bundle_forSplitIntervals\n\tpath gatk_bundle_wgs_interval_list\n\n\toutput:\n\tpath \"splitIntervals/*-split.interval_list\" into split_intervals mode flatten\n\n\tscript:\n\t\"\"\"\n\tgatk SplitIntervals \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--reference \"${reference_genome_fasta_forSplitIntervals}\" \\\n\t--intervals \"${gatk_bundle_wgs_interval_list}\" \\\n\t--subdivision-mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION \\\n\t--extension -split.interval_list \\\n\t--scatter-count 20 \\\n\t--output splitIntervals\n\t\"\"\"\n}", "process collect_starSE {\n  tag \"$bam\"\n  label 'small'\n  cpus \"$params.cpus\"\n\n\tpublishDir \"${params.out}/featureCounts\", mode: 'copy'\n\n  conda \"$baseDir/conda-envs/subread-env.yaml\"\n\n\tinput:\n\t  path bam\n\t  path gtf\n  output:\n    path \"${bam}_gene.featureCounts.txt*\"\n  script:\n    \"\"\"\n    featureCounts \\\n    -T ${task.cpus} \\\n    -s ${params.featureCounts_direction} \\\n    -a $gtf \\\n    -o ${bam}_gene.featureCounts.txt \\\n    ${bam}/*.out.bam\n    \"\"\"\n\n}"], "list_proc": ["pblaney/mgp1000/splitIntervalList_gatk", "palfalvi/rnaseq/collect_starSE"], "list_wf_names": ["pblaney/mgp1000", "palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_fastp_qc {\ntag \"$sample_id\"\nlabel 'small_plus'\ncpus \"$params.cpus\"\npublishDir \"${params.out}/fastp_qc\", mode: 'copy', pattern: '*.json'\nconda \"$baseDir/conda-envs/trim-env.yaml\"\n\nwhen:\n  !kip_qc\ninput:\n  tuple val(sample_id), file(reads)\noutput:\n  path \"*.json\", emit: json\nscript:\n  \"\"\"\n  fastp \\\n  -w ${task.cpus} \\\n  -i ${reads[0]} \\\n  -I ${reads[1]} \\\n  --overrepresentation_analysis \\\n  --json ${sample_id}_fastp.json\n  \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_fastp_qc"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process salmon_idx {\n  tag \"$transcriptome\"\n  label 'small'\n  cpus \"$params.cpus\"\n  publishDir path: { params.save_index ? \"${params.out}/salmon_index\" : params.out },\n              mode: 'copy', saveAs: { params.save_index ? it : null }\n  conda \"$baseDir/conda-envs/salmon-env.yaml\"\n\n  input:\n    path transcriptome\n  output:\n    path 'index'\n  script:\n    \"\"\"\n    salmon index -p ${task.cpus} -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/salmon_idx"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process kallisto_quantSE {\n\ttag \"$reads.simpleName\"\n\tlabel 'small'\n\tpublishDir \"${params.out}/kallisto\", mode: 'copy'\n\tcpus \"$params.cpus\"\n  conda \"$baseDir/conda-envs/kallisto-env.yaml\"\n\n  input:\n  \tpath transcriptome_idx\n    path reads\n  output:\n    path \"$reads.simpleName\"\n  script:\n    \"\"\"\n    kallisto quant \\\n\t\t-t ${task.cpus} \\\n\t\t--single \\\n\t\t-l ${params.fragment_length} \\\n\t\t-s ${params.fragment_sd} \\\n\t\t-i $transcriptome_idx \\\n\t\t-o $reads.simpleName ${reads} \\\n\t\t&> ${reads.simpleName}.log\n\n    mv ${reads.simpleName}.log ${reads.simpleName}/\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/kallisto_quantSE"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process star_align {\n\ttag \"$sample_id\"\n\tlabel 'small_plus'\n\tpublishDir \"${params.out}/star\", mode: 'copy'\n\tcpus \"$params.cpus\"\n  conda \"$baseDir/conda-envs/star-env.yaml\"\n\n  input:\n    path genome_idx\n    tuple val(sample_id), file(reads)\n  output:\n    path sample_id\n  script:\n    \"\"\"\n    mkdir $sample_id\n    STAR \\\n\t\t--runThreadN $task.cpus \\\n\t\t--genomeDir $genome_idx \\\n\t\t--readFilesIn $reads \\\n\t\t--readFilesCommand zcat \\\n\t\t--outFileNamePrefix ${sample_id}/${sample_id}_ \\\n\t\t--quantMode GeneCounts \\\n\t\t--outSAMtype BAM SortedByCoordinate\n\t\t\"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/star_align"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process collect_star {\n  tag \"$bam\"\n  label 'small'\n  cpus \"$params.cpus\"\n\n\tpublishDir \"${params.out}/featureCounts\", mode: 'copy'\n\n  conda \"$baseDir/conda-envs/subread-env.yaml\"\n\n\tinput:\n\t  path bam\n\t  path gtf\n  output:\n    path \"${bam}_gene.featureCounts.txt*\"\n  script:\n    \"\"\"\n    featureCounts \\\n    -p \\\n    -T ${task.cpus} \\\n    -s ${params.featureCounts_direction} \\\n    -a $gtf \\\n    -o ${bam}_gene.featureCounts.txt \\\n    ${bam}/*.out.bam\n    \"\"\"\n\n}"], "list_proc": ["palfalvi/rnaseq/collect_star"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["\nprocess kallisto_idx {\n  tag \"$transcriptome\"\n  publishDir path: { params.save_index ? \"${params.out}/kallisto_index\" : params.out },\n             mode: 'copy', saveAs: { params.save_index ? it : null }\n  conda \"$baseDir/conda-envs/kallisto-env.yaml\"\n\n  input:\n    path transcriptome\n  output:\n    path 'index'\n  script:\n    \"\"\"\n    kallisto index -i index $transcriptome\n    \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/kallisto_idx"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process star_alignSE {\n\ttag \"$reads.simpleName\"\n\tlabel 'small_plus'\n\tpublishDir \"${params.out}/star\", mode: 'copy'\n\tcpus \"$params.cpus\"\n  conda \"$baseDir/conda-envs/star-env.yaml\"\n\n  input:\n\t\tpath genome_idx\n    path reads\n  output:\n    path \"$reads.simpleName\"\n  script:\n    \"\"\"\n    mkdir \"${reads.simpleName}\"\n    STAR \\\n\t\t--runThreadN $task.cpus \\\n\t\t--genomeDir $genome_idx \\\n\t\t--readFilesIn $reads \\\n\t\t--readFilesCommand zcat \\\n\t\t--outFileNamePrefix ${reads.simpleName}/${reads.simpleName}_ \\\n\t\t--quantMode GeneCounts \\\n\t\t--outSAMtype BAM SortedByCoordinate\n\t  \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/star_alignSE"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["palfalvi"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["palfalvi"], "nb_contrib": 1, "codes": ["process run_fastqc {\n  tag \"$sample_id\"\n  cpus \"$params.cpus\"\n  publishDir \"${params.out}/fastqc\", mode: 'copy'\n  conda \"$baseDir/conda-envs/fastqc-env.yaml\"\n\n\twhen:\n                !params.skip_qc\n\tinput:\n                tuple val(sample_id), file(reads)\n        output:\n                path 'fastqc_*'\n        script:\n                \"\"\"\n                mkdir fastqc_${sample_id}\n\t\tfastqc -t ${task.cpus} -o fastqc_${sample_id} $reads\n                \"\"\"\n}"], "list_proc": ["palfalvi/rnaseq/run_fastqc"], "list_wf_names": ["palfalvi/rnaseq"]}, {"nb_reuse": 1, "tools": ["SOAPnuke"], "nb_own": 1, "list_own": ["pandora414"], "nb_wf": 1, "list_wf": ["dna"], "list_contrib": ["zhumiao1989", "lucky-nj"], "nb_contrib": 2, "codes": ["\nprocess soapnuke{\n    tag { sample_name }\n    input:\n        val sample_name from sample\n        file 'sample_R1.fq.gz' from reads1\n        file 'sample_R2.fq.gz' from reads2\n\n    output:\n        set sample_name,file(\"*.fastq.gz\") into clean_samples\n\n    script:\n    \"\"\"\n    SOAPnuke filter -1 sample_R1.fq.gz -2 sample_R2.fq.gz -l 15 -q 0.5 -Q 2 -o . \\\n        -C sample.clean1.fastq.gz -D sample.clean2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["pandora414/dna/soapnuke"], "list_wf_names": ["pandora414/dna"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["pandora414"], "nb_wf": 1, "list_wf": ["dna"], "list_contrib": ["zhumiao1989", "lucky-nj"], "nb_contrib": 2, "codes": ["\nprocess bwa{\n    input:\n        set sample_name,files from clean_samples\n    \n    output:\n        set sample_name,file('sample.clean.sam') into sam_res\n\n    script:\n    \"\"\"\n    bwa mem -t ${params.bwa_cpu} -M -T 30 ${params.bwa_db_prefix} \\\n        -R \"@RG\\tID:${sample_name}\\tSM:${sample_name}\\tPL:ILLUMINA\\tLB:DG\\tPU:illumina\" \\\n        ${files[0]} ${files[1]} > sample.clean.sam\n    \"\"\"     \n}"], "list_proc": ["pandora414/dna/bwa"], "list_wf_names": ["pandora414/dna"]}, {"nb_reuse": 10, "tools": ["BCFtools", "SAMtools", "BLASTP-ACC", "Minimap2", "MultiQC", "FastQC", "STAR"], "nb_own": 8, "list_own": ["rafalcode", "sarseq", "paulstretenowich", "vpeddu", "vibbits", "zamanianlab", "stevekm", "tamara-hodgetts"], "nb_wf": 8, "list_wf": ["chipseq-nextflow", "nxtflo", "nf-core-viralrecon", "nextflow-pipeline-demo", "vcf-filter-annotate", "ev-meta", "nf-atac-seq", "sarseq2", "Core_RNAseq-nf"], "list_contrib": ["mzamanian", "sarseq", "tmuylder", "vpeddu", "chenthorn", "stevekm", "drpatelh", "tamara-hodgetts"], "nb_contrib": 8, "codes": ["\nprocess normalize_vcf {\n                                          \n    tag \"${caller}-${sampleID}\"\n    publishDir \"${params.output_dir}/samples/${sampleID}/${caller}\", mode: 'copy', overwrite: true\n    publishDir \"${params.output_dir}/analysis/normalize_vcf\", overwrite: true\n\n    input:\n    set val(caller), val(sampleID), file(sample_vcf), file(ref_fasta) from sample_variants.combine(ref_fasta)\n\n    output:\n    set val(caller), val(sampleID), file(\"${prefix}.norm.vcf\") into (normalized_variants, normalized_variants2)\n    file(\"${prefix}.bcftools.multiallelics.stats.txt\")\n    file(\"${prefix}.bcftools.realign.stats.txt\")\n\n    script:\n    prefix = \"${sampleID}.${caller}\"\n    \"\"\"\n    cat ${sample_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${prefix}.bcftools.multiallelics.stats.txt\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${prefix}.bcftools.realign.stats.txt\" > \\\n    \"${prefix}.norm.vcf\"\n    \"\"\"\n}", "\nprocess star_align {\n\n    publishDir \"${output}/${params.dir}/star\", mode: 'copy', pattern: '*.Log.final.out'\n    publishDir \"${output}/${params.dir}/star\", mode: 'copy', pattern: '*.flagstat.txt'\n    publishDir \"${output}/${params.dir}/counts\", mode: 'copy', pattern: '*.ReadsPerGene.tab'\n                                                                               \n                                                                                   \n\n    cpus big\n    tag { id }\n    maxForks 6\n\n    when:\n      params.star\n\n    input:\n        file(\"STAR_index/*\") from star_indices\n        tuple val(id), file(forward), file(reverse) from trimmed_reads_star\n\n    output:\n        tuple file(\"${id}.Log.final.out\"), file(\"${id}.flagstat.txt\") into alignment_logs_star\n        tuple id, file(\"${id}.bam\"), file(\"${id}.bam.bai\") into bam_files_star\n        file(\"${id}.ReadsPerGene.tab\") into star_counts\n\n    script:\n\n        \"\"\"\n          STAR --runThreadN ${task.cpus} --runMode alignReads --genomeDir STAR_index\\\n            --outSAMtype BAM Unsorted --readFilesCommand zcat \\\n            --outFileNamePrefix ${id}. --readFilesIn ${forward} ${reverse}\\\n            --peOverlapNbasesMin 10 \\\n            --quantMode GeneCounts --outSAMattrRGline ID:${id}\n          samtools sort -@ ${task.cpus} -m 24G -o ${id}.bam ${id}.Aligned.out.bam\n          rm *.Aligned.out.bam\n          samtools index -@ ${task.cpus} -b ${id}.bam\n          samtools flagstat ${id}.bam > ${id}.flagstat.txt\n          cat ${id}.ReadsPerGene.out.tab | cut -f 1,2 > ${id}.ReadsPerGene.tab\n        \"\"\"\n                \n}", "\nprocess blastSearch {\n    input:\n    file query from query_ch\n                                                                    \n                                                      \n                      \n                                                               \n              \n\n    output:\n    file \"top_hits.txt\" into top_hits_ch\n\n    \"\"\"\n    blastp -db $db -query $query -outfmt 6 > blast_result\n    cat blast_result | head -n 10 | cut -f 2 > top_hits.txt\n    \"\"\"\n                                                                         \n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess SAMTOOLS_FAIDX {\n    tag \"$fasta\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"*.fai\"        , emit: fai\n    path \"*.version.txt\", emit: version\n    path '*.sizes'      , emit: sizes                                     \n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess aln2tile {\n\n    tag \"${setname}\"\n\n    input:\n    set val(setname), file(reads) from ch_r2t_fastq_set\n    file index from ch_tile_indices\n\n    output:\n    set file(\"*bam\"), file(\"*flagstat\") optional true into ch_aln2tile_bam_log\n    file(\"*flagstat\") optional true into ch_aln2tile_multiqc\n\n\n    script:\n    \"\"\"\n    for infile in $reads; do\n\n      if [ \\$(wc -l \\$infile | cut -f 1 -d \" \") -ge \"${params.sample_minreads_fastq_paired}\" ]; then\n        name=\\$(basename \\$infile | sed 's/.R12.fastq//')\n\n        tile=\\$(echo \\$infile | sed 's/__.*\\$//')\n\n        minimap2 -ax sr -t $task.cpus indices/\\$tile/sequence.mmi \\$infile | \\\\\n          samtools view -@ $task.cpus -b -h -F 0x0100 - | samtools sort -o \\$name.bam -\n\n        samtools index \\$name.bam\n\n        samtools flagstat -@ $task.cpus \\$name.bam > \\$name.flagstat\n      fi\n    done\n    \"\"\"\n\n}", "\nprocess Extract_true_novel { \n                                \npublishDir \"${params.OUTPUT}/novel_reads/${base}\", mode: 'symlink'\ncontainer \"quay.io/vpeddu/evmeta:latest\"\nbeforeScript 'chmod o+rw .'\ncpus 24\ninput: \n    tuple val(base), file(unassigned_fastq), file(metaflye_contigs)\noutput: \n    tuple val(\"${base}\"), file(\"${base}.unassembled.unclassified.fastq.gz\")\n\n\nscript:\n\"\"\"\n#!/bin/bash\n\n#logging\necho \"ls of directory\" \nls -lah \n\necho \"remapping ${base} to contigs to find unassembled reads\"\n#TODO: FILL IN MINIMAP2 COMMAND \nminimap2 \\\n    -ax map-ont \\\n    -t \"\\$((${task.cpus}-4))\" \\\n    -2 \\\n    --split-prefix \\\n    ${unassigned_fastq} \\\n    ${metaflye_contigs}| samtools view -Sb -f 4 -@ 4 - > ${base}.unassembled.unclassified.bam\n\nsamtools fastq -@ 4 ${base}.unassembled.unclassified.bam | gzip > ${base}.unassembled.unclassified.fastq.gz\n\n\"\"\"\n}", "\nprocess multiqc {\n    publishDir(\"$params.outdir/multiqc/\", mode: 'copy', overwrite: true)\n    label 'low'\n    container 'quay.io/biocontainers/multiqc:1.9--py_1'\n\n    input:\n    path (inputfiles)\n\n    output:\n    path \"multiqc_report.html\"\t\t\t\t\t\n\n    script:\n                                      \n    \"\"\"\n    multiqc .\n    \"\"\"\n}", "\nprocess sambamba_dedup {\n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam-bwa-dd\", mode: 'copy', overwrite: true\n    clusterOptions '-pe threaded 1-8 -l mem_free=40G -l mem_token=4G'\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'samtools/1.3'\n\n    input:\n    set val(sample_ID), file(sample_bam) from samples_bam2\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.bam\") into samples_dd_bam, samples_dd_bam2, samples_dd_bam3, samples_dd_bam4, samples_dd_bam5, samples_dd_bam6, samples_dd_bam7\n    file(\"${sample_ID}.dd.bam.bai\")\n\n    script:\n    \"\"\"\n    \"${params.sambamba_bin}\" markdup --remove-duplicates --nthreads \\${NSLOTS:-1} --hash-table-size 525000 --overflow-list-size 525000 \"${sample_bam}\" \"${sample_ID}.dd.bam\"\n    samtools view \"${sample_ID}.dd.bam\"\n    \"\"\"\n}", "\nprocess bam_ra_rc_gatk {\n                                               \n                                                                                                                                            \n                                                                                                                                             \n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam_dd_ra_rc_gatk\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n\n    input:\n    set val(sample_ID), file(sample_bam), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(gatk_1000G_phase1_indels_vcf), file(mills_and_1000G_gold_standard_indels_vcf), file(dbsnp_ref_vcf) from samples_dd_bam_ref_gatk\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.ra.rc.bam\"), file(\"${sample_ID}.dd.ra.rc.bam.bai\") into samples_dd_ra_rc_bam, samples_dd_ra_rc_bam2, samples_dd_ra_rc_bam3\n    file \"${sample_ID}.intervals\"\n    file \"${sample_ID}.table1.txt\"\n    file \"${sample_ID}.table2.txt\"\n    file \"${sample_ID}.csv\"\n    file \"${sample_ID}.pdf\"\n\n    script:\n    \"\"\"\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T RealignerTargetCreator \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    -nt \\${NSLOTS:-1} \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.intervals\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T IndelRealigner \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --maxReadsForRealignment 50000 \\\n    -known \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -known \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -targetIntervals \"${sample_ID}.intervals\" \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.dd.ra.bam\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.table1.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T BaseRecalibrator \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -knownSites \"${gatk_1000G_phase1_indels_vcf}\" \\\n    -knownSites \"${mills_and_1000G_gold_standard_indels_vcf}\" \\\n    -knownSites \"${dbsnp_ref_vcf}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --out \"${sample_ID}.table2.txt\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T AnalyzeCovariates \\\n    --logging_level ERROR \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -before \"${sample_ID}.table1.txt\" \\\n    -after \"${sample_ID}.table2.txt\" \\\n    -csv \"${sample_ID}.csv\" \\\n    -plots \"${sample_ID}.pdf\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T PrintReads \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    -rf BadCigar \\\n    --reference_sequence \"${ref_fasta}\" \\\n    -BQSR \"${sample_ID}.table1.txt\" \\\n    --input_file \"${sample_ID}.dd.ra.bam\" \\\n    --out \"${sample_ID}.dd.ra.rc.bam\"\n\n    samtools index \"${sample_ID}.dd.ra.rc.bam\"\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["stevekm/vcf-filter-annotate/normalize_vcf", "zamanianlab/Core_RNAseq-nf/star_align", "rafalcode/nxtflo/blastSearch", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_FAIDX", "sarseq/sarseq2/aln2tile", "vpeddu/ev-meta/Extract_true_novel", "vibbits/chipseq-nextflow/multiqc", "stevekm/nextflow-pipeline-demo/sambamba_dedup", "stevekm/nextflow-pipeline-demo/bam_ra_rc_gatk", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_INDEX"], "list_wf_names": ["sarseq/sarseq2", "vpeddu/ev-meta", "stevekm/nextflow-pipeline-demo", "stevekm/vcf-filter-annotate", "zamanianlab/Core_RNAseq-nf", "rafalcode/nxtflo", "vibbits/chipseq-nextflow", "tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 2, "tools": ["BCFtools", "Trimmomatic", "MultiQC"], "nb_own": 3, "list_own": ["paulstretenowich", "vibbits", "tdelhomme"], "nb_wf": 2, "list_wf": ["chipseq-nextflow", "vcf_ancestry-nf", "nf-core-viralrecon"], "list_contrib": ["tmuylder", "drpatelh", "tdelhomme"], "nb_contrib": 3, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess merge_imputed_vcf {\n\n  input:\n  file fasta_ref\n  file all_chr from imp_res.collect()\n\n  output:\n  file(\"*.vcf.gz\") into final_vcf\n\n  shell:\n  '''\n  for f in `ls *vcf.gz`\n  do\n    filename=`basename $f`\n    tabix -p vcf $f\n    bcftools filter -i 'INFO/DR2>0.3' $f | bgzip -c > ${filename}_DR2filter.vcf.gz\n  done\n  bcftools concat *{1..22}_DR2filter.vcf.gz | bcftools norm -m - -Oz -f !{fasta_ref} > !{params.cohort}_plinkfilt_imputed_allchr_DR2filter_norm.vcf.gz\n  '''\n\n}", "\nprocess trimmomatic {\n    publishDir \"$params.outdir/trimmed-reads\", mode: 'copy' , overwrite: true\n    label 'low'\n    container 'quay.io/biocontainers/trimmomatic:0.35--6'\n\n                                                                       \n    input:\n    tuple val(sample), path(reads) \n\n    output:\n    tuple val(\"${sample}\"), path(\"${sample}*.paired.fq\"), emit: trim_fq\n    tuple val(\"${sample}\"), path(\"${sample}*.unpaired.fq\"), emit: untrim_fq\n    \n    script:\n    \"\"\"\n    mkdir -p $params.outdir/trimmed-reads/\n    trimmomatic PE \\\\\n        -threads $params.threads \\\\\n        ${reads[0]} \\\\\n        ${reads[1]} \\\\\n        ${sample}1.paired.fq \\\\\n        ${sample}1.unpaired.fq \\\\\n        ${sample}2.paired.fq \\\\\n        ${sample}2.unpaired.fq \\\\\n        $params.illuminaclip \\\\\n        $params.leading \\\\\n        $params.trailing \\\\\n        $params.slidingwindow \\\\\n        $params.minlen \n    \"\"\"\n}"], "list_proc": ["tdelhomme/vcf_ancestry-nf/merge_imputed_vcf", "vibbits/chipseq-nextflow/trimmomatic"], "list_wf_names": ["tdelhomme/vcf_ancestry-nf", "vibbits/chipseq-nextflow"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess haplotypeCaller_gatk {\n\ttag \"${sample_id}.${interval_id}\"\n\n\tinput:\n\ttuple path(bam_preprocessed), path(reference_genome_fasta_forHaplotypeCaller), path(reference_genome_fasta_index_forHaplotypeCaller), path(reference_genome_fasta_dict_forHaplotypeCaller), path(interval) from input_bams_and_reference_fasta_forHaplotypeCaller.combine(split_intervals)\n\n\toutput:\n\ttuple val(sample_id), path(gvcf_per_interval_raw), path(gvcf_per_interval_raw_index) into raw_gvcfs\n\n\tscript:\n\tsample_id = \"${bam_preprocessed}\".replaceFirst(/\\.final\\..*bam/, \"\")\n\tinterval_id = \"${interval}\".replaceFirst(/-split\\.interval_list/, \"_int\")\n\tgvcf_per_interval_raw = \"${sample_id}.${interval_id}.g.vcf.gz\"\n\tgvcf_per_interval_raw_index = \"${gvcf_per_interval_raw}.tbi\"\n\t\"\"\"\n\tgatk HaplotypeCaller \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--max-alternate-alleles 3 \\\n\t--standard-min-confidence-threshold-for-calling 50 \\\n\t--emit-ref-confidence GVCF \\\n\t--reference \"${reference_genome_fasta_forHaplotypeCaller}\" \\\n\t--intervals \"${interval}\" \\\n\t--input \"${bam_preprocessed}\" \\\n\t--output \"${gvcf_per_interval_raw}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/haplotypeCaller_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess combineAllGvcfs_gatk {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\tpath gvcf_merged_raw from merged_raw_gcvfs.toList()\n\tpath gvcf_merged_raw_index from merged_raw_gcvfs_indicies.collect()\n\ttuple path(reference_genome_fasta_forCombineGvcfs), path(reference_genome_fasta_index_forCombineGvcfs), path(reference_genome_fasta_dict_forCombineGvcfs) from reference_genome_bundle_forCombineGvcfs\n\n\toutput:\n\ttuple path(gvcf_cohort_combined), path(gvcf_cohort_combined_index) into combined_cohort_gvcf\n\n\tscript:\n\tgvcf_cohort_combined = \"${params.cohort_name}.g.vcf.gz\"\n\tgvcf_cohort_combined_index = \"${gvcf_cohort_combined}.tbi\"\n\t\"\"\"\n\tgatk CombineGVCFs \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--reference \"${reference_genome_fasta_forCombineGvcfs}\" \\\n\t${gvcf_merged_raw.collect {\" --variant $it\" }.join()} \\\n\t--output \"${gvcf_cohort_combined}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/combineAllGvcfs_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess jointGenotyping_gatk {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(gvcf_cohort_combined), path(gvcf_cohort_combined_index) from combined_cohort_gvcf\n\ttuple path(reference_genome_fasta_forJointGenotyping), path(reference_genome_fasta_index_forJointGenotyping), path(reference_genome_fasta_dict_forJointGenotyping) from reference_genome_bundle_forJointGenotyping\n\ttuple path(gatk_bundle_dbsnp138), path(gatk_bundle_dbsnp138_index) from gatk_reference_bundle_forJointGenotyping\n\n\toutput:\n\ttuple path(vcf_joint_genotyped), path(vcf_joint_genotyped_index) into joint_genotyped_vcfs\n\n\tscript:\n\tvcf_joint_genotyped = \"${params.cohort_name}.vcf.gz\"\n\tvcf_joint_genotyped_index = \"${vcf_joint_genotyped}.tbi\"\n\t\"\"\"\n\tgatk GenotypeGVCFs \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--dbsnp \"${gatk_bundle_dbsnp138}\" \\\n\t--reference \"${reference_genome_fasta_forJointGenotyping}\" \\\n\t--standard-min-confidence-threshold-for-calling 50 \\\n\t--annotation-group StandardAnnotation \\\n\t--annotation-group AS_StandardAnnotation \\\n\t--variant \"${gvcf_cohort_combined}\" \\\n\t--output \"${vcf_joint_genotyped}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/jointGenotyping_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess excessHeterozygosityHardFilter_gatk {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(vcf_joint_genotyped), path(vcf_joint_genotyped_index) from joint_genotyped_vcfs\n\n\toutput:\n\ttuple path(vcf_hard_filtered), path(vcf_hard_filtered_index) into hard_filtered_vcfs_forIndelVariantRecalibration, hard_filtered_vcfs_forSnpVariantRecalibration, hard_filtered_vcfs_forApplyVqsr\n\n\tscript:\n\tvcf_hard_filtered_marked = \"${vcf_joint_genotyped}\".replaceFirst(/\\.vcf\\.gz/, \".filtermarked.vcf.gz\")\n\tvcf_hard_filtered = \"${vcf_joint_genotyped}\".replaceFirst(/\\.vcf\\.gz/, \".filtered.vcf.gz\")\n\tvcf_hard_filtered_index = \"${vcf_hard_filtered}.tbi\"\n\t\"\"\"\n\tgatk VariantFiltration \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--filter-name ExcessHet \\\n\t--filter-expression \"ExcessHet > 54.69\" \\\n\t--variant \"${vcf_joint_genotyped}\" \\\n\t--output \"${vcf_hard_filtered_marked}\"\n\n\tgatk SelectVariants \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--exclude-filtered \\\n\t--variant \"${vcf_hard_filtered_marked}\" \\\n\t--output \"${vcf_hard_filtered}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/excessHeterozygosityHardFilter_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess indelVariantRecalibration_gatk {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(vcf_hard_filtered), path(vcf_hard_filtered_index) from hard_filtered_vcfs_forIndelVariantRecalibration\n\ttuple path(gatk_bundle_mills_1000G), path(gatk_bundle_mills_1000G_index), path(gatk_bundle_axiom), path(gatk_bundle_axiom_index), path(gatk_bundle_dbsnp138_forIndelVariantRecalibration), path(gatk_bundle_dbsnp138_index_forIndelVariantRecalibration) from gatk_reference_bundle_forIndelVariantRecalibration\n\ttuple path(reference_genome_fasta_forIndelVariantRecalibration), path(reference_genome_fasta_index_forIndelVariantRecalibration), path(reference_genome_fasta_dict_forIndelVariantRecalibration) from reference_genome_bundle_forIndelVariantRecalibration\n\n\toutput:\n\ttuple path(indel_vqsr_table), path(indel_vqsr_table_index), path(indel_vqsr_tranches) into indel_vqsr_files\n\n\tscript:\n\tindel_vqsr_table = \"${params.cohort_name}.indel.recaldata.table\"\n\tindel_vqsr_table_index = \"${indel_vqsr_table}.idx\"\n\tindel_vqsr_tranches = \"${params.cohort_name}.indel.tranches\"\n\t\"\"\"\n\tgatk VariantRecalibrator \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--mode INDEL \\\n\t-an QD -an DP -an FS -an SOR -an ReadPosRankSum -an MQRankSum \\\n\t--max-gaussians 4 \\\n\t--trust-all-polymorphic \\\n\t--reference \"${reference_genome_fasta_forIndelVariantRecalibration}\" \\\n\t--resource:mills,known=false,training=true,truth=true,prior=12 \"${gatk_bundle_mills_1000G}\" \\\n\t--resource:axiomPoly,known=false,training=true,truth=false,prior=10 \"${gatk_bundle_axiom}\" \\\n\t--resource:dbsnp,known=true,training=false,truth=false,prior=2 \"${gatk_bundle_dbsnp138_forIndelVariantRecalibration}\" \\\n\t--variant \"${vcf_hard_filtered}\" \\\n\t--output \"${indel_vqsr_table}\" \\\n\t--tranches-file \"${indel_vqsr_tranches}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/indelVariantRecalibration_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess snpVariantRecalibration_gatk {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(vcf_hard_filtered), path(vcf_hard_filtered_index) from hard_filtered_vcfs_forSnpVariantRecalibration\n\ttuple path(gatk_bundle_hapmap), path(gatk_bundle_hapmap_index), path(gatk_bundle_1000G_omni), path(gatk_bundle_1000G_omni_index), path(gatk_bundle_1000G_snps), path(gatk_bundle_1000G_snps_index), path(gatk_bundle_dbsnp138_forSnpVariantRecalibration), path(gatk_bundle_dbsnp138_index_forSnpVariantRecalibration) from gatk_reference_bundle_forSnpVariantRecalibration\n\ttuple path(reference_genome_fasta_forSnpVariantRecalibration), path(reference_genome_fasta_index_forSnpVariantRecalibration), path(reference_genome_fasta_dict_forSnpVariantRecalibration) from reference_genome_bundle_forSnpVariantRecalibration\n\n\toutput:\n\ttuple path(snp_vqsr_table), path(snp_vqsr_table_index), path(snp_vqsr_tranches) into snp_vqsr_files\n\n\tscript:\n\tsnp_vqsr_table = \"${params.cohort_name}.snp.recaldata.table\"\n\tsnp_vqsr_table_index = \"${snp_vqsr_table}.idx\"\n\tsnp_vqsr_tranches = \"${params.cohort_name}.snp.tranches\"\t\n\t\"\"\"\n\tgatk VariantRecalibrator \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--mode SNP \\\n\t-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP \\\n\t--max-gaussians 6 \\\n\t--trust-all-polymorphic \\\n\t--reference \"${reference_genome_fasta_forSnpVariantRecalibration}\" \\\n\t--resource:hapmap,known=false,training=true,truth=true,prior=15 \"${gatk_bundle_hapmap}\" \\\n\t--resource:omni,known=false,training=true,truth=true,prior=12 \"${gatk_bundle_1000G_omni}\" \\\n\t--resource:1000G,known=false,training=true,truth=false,prior=10 \"${gatk_bundle_1000G_snps}\" \\\n\t--resource:dbsnp,known=true,training=false,truth=false,prior=7 \"${gatk_bundle_dbsnp138_forSnpVariantRecalibration}\" \\\n\t--variant \"${vcf_hard_filtered}\" \\\n\t--output \"${snp_vqsr_table}\" \\\n\t--tranches-file \"${snp_vqsr_tranches}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/snpVariantRecalibration_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess applyIndelAndSnpVqsr_gatk {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(vcf_hard_filtered), path(vcf_hard_filtered_index) from hard_filtered_vcfs_forApplyVqsr\n\ttuple path(indel_vqsr_table), path(indel_vqsr_table_index), path(indel_vqsr_tranches) from indel_vqsr_files\n\ttuple path(snp_vqsr_table), path(snp_vqsr_table_index), path(snp_vqsr_tranches) from snp_vqsr_files\n\n\toutput:\n\ttuple path(final_vqsr_germline_vcf), path(final_vqsr_germline_vcf_index) into vqsr_germline_vcfs\n\n\tscript:\n\tintermediate_vqsr_germline_vcf = \"${vcf_hard_filtered}\".replaceFirst(/\\.filtered\\.vcf\\.gz/, \".intermediate.vqsr.vcf.gz\")\n\tfinal_vqsr_germline_vcf = \"${vcf_hard_filtered}\".replaceFirst(/\\.filtered\\.vcf\\.gz/, \".final.vqsr.vcf.gz\")\n\tfinal_vqsr_germline_vcf_index = \"${final_vqsr_germline_vcf}.tbi\"\n\t\"\"\"\n\tgatk ApplyVQSR \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--mode INDEL \\\n\t--truth-sensitivity-filter-level 99.0 \\\n\t--exclude-filtered \\\n\t--variant \"${vcf_hard_filtered}\" \\\n\t--recal-file \"${indel_vqsr_table}\" \\\n\t--tranches-file \"${indel_vqsr_tranches}\" \\\n\t--create-output-variant-index true \\\n\t--output \"${intermediate_vqsr_germline_vcf}\"\n\n\tgatk ApplyVQSR \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--mode SNP \\\n\t--truth-sensitivity-filter-level 99.5 \\\n\t--exclude-filtered \\\n\t--variant \"${intermediate_vqsr_germline_vcf}\" \\\n\t--recal-file \"${snp_vqsr_table}\" \\\n\t--tranches-file \"${snp_vqsr_tranches}\" \\\n\t--create-output-variant-index true \\\n\t--output \"${final_vqsr_germline_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/applyIndelAndSnpVqsr_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess splitMultiallelicAndLeftNormalizeVcf_bcftools {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(final_vqsr_germline_vcf), path(final_vqsr_germline_vcf_index) from vqsr_germline_vcfs\n\ttuple path(reference_genome_fasta_forSplitAndNorm), path(reference_genome_fasta_index_forSplitAndNorm), path(reference_genome_fasta_dict_forSplitAndNorm) from reference_genome_bundle_forSplitAndNorm\n\n\toutput:\n\ttuple path(final_germline_vcf), path(final_germline_vcf_index) into final_germline_vcf_forAnnotation, final_germline_vcf_forAdmixture\n\tpath multiallelics_stats\n\tpath realign_normalize_stats\n\n\tscript:\n\tfinal_germline_vcf = \"${final_vqsr_germline_vcf}\".replaceFirst(/\\.final\\.vqsr\\.vcf\\.gz/, \".germline.vcf.gz\")\n\tfinal_germline_vcf_index = \"${final_germline_vcf}.tbi\"\n\tmultiallelics_stats = \"${params.cohort_name}.multiallelicsstats.txt\"\n\trealign_normalize_stats = \"${params.cohort_name}.realignnormalizestats.txt\"\n\t\"\"\"\n\tzcat \"${final_vqsr_germline_vcf}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics -both \\\n\t--output-type z \\\n\t- 2>\"${multiallelics_stats}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--fasta-ref \"${reference_genome_fasta_forSplitAndNorm}\" \\\n\t--output-type z \\\n\t- 2>\"${realign_normalize_stats}\" \\\n\t--output \"${final_germline_vcf}\"\n\n\ttabix \"${final_germline_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/splitMultiallelicAndLeftNormalizeVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess referenceVcfPrep_bcftools {\n\tpublishDir \"references/hg38\", mode: 'copy'\n\n\tinput:\n\tpath per_chromosome_ref_vcf from reference_vcf_1000G_per_chromosome\n\tpath per_chromosome_ref_vcf_index from reference_vcf_1000G_per_chromosome_index.toList()\n\tpath chr_name_conversion_map from reference_vcf_1000G_chr_name_conversion_map\n\n\toutput:\n\ttuple path(whole_genome_ref_vcf), path(whole_genome_ref_vcf_index) into reference_vcf_1000G_fromProcess\n\n\twhen:\n\tparams.ref_vcf_concatenated == \"no\"\n\n\tscript:\n\twhole_genome_ref_vcf = \"ALL.wgs.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz\"\n\twhole_genome_ref_vcf_index = \"ALL.wgs.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz.tbi\"\n\t\"\"\"\n\tbcftools concat \\\n\t--threads ${task.cpus} \\\n\t--output-type z \\\n\t${per_chromosome_ref_vcf.collect { \"$it \"}.join()} \\\n\t| \\\n\tbcftools annotate \\\n\t--threads ${task.cpus} \\\n\t--output-type z \\\n\t--rename-chrs \"${chr_name_conversion_map}\" \\\n\t- > \"${whole_genome_ref_vcf}\"\n\n\ttabix \"${whole_genome_ref_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/referenceVcfPrep_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mergeCohortAndReferenceVcf_bcftools {\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple path(vcf_germline_final), path(vcf_germline_final_index) from final_germline_vcf_forAdmixture\n\ttuple path(whole_genome_ref_vcf), path(whole_genome_ref_vcf_index) from reference_vcf_1000G\n\n\toutput:\n\tpath sample_ref_merged_vcf into merged_unfiltered_vcf\n\n\tscript:\n\tsample_ref_merged_vcf = \"${params.cohort_name}.refmerged.vcf.gz\"\n\tvcf_germline_final_gt_only = \"${vcf_germline_final}\".replaceFirst(/\\.germline\\.vcf\\.gz/, \".germline.gto.vcf.gz\")\n\t\"\"\"\n\tbcftools annotate \\\n\t--threads ${task.cpus} \\\n\t--remove FORMAT \\\n\t--output-type z \\\n\t--output \"${vcf_germline_final_gt_only}\" \\\n\t\"${vcf_germline_final}\"\n\n\ttabix \"${vcf_germline_final_gt_only}\"\n\n\tbcftools merge \\\n\t--threads ${task.cpus} \\\n\t--merge none \\\n\t--missing-to-ref \\\n\t--output-type z \\\n\t--output \"${sample_ref_merged_vcf}\" \\\n\t\"${vcf_germline_final_gt_only}\" \"${whole_genome_ref_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mergeCohortAndReferenceVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess hardFilterCohortReferenceMergedVcf_vcftools {\n\tpublishDir \"${params.output_dir}/germline/${params.cohort_name}\", mode: 'copy', pattern: '*.{stats.txt}'\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\tpath sample_ref_merged_vcf from merged_unfiltered_vcf\n\n\toutput:\n\ttuple val(hard_filtered_plink_file_prefix), path(hard_filtered_plink_ped_file), path(hard_filtered_plink_map_file) into hard_filtered_refmerged_plink_files\n\tpath hard_filtered_stats_file\n\n\tscript:\n\thard_filtered_plink_file_prefix = \"${params.cohort_name}.hardfiltered.refmerged\"\n\thard_filtered_plink_ped_file = \"${hard_filtered_plink_file_prefix}.ped\"\n\thard_filtered_plink_map_file = \"${hard_filtered_plink_file_prefix}.map\"\n\thard_filtered_stats_file = \"${hard_filtered_plink_file_prefix}.stats.txt\"\n\t\"\"\"\n\tvcftools \\\n\t--gzvcf \"${sample_ref_merged_vcf}\" \\\n\t--thin 2000 \\\n\t--min-alleles 2 \\\n\t--max-alleles 2 \\\n\t--non-ref-ac 2 \\\n\t--max-missing 1.0 \\\n\t--plink \\\n\t--temp . \\\n\t--out \"${hard_filtered_plink_file_prefix}\" \\\n\t2>\"${hard_filtered_stats_file}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/hardFilterCohortReferenceMergedVcf_vcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["Admixture"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess ancestryEstimation_admixture {\n\tpublishDir \"${params.output_dir}/germline/${params.cohort_name}\", mode: 'copy'\n\ttag \"${params.cohort_name}\"\n\n\tinput:\n\ttuple val(pruned_filtered_plink_file_prefix), path(pruned_filtered_plink_bed_file), path(pruned_filtered_plink_bim_file), path(pruned_filtered_plink_fam_file) from pruned_filtered_refmerged_plink_files\n\tpath known_ancestry_file_ref_vcf from reference_vcf_1000G_known_ancestry\n\n\toutput:\n\tpath admixture_supervised_analysis_pop_file\n\tpath admixture_ancestry_fractions\n\tpath admixture_allele_frequencies\n\tpath admixture_standard_error\n\n\tscript:\n\tancestry_groups = 26\n\tadmixture_supervised_analysis_pop_file = \"${pruned_filtered_plink_file_prefix}.pop\"\n\tadmixture_ancestry_fractions = \"${pruned_filtered_plink_file_prefix}.${ancestry_groups}.Q\"\n\tadmixture_allele_frequencies = \"${pruned_filtered_plink_file_prefix}.${ancestry_groups}.P\"\n\tadmixture_standard_error = \"${pruned_filtered_plink_file_prefix}.${ancestry_groups}.Q_se\"\n\t\"\"\"\n\tcohort_pop_file_creator.sh \\\n\t\"${pruned_filtered_plink_file_prefix}.fam\" \\\n\t\"${known_ancestry_file_ref_vcf}\" \\\n\t\"${admixture_supervised_analysis_pop_file}\"\n\n\tadmixture \\\n\t-j${task.cpus}\\\n\t-B200 \\\n\t--supervised \\\n\t--haploid=\"male:23\" \\\n\t\"${pruned_filtered_plink_file_prefix}.bed\" \\\n\t\"${ancestry_groups}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/ancestryEstimation_admixture"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess snvAndIndelCalling_varscan {\n\ttag \"${tumor_normal_sample_id} C=${chromosome}\"\n\n\tinput:\n\ttuple path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(reference_genome_fasta_forVarscanSamtoolsMpileup), path(reference_genome_fasta_index_forVarscanSamtoolsMpileup), path(reference_genome_fasta_dict_forVarscanSamtoolsMpileup), path(gatk_bundle_wgs_bed_forVarscanSamtoolsMpileup) from tumor_normal_pair_forVarscanSamtoolsMpileup.combine(reference_genome_bundle_and_bed_forVarscanSamtoolsMpileup)\n\teach chromosome from chromosome_list_forVarscanSamtoolsMpileup\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(raw_per_chromosome_snv_vcf), path(raw_per_chromosome_snv_vcf_index), path(raw_per_chromosome_indel_vcf), path(raw_per_chromosome_indel_vcf_index) into raw_per_chromosome_vcfs_forVarscanBcftools\n\n\twhen:\n\tparams.varscan == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\ttumor_normal_sample_id = \"${tumor_id}_vs_${normal_id}\"\n\traw_per_chromosome_snv_vcf = \"${tumor_normal_sample_id}.${chromosome}.snv.vcf.gz\"\n\traw_per_chromosome_snv_vcf_index = \"${raw_per_chromosome_snv_vcf}.tbi\"\n\traw_per_chromosome_indel_vcf = \"${tumor_normal_sample_id}.${chromosome}.indel.vcf.gz\"\n\traw_per_chromosome_indel_vcf_index = \"${raw_per_chromosome_indel_vcf}.tbi\"\n\t\"\"\"\n\tsamtools mpileup \\\n\t--no-BAQ \\\n\t--min-MQ 1 \\\n\t--positions \"${gatk_bundle_wgs_bed_forVarscanSamtoolsMpileup}\" \\\n\t--region \"${chromosome}\" \\\n\t--fasta-ref \"${reference_genome_fasta_forVarscanSamtoolsMpileup}\" \\\n\t\"${normal_bam}\" \"${tumor_bam}\" \\\n\t| \\\n\tjava -jar \\${VARSCAN} somatic \\\n\t--mpileup 1 \\\n\t--min-coverage-normal 8 \\\n\t--min-coverage-tumor 6 \\\n\t--min-var-freq 0.10 \\\n\t--min-freq-for-hom 0.75 \\\n\t--normal-purity 1.00 \\\n\t--tumor-purity 1.00 \\\n\t--p-value 0.99 \\\n\t--somatic-p-value 0.05 \\\n\t--strand-filter 0 \\\n\t--output-vcf \\\n\t--output-snp \"${tumor_normal_sample_id}.${chromosome}.snv\" \\\n\t--output-indel \"${tumor_normal_sample_id}.${chromosome}.indel\"\n\n\tbgzip < \"${tumor_normal_sample_id}.${chromosome}.snv.vcf\" > \"${raw_per_chromosome_snv_vcf}\"\n\ttabix \"${raw_per_chromosome_snv_vcf}\"\n\n\tbgzip < \"${tumor_normal_sample_id}.${chromosome}.indel.vcf\" > \"${raw_per_chromosome_indel_vcf}\"\n\ttabix \"${raw_per_chromosome_indel_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/snvAndIndelCalling_varscan"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools", "TumorCNV"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess bamReadcountForVarscanFpFilter_bamreadcount {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(high_confidence_snv_vcf), path(high_confidence_snv_vcf_index), path(high_confidence_indel_vcf), path(high_confidence_indel_vcf_index), path(reference_genome_fasta_forVarscanBamReadcount), path(reference_genome_fasta_index_forVarscanBamReadcount), path(reference_genome_fasta_dict_forVarscanBamReadcount) from bams_forVarscanBamReadcount.join(high_confidence_vcfs_forVarscanBamReadcount).combine(reference_genome_bundle_forVarscanBamReadcount)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(snv_readcount_file), path(indel_readcount_file) into readcount_forVarscanFpFilter\n\n\twhen:\n\tparams.varscan == \"on\"\n\n\tscript:\n\tsnv_readcount_file = \"${tumor_normal_sample_id}_bam_readcount_snv.tsv\"\n\tindel_readcount_file = \"${tumor_normal_sample_id}_bam_readcount_indel.tsv\"\n\t\"\"\"\n\tbcftools concat \\\n\t--threads ${task.cpus} \\\n\t--allow-overlaps \\\n\t--output-type z \\\n\t--output \"${tumor_normal_sample_id}.somatic.hc.vcf.gz\" \\\n\t\"${high_confidence_snv_vcf}\" \"${high_confidence_indel_vcf}\"\n\n\ttabix \"${tumor_normal_sample_id}.somatic.hc.vcf.gz\"\n\n\tbam_readcount_helper.py \\\n\t\"${tumor_normal_sample_id}.somatic.hc.vcf.gz\" \\\n\tTUMOR \\\n\t\"${reference_genome_fasta_forVarscanBamReadcount}\" \\\n\t\"${tumor_bam}\" \\\n\t.\n\n\tmv TUMOR_bam_readcount_snv.tsv \"${snv_readcount_file}\"\n\tmv TUMOR_bam_readcount_indel.tsv \"${indel_readcount_file}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/bamReadcountForVarscanFpFilter_bamreadcount"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess splitMultiallelicAndLeftNormalizeVarscanVcf_bcftools {\n\tpublishDir \"${params.output_dir}/somatic/varscan\", mode: 'copy', pattern: '*.{vcf.gz,tbi,txt}'\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(fp_filtered_snv_vcf), path(fp_filtered_indel_vcf), path(reference_genome_fasta_forVarscanBcftoolsNorm), path(reference_genome_fasta_index_forVarscanBcftoolsNorm), path(reference_genome_fasta_dict_forVarscanBcftoolsNorm) from filtered_vcfs_forVarscanBcftools.combine(reference_genome_bundle_forVarscanBcftoolsNorm)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(final_varscan_snv_vcf), path(final_varscan_snv_vcf_index) into final_varscan_snv_vcf_forConsensus\n\ttuple val(tumor_normal_sample_id), path(final_varscan_indel_vcf), path(final_varscan_indel_vcf_index) into final_varscan_indel_vcf_forConsensus\n\tpath varscan_snv_multiallelics_stats\n\tpath varscan_indel_multiallelics_stats\n\tpath varscan_indel_realign_normalize_stats\n\n\twhen:\n\tparams.varscan == \"on\"\n\n\tscript:\n\tfinal_varscan_snv_vcf = \"${tumor_normal_sample_id}.varscan.somatic.snv.vcf.gz\"\n\tfinal_varscan_snv_vcf_index =\"${final_varscan_snv_vcf}.tbi\"\n\tfinal_varscan_indel_vcf = \"${tumor_normal_sample_id}.varscan.somatic.indel.vcf.gz\"\n\tfinal_varscan_indel_vcf_index = \"${final_varscan_indel_vcf}.tbi\"\n\tvarscan_snv_multiallelics_stats = \"${tumor_normal_sample_id}.varscan.snv.multiallelicsstats.txt\"\n\tvarscan_indel_multiallelics_stats = \"${tumor_normal_sample_id}.varscan.indel.multiallelicsstats.txt\"\n\tvarscan_indel_realign_normalize_stats = \"${tumor_normal_sample_id}.varscan.indel.realignnormalizestats.txt\"\n\t\"\"\"\n\tbgzip --stdout < \"${fp_filtered_snv_vcf}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics -snps \\\n\t--output-type z \\\n\t--output \"${final_varscan_snv_vcf}\" \\\n\t- 2>\"${varscan_snv_multiallelics_stats}\"\n\n\ttabix \"${final_varscan_snv_vcf}\"\n\t\n\tbgzip --stdout < \"${fp_filtered_indel_vcf}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics -indels \\\n\t--output-type z \\\n\t- 2>\"${varscan_indel_multiallelics_stats}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--fasta-ref \"${reference_genome_fasta_forVarscanBcftoolsNorm}\" \\\n\t--output-type z \\\n\t--output \"${final_varscan_indel_vcf}\" \\\n\t- 2>\"${varscan_indel_realign_normalize_stats}\"\n\n\ttabix \"${final_varscan_indel_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/splitMultiallelicAndLeftNormalizeVarscanVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mutect2GnomadReferenceVcfPrep_bcftools {\n\tpublishDir \"references/hg38\", mode: 'copy'\n\n\tinput:\n\tpath gnomad_ref_vcf_chromosomes1_9\n\tpath gnomad_ref_vcf_chromosomes1_9_index\n\tpath gnomad_ref_vcf_chromosomes10_22\n\tpath gnomad_ref_vcf_chromosomesXYM_alts\n\tpath gnomad_ref_vcf_chromosomesXYM_alts_index\n\n\toutput:\n\ttuple path(mutect_gnomad_ref_vcf), path(mutect_gnomad_ref_vcf_index) into mutect_gnomad_ref_vcf_fromProcess\n\n\twhen:\n\tparams.mutect == \"on\" && params.mutect_ref_vcf_concatenated == \"no\"\n\n\tscript:\n\tmutect_gnomad_ref_vcf = \"af-only-gnomad.hg38.vcf.gz\"\n\tmutect_gnomad_ref_vcf_index = \"${mutect_gnomad_ref_vcf}.tbi\"\n\t\"\"\"\n\tbcftools concat \\\n\t--threads ${task.cpus} \\\n\t--output-type z \\\n\t--output \"${mutect_gnomad_ref_vcf}\" \\\n\t\"${gnomad_ref_vcf_chromosomes1_9}\" \\\n\t\"${gnomad_ref_vcf_chromosomes10_22}\" \\\n\t\"${gnomad_ref_vcf_chromosomesXYM_alts}\"\n\n\ttabix \"${mutect_gnomad_ref_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mutect2GnomadReferenceVcfPrep_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess snvAndIndelCalling_gatk {\n\ttag \"${tumor_normal_sample_id} C=${chromosome} \"\n\n\tinput:\n\ttuple path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(reference_genome_fasta_forMutectCalling), path(reference_genome_fasta_index_forMutectCalling), path(reference_genome_fasta_dict_forMutectCalling), path(gatk_bundle_wgs_bed_forMutectCalling), path(mutect_gnomad_ref_vcf), path(mutect_gnomad_ref_vcf_index), path(panel_of_normals_1000G), path(panel_of_normals_1000G_index) from tumor_normal_pair_forMutectCalling.combine(reference_genome_bed_and_vcfs_forMutectCalling)\n\teach chromosome from chromosome_list_forMutectCalling\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(raw_per_chromosome_vcf), path(raw_per_chromosome_vcf_index) into raw_per_chromosome_vcfs_forMutectVcfMerge\n\ttuple val(tumor_normal_sample_id), path(raw_per_chromosome_mutect_stats_file) into raw_per_chromosome_mutect_stats_forMutectStatsMerge\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\ttumor_normal_sample_id = \"${tumor_id}_vs_${normal_id}\"\n\tper_chromosome_bed_file = \"${gatk_bundle_wgs_bed_forMutectCalling}\".replaceFirst(/\\.bed/, \".${chromosome}.bed\")\n\traw_per_chromosome_vcf = \"${tumor_normal_sample_id}.${chromosome}.vcf.gz\"\n\traw_per_chromosome_vcf_index = \"${raw_per_chromosome_vcf}.tbi\"\n\traw_per_chromosome_mutect_stats_file = \"${tumor_normal_sample_id}.${chromosome}.vcf.gz.stats\"\n\t\"\"\"\n\tgrep -w '${chromosome}' \"${gatk_bundle_wgs_bed_forMutectCalling}\" > \"${per_chromosome_bed_file}\"\n\n\tgatk Mutect2 \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--native-pair-hmm-threads ${task.cpus} \\\n\t--af-of-alleles-not-in-resource 0.00003125 \\\n\t--seconds-between-progress-updates 600 \\\n\t--reference \"${reference_genome_fasta_forMutectCalling}\" \\\n\t--intervals \"${per_chromosome_bed_file}\" \\\n\t--germline-resource \"${mutect_gnomad_ref_vcf}\" \\\n\t--panel-of-normals \"${panel_of_normals_1000G}\" \\\n\t--input \"${tumor_bam}\" \\\n\t--input \"${normal_bam}\" \\\n\t--normal-sample \"${normal_id}\" \\\n\t--output \"${raw_per_chromosome_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/snvAndIndelCalling_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mergeAndSortMutect2Vcfs_gatk {\n\ttag \"${tumor_normal_sample_id}\"\n\t\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(raw_per_chromosome_vcf), path(raw_per_chromosome_vcf_index) from raw_per_chromosome_vcfs_forMutectVcfMerge.groupTuple()\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(merged_raw_vcf), path(merged_raw_vcf_index) into merged_raw_vcfs_forMutectFilter\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\tmerged_raw_vcf = \"${tumor_normal_sample_id}.vcf.gz\"\n\tmerged_raw_vcf_index = \"${merged_raw_vcf}.tbi\"\n\t\"\"\"\n\tgatk SortVcf \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--TMP_DIR . \\\n\t--MAX_RECORDS_IN_RAM 4000000 \\\n\t${raw_per_chromosome_vcf.collect { \"--INPUT $it \" }.join()} \\\n\t--OUTPUT \"${merged_raw_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mergeAndSortMutect2Vcfs_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mergeMutect2StatsForFiltering_gatk {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(raw_per_chromosome_mutect_stats_file) from raw_per_chromosome_mutect_stats_forMutectStatsMerge.groupTuple()\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(merged_mutect_stats_file) into merged_mutect_stats_file_forMutectFilter\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\tmerged_mutect_stats_file = \"${tumor_normal_sample_id}.vcf.gz.stats\"\n\t\"\"\"\n\tgatk MergeMutectStats \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t${raw_per_chromosome_mutect_stats_file.collect { \"--stats $it \" }.join()} \\\n\t--output \"${merged_mutect_stats_file}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mergeMutect2StatsForFiltering_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess pileupSummariesForMutect2Contamination_gatk {\n\ttag \"${tumor_normal_sample_id} C=${chromosome}\"\n\n\tinput:\n\ttuple path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(gatk_bundle_wgs_bed_forMutectPileup), path(exac_common_sites_ref_vcf), path(exac_common_sites_ref_vcf_index) from tumor_normal_pair_forMutectPileup.combine(bed_and_resources_vcfs_forMutectPileup)\n\teach chromosome from chromosome_list_forMutectPileup\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(per_chromosome_tumor_pileup) into per_chromosome_tumor_pileups_forMutectPileupGather\n\ttuple val(tumor_normal_sample_id), path(per_chromosome_normal_pileup) into per_chromosome_normal_pileups_forMutectPileupGather\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\ttumor_normal_sample_id = \"${tumor_id}_vs_${normal_id}\"\n\tper_chromosome_bed_file = \"${gatk_bundle_wgs_bed_forMutectPileup}\".replaceFirst(/\\.bed/, \".${chromosome}.bed\")\n\tper_chromosome_tumor_pileup = \"${tumor_id}.${chromosome}.pileup\"\n\tper_chromosome_normal_pileup = \"${normal_id}.${chromosome}.pileup\"\n\t\"\"\"\n\tgrep -w '${chromosome}' \"${gatk_bundle_wgs_bed_forMutectPileup}\" > \"${per_chromosome_bed_file}\"\n\n\tgatk GetPileupSummaries \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--intervals \"${per_chromosome_bed_file}\" \\\n\t--variant \"${exac_common_sites_ref_vcf}\" \\\n\t--input \"${tumor_bam}\" \\\n\t--output \"${per_chromosome_tumor_pileup}\"\n\n\tgatk GetPileupSummaries \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--intervals \"${per_chromosome_bed_file}\" \\\n\t--variant \"${exac_common_sites_ref_vcf}\" \\\n\t--input \"${normal_bam}\" \\\n\t--output \"${per_chromosome_normal_pileup}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/pileupSummariesForMutect2Contamination_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess gatherTumorPileupSummariesForMutect2Contamination_gatk {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(per_chromosome_tumor_pileup), path(reference_genome_fasta_dict) from per_chromosome_tumor_pileups_forMutectPileupGather.groupTuple().combine(reference_genome_fasta_dict_forMutectPileupGatherTumor)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(tumor_pileup) into tumor_pileups_forMutectContamination\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_normal_sample_id}\".replaceFirst(/\\_vs\\_.*/, \"\")\n\ttumor_pileup = \"${tumor_id}.pileup\"\n\t\"\"\"\n\tgatk GatherPileupSummaries \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--sequence-dictionary \"${reference_genome_fasta_dict}\" \\\n\t${per_chromosome_tumor_pileup.collect { \"--I $it \" }.join()} \\\n\t--O \"${tumor_pileup}\"\n\t\"\"\"\n}", "\nprocess gatherNormalPileupSummariesForMutect2Contamination_gatk {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(per_chromosome_normal_pileup), path(reference_genome_fasta_dict) from per_chromosome_normal_pileups_forMutectPileupGather.groupTuple().combine(reference_genome_fasta_dict_forMutectPileupGatherNormal)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(normal_pileup) into normal_pileups_forMutectContamination\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\tnormal_id = \"${tumor_normal_sample_id}\".replaceFirst(/.*\\_vs\\_/, \"\")\n\tnormal_pileup = \"${normal_id}.pileup\"\n\t\"\"\"\n\tgatk GatherPileupSummaries \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--sequence-dictionary \"${reference_genome_fasta_dict}\" \\\n\t${per_chromosome_normal_pileup.collect { \"--I $it \" }.join()} \\\n\t--O \"${normal_pileup}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/gatherTumorPileupSummariesForMutect2Contamination_gatk", "pblaney/mgp1000/gatherNormalPileupSummariesForMutect2Contamination_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mutect2ContaminationCalculation_gatk {\n\tpublishDir \"${params.output_dir}/somatic/mutect\", mode: 'copy', pattern: '*.{txt}'\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(tumor_pileup), path(normal_pileup) from tumor_pileups_forMutectContamination.join(normal_pileups_forMutectContamination)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(mutect_contamination_file) into contamination_file_forMutectFilter, mutect_output_forConsensusMetadata\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\tmutect_contamination_file = \"${tumor_normal_sample_id}.mutect.contamination.txt\" \n\t\"\"\"\n\tgatk CalculateContamination \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--input \"${tumor_pileup}\" \\\n\t--matched-normal \"${normal_pileup}\" \\\n\t--output \"${mutect_contamination_file}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mutect2ContaminationCalculation_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mutect2VariantFiltration_gatk {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(merged_raw_vcf), path(merged_raw_vcf_index), path(merged_mutect_stats_file), path(mutect_contamination_file), path(reference_genome_fasta_forMutectFilter), path(reference_genome_fasta_index_forMutectFilter), path(reference_genome_fasta_dict_forMutectFilter) from input_vcf_stats_and_contamination_forMutectFilter.combine(reference_genome_bundle_forMutectFilter)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(filtered_vcf), path(filtered_vcf_index) into filtered_vcf_forMutectBcftools\n\tpath filter_stats_file\n\n\twhen:\n\tparams.mutect == \"on\"\n\n\tscript:\n\tfiltered_vcf = \"${tumor_normal_sample_id}.filtered.vcf.gz\"\n\tfiltered_vcf_index = \"${filtered_vcf}.tbi\"\n\tfilter_stats_file = \"${tumor_normal_sample_id}.filterstats.txt\"\n\t\"\"\"\n\tgatk FilterMutectCalls \\\n\t--java-options \"-Xmx${task.memory.toGiga()}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--unique-alt-read-count 5 \\\n\t--reference \"${reference_genome_fasta_forMutectFilter}\" \\\n\t--stats \"${merged_mutect_stats_file}\" \\\n\t--variant \"${merged_raw_vcf}\" \\\n\t--contamination-table \"${mutect_contamination_file}\" \\\n\t--output \"${filtered_vcf}\" \\\n\t--filtering-stats \"${filter_stats_file}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mutect2VariantFiltration_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess bamMpileupForControlFreec_samtools {\n\ttag \"${tumor_normal_sample_id} C=${chromosome}\"\n\n\tinput:\n\ttuple path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(reference_genome_fasta_forControlFreecSamtoolsMpileup), path(reference_genome_fasta_index_forControlFreecSamtoolsMpileup), path(reference_genome_fasta_dict_forControlFreecSamtoolsMpileup), path(gatk_bundle_wgs_bed_forControlFreecSamtoolsMpileup) from tumor_normal_pair_forControlFreecSamtoolsMpileup.combine(reference_genome_bundle_and_bed_forControlFreecSamtoolsMpileup)\n\teach chromosome from chromosome_list_forControlFreecSamtoolsMpileup\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), val(tumor_id), val(normal_id), path(tumor_pileup_per_chromosome), path(normal_pileup_per_chromosome) into per_chromosome_tumor_normal_pileups_forControlFreecMerge\n\n\twhen:\n\tparams.controlfreec == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\ttumor_normal_sample_id = \"${tumor_id}_vs_${normal_id}\"\n\ttumor_pileup_per_chromosome = \"${tumor_id}.${chromosome}.pileup.gz\"\n\tnormal_pileup_per_chromosome = \"${normal_id}.${chromosome}.pileup.gz\"\n\t\"\"\"\n\tsamtools mpileup \\\n\t--positions \"${gatk_bundle_wgs_bed_forControlFreecSamtoolsMpileup}\" \\\n\t--fasta-ref \"${reference_genome_fasta_forControlFreecSamtoolsMpileup}\" \\\n\t--region \"${chromosome}\" \\\n\t\"${tumor_bam}\" \\\n\t| \\\n\tbgzip > \"${tumor_pileup_per_chromosome}\"\n\n\tsamtools mpileup \\\n\t--positions \"${gatk_bundle_wgs_bed_forControlFreecSamtoolsMpileup}\" \\\n\t--fasta-ref \"${reference_genome_fasta_forControlFreecSamtoolsMpileup}\" \\\n\t--region \"${chromosome}\" \\\n\t\"${normal_bam}\" \\\n\t| \\\n\tbgzip > \"${normal_pileup_per_chromosome}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/bamMpileupForControlFreec_samtools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["MSClust"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess bamprocessPerChromosome_sclust {\n\ttag \"${tumor_normal_sample_id} C=${chromosome}\"\n\n\tinput:\n\ttuple path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index) from tumor_normal_pair_forSclustBamprocess\n\teach chromosome from chromosome_list_forSclustBamprocess\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(bamprocess_data_per_chromosome) into per_chromosome_bamprocess_data\n\n\twhen:\n\tparams.sclust == \"on\" && params.mutect == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\ttumor_normal_sample_id = \"${tumor_id}_vs_${normal_id}\"\n\tbamprocess_data_per_chromosome = \"${tumor_normal_sample_id}_${chromosome}_bamprocess_data.txt\"\n\t\"\"\"\n\tSclust bamprocess \\\n\t-t \"${tumor_bam}\" \\\n\t-n \"${normal_bam}\" \\\n\t-o \"${tumor_normal_sample_id}\" \\\n\t-build hg38 \\\n\t-part 2 \\\n\t-r \"${chromosome}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/bamprocessPerChromosome_sclust"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["MSClust"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mergeBamprocessData_sclust {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(bamprocess_data_per_chromosome) from per_chromosome_bamprocess_data.groupTuple()\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(read_count_file), path(common_snp_count_file) into read_count_and_snp_count_files\n\n\twhen:\n\tparams.sclust == \"on\" && params.mutect == \"on\"\n\n\tscript:\n\tread_count_file = \"${tumor_normal_sample_id}_rcount.txt\"\n\tcommon_snp_count_file = \"${tumor_normal_sample_id}_snps.txt\"\n\t\"\"\"\n\tSclust bamprocess \\\n\t-build hg38 \\\n\t-i \"${tumor_normal_sample_id}\" \\\n\t-o \"${tumor_normal_sample_id}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mergeBamprocessData_sclust"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["MSClust"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mutationalClustering_sclust {\n\tpublishDir \"${params.output_dir}/somatic/sclust\", mode: 'copy', pattern: '*.{txt,pdf}'\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(read_count_file), path(common_snp_count_file), path(sclust_allelic_states_file), path(sclust_subclones_file), path(sclust_cnv_summary_file), path(mutations_exp_af_file), path(sclust_cnv_segments_file), path(mutations_vcf) from sclust_cn_output_forClustering.join(vcf_forSclustClustering)\n\n\toutput:\n\tpath mutation_clusters_file\n\tpath mutation_clusters_pdf\n\tpath cluster_assignment_file\n\n\twhen:\n\tparams.sclust == \"on\" && params.mutect == \"on\" && params.sclust_mutclustering == \"on\"\n\n\tscript:\n\tsclust_lambda = params.sclust_lambda ? \"-lambda ${params.sclust_lambda}\" : \"\"\n\tmutation_clusters_file = \"${tumor_normal_sample_id}.sclust.mutclusters.txt\"\n\tmutation_clusters_pdf = \"${tumor_normal_sample_id}.sclust.mutclusters.pdf\"\n\tcluster_assignment_file = \"${tumor_normal_sample_id}.sclust.clusterassignments.txt\"\n\t\"\"\"\n\tgunzip -f \"${mutations_vcf}\"\n\t\n\tSclust cluster \\\n\t-i \"${tumor_normal_sample_id}\" \\\n\t${sclust_lambda}\n\n\tmv \"${tumor_normal_sample_id}_mclusters.txt\" \"${mutation_clusters_file}\"\n\tmv \"${tumor_normal_sample_id}_mcluster.pdf\" \"${mutation_clusters_pdf}\"\n\tmv \"${tumor_normal_sample_id}_cluster_assignments.txt\" \"${cluster_assignment_file}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mutationalClustering_sclust"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess filterAndPostprocessMantaVcf_bcftools {\n    tag \"${tumor_normal_sample_id}\"\n\n    input:\n    tuple val(tumor_normal_sample_id), val(tumor_id), val(normal_id), path(manta_somatic_sv_vcf), path(manta_somatic_sv_vcf_index) from manta_sv_vcf_forPostprocessing\n\n    output:\n    tuple val(tumor_normal_sample_id), val(tumor_id), path(final_manta_somatic_sv_vcf) into manta_sv_vcf_forSurvivor\n    tuple val(tumor_normal_sample_id), path(final_manta_somatic_sv_read_support) into manta_sv_read_support_forAnnotation\n\n    when:\n    params.manta == \"on\"\n\n    script:\n    final_manta_somatic_sv_vcf = \"${tumor_normal_sample_id}.manta.somatic.sv.vcf\"\n    final_manta_somatic_sv_read_support = \"${tumor_normal_sample_id}.manta.somatic.sv.readsupp.txt\"\n    \"\"\"\n    touch name.txt\n    echo \"${normal_id}\" >> name.txt\n\n    bcftools filter \\\n    --output-type v \\\n    --exclude 'FORMAT/SR[@name.txt:1]>2 || FORMAT/PR[@name.txt:1]>2' \\\n    \"${manta_somatic_sv_vcf}\" \\\n    | \\\n    bcftools view \\\n    --output-type v \\\n    --samples \"${tumor_id}\" \\\n    --output-file \"${final_manta_somatic_sv_vcf}\"\n\n    bcftools query \\\n    --format '%ID\\t[%PR{1}]\\t[%SR{1}]\\n' \\\n    --output \"${final_manta_somatic_sv_read_support}\" \\\n    \"${final_manta_somatic_sv_vcf}\"\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/filterAndPostprocessMantaVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess splitMultiallelicAndLeftNormalizeStrelkaVcf_bcftools {\n\tpublishDir \"${params.output_dir}/somatic/strelka\", mode: 'copy', pattern: '*.{vcf.gz,tbi,txt}'\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(unfiltered_strelka_snv_vcf), path(unfiltered_strelka_snv_vcf_index), path(unfiltered_strelka_indel_vcf), path(unfiltered_strelka_indel_vcf_index), path(reference_genome_fasta_forStrelkaBcftools), path(reference_genome_fasta_index_forStrelkaBcftools), path(reference_genome_fasta_dict_forStrelkaBcftools) from unfiltered_snv_and_indel_vcfs_forStrelkaBcftools.combine(reference_genome_bundle_forStrelkaBcftools)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(final_strelka_snv_vcf), path(final_strelka_snv_vcf_index) into final_strelka_snv_vcf_forConsensus\n\ttuple val(tumor_normal_sample_id), path(final_strelka_indel_vcf), path(final_strelka_indel_vcf_index) into final_strelka_indel_vcf_forConsensus\n\tpath strelka_snv_multiallelics_stats\n\tpath strelka_indel_multiallelics_stats\n\tpath strelka_indel_realign_normalize_stats\n\n\twhen:\n\tparams.strelka == \"on\" && params.manta == \"on\"\n\n\tscript:\n\tfinal_strelka_snv_vcf = \"${tumor_normal_sample_id}.strelka.somatic.snv.vcf.gz\"\n\tfinal_strelka_snv_vcf_index =\"${final_strelka_snv_vcf}.tbi\"\n\tfinal_strelka_indel_vcf = \"${tumor_normal_sample_id}.strelka.somatic.indel.vcf.gz\"\n\tfinal_strelka_indel_vcf_index = \"${final_strelka_indel_vcf}.tbi\"\n\tstrelka_snv_multiallelics_stats = \"${tumor_normal_sample_id}.strelka.snv.multiallelicsstats.txt\"\n\tstrelka_indel_multiallelics_stats = \"${tumor_normal_sample_id}.strelka.indel.multiallelicsstats.txt\"\n\tstrelka_indel_realign_normalize_stats = \"${tumor_normal_sample_id}.strelka.indel.realignnormalizestats.txt\"\n\t\"\"\"\n\tzgrep -E \"^#|PASS\" \"${unfiltered_strelka_snv_vcf}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics -snps \\\n\t--output-type z \\\n\t--output \"${final_strelka_snv_vcf}\" \\\n\t- 2>\"${strelka_snv_multiallelics_stats}\"\n\n\ttabix \"${final_strelka_snv_vcf}\"\n\t\n\tzgrep -E \"^#|PASS\" \"${unfiltered_strelka_indel_vcf}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics -indels \\\n\t--output-type z \\\n\t- 2>\"${strelka_indel_multiallelics_stats}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--fasta-ref \"${reference_genome_fasta_forStrelkaBcftools}\" \\\n\t--output-type z \\\n\t--output \"${final_strelka_indel_vcf}\" \\\n\t- 2>\"${strelka_indel_realign_normalize_stats}\"\n\n\ttabix \"${final_strelka_indel_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/splitMultiallelicAndLeftNormalizeStrelkaVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess filterAndPostprocessSvabaVcf_bcftools {\n    tag \"${tumor_normal_sample_id}\"\n\n    input:\n    tuple val(tumor_normal_sample_id), val(tumor_id), path(svaba_somatic_sv_vcf), path(svaba_somatic_sv_vcf_index), path(svaba_somatic_sv_unclassified_vcf), path(sample_renaming_file) from svaba_sv_vcf_forPostprocessing\n\n    output:\n    tuple val(tumor_normal_sample_id), val(tumor_id), path(final_svaba_somatic_sv_vcf) into svaba_sv_vcf_forSurvivor\n    tuple val(tumor_normal_sample_id), path(final_svaba_somatic_sv_read_support) into svaba_sv_read_support_forAnnotation\n\n    when:\n    params.svaba == \"on\"\n\n    script:\n    final_svaba_somatic_sv_vcf = \"${tumor_normal_sample_id}.svaba.somatic.sv.vcf\"\n    final_svaba_somatic_sv_read_support = \"${tumor_normal_sample_id}.svaba.somatic.sv.readsupp.txt\"\n    \"\"\"\n    bcftools filter \\\n    --output-type v \\\n    --exclude 'QUAL<6' \\\n    \"${svaba_somatic_sv_vcf}\" \\\n    | \\\n    bcftools filter \\\n    --output-type v \\\n    --include 'INFO/MAPQ=60 || INFO/DISC_MAPQ=60' \\\n    | \\\n    bcftools reheader \\\n    --samples \"${sample_renaming_file}\" \\\n    | \\\n    bcftools view \\\n    --output-type v \\\n    --samples \"${tumor_id}\" \\\n    --output-file \"${final_svaba_somatic_sv_vcf}\"\n\n    svaba_interchromosomal_mate_finder.sh \\\n    \"${final_svaba_somatic_sv_vcf}\" \\\n    \"${svaba_somatic_sv_unclassified_vcf}\" > \"${tumor_normal_sample_id}.svaba.missingmates.txt\"\n\n    cat \"${tumor_normal_sample_id}.svaba.missingmates.txt\" >> \"${final_svaba_somatic_sv_vcf}\"\n\n    bcftools query \\\n    --format '%ID\\t[%DR]\\t[%SR]\\n' \\\n    --output \"${final_svaba_somatic_sv_read_support}\" \\\n    \"${final_svaba_somatic_sv_vcf}\"\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/filterAndPostprocessSvabaVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess leftNormalizeSvabaVcf_bcftools {\n\tpublishDir \"${params.output_dir}/somatic/svaba\", mode: 'copy', pattern: '*.{vcf.gz,tbi,txt}'\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(filtered_somatic_indel_vcf), path(filtered_somatic_indel_vcf_index), path(reference_genome_fasta_forSvabaBcftools), path(reference_genome_fasta_index_forSvabaBcftools), path(reference_genome_fasta_dict_forSvabaBcftools) from filtered_indel_vcf_forSvabaBcftools.combine(reference_genome_bundle_forSvabaBcftools)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(final_svaba_indel_vcf), path(final_svaba_indel_vcf_index) into final_svaba_indel_vcf_forConsensus\n\tpath svaba_realign_normalize_stats\n\n\twhen:\n\tparams.svaba == \"on\"\n\n\tscript:\n\tfinal_svaba_indel_vcf = \"${tumor_normal_sample_id}.svaba.somatic.indel.vcf.gz\"\n\tfinal_svaba_indel_vcf_index = \"${final_svaba_indel_vcf}.tbi\"\n\tsvaba_realign_normalize_stats = \"${tumor_normal_sample_id}.svaba.realignnormalizestats.txt\"\n\t\"\"\"\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--fasta-ref \"${reference_genome_fasta_forSvabaBcftools}\" \\\n\t--output-type z \\\n\t--output \"${final_svaba_indel_vcf}\" \\\n\t\"${filtered_somatic_indel_vcf}\" 2>\"${svaba_realign_normalize_stats}\"\n\n\ttabix \"${final_svaba_indel_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/leftNormalizeSvabaVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools", "Delly2"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess svAndIndelCalling_delly {\n\tpublishDir \"${params.output_dir}/somatic/delly\", mode: 'copy', pattern: '*.{vcf.gz,tbi}'\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(reference_genome_fasta_forDelly), path(reference_genome_fasta_index_forDelly), path(reference_genome_fasta_dict_forDelly), path(gatk_bundle_wgs_bed_blacklist_0based_forDelly) from tumor_normal_pair_forDelly.combine(reference_genome_and_blacklist_bundle_forDelly)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), val(tumor_id), path(delly_somatic_sv_vcf), path(delly_somatic_sv_vcf_index) into delly_sv_vcf_forPostprocessing\n\tpath delly_germline_sv_vcf\n\tpath delly_germline_sv_vcf_index\n\n\twhen:\n\tparams.delly == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\ttumor_normal_sample_id = \"${tumor_id}_vs_${normal_id}\"\n\tdelly_somatic_sv_vcf = \"${tumor_normal_sample_id}.delly.somatic.sv.unprocessed.vcf.gz\"\n\tdelly_somatic_sv_vcf_index = \"${delly_somatic_sv_vcf}.tbi\"\n\tdelly_germline_sv_vcf = \"${tumor_normal_sample_id}.delly.germline.sv.vcf.gz\"\n\tdelly_germline_sv_vcf_index = \"${delly_germline_sv_vcf}.tbi\"\n\t\"\"\"\n\tdelly call \\\n\t--genome \"${reference_genome_fasta_forDelly}\" \\\n\t--exclude \"${gatk_bundle_wgs_bed_blacklist_0based_forDelly}\" \\\n\t--outfile \"${tumor_normal_sample_id}.delly.sv.unfiltered.bcf\" \\\n\t\"${tumor_bam}\" \"${normal_bam}\"\n\n\ttouch samples.tsv\n\techo \"${tumor_id}\\ttumor\" >> samples.tsv\n\techo \"${normal_id}\\tcontrol\" >> samples.tsv\n\n\tdelly filter \\\n\t--filter somatic \\\n\t--pass \\\n\t--altaf 0.1 \\\n\t--minsize 51 \\\n\t--coverage 10 \\\n\t--samples samples.tsv \\\n\t--outfile \"${tumor_normal_sample_id}.delly.somatic.sv.unprocessed.bcf\" \\\n\t\"${tumor_normal_sample_id}.delly.sv.unfiltered.bcf\"\n\n\tbcftools isec \\\n\t--nfiles 1 \\\n\t--complement \\\n\t--write 1 \\\n\t--output-type v \\\n\t\"${tumor_normal_sample_id}.delly.sv.unfiltered.bcf\" \\\n\t\"${tumor_normal_sample_id}.delly.somatic.sv.unprocessed.bcf\" \\\n\t| \\\n\tbcftools filter \\\n\t--output-type v \\\n\t--include 'FILTER=\"PASS\"' \\\n\t| \\\n\tbgzip > \"${delly_germline_sv_vcf}\"\n\n\ttabix \"${delly_germline_sv_vcf}\"\n\n\tbcftools view \\\n\t--output-type z \\\n\t\"${tumor_normal_sample_id}.delly.somatic.sv.unprocessed.bcf\" > \"${delly_somatic_sv_vcf}\"\n\n\ttabix \"${delly_somatic_sv_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/svAndIndelCalling_delly"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess filterAndPostprocessDellyVcf_bcftools {\n    tag \"${tumor_normal_sample_id}\"\n\n    input:\n    tuple val(tumor_normal_sample_id), val(tumor_id), path(delly_somatic_sv_vcf), path(delly_somatic_sv_vcf_index) from delly_sv_vcf_forPostprocessing\n\n    output:\n    tuple val(tumor_normal_sample_id), val(tumor_id), path(final_delly_somatic_sv_vcf) into delly_sv_vcf_forSurvivor\n    tuple val(tumor_normal_sample_id), path(final_delly_somatic_sv_read_support) into delly_sv_read_support_forAnnotation\n\n    when:\n    params.delly == \"on\"\n\n    script:\n    final_delly_somatic_sv_vcf = \"${tumor_normal_sample_id}.delly.somatic.sv.vcf\"\n    final_delly_somatic_sv_read_support = \"${tumor_normal_sample_id}.delly.somatic.sv.readsupp.txt\"\n    \"\"\"\n    bcftools filter \\\n    --output-type v \\\n    --include 'INFO/MAPQ=60 || INFO/SRMAPQ=60' \\\n    \"${delly_somatic_sv_vcf}\" \\\n    | \\\n    bcftools filter \\\n    --output-type v \\\n    --include 'INFO/PE>3 || INFO/SR>3' \\\n    | \\\n    bcftools view \\\n    --output-type v \\\n    --samples \"${tumor_id}\" \\\n    --output-file \"${final_delly_somatic_sv_vcf}\"\n\n    touch \"${tumor_normal_sample_id}.delly.splitmates.txt\"\n    delly_interchromosomal_record_splitter.sh \\\n    \"${final_delly_somatic_sv_vcf}\" > \"${tumor_normal_sample_id}.delly.splitmates.txt\"\n\n    cat \"${tumor_normal_sample_id}.delly.splitmates.txt\" >> \"${final_delly_somatic_sv_vcf}\"\n\n    bcftools query \\\n    --format '%ID\\t%PE\\t%SR\\n' \\\n    --output \"${final_delly_somatic_sv_read_support}\" \\\n    \"${final_delly_somatic_sv_vcf}\"\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/filterAndPostprocessDellyVcf_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 2, "tools": ["Salmon", "BCFtools", "Dindel"], "nb_own": 2, "list_own": ["pblaney", "pditommaso"], "nb_wf": 2, "list_wf": ["mgp1000", "nf-course-unimi-2021"], "list_contrib": ["pblaney", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess consensusSnvMpileup_bcftools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(consensus_somatic_snv_nosamples_badheader_noformat_vcf), path(snv_vcf_base_header), path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(reference_genome_fasta_forConsensusSnvMpileup), path(reference_genome_fasta_index_forConsensusSnvMpileup), path(reference_genome_fasta_dict_forConsensusSnvMpileup) from consensus_snv_vcf_forConsensusSnvMpileup.join(bams_forConsensusSnvMpileup).combine(reference_genome_bundle_forConsensusSnvMpileup)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), val(tumor_id), val(normal_id), path(mpileup_supported_consensus_somatic_snv_nosamples_noformat_vcf) into consensus_snv_vcf_forAddSamples\n\ttuple val(tumor_normal_sample_id), path(snv_mpileup_info_dp_metrics), path(snv_mpileup_normal_format_metrics), path(snv_mpileup_normal_format_metrics_index), path(snv_mpileup_tumor_format_metrics), path(snv_mpileup_tumor_format_metrics_index) into consensus_snv_mpileup_metrics_forAddFormat\n\n\twhen:\n\tparams.varscan == \"on\" && params.mutect == \"on\" && params.strelka == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tfull_snv_vcf_header = \"full_snv_vcf_header.txt\"\n\tmpileup_supported_consensus_somatic_snv_nosamples_noformat_vcf = \"${tumor_normal_sample_id}.ms.consensus.somatic.snv.nosamples.noformat.vcf\"\n\tsnv_mpileup_info_dp_metrics = \"${tumor_normal_sample_id}.snv.mpileup.info.dp.metrics.txt\"\n\tsnv_mpileup_normal_format_metrics = \"${tumor_normal_sample_id}.snv.mpileup.normal.format.metrics.txt.gz\"\n\tsnv_mpileup_normal_format_metrics_index = \"${snv_mpileup_normal_format_metrics}.tbi\"\n\tsnv_mpileup_tumor_format_metrics = \"${tumor_normal_sample_id}.snv.mpileup.tumor.format.metrics.txt.gz\"\n\tsnv_mpileup_tumor_format_metrics_index = \"${snv_mpileup_tumor_format_metrics}.tbi\"\n\t\"\"\"\n\tbcftools mpileup \\\n\t--no-BAQ \\\n\t--max-depth 5000 \\\n\t--threads ${task.cpus} \\\n\t--fasta-ref \"${reference_genome_fasta_forConsensusSnvMpileup}\" \\\n\t--regions-file \"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}\" \\\n\t--samples ${normal_id},${tumor_id} \\\n\t--annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP \\\n\t\"${normal_bam}\" \"${tumor_bam}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics - \\\n\t--fasta-ref \"${reference_genome_fasta_forConsensusSnvMpileup}\" \\\n\t- \\\n\t| \\\n\tbcftools filter \\\n\t--exclude 'FORMAT/DP == 0' \\\n\t- \\\n\t| \\\n\tgrep -Ev '<.>|INDEL;' \\\n\t| \\\n\tbgzip > \"${tumor_normal_sample_id}.consensus.somatic.snv.mpileup.vcf.gz\"\n\ttabix \"${tumor_normal_sample_id}.consensus.somatic.snv.mpileup.vcf.gz\"\n\n\tbgzip < \"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}\" > \"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}.gz\"\n\ttabix \"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}.gz\"\n\n\ttouch \"${full_snv_vcf_header}\"\n\tcat \"${snv_vcf_base_header}\" >> \"${full_snv_vcf_header}\"\n\tzgrep '##FILTER=<ID=LOWSUPPORT' \"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}.gz\" >> \"${full_snv_vcf_header}\"\n\tzgrep '##INFO=' \"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}.gz\" >> \"${full_snv_vcf_header}\"\n\techo -e '#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO' >> \"${full_snv_vcf_header}\"\n\n\tbcftools isec \\\n\t--nfiles =2 \\\n\t--write 1 \\\n\t\"${consensus_somatic_snv_nosamples_badheader_noformat_vcf}.gz\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.snv.mpileup.vcf.gz\" \\\n\t| \\\n\tbcftools reheader \\\n\t--header \"${full_snv_vcf_header}\" \\\n\t--output \"${mpileup_supported_consensus_somatic_snv_nosamples_noformat_vcf}\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t%REF\\t%ALT\\t%INFO/DP\\n' \\\n\t--output \"${snv_mpileup_info_dp_metrics}\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.snv.mpileup.vcf.gz\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t%REF\\t%ALT\\t[%DP]\\t[%AD]\\t[%ADF]\\t[%ADR]\\n' \\\n\t--samples \"${normal_id}\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.snv.mpileup.vcf.gz\" \\\n\t| \\\n\tbgzip > \"${snv_mpileup_normal_format_metrics}\"\n\ttabix -s1 -b2 -e2 \"${snv_mpileup_normal_format_metrics}\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t%REF\\t%ALT\\t[%DP]\\t[%AD]\\t[%ADF]\\t[%ADR]\\n' \\\n\t--samples \"${tumor_id}\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.snv.mpileup.vcf.gz\" \\\n\t| \\\n\tbgzip > \"${snv_mpileup_tumor_format_metrics}\"\n\ttabix -s1 -b2 -e2 \"${snv_mpileup_tumor_format_metrics}\"\n\t\"\"\"\n}", "\nprocess quantification {\n     \n    input:\n    path index from index_ch\n    tuple val(pair_id), path(reads) from read_pairs_ch\n \n    output:\n    path(pair_id) into quant_ch\n \n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/consensusSnvMpileup_bcftools", "pditommaso/nf-course-unimi-2021/quantification"], "list_wf_names": ["pditommaso/nf-course-unimi-2021", "pblaney/mgp1000"]}, {"nb_reuse": 2, "tools": ["BCFtools", "MultiQC"], "nb_own": 2, "list_own": ["pblaney", "pditommaso"], "nb_wf": 2, "list_wf": ["mgp1000", "nf-course-unimi-2021"], "list_contrib": ["pblaney", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess annotateConsensusSnvVcfFormatColumnAndFilter_bcftools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), val(tumor_id), val(normal_id), path(mpileup_supported_consensus_somatic_snv_noformat_vcf), path(snv_mpileup_info_dp_metrics), path(snv_mpileup_normal_format_metrics), path(snv_mpileup_normal_format_metrics_index), path(snv_mpileup_tumor_format_metrics), path(snv_mpileup_tumor_format_metrics_index) from consensus_snv_vcf_forAddFormat.join(consensus_snv_mpileup_metrics_forAddFormat)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(snv_consensus_vcf), path(snv_consensus_vcf_index), path(snv_strand_metrics) into consensus_snv_forBedFilters\n\n\twhen:\n\tparams.varscan == \"on\" && params.mutect == \"on\" && params.strelka == \"on\"\n\n\tscript:\n\tsnv_consensus_vcf_info_header = \"snv_consensus_vcf_info_header.txt\"\n\tsnv_consensus_vcf_format_headers = \"snv_consensus_vcf_format_header.txt\"\n\tsnv_consensus_vcf = \"${tumor_normal_sample_id}.consensus.somatic.snv.vcf.gz\"\n\tsnv_consensus_vcf_index = \"${snv_consensus_vcf}.tbi\"\n\tsnv_strand_metrics = \"${tumor_normal_sample_id}.snv.strand.metrics.txt\"\n\t\"\"\"\n\tcat \"${snv_mpileup_info_dp_metrics}\" \\\n\t| \\\n\tpaste - <(zcat \"${snv_mpileup_tumor_format_metrics}\" | cut -f 6 | awk '{split(\\$0,x,\",\"); print x[2]}') > \"${tumor_normal_sample_id}.snv.mpileup.info.dp.ac.metrics.txt\"\n\n\tcat \"${tumor_normal_sample_id}.snv.mpileup.info.dp.ac.metrics.txt\" \\\n\t| \\\n\tpaste - <(zcat \"${snv_mpileup_tumor_format_metrics}\" | cut -f 5) \\\n\t| \\\n\tawk 'BEGIN {OFS=\"\\t\"} {print \\$1,\\$2,\\$3,\\$4,\\$5,\\$6,\\$6/\\$7}' \\\n\t| \\\n\tbgzip > \"${tumor_normal_sample_id}.snv.mpileup.info.metrics.txt.gz\"\n\ttabix -s1 -b2 -e2 \"${tumor_normal_sample_id}.snv.mpileup.info.metrics.txt.gz\"\n\n\ttouch \"${snv_consensus_vcf_info_header}\"\n\techo '##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total read depth across samples (normal sample DP + tumor sample DP)\">' >> \"${snv_consensus_vcf_info_header}\"\n\techo '##INFO=<ID=AC,Number=1,Type=Integer,Description=\"Count of ALT allele reads in tumor sample\">' >> \"${snv_consensus_vcf_info_header}\"\n\techo '##INFO=<ID=VAF,Number=1,Type=Float,Description=\"Variant allele frequency, expressed as fraction of ALT allele reads in total read depth in tumor sample (tumor sample ALT AC / tumor sample DP)\">' >> \"${snv_consensus_vcf_info_header}\"\n\n\tbcftools annotate \\\n\t--output-type z \\\n\t--annotations \"${tumor_normal_sample_id}.snv.mpileup.info.metrics.txt.gz\" \\\n\t--header-lines \"${snv_consensus_vcf_info_header}\" \\\n\t--columns CHROM,POS,REF,ALT,INFO/DP,INFO/AC,INFO/VAF \\\n\t--output \"${tumor_normal_sample_id}.ms.consensus.somatic.snv.info.noformat.vcf.gz\" \\\n\t\"${mpileup_supported_consensus_somatic_snv_noformat_vcf}\"\n\n\ttouch \"${snv_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=DPS,Number=1,Type=Integer,Description=\"Total read depth in sample\">' >> \"${snv_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=ACS,Number=R,Type=Integer,Description=\"Count of REF,ALT allele reads in sample\">' >> \"${snv_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=ACFS,Number=R,Type=Integer,Description=\"Count of REF,ALT allele reads on forward(+) strand in sample\">' >> \"${snv_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=ACRS,Number=R,Type=Integer,Description=\"Count of REF,ALT allele reads on reverse(-) strand in sample\">' >> \"${snv_consensus_vcf_format_headers}\"\n\n\tbcftools annotate \\\n\t--output-type z \\\n\t--samples \"${normal_id}\" \\\n\t--annotations \"${snv_mpileup_normal_format_metrics}\" \\\n\t--header-lines \"${snv_consensus_vcf_format_headers}\" \\\n\t--columns CHROM,POS,REF,ALT,FORMAT/DPS,FORMAT/ACS,FORMAT/ACFS,FORMAT/ACRS \\\n\t--output \"${tumor_normal_sample_id}.ms.consensus.somatic.snv.info.halfformat.vcf.gz\" \\\n\t\"${tumor_normal_sample_id}.ms.consensus.somatic.snv.info.noformat.vcf.gz\"\n\n\tbcftools annotate \\\n\t--output-type z \\\n\t--samples \"${tumor_id}\" \\\n\t--annotations \"${snv_mpileup_tumor_format_metrics}\" \\\n\t--header-lines \"${snv_consensus_vcf_format_headers}\" \\\n\t--columns CHROM,POS,REF,ALT,FORMAT/DPS,FORMAT/ACS,FORMAT/ACFS,FORMAT/ACRS \\\n\t--remove FORMAT/GT \\\n\t--output \"${tumor_normal_sample_id}.ms.consensus.somatic.snv.info.format.vcf.gz\" \\\n\t\"${tumor_normal_sample_id}.ms.consensus.somatic.snv.info.halfformat.vcf.gz\"\n\n\tbcftools filter \\\n\t--output-type v \\\n\t--exclude 'INFO/AC<3 | INFO/VAF<0.01' \\\n\t\"${tumor_normal_sample_id}.ms.consensus.somatic.snv.info.format.vcf.gz\" \\\n\t| \\\n\tbcftools filter \\\n\t--output-type v \\\n\t--exclude 'FILTER=\"LOWSUPPORT\"' - \\\n\t| \\\n\tgrep -v '##bcftools_annotate' \\\n\t| \\\n\tbgzip > \"${snv_consensus_vcf}\"\n\ttabix \"${snv_consensus_vcf}\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t[%ACFS]\\t[%ACRS]\\n' \\\n\t--samples \"${tumor_id}\" \\\n\t--output \"${snv_strand_metrics}\" \\\n\t\"${snv_consensus_vcf}\"\n\t\"\"\"\n}", "\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n       \n    input:\n    path('*') from quant_ch.mix(fastqc_ch).collect()\n    \n    output:\n    path('multiqc_report.html')  \n     \n    script:\n    \"\"\"\n    multiqc . \n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/annotateConsensusSnvVcfFormatColumnAndFilter_bcftools", "pditommaso/nf-course-unimi-2021/multiqc"], "list_wf_names": ["pditommaso/nf-course-unimi-2021", "pblaney/mgp1000"]}, {"nb_reuse": 2, "tools": ["PncStress", "VCFtools"], "nb_own": 2, "list_own": ["pblaney", "pditommaso"], "nb_wf": 2, "list_wf": ["nf-foo", "mgp1000"], "list_contrib": ["pblaney", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess repeatsAndStrandBiasFilterSnvs_vcftools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(snv_consensus_vcf), path(snv_consensus_vcf_index), path(snv_strand_metrics), path(simple_and_centromeric_repeats_bed) from consensus_snv_forBedFilters.combine(simple_and_centromeric_repeats_bed_forSnvBedFilter)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(hq_snv_consensus_vcf), path(hq_snv_consensus_vcf_index) into high_quality_consensus_snv_forAnnotation\n\n\twhen:\n\tparams.varscan == \"on\" && params.mutect == \"on\" && params.strelka == \"on\"\n\n\tscript:\n\tstrand_bias_filter_bed = \"${tumor_normal_sample_id}.snv.strandbias.bed\"\n\thq_snv_consensus_vcf = \"${tumor_normal_sample_id}.hq.consensus.somatic.snv.vcf.gz\"\n\thq_snv_consensus_vcf_index = \"${hq_snv_consensus_vcf}.tbi\"\n\t\"\"\"\n\tRscript --vanilla  \\\n\t${workflow.projectDir}/bin/strand_bias_proportion_tester.R \\\n\t\"${snv_strand_metrics}\" \\\n\t\"${strand_bias_filter_bed}\"\n\n\tvcftools \\\n\t--gzvcf \"${snv_consensus_vcf}\" \\\n\t--exclude-bed \"${strand_bias_filter_bed}\" \\\n\t--recode \\\n\t--recode-INFO-all \\\n\t--stdout \\\n\t| \\\n\tvcftools \\\n\t--vcf - \\\n\t--exclude-bed \"${simple_and_centromeric_repeats_bed}\" \\\n\t--recode \\\n\t--recode-INFO-all \\\n\t--stdout \\\n\t| \\\n\tbgzip > \"${hq_snv_consensus_vcf}\"\n\n\ttabix \"${hq_snv_consensus_vcf}\"\n\t\"\"\"\n}", "\nprocess stress_2cpu {\n  cpus 2\n  input: val x from Channel.from(1,2,1,2)\n  \"\"\"\n  stress -c $x -t 10\n  \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/repeatsAndStrandBiasFilterSnvs_vcftools", "pditommaso/nf-foo/stress_2cpu"], "list_wf_names": ["pditommaso/nf-foo", "pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools", "Dindel"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess consensusIndelMpileup_bcftools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(consensus_somatic_indel_nosamples_badheader_noformat_vcf), path(indel_vcf_base_header), path(tumor_bam), path(tumor_bam_index), path(normal_bam), path(normal_bam_index), path(reference_genome_fasta_forConsensusIndelMpileup), path(reference_genome_fasta_index_forConsensusIndelMpileup), path(reference_genome_fasta_dict_forConsensusIndelMpileup) from consensus_indel_vcf_forConsensusIndelMpileup.join(bams_forConsensusIndelMpileup).combine(reference_genome_bundle_forConsensusIndelMpileup)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), val(tumor_id), val(normal_id), path(mpileup_supported_consensus_somatic_indel_nosamples_noformat_vcf) into consensus_indel_vcf_forAddSamples\n\ttuple val(tumor_normal_sample_id), path(indel_mpileup_info_dp_metrics), path(indel_mpileup_normal_format_metrics), path(indel_mpileup_normal_format_metrics_index), path(indel_mpileup_tumor_format_metrics), path(indel_mpileup_tumor_format_metrics_index) into consensus_indel_mpileup_metrics_forAddFormat\n\n\twhen:\n\tparams.varscan == \"on\" && params.mutect == \"on\" && params.strelka == \"on\" && params.svaba == \"on\"\n\n\tscript:\n\ttumor_id = \"${tumor_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tnormal_id = \"${normal_bam.baseName}\".replaceFirst(/\\..*$/, \"\")\n\tfull_indel_vcf_header = \"full_indel_vcf_header.txt\"\n\tmpileup_supported_consensus_somatic_indel_nosamples_noformat_vcf = \"${tumor_normal_sample_id}.ms.consensus.somatic.indel.nosamples.noformat.vcf\"\n\tindel_mpileup_info_dp_metrics = \"${tumor_normal_sample_id}.indel.mpileup.info.dp.metrics.txt\"\n\tindel_mpileup_normal_format_metrics = \"${tumor_normal_sample_id}.indel.mpileup.normal.format.metrics.txt.gz\"\n\tindel_mpileup_normal_format_metrics_index = \"${indel_mpileup_normal_format_metrics}.tbi\"\n\tindel_mpileup_tumor_format_metrics = \"${tumor_normal_sample_id}.indel.mpileup.tumor.format.metrics.txt.gz\"\n\tindel_mpileup_tumor_format_metrics_index = \"${indel_mpileup_tumor_format_metrics}.tbi\"\n\t\"\"\"\n\tbcftools mpileup \\\n\t--no-BAQ \\\n\t--max-depth 5000 \\\n\t--threads ${task.cpus} \\\n\t--fasta-ref \"${reference_genome_fasta_forConsensusIndelMpileup}\" \\\n\t--regions-file \"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}\" \\\n\t--samples ${normal_id},${tumor_id} \\\n\t--annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP \\\n\t\"${normal_bam}\" \"${tumor_bam}\" \\\n\t| \\\n\tbcftools norm \\\n\t--threads ${task.cpus} \\\n\t--multiallelics - \\\n\t--fasta-ref \"${reference_genome_fasta_forConsensusIndelMpileup}\" \\\n\t- \\\n\t| \\\n\tbcftools filter \\\n\t--exclude 'FORMAT/DP == 0' \\\n\t- \\\n\t| \\\n\tgrep -E '^#|INDEL;' \\\n\t| \\\n\tbgzip > \"${tumor_normal_sample_id}.consensus.somatic.indel.mpileup.vcf.gz\"\n\ttabix \"${tumor_normal_sample_id}.consensus.somatic.indel.mpileup.vcf.gz\"\n\n\tbgzip < \"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}\" > \"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}.gz\"\n\ttabix \"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}.gz\"\n\n\ttouch \"${full_indel_vcf_header}\"\n\tcat \"${indel_vcf_base_header}\" >> \"${full_indel_vcf_header}\"\n\tzgrep '##FILTER=<ID=LOWSUPPORT' \"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}.gz\" >> \"${full_indel_vcf_header}\"\n\tzgrep '##INFO=' \"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}.gz\" >> \"${full_indel_vcf_header}\"\n\techo -e '#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO' >> \"${full_indel_vcf_header}\"\n\n\tbcftools isec \\\n\t--nfiles =2 \\\n\t--write 1 \\\n\t\"${consensus_somatic_indel_nosamples_badheader_noformat_vcf}.gz\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.indel.mpileup.vcf.gz\" \\\n\t| \\\n\tbcftools reheader \\\n\t--header \"${full_indel_vcf_header}\" \\\n\t--output \"${mpileup_supported_consensus_somatic_indel_nosamples_noformat_vcf}\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t%REF\\t%ALT\\t%INFO/DP\\n' \\\n\t--output \"${indel_mpileup_info_dp_metrics}\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.indel.mpileup.vcf.gz\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t%REF\\t%ALT\\t[%DP]\\t[%AD]\\t[%ADF]\\t[%ADR]\\n' \\\n\t--samples \"${normal_id}\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.indel.mpileup.vcf.gz\" \\\n\t| \\\n\tbgzip > \"${indel_mpileup_normal_format_metrics}\"\n\ttabix -s1 -b2 -e2 \"${indel_mpileup_normal_format_metrics}\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t%REF\\t%ALT\\t[%DP]\\t[%AD]\\t[%ADF]\\t[%ADR]\\n' \\\n\t--samples \"${tumor_id}\" \\\n\t\"${tumor_normal_sample_id}.consensus.somatic.indel.mpileup.vcf.gz\" \\\n\t| \\\n\tbgzip > \"${indel_mpileup_tumor_format_metrics}\"\n\ttabix -s1 -b2 -e2 \"${indel_mpileup_tumor_format_metrics}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/consensusIndelMpileup_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess annotateConsensusIndelVcfFormatColumnAndFilter_bcftools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), val(tumor_id), val(normal_id), path(mpileup_supported_consensus_somatic_indel_noformat_vcf), path(indel_mpileup_info_dp_metrics), path(indel_mpileup_normal_format_metrics), path(indel_mpileup_normal_format_metrics_index), path(indel_mpileup_tumor_format_metrics), path(indel_mpileup_tumor_format_metrics_index) from consensus_indel_vcf_forAddFormat.join(consensus_indel_mpileup_metrics_forAddFormat)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(indel_consensus_vcf), path(indel_consensus_vcf_index), path(indel_strand_metrics) into consensus_indel_forBedFilters\n\n\twhen:\n\tparams.varscan == \"on\" && params.mutect == \"on\" && params.strelka == \"on\" && params.svaba == \"on\"\n\n\tscript:\n\tindel_consensus_vcf_info_header = \"indel_consensus_vcf_info_header.txt\"\n\tindel_consensus_vcf_format_headers = \"indel_consensus_vcf_format_header.txt\"\n\tindel_consensus_vcf = \"${tumor_normal_sample_id}.consensus.somatic.indel.vcf.gz\"\n\tindel_consensus_vcf_index = \"${indel_consensus_vcf}.tbi\"\n\tindel_strand_metrics = \"${tumor_normal_sample_id}.indel.strand.mertrics.txt\"\n\t\"\"\"\n\tcat \"${indel_mpileup_info_dp_metrics}\" \\\n\t| \\\n\tpaste - <(zcat \"${indel_mpileup_tumor_format_metrics}\" | cut -f 6 | awk '{split(\\$0,x,\",\"); print x[2]}') > \"${tumor_normal_sample_id}.indel.mpileup.info.dp.ac.metrics.txt\"\n\n\tcat \"${tumor_normal_sample_id}.indel.mpileup.info.dp.ac.metrics.txt\" \\\n\t| \\\n\tpaste - <(zcat \"${indel_mpileup_tumor_format_metrics}\" | cut -f 5) \\\n\t| \\\n\tawk 'BEGIN {OFS=\"\\t\"} {print \\$1,\\$2,\\$3,\\$4,\\$5,\\$6,\\$6/\\$7}' \\\n\t| \\\n\tbgzip > \"${tumor_normal_sample_id}.indel.mpileup.info.metrics.txt.gz\"\n\ttabix -s1 -b2 -e2 \"${tumor_normal_sample_id}.indel.mpileup.info.metrics.txt.gz\"\n\n\ttouch \"${indel_consensus_vcf_info_header}\"\n\techo '##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total read depth across samples (normal sample DP + tumor sample DP)\">' >> \"${indel_consensus_vcf_info_header}\"\n\techo '##INFO=<ID=AC,Number=1,Type=Integer,Description=\"Count of ALT allele reads in tumor sample\">' >> \"${indel_consensus_vcf_info_header}\"\n\techo '##INFO=<ID=VAF,Number=1,Type=Float,Description=\"Variant allele frequency, expressed as fraction of ALT allele reads in total read depth in tumor sample (tumor sample ALT AC / tumor sample DP)\">' >> \"${indel_consensus_vcf_info_header}\"\n\n\tbcftools annotate \\\n\t--output-type z \\\n\t--annotations \"${tumor_normal_sample_id}.indel.mpileup.info.metrics.txt.gz\" \\\n\t--header-lines \"${indel_consensus_vcf_info_header}\" \\\n\t--columns CHROM,POS,REF,ALT,INFO/DP,INFO/AC,INFO/VAF \\\n\t--output \"${tumor_normal_sample_id}.ms.consensus.somatic.indel.info.noformat.vcf.gz\" \\\n\t\"${mpileup_supported_consensus_somatic_indel_noformat_vcf}\"\n\n\ttouch \"${indel_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=DPS,Number=1,Type=Integer,Description=\"Total read depth in sample\">' >> \"${indel_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=ACS,Number=R,Type=Integer,Description=\"Count of REF,ALT allele reads in sample\">' >> \"${indel_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=ACFS,Number=R,Type=Integer,Description=\"Count of REF,ALT allele reads on forward(+) strand in sample\">' >> \"${indel_consensus_vcf_format_headers}\"\n\techo '##FORMAT=<ID=ACRS,Number=R,Type=Integer,Description=\"Count of REF,ALT allele reads on reverse(-) strand in sample\">' >> \"${indel_consensus_vcf_format_headers}\"\n\n\tbcftools annotate \\\n\t--output-type z \\\n\t--samples \"${normal_id}\" \\\n\t--annotations \"${indel_mpileup_normal_format_metrics}\" \\\n\t--header-lines \"${indel_consensus_vcf_format_headers}\" \\\n\t--columns CHROM,POS,REF,ALT,FORMAT/DPS,FORMAT/ACS,FORMAT/ACFS,FORMAT/ACRS \\\n\t--output \"${tumor_normal_sample_id}.ms.consensus.somatic.indel.info.halfformat.vcf.gz\" \\\n\t\"${tumor_normal_sample_id}.ms.consensus.somatic.indel.info.noformat.vcf.gz\"\n\n\tbcftools annotate \\\n\t--output-type z \\\n\t--samples \"${tumor_id}\" \\\n\t--annotations \"${indel_mpileup_tumor_format_metrics}\" \\\n\t--header-lines \"${indel_consensus_vcf_format_headers}\" \\\n\t--columns CHROM,POS,REF,ALT,FORMAT/DPS,FORMAT/ACS,FORMAT/ACFS,FORMAT/ACRS \\\n\t--remove FORMAT/GT \\\n\t--output \"${tumor_normal_sample_id}.ms.consensus.somatic.indel.info.format.vcf.gz\" \\\n\t\"${tumor_normal_sample_id}.ms.consensus.somatic.indel.info.halfformat.vcf.gz\"\n\n\tbcftools filter \\\n\t--output-type v \\\n\t--exclude 'INFO/AC<3 | INFO/VAF<0.01' \\\n\t\"${tumor_normal_sample_id}.ms.consensus.somatic.indel.info.format.vcf.gz\" \\\n\t| \\\n\tbcftools filter \\\n\t--output-type v \\\n\t--exclude 'FILTER=\"LOWSUPPORT\"' - \\\n\t| \\\n\tgrep -v '##bcftools_annotate' \\\n\t| \\\n\tbgzip > \"${indel_consensus_vcf}\"\n\ttabix \"${indel_consensus_vcf}\"\n\n\tbcftools query \\\n\t--format '%CHROM\\t%POS\\t[%ACFS]\\t[%ACRS]\\n' \\\n\t--samples \"${tumor_id}\" \\\n\t--output \"${indel_strand_metrics}\" \\\n\t\"${indel_consensus_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/annotateConsensusIndelVcfFormatColumnAndFilter_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess repeatsAndStrandBiasFilterIndels_vcftools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(indel_consensus_vcf), path(indel_consensus_vcf_index), path(indel_strand_metrics), path(simple_and_centromeric_repeats_bed) from consensus_indel_forBedFilters.combine(simple_and_centromeric_repeats_bed_forIndelBedFilter)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(hq_indel_consensus_vcf), path(hq_indel_consensus_vcf_index) into high_quality_consensus_indel_forAnnotation\n\n\twhen:\n\tparams.varscan == \"on\" && params.mutect == \"on\" && params.strelka == \"on\" && params.svaba == \"on\"\n\n\tscript:\n\tstrand_bias_filter_bed = \"${tumor_normal_sample_id}.indel.strandbias.bed\"\n\thq_indel_consensus_vcf = \"${tumor_normal_sample_id}.hq.consensus.somatic.indel.vcf.gz\"\n\thq_indel_consensus_vcf_index = \"${hq_indel_consensus_vcf}.tbi\"\n\t\"\"\"\n\tRscript --vanilla  \\\n\t${workflow.projectDir}/bin/strand_bias_proportion_tester.R \\\n\t\"${indel_strand_metrics}\" \\\n\t\"${strand_bias_filter_bed}\"\n\n\tvcftools \\\n\t--gzvcf \"${indel_consensus_vcf}\" \\\n\t--exclude-bed \"${strand_bias_filter_bed}\" \\\n\t--recode \\\n\t--recode-INFO-all \\\n\t--stdout \\\n\t| \\\n\tvcftools \\\n\t--vcf - \\\n\t--exclude-bed \"${simple_and_centromeric_repeats_bed}\" \\\n\t--recode \\\n\t--recode-INFO-all \\\n\t--stdout \\\n\t| \\\n\tbgzip > \"${hq_indel_consensus_vcf}\"\n\n\ttabix \"${hq_indel_consensus_vcf}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/repeatsAndStrandBiasFilterIndels_vcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess mergeAndGenerateConsensusCnvCalls_bedtools {\n\ttag \"${tumor_normal_sample_id}\"\n\n\tinput:\n\ttuple val(tumor_normal_sample_id), path(ascat_cnv_profile_final), path(control_freec_bedgraph), path(control_freec_cnv_profile_final), path(sclust_allelic_states_file), path(reference_genome_fasta_index_forConsensusCnv) from final_ascat_cnv_profile_forConsensus.join(final_control_freec_cnv_profile_forConsensus).join(final_sclust_cnv_profile_forConsensus).combine(reference_genome_fasta_index_forConsensusCnv)\n\n\toutput:\n\ttuple val(tumor_normal_sample_id), path(consensus_merged_cnv_alleles_bed) into consensus_cnv_and_allele_bed_forConsensusCnvTransform\n\n\twhen:\n\tparams.ascatngs == \"on\" & params.controlfreec == \"on\" & params.sclust == \"on\"\n\n\tscript:\n\tascat_somatic_cnv_bed = \"${tumor_normal_sample_id}.ascat.somatic.cnv.bed\"\n\tascat_somatic_alleles_bed = \"${tumor_normal_sample_id}.ascat.somatic.alleles.bed\"\n\tcontrol_freec_somatic_cnv_bed = \"${tumor_normal_sample_id}.controlfreec.somatic.cnv.bed\"\n\tcontrol_freec_somatic_alleles_bed = \"${tumor_normal_sample_id}.controlfreec.somatic.alleles.bed\"\n\tsclust_somatic_cnv_bed = \"${tumor_normal_sample_id}.sclust.somatic.cnv.bed\"\n\tsclust_somatic_alleles_bed = \"${tumor_normal_sample_id}.sclust.somatic.alleles.bed\"\n\tmerged_cnv_bed = \"${tumor_normal_sample_id}.merged.somatic.cnv.bed\"\n\tmerged_alleles_bed = \"${tumor_normal_sample_id}.merged.somatic.alleles.bed\"\n\tconsensus_cnv_bed = \"${tumor_normal_sample_id}.consensus.somatic.cnv.bed\"\n\tconsensus_alleles_bed = \"${tumor_normal_sample_id}.consensus.somatic.alleles.bed\"\n\tconsensus_merged_cnv_alleles_bed = \"${tumor_normal_sample_id}.consensus.somatic.cnv.alleles.merged.bed\"\n\t\"\"\"\n\t### Prep ASCAT files ###\n\t# total copy number per segment\n\tgrep -v 'segment_number' \"${ascat_cnv_profile_final}\" \\\n\t| \\\n\tawk -F, 'BEGIN {OFS=\"\\t\"} {print \\$2,\\$3,\\$4,\\$7}' > \"${ascat_somatic_cnv_bed}\"\n\n\t# major/minor alleles per segment\n\tgrep -v 'segment_number' \"${ascat_cnv_profile_final}\" \\\n\t| \\\n\tawk -F, 'BEGIN {OFS=\"\\t\"} {print \\$2,\\$3,\\$4,\\$7-\\$8\"/\"\\$8}' > \"${ascat_somatic_alleles_bed}\"\n\n\n\t### Prep Control-FREEC files ###\n\tcontrol_freec_cnv_and_allele_preparer.sh \\\n\t\"${tumor_normal_sample_id}\" \\\n\t\"${control_freec_cnv_profile_final}\" \\\n\t\"${control_freec_bedgraph}\" \\\n\t\"${reference_genome_fasta_index_forConsensusCnv}\"\n\n\tcontrol_freec_segment_refiner.py \\\n\t\"${tumor_normal_sample_id}.controlfreec.complete.cnv.alleles.merged.bed\" \\\n\t\"${control_freec_somatic_alleles_bed}\" \\\n\t\"${control_freec_somatic_cnv_bed}\"\n\n\n\t### Prep Sclust files ###\n\t# total copy number per segment\n\tgrep -v 'Sample' \"${sclust_allelic_states_file}\" \\\n\t| \\\n\tcut -f 2-4,6 > \"${sclust_somatic_cnv_bed}\"\n\n\t# major/minor alleles per segment\n\tgrep -v 'Sample' \"${sclust_allelic_states_file}\" \\\n\t| \\\n\tawk 'BEGIN {OFS=\"\\t\"} {print \\$2,\\$3,\\$4,\\$7\"/\"\\$8}' > \"${sclust_somatic_alleles_bed}\"\n\n\n\t### Create consensus total copy number file ###\n\tbedtools unionbedg \\\n\t-filler . \\\n\t-i \"${ascat_somatic_cnv_bed}\" \"${control_freec_somatic_cnv_bed}\" \"${sclust_somatic_cnv_bed}\" \\\n\t-header \\\n\t-names ascat_total_cn controlfreec_total_cn sclust_total_cn > \"${merged_cnv_bed}\"\n\n\tconsensus_cnv_generator.py \\\n\t<(grep -v 'chrom' \"${merged_cnv_bed}\") \\\n\t\"${consensus_cnv_bed}\"\n\n\n\t### Create consensus major and minor allele file ###\n\tbedtools unionbedg \\\n\t-filler . \\\n\t-i \"${ascat_somatic_alleles_bed}\" \"${control_freec_somatic_alleles_bed}\" \"${sclust_somatic_alleles_bed}\" \\\n\t-header \\\n\t-names ascat_major_minor_alleles controlfreec_major_minor_alleles sclust_major_minor_alleles > \"${merged_alleles_bed}\"\n\n\tconsensus_allele_generator.py \\\n\t<(grep -v 'chrom' \"${merged_alleles_bed}\") \\\n\t\"${consensus_alleles_bed}\"\n\n\n\t### Merge both consensus CNV and called alleles per segment ###\n\tpaste \"${consensus_cnv_bed}\" <(cut -f 4-9 \"${consensus_alleles_bed}\") \\\n\t| \\\n\tawk 'BEGIN {OFS=\"\\t\"} {print \\$1,\\$2,\\$3,\\$4,\\$9,\\$10,\\$5,\\$11,\\$6,\\$12,\\$7,\\$13,\\$8,\\$14}' > \"${consensus_merged_cnv_alleles_bed}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/mergeAndGenerateConsensusCnvCalls_bedtools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["AnnotSV"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess annotateConsensusCnvCalls_annotsv {\n    publishDir \"${params.output_dir}/somatic/consensus/${tumor_normal_sample_id}\", mode: 'copy', pattern: '*.{bed}'\n    tag  \"${tumor_normal_sample_id}\"\n\n    input:\n    tuple val(tumor_normal_sample_id), path(hq_consensus_cnv_bed), path(annotsv_ref_dir_bundle) from hq_consensus_cnv_bed_forAnnotation.combine(annotsv_ref_dir_forCnvAnnotation)\n\n    output:\n    path hq_consensus_cnv_annotated_bed\n\n    when:\n    params.ascatngs == \"on\" & params.controlfreec == \"on\" & params.sclust == \"on\"\n\n    script:\n    hq_consensus_cnv_annotated_bed = \"${tumor_normal_sample_id}.hq.consensus.somatic.cnv.annotated.bed\"\n    \"\"\"\n    \\$ANNOTSV/bin/AnnotSV \\\n    -annotationsDir \"${annotsv_ref_dir_bundle}\" \\\n    -annotationMode full \\\n    -genomeBuild GRCh38 \\\n    -outputDir . \\\n    -outputFile \"${tumor_normal_sample_id}.hq.consensus.somatic.cnv.annotated\" \\\n    -SVinputFile \"${hq_consensus_cnv_bed}\" \\\n    -SVminSize 1 \\\n    -includeCI 0 \\\n    -tx ENSEMBL\n\n    touch \"${hq_consensus_cnv_annotated_bed}\"\n    echo \"chr\\tstart\\tend\\tconsensus_total_cn\\tconsensus_major_allele\\tconsensus_minor_allele\\ttype\\tclass\\tallele_status\\tconsensus_conf_rating\\tcytoband\\tgenes\" >> \"${hq_consensus_cnv_annotated_bed}\"\n    grep -v 'SV_chrom' \"${tumor_normal_sample_id}.hq.consensus.somatic.cnv.annotated.tsv\" \\\n    | \\\n    awk 'BEGIN {OFS=\"\\t\"} {print \"chr\"\\$2,\\$3,\\$4,\\$5,\\$6,\\$7,\\$8,\\$9,\\$10,\\$11,\\$13,\\$14}' \\\n    | \\\n    sort -k1,1V -k2,2n >> \"${hq_consensus_cnv_annotated_bed}\"\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/annotateConsensusCnvCalls_annotsv"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess extractFpFilterPassingSvCalls_bcftools {\n    tag \"${tumor_normal_sample_id}\"\n\n    input:\n    tuple val(tumor_normal_sample_id), path(consensus_somatic_sv_fpmarked_vcf) from consensus_sv_vcf_forFpFiltering\n\n    output:\n    tuple val(tumor_normal_sample_id), path(consensus_somatic_sv_fpfiltered_vcf) into consensus_sv_vcf_forAnnotation\n\n    when:\n    params.manta == \"on\" && params.svaba == \"on\" && params.delly == \"on\"\n\n    script:\n    consensus_somatic_sv_fpfiltered_vcf = \"${tumor_normal_sample_id}.consensus.somatic.sv.fpfiltered.vcf\"\n    \"\"\"\n    bcftools filter \\\n    --output-type v \\\n    --exclude 'INFO/SVTYPE=\"DEL\" && FORMAT/DHFFC>0.7' \\\n    \"${consensus_somatic_sv_fpmarked_vcf}\" \\\n    | \\\n    bcftools filter \\\n    --output-type v \\\n    --exclude 'INFO/SVTYPE=\"DUP\" && FORMAT/DHBFC<1.3' \\\n    --output \"${consensus_somatic_sv_fpfiltered_vcf}\"\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/extractFpFilterPassingSvCalls_bcftools"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["AnnotSV"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess annotateConsensusSvCalls_annotsv {\n    publishDir \"${params.output_dir}/somatic/consensus/${tumor_normal_sample_id}\", mode: 'copy', pattern: '*.{sv.annotated.genesplit.bed,bedpe}'\n    tag  \"${tumor_normal_sample_id}\"\n\n    input:\n    tuple val(tumor_normal_sample_id), path(consensus_somatic_sv_fpfiltered_vcf), path(final_manta_somatic_sv_read_support), path(final_svaba_somatic_sv_read_support), path(final_delly_somatic_sv_read_support), path(annotsv_ref_dir_bundle) from consensus_sv_vcf_forAnnotation.join(manta_sv_read_support_forAnnotation).join(svaba_sv_read_support_forAnnotation).join(delly_sv_read_support_forAnnotation).combine(annotsv_ref_dir_forSvAnnotation)\n\n    output:\n    path gene_split_annotated_consensus_sv_bed\n    path hq_consensus_sv_bedpe\n\n    when:\n    params.manta == \"on\" && params.svaba == \"on\" && params.delly == \"on\"\n\n    script:\n    gene_split_annotated_consensus_sv_bed = \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.annotated.genesplit.bed\"\n    collapsed_annotated_consensus_sv_bed = \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.annotated.collapsed.bed\"\n    hq_consensus_sv_bedpe = \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.annotated.bedpe\"\n    \"\"\"\n    grep -v 'SVTYPE=BND' \"${consensus_somatic_sv_fpfiltered_vcf}\" > \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.vcf\"\n\n\t\\$ANNOTSV/bin/AnnotSV \\\n\t-annotationsDir \"${annotsv_ref_dir_bundle}\" \\\n\t-annotationMode split \\\n\t-genomeBuild GRCh38 \\\n\t-hpo HP:0006775 \\\n\t-outputDir . \\\n\t-outputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit\" \\\n\t-SVinputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.vcf\" \\\n\t-SVminSize 1 \\\n\t-includeCI 0 \\\n\t-tx ENSEMBL\n\n    paste \\\n\t<(cut -f 2 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\" | awk 'BEGIN {OFS=\"\\t\"} {print \"chr\"\\$1}') \\\n\t<(cut -f 3-4 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\" | awk 'BEGIN {OFS=\"\\t\"} {print \\$1-1,\\$2}') \\\n\t<(cut -f 20 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\") \\\n\t<(cut -f 5-6 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\") \\\n\t<(cut -f 1 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\") \\\n\t<(cut -f 21,23-36,64-65 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\" | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\$|\\t.|') \\\n\t<(cut -f 44-45,48-49,52-63 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.genesplit.tsv\" | sed 's|^\\t|.\\t|' | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\t|\\t.\\t|g') \\\n\t| \\\n\tsed 's|\\t\\$|\\t.|' \\\n\t| \\\n\tsed 's|chrSV_chrom\\t-1|SV_chrom\\tSV_start|' \\\n\t| \\\n\tsort -k1,1V -k2,2n > \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.nonbreakend.annotated.genesplit.bed\"\n\n\t\\$ANNOTSV/bin/AnnotSV \\\n\t-annotationsDir \"${annotsv_ref_dir_bundle}\" \\\n\t-annotationMode full \\\n\t-genomeBuild GRCh38 \\\n\t-hpo HP:0006775 \\\n\t-outputDir . \\\n\t-outputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed\" \\\n\t-SVinputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.vcf\" \\\n\t-SVminSize 1 \\\n\t-includeCI 0 \\\n\t-tx ENSEMBL\n\n\tpaste \\\n\t<(cut -f 2 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed.tsv\" | awk 'BEGIN {OFS=\"\\t\"} {print \"chr\"\\$1}') \\\n\t<(cut -f 3-4 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed.tsv\") \\\n\t<(cut -f 20 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed.tsv\") \\\n\t<(cut -f 5-6 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed.tsv\") \\\n\t<(cut -f 1,106,108 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed.tsv\" | sed 's|\\t\\t|\\t.\\t|g') \\\n\t<(cut -f 8,13,15-17,21-22,66-79 \"${tumor_normal_sample_id}.consensus.somatic.sv.nonbreakend.annotated.collapsed.tsv\" | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\t|\\t.\\t|g') \\\n\t| \\\n\tsed 's|\\t\\$|\\t.|' \\\n\t| \\\n\tsed 's|chrSV_chrom|SV_chrom|' \\\n\t| \\\n\tsort -k1,1V -k2,2n > \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.nonbreakend.annotated.collapsed.bed\"\n\n\n\tgrep -E '^#|SVTYPE=BND' \"${consensus_somatic_sv_fpfiltered_vcf}\" > \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.vcf\"\n\n\t\\$ANNOTSV/bin/AnnotSV \\\n\t-annotationsDir \"${annotsv_ref_dir_bundle}\" \\\n\t-annotationMode split \\\n\t-genomeBuild GRCh38 \\\n\t-hpo HP:0006775 \\\n\t-outputDir . \\\n\t-outputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit\" \\\n\t-SVinputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.vcf\" \\\n\t-SVminSize 1 \\\n\t-includeCI 0 \\\n\t-tx ENSEMBL\n\n\tpaste \\\n    <(cut -f 2 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\" | awk 'BEGIN {OFS=\"\\t\"} {print \"chr\"\\$1}') \\\n    <(cut -f 3-4 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\" | awk 'BEGIN {OFS=\"\\t\"} {print \\$1-1,\\$2}') \\\n    <(cut -f 20 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\") \\\n    <(cut -f 5-6 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\") \\\n    <(cut -f 1 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\") \\\n    <(cut -f 21,23-36,64-65 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\" | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\$|\\t.|') \\\n    <(cut -f 44-45,48-49,52-63 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.genesplit.tsv\" | sed 's|^\\t|.\\t|' | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\t|\\t.\\t|g') \\\n    | \\\n    sed 's|\\t\\$|\\t.|' \\\n    | \\\n    sed 's|chrSV_chrom\\t-1|SV_chrom\\tSV_start|' \\\n    | \\\n    sort -k1,1V -k2,2n > \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.breakend.annotated.genesplit.bed\"\n\n    \\$ANNOTSV/bin/AnnotSV \\\n    -annotationsDir \"${annotsv_ref_dir_bundle}\" \\\n    -annotationMode full \\\n    -genomeBuild GRCh38 \\\n    -hpo HP:0006775 \\\n    -outputDir . \\\n    -outputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed\" \\\n    -SVinputFile \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.vcf\" \\\n    -SVminSize 1 \\\n\t-includeCI 0 \\\n    -tx ENSEMBL\n\n    paste \\\n    <(cut -f 2 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed.tsv\" | awk 'BEGIN {OFS=\"\\t\"} {print \"chr\"\\$1}') \\\n    <(cut -f 3-4 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed.tsv\") \\\n    <(cut -f 20 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed.tsv\") \\\n    <(cut -f 5-6 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed.tsv\") \\\n    <(cut -f 1,106,108 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed.tsv\" | sed 's|\\t\\t|\\t.\\t|g') \\\n    <(cut -f 8,13,15-17,21-22,66-79 \"${tumor_normal_sample_id}.consensus.somatic.sv.breakend.annotated.collapsed.tsv\" | sed 's|\\t\\t|\\t.\\t|g' | sed 's|\\t\\t|\\t.\\t|g') \\\n    | \\\n    sed 's|\\t\\$|\\t.|' \\\n    | \\\n    sed 's|chrSV_chrom|SV_chrom|' \\\n    | \\\n    sort -k1,1V -k2,2n > \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.breakend.annotated.collapsed.bed\"\n\n    cat \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.nonbreakend.annotated.genesplit.bed\" \\\n    <(grep -v 'SV_chrom' \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.breakend.annotated.genesplit.bed\") \\\n    | \\\n    sort -k1,1V -k2,2n > \"${gene_split_annotated_consensus_sv_bed}\"\n\n    cat \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.nonbreakend.annotated.collapsed.bed\" \\\n    <(grep -v 'SV_chrom' \"${tumor_normal_sample_id}.hq.consensus.somatic.sv.breakend.annotated.collapsed.bed\") \\\n    | \\\n    sort -k1,1V -k2,2n > \"${collapsed_annotated_consensus_sv_bed}\"\n\n\n    # Transform collapsed annotation BED to high quality BEDPE\n    high_quality_bedpe_transformer.py \\\n    <(grep -v 'SV_chrom' \"${collapsed_annotated_consensus_sv_bed}\") \\\n    \"${final_manta_somatic_sv_read_support}\" \\\n    \"${final_svaba_somatic_sv_read_support}\" \\\n    \"${final_delly_somatic_sv_read_support}\" \\\n    \"${hq_consensus_sv_bedpe}\"\n    \"\"\"\n}"], "list_proc": ["pblaney/mgp1000/annotateConsensusSvCalls_annotsv"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess revertMappedBam_gatk {\n\ttag \"${bam_mapped.baseName}\"\n\n\tinput:\n\tpath bam_mapped from input_mapped_bams\n\n\toutput:\n\tpath bam_unmapped into unmapped_bams\n\n\twhen:\n\tparams.input_format == \"bam\"\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tbam_unmapped = \"${bam_mapped}\".replaceFirst(/\\..*bam/, \".unmapped.bam\")\n\t\"\"\"\n\tgatk RevertSam \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 2}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--VALIDATION_STRINGENCY LENIENT \\\n\t--MAX_RECORDS_IN_RAM 4000000 \\\n\t--TMP_DIR . \\\n\t--SANITIZE true \\\n\t--ATTRIBUTE_TO_CLEAR XT \\\n\t--ATTRIBUTE_TO_CLEAR XN \\\n\t--ATTRIBUTE_TO_CLEAR OC \\\n\t--ATTRIBUTE_TO_CLEAR OP \\\n\t--INPUT \"${bam_mapped}\" \\\n\t--OUTPUT \"${bam_unmapped}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/revertMappedBam_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess fastqTrimming_trimmomatic {\n\tpublishDir \"${params.output_dir}/preprocessing/trimLogs\", mode: 'copy', pattern: '*.{log}'\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple val(sample_id), path(input_R1_fastqs), path(input_R2_fastqs), path(trimmomatic_contaminants) from input_fastqs_forTrimming.combine(trimmomatic_contaminants)\n\n\toutput:\n\ttuple val(sample_id), path(fastq_R1_trimmed), path(fastq_R2_trimmed) into trimmed_fastqs_forFastqc, trimmed_fastqs_forAlignment\n\tpath fastq_trim_log\n\n\tscript:\n\tfastq_R1_trimmed = \"${sample_id}_R1_trim.fastq.gz\"\n\tfastq_R2_trimmed = \"${sample_id}_R2_trim.fastq.gz\"\n\tfastq_R1_unpaired = \"${sample_id}_R1_unpaired.fastq.gz\"\n\tfastq_R2_unpaired = \"${sample_id}_R2_unpaired.fastq.gz\"\n\tfastq_trim_log = \"${sample_id}.trim.log\"\n\t\"\"\"\n\ttrimmomatic PE \\\n\t-threads ${task.cpus} \\\n\t\"${input_R1_fastqs}\" \\\n\t\"${input_R2_fastqs}\" \\\n\t\"${fastq_R1_trimmed}\" \\\n\t\"${fastq_R1_unpaired}\" \\\n\t\"${fastq_R2_trimmed}\" \\\n\t\"${fastq_R2_unpaired}\" \\\n\tILLUMINACLIP:${trimmomatic_contaminants}:2:30:10:1:true \\\n\tTRAILING:5 \\\n\tSLIDINGWINDOW:4:15 \\\n\tMINLEN:35 \\\n\t2> \"${fastq_trim_log}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/fastqTrimming_trimmomatic"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess fastqQualityControlMetrics_fastqc {\n\tpublishDir \"${params.output_dir}/preprocessing/fastqc\", mode: 'copy'\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple val(sample_id), path(fastq_R1), path(fastq_R2) from trimmed_fastqs_forFastqc\n\n\toutput:\n\ttuple path(fastqc_R1_html), path(fastqc_R2_html)\n\ttuple path(fastqc_R1_zip), path(fastqc_R2_zip)\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tfastqc_R1_html = \"${fastq_R1}\".replaceFirst(/\\.*fastq.gz/, \"_fastqc.html\")\n\tfastqc_R1_zip = \"${fastq_R1}\".replaceFirst(/\\.*fastq.gz/, \"_fastqc.zip\")\n\tfastqc_R2_html = \"${fastq_R2}\".replaceFirst(/\\.*fastq.gz/, \"_fastqc.html\")\n\tfastqc_R2_zip = \"${fastq_R2}\".replaceFirst(/\\.*fastq.gz/, \"_fastqc.zip\")\n\t\"\"\"\n\tfastqc --outdir . \"${fastq_R1}\"\n\tfastqc --outdir . \"${fastq_R2}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/fastqQualityControlMetrics_fastqc"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["Sambamba", "BWA"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess alignment_bwa {\n\tpublishDir \"${params.output_dir}/preprocessing/alignmentFlagstats\", mode: 'copy', pattern: \"*${bam_flagstat_log}\"\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple val(sample_id), path(fastq_R1), path(fastq_R2), path(bwa_reference_dir) from trimmed_fastqs_forAlignment.combine(bwa_reference_dir)\n\n\toutput:\n\tpath bam_aligned into aligned_bams\n\tpath bam_flagstat_log\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tbam_aligned = \"${sample_id}.bam\"\n\tbam_flagstat_log = \"${sample_id}.alignment.flagstat.log\"\n\t\"\"\"\n\tbwa mem \\\n\t-M \\\n\t-K 100000000 \\\n\t-v 1 \\\n\t-t ${task.cpus - 2} \\\n\t-R '@RG\\\\tID:${sample_id}\\\\tSM:${sample_id}\\\\tLB:${sample_id}\\\\tPL:ILLUMINA' \\\n\t\"${bwa_reference_dir}/Homo_sapiens_assembly38.fasta\" \\\n\t\"${fastq_R1}\" \"${fastq_R2}\" \\\n\t| \\\n\tsambamba view \\\n\t--sam-input \\\n\t--nthreads=${task.cpus - 2} \\\n\t--filter='mapping_quality>=10' \\\n\t--format=bam \\\n\t--compression-level=0 \\\n\t/dev/stdin \\\n\t| \\\n\tsambamba sort \\\n\t--nthreads=${task.cpus - 2} \\\n\t--tmpdir=. \\\n\t--memory-limit=8GB \\\n\t--sort-by-name \\\n\t--out=${bam_aligned} \\\n\t/dev/stdin\n\n\tsambamba flagstat \\\n\t\"${bam_aligned}\" > \"${bam_flagstat_log}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/alignment_bwa"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess fixMateInformationAndSort_gatk {\n\ttag \"${bam_aligned.baseName}\"\n\n\tinput:\n\tpath bam_aligned from aligned_bams\n\n\toutput:\n\tpath bam_fixed_mate into fixed_mate_bams\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tbam_fixed_mate_unsorted = \"${bam_aligned}\".replaceFirst(/\\.bam/, \".unsorted.fixedmate.bam\")\n\tbam_fixed_mate = \"${bam_aligned}\".replaceFirst(/\\.bam/, \".fixedmate.bam\")\n\t\"\"\"\n\tgatk FixMateInformation \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 4}G -Djava.io.tmpdir=. -XX:ParallelGCThreads=2 -XX:-UseGCOverheadLimit\" \\\n\t--VERBOSITY ERROR \\\n\t--VALIDATION_STRINGENCY SILENT \\\n\t--ADD_MATE_CIGAR true \\\n\t--MAX_RECORDS_IN_RAM 4000000 \\\n\t--ASSUME_SORTED true \\\n\t--TMP_DIR . \\\n\t--INPUT \"${bam_aligned}\" \\\n\t--OUTPUT \"${bam_fixed_mate_unsorted}\"\n\n\tgatk SortSam \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 4}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--TMP_DIR . \\\n\t--SORT_ORDER coordinate \\\n\t--INPUT \"${bam_fixed_mate_unsorted}\" \\\n\t--OUTPUT \"${bam_fixed_mate}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/fixMateInformationAndSort_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess markDuplicatesAndIndex_sambamba {\n\tpublishDir \"${params.output_dir}/preprocessing/markdupFlagstats\", mode: 'copy', pattern: '*.{log}'\n\ttag \"${sample_id}\"\n\n\tinput:\n\tpath bam_fixed_mate from fixed_mate_bams\n\n\toutput:\n\ttuple val(sample_id), path(bam_marked_dup) into marked_dup_bams_forDownsampleBam, marked_dup_bams_forApplyBqsr\n\tpath bam_marked_dup_index\n\tpath markdup_output_log\n\tpath bam_markdup_flagstat_log\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tsample_id = \"${bam_fixed_mate}\".replaceFirst(/\\.fixedmate\\.bam/, \"\")\n\tbam_marked_dup = \"${sample_id}.markdup.bam\"\n\tbam_marked_dup_index = \"${bam_marked_dup}.bai\"\n\tmarkdup_output_log = \"${sample_id}.markdup.log\"\n\tbam_markdup_flagstat_log = \"${sample_id}.markdup.flagstat.log\"\n\t\"\"\"\n\tsambamba markdup \\\n\t--remove-duplicates \\\n\t--nthreads ${task.cpus} \\\n\t--hash-table-size 1000000 \\\n\t--overflow-list-size 1000000 \\\n\t--tmpdir . \\\n\t\"${bam_fixed_mate}\" \\\n\t\"${bam_marked_dup}\" \\\n\t2> \"${markdup_output_log}\"\n\n\tsambamba flagstat \\\n\t\"${bam_marked_dup}\" > \"${bam_markdup_flagstat_log}\"\n\n\tsambamba index \\\n\t\"${bam_marked_dup}\" \"${bam_marked_dup_index}\"\n\t\"\"\"\t\n}"], "list_proc": ["pblaney/mgp1000/markDuplicatesAndIndex_sambamba"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess downsampleBam_gatk {\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple val(sample_id), path(bam_marked_dup) from marked_dup_bams_forDownsampleBam\n\n\toutput:\n\tpath bam_marked_dup_downsampled into downsampled_makred_dup_bams\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tbam_marked_dup_downsampled = \"${sample_id}.markdup.downsampled.bam\"\n\t\"\"\"\n\tgatk DownsampleSam \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 2}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--MAX_RECORDS_IN_RAM 4000000 \\\n\t--TMP_DIR . \\\n\t--STRATEGY ConstantMemory \\\n\t--RANDOM_SEED 1000 \\\n\t--CREATE_INDEX \\\n\t--VALIDATION_STRINGENCY SILENT \\\n\t--PROBABILITY 0.1 \\\n\t--INPUT \"${bam_marked_dup}\" \\\n\t--OUTPUT \"${bam_marked_dup_downsampled}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/downsampleBam_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess baseRecalibrator_gatk {\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple path(bam_marked_dup_downsampled), path(reference_genome_fasta_forBaseRecalibrator), path(reference_genome_fasta_index_forBaseRecalibrator), path(reference_genome_fasta_dict_forBaseRecalibrator), path(gatk_bundle_wgs_interval_list), path(gatk_bundle_mills_1000G), path(gatk_bundle_mills_1000G_index), path(gatk_bundle_known_indels), path(gatk_bundle_known_indels_index), path(gatk_bundle_dbsnp138), path(gatk_bundle_dbsnp138_index) from input_and_reference_files_forBaseRecalibrator\n\n\toutput:\n\ttuple val(sample_id), path(bqsr_table) into base_quality_score_recalibration_data\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tsample_id = \"${bam_marked_dup_downsampled}\".replaceFirst(/\\.markdup\\.downsampled\\.bam/, \"\")\n\tbqsr_table = \"${sample_id}.recaldata.table\"\n\t\"\"\"\n\tgatk BaseRecalibrator \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 2}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--read-filter GoodCigarReadFilter \\\n\t--reference \"${reference_genome_fasta_forBaseRecalibrator}\" \\\n\t--intervals \"${gatk_bundle_wgs_interval_list}\" \\\n\t--input \"${bam_marked_dup_downsampled}\" \\\n\t--output \"${bqsr_table}\" \\\n\t--known-sites \"${gatk_bundle_mills_1000G}\" \\\n\t--known-sites \"${gatk_bundle_known_indels}\" \\\n\t--known-sites \"${gatk_bundle_dbsnp138}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/baseRecalibrator_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess applyBqsr_gatk {\n\tpublishDir \"${params.output_dir}/preprocessing/finalPreprocessedBams\", mode: 'copy', pattern: '*.{final.bam,bai}'\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple val(sample_id), path(bam_marked_dup), path(bqsr_table), path(reference_genome_fasta_forApplyBqsr), path(reference_genome_fasta_index_forApplyBqsr), path(reference_genome_fasta_dict_forApplyBqsr) from input_and_reference_files_forApplyBqsr\n\n\toutput:\n\tpath bam_preprocessed_final into final_preprocessed_bams_forCollectWgsMetrics, final_preprocessed_bams_forCollectGcBiasMetrics\n\tpath bam_preprocessed_final_index\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tbam_preprocessed_final = \"${bam_marked_dup}\".replaceFirst(/\\.markdup\\.bam/, \".final.bam\")\n\tbam_preprocessed_final_index = \"${bam_preprocessed_final}\".replaceFirst(/\\.bam$/, \".bai\")\n\t\"\"\"\n\tgatk ApplyBQSR \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 2}G -Djava.io.tmpdir=.\" \\\n\t--verbosity ERROR \\\n\t--tmp-dir . \\\n\t--read-filter GoodCigarReadFilter \\\n\t--reference \"${reference_genome_fasta_forApplyBqsr}\" \\\n\t--input \"${bam_marked_dup}\" \\\n\t--output \"${bam_preprocessed_final}\" \\\n\t--bqsr-recal-file \"${bqsr_table}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/applyBqsr_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess collectWgsMetrics_gatk {\n\tpublishDir \"${params.output_dir}/preprocessing/coverageMetrics\", mode: 'copy'\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple path(bam_preprocessed_final), path(reference_genome_fasta_forCollectWgsMetrics), path(reference_genome_fasta_index_forCollectWgsMetrics), path(reference_genome_fasta_dict_forCollectWgsMetrics), path(autosome_chromosome_list) from final_preprocessed_bams_forCollectWgsMetrics.combine( reference_genome_bundle_forCollectWgsMetrics)\n\n\toutput:\n\tpath coverage_metrics\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tsample_id = \"${bam_preprocessed_final}\".replaceFirst(/\\.final\\.bam/, \"\")\n\tcoverage_metrics = \"${sample_id}.coverage.metrics.txt\"\n\t\"\"\"\n\tgatk CollectWgsMetrics \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 2}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--TMP_DIR . \\\n\t--INCLUDE_BQ_HISTOGRAM \\\n\t--MINIMUM_BASE_QUALITY 20 \\\n\t--MINIMUM_MAPPING_QUALITY 20 \\\n\t--REFERENCE_SEQUENCE \"${reference_genome_fasta_forCollectWgsMetrics}\" \\\n\t--INTERVALS \"${autosome_chromosome_list}\" \\\n\t--INPUT \"${bam_preprocessed_final}\" \\\n\t--OUTPUT \"${coverage_metrics}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/collectWgsMetrics_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess collectGcBiasMetrics_gatk {\n\tpublishDir \"${params.output_dir}/preprocessing/gcBiasMetrics\", mode: 'copy'\n\ttag \"${sample_id}\"\n\n\tinput:\n\ttuple path(bam_preprocessed_final), path(reference_genome_fasta_forCollectGcBiasMetrics), path(reference_genome_fasta_index_forCollectGcBiasMetrics), path(reference_genome_fasta_dict_forCollectGcBiasMetrics) from final_preprocessed_bams_forCollectGcBiasMetrics.combine(reference_genome_bundle_forCollectGcBiasMetrics)\n\n\toutput:\n\tpath gc_bias_metrics\n\tpath gc_bias_chart\n\tpath gc_bias_summary\n\n\twhen:\n\tparams.skip_to_qc == \"no\"\n\n\tscript:\n\tsample_id = \"${bam_preprocessed_final}\".replaceFirst(/\\.final\\.bam/, \"\")\n\tgc_bias_metrics = \"${sample_id}.gcbias.metrics.txt\"\n\tgc_bias_chart = \"${sample_id}.gcbias.metrics.pdf\"\n\tgc_bias_summary = \"${sample_id}.gcbias.summary.txt\"\n\t\"\"\"\n\tgatk CollectGcBiasMetrics \\\n\t--java-options \"-Xmx${task.memory.toGiga() - 2}G -Djava.io.tmpdir=.\" \\\n\t--VERBOSITY ERROR \\\n\t--TMP_DIR . \\\n\t--REFERENCE_SEQUENCE \"${reference_genome_fasta_forCollectGcBiasMetrics}\" \\\n\t--INPUT \"${bam_preprocessed_final}\" \\\n\t--OUTPUT \"${gc_bias_metrics}\" \\\n\t--CHART_OUTPUT \"${gc_bias_chart}\" \\\n\t--SUMMARY_OUTPUT \"${gc_bias_summary}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/collectGcBiasMetrics_gatk"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["pblaney"], "nb_wf": 1, "list_wf": ["mgp1000"], "list_contrib": ["pblaney"], "nb_contrib": 1, "codes": ["\nprocess extremeBamQualityControl_qualimap {\n\tpublishDir \"${params.output_dir}/preprocessing/qualimapBamQc\", mode: 'copy'\n\ttag \"${bam_mapped_high_coverage.baseName}\"\n\n\tinput:\n\tpath bam_mapped_high_coverage from input_mapped_bams_forQaulimap\n\n\toutput:\n\tpath \"${bam_mapped_high_coverage.baseName}\"\n\n\twhen:\n\tparams.input_format == \"bam\"\n\tparams.skip_to_qc == \"yes\"\n\n\tscript:\n\t\"\"\"\n\tqualimap --java-mem-size=12G \\\n\tbamqc \\\n\t-bam \"${bam_mapped_high_coverage}\" \\\n\t--paint-chromosome-limits \\\n\t--genome-gc-distr HUMAN \\\n\t-nt ${task.cpus} \\\n\t-outformat HTML \\\n\t-outdir \"${bam_mapped_high_coverage.baseName}\"\n\t\"\"\"\n}"], "list_proc": ["pblaney/mgp1000/extremeBamQualityControl_qualimap"], "list_wf_names": ["pblaney/mgp1000"]}, {"nb_reuse": 2, "tools": ["Salmon", "QUAST"], "nb_own": 2, "list_own": ["telatin", "pditommaso"], "nb_wf": 2, "list_wf": ["nextflow-example", "nf-course-unimi-2021"], "list_contrib": ["telatin", "pditommaso", "vmikk"], "nb_contrib": 3, "codes": ["\nprocess quast  {\n    tag \"quast\"\n    \n    publishDir \"$params.outdir/\", \n        mode: 'copy'\n    \n    input:\n    path(\"*\")  \n    \n    \n    output:\n    path(\"quast\")\n\n    script:\n    \"\"\"\n    quast --threads ${task.cpus} --output-dir quast *.fa\n    \"\"\"\n}", "\nprocess index {\n    \n    input:\n    path transcriptome from params.transcript\n     \n    output:\n    path 'index' into index_ch\n\n    script:       \n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["telatin/nextflow-example/quast", "pditommaso/nf-course-unimi-2021/index"], "list_wf_names": ["pditommaso/nf-course-unimi-2021", "telatin/nextflow-example"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FastQC", "MultiQC"], "nb_own": 3, "list_own": ["sickle-in-africa", "trev-f", "pditommaso"], "nb_wf": 3, "list_wf": ["saw.sarek", "nf-course-unimi-2021", "SRAtac"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "trev-f", "skrakau", "lescai", "pditommaso", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "t-f-freeman", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) from read_pairs2_ch\n\n    output:\n    path(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"  \n}", "process ReadsMultiQC {\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}/readsQC/trim\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/readsQC/trim\", mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        file fastqc\n        val runName\n        val toolIDs\n\n    output:\n        path \"*\"\n\n    script:\n                                       \n        toolIDs += 'mqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        \"\"\"\n        multiqc \\\n            -n ${runName}${suffix} \\\n            --module fastqc \\\n            ${fastqc}\n        \"\"\"\n}", "\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}"], "list_proc": ["pditommaso/nf-course-unimi-2021/fastqc", "trev-f/SRAtac/ReadsMultiQC", "sickle-in-africa/saw.sarek/UMIMapBamFile"], "list_wf_names": ["pditommaso/nf-course-unimi-2021", "sickle-in-africa/saw.sarek", "trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["PncStress"], "nb_own": 1, "list_own": ["pditommaso"], "nb_wf": 1, "list_wf": ["nf-foo"], "list_contrib": ["pditommaso"], "nb_contrib": 1, "codes": ["\nprocess stress_1cpu {\n  \"\"\"\n  stress -c 1 -t 5\n  \"\"\"\n}"], "list_proc": ["pditommaso/nf-foo/stress_1cpu"], "list_wf_names": ["pditommaso/nf-foo"]}, {"nb_reuse": 1, "tools": ["PncStress"], "nb_own": 1, "list_own": ["pditommaso"], "nb_wf": 1, "list_wf": ["nf-foo"], "list_contrib": ["pditommaso"], "nb_contrib": 1, "codes": ["\nprocess stress_and_fail {\n  errorStrategy 'ignore'\n  input: val x from Channel.from(1,2,3)\n  when:\n  params.fail \n  \"\"\"\n  stress -c 1 -t $x\n  exit $x\n  \"\"\"\n}"], "list_proc": ["pditommaso/nf-foo/stress_and_fail"], "list_wf_names": ["pditommaso/nf-foo"]}, {"nb_reuse": 1, "tools": ["PncStress"], "nb_own": 1, "list_own": ["pditommaso"], "nb_wf": 1, "list_wf": ["nf-foo"], "list_contrib": ["pditommaso"], "nb_contrib": 1, "codes": ["\nprocess stress_100mega {\n  memory { 150.MB * x } \n  input: val x from Channel.from(1,2,3)\n  \"\"\"\n  stress -m 1 --vm-bytes ${x}00000000 -t 10\n  \"\"\"\n}"], "list_proc": ["pditommaso/nf-foo/stress_100mega"], "list_wf_names": ["pditommaso/nf-foo"]}, {"nb_reuse": 1, "tools": ["PncStress"], "nb_own": 1, "list_own": ["pditommaso"], "nb_wf": 1, "list_wf": ["nf-foo"], "list_contrib": ["pditommaso"], "nb_contrib": 1, "codes": ["\nprocess stress_200mega {\n  memory 250.MB\n                                              \n  \"\"\"\n  stress -m 1 --vm-bytes 200000000 -t 5\n  stress -m 1 --vm-bytes 100000000 -t 5\n  \"\"\"\n}"], "list_proc": ["pditommaso/nf-foo/stress_200mega"], "list_wf_names": ["pditommaso/nf-foo"]}, {"nb_reuse": 1, "tools": ["PncStress"], "nb_own": 1, "list_own": ["pditommaso"], "nb_wf": 1, "list_wf": ["nf-foo"], "list_contrib": ["pditommaso"], "nb_contrib": 1, "codes": ["\nprocess stress_300mega {\n  memory 350.MB\n  cpus 2\n                                                  \n  \"\"\"\n  stress -m 2 --vm-bytes 150000000 -t 5\n  \"\"\"\n}"], "list_proc": ["pditommaso/nf-foo/stress_300mega"], "list_wf_names": ["pditommaso/nf-foo"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peikovakate"], "nb_wf": 1, "list_wf": ["fastq"], "list_contrib": ["peikovakate"], "nb_contrib": 1, "codes": ["\nprocess cram_to_fastq{\n    publishDir \"${params.outdir}/cram_to_fastq_results/\", mode: 'copy'\n    memory '7 GB'\n    cpus 2\n\n    input:\n    file cram_file from crams\n    path \"ref.fa\" from params.fa_ref\n\n    output:\n    file \"${cram_file.simpleName}_1.fastq.gz\"\n    file \"${cram_file.simpleName}_2.fastq.gz\"\n\n    script:\n    \"\"\"\n    samtools collate $cram_file ${cram_file.simpleName}.collated\n    samtools fastq -c 6 -1 ${cram_file.simpleName}_1.fastq.gz -2 ${cram_file.simpleName}_2.fastq.gz --reference ref.fa ${cram_file.simpleName}.collated.bam\n    \"\"\"\n}"], "list_proc": ["peikovakate/fastq/cram_to_fastq"], "list_wf_names": ["peikovakate/fastq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["peikovakate"], "nb_wf": 1, "list_wf": ["fastq"], "list_contrib": ["peikovakate"], "nb_contrib": 1, "codes": ["\nprocess alignWithHisat2 { \n    input: \n    file hs2_indices from hs2_indices.collect()\n    tuple val(sample), path(read_1), path(read_2) from align_fastq\n\n    output:\n    tuple val(sample), path(\"${sample}.bam\") into aligned_bam\n    \n    script:\n    index_base = hs2_indices[0].toString() - ~/.\\d.ht2/\n\n    \"\"\"\n    hisat2 -x ${index_base} -1 ${read_1} -2 ${read_2} \\\\\n        | samtools view -b > ${sample}.bam\n    \"\"\"\n\n}"], "list_proc": ["peikovakate/fastq/alignWithHisat2"], "list_wf_names": ["peikovakate/fastq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peikovakate"], "nb_wf": 1, "list_wf": ["fastq"], "list_contrib": ["peikovakate"], "nb_contrib": 1, "codes": ["\nprocess sortBam {\n    input:\n    tuple val(sample), path(aligned) from aligned_bam\n\n    output:\n    tuple val(sample), path(\"${sample}_sorted.bam\") into bin_scores\n\n    \"\"\"\n    samtools sort ${aligned} -O bam -o ${sample}_sorted.bam\n    \"\"\"\n}"], "list_proc": ["peikovakate/fastq/sortBam"], "list_wf_names": ["peikovakate/fastq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peikovakate"], "nb_wf": 1, "list_wf": ["fastq"], "list_contrib": ["peikovakate"], "nb_contrib": 1, "codes": ["\nprocess bamToCram {\n    publishDir \"${params.outdir}\", mode: \"copy\"\n\n    input:\n    tuple val(sample), path(qualbin) from to_cram\n    path \"ref.fa\" from params.fa_ref\n\n    output:\n    path \"${sample}.cram\"\n\n    script:\n    \"\"\"\n    samtools view ${qualbin} -C -T ref.fa > ${sample}.cram\n    \"\"\"\n}"], "list_proc": ["peikovakate/fastq/bamToCram"], "list_wf_names": ["peikovakate/fastq"]}, {"nb_reuse": 1, "tools": ["PanelApp"], "nb_own": 1, "list_own": ["perllb"], "nb_wf": 1, "list_wf": ["ctg-exome"], "list_contrib": ["perllb"], "nb_contrib": 1, "codes": ["\nprocess dragen_align_vc {\n\n    tag \"${sid}_${projid}\"\n    label 'dragen' \n\n    input:\n    val x from run_analysis\n    set sid, projid, ref, panel, annotate from analyze_csv\n\n    output:\n    val x into done_analyze\n    val projid into dragen_metrics\n    set sid, projid, ref, annotate, val(\"${OUTDIR}/${projid}/dragen/vcf/${sid}/${sid}.hard-filtered.vcf.gz\"), val(\"${OUTDIR}/${projid}/dragen/vcf/${sid}/${sid}.diploidSV.vcf.gz\") into annotate_vcf\n    \n    \"\"\"\n    export LC_ALL=C\n\n    R1=\\$(echo ${OUTDIR}/${projid}/fastq/${sid}/${sid}*_R1_*fastq.gz)\n    R2=\\$(echo ${OUTDIR}/${projid}/fastq/${sid}/${sid}*_R2_*fastq.gz)\n\n    # Get target panel file\n    if [ $panel == \"comprehensive\" ] || [ $panel == \"Comprehensive\" ]\n    then\n\tif [ $ref == 'hg38' ]; then\n\t    targetfile=${params.target_twist_comprehensive_hg38}\n\telif [ $ref == 'mm10' ]; then\n\t    targetfile=${params.target_twist_comprehensive_mm10}\n        fi\n    elif [ $panel == \"core\" ] || [ $panel == \"Core\" ]\n    then\n        if [ $ref == 'hg38' ]; then\n\t    targetfile=${params.target_twist_core_hg38}\n\telif [ $ref == 'mm10' ]; then\n\t    targetfile=${params.target_twist_core_mm10}\n        fi\n    elif [ $panel == \"custom\"  ] || [ $panel == \"Custom\" ] \n    then\n        targetfile=${params.custom_target}\n    else\n        echo '>PANEL NOT RECOGNIZED!'\n\techo 'in samplesheet - only 'comprehensive', 'core' and 'custom' can be specified in 'panel' section'\n        targetfile='ERR'\n    fi\n\n    # Get species based on ref\n    if [[ $ref == hg* ]]\n    then \n    \tspecies='human'\n    elif [[ $ref == mm* ]]\n    then\n\tspecies='mouse'\n    else\n        echo 'species cannot be extracted from reference: $ref'\n\texit 1;\n    fi\n\n    dragendir=${OUTDIR}/${projid}/dragen/\n    mkdir -p \\$dragendir\n    mkdir -p \\${dragendir}/metrics\n    mkdir -p \\${dragendir}/vcf/\n    mkdir -p \\${dragendir}/vcf/${sid}\n\n    outdir=\\${dragendir}/metrics/${sid}\n    mkdir -p \\$outdir\n\n    echo \"R1: '\\${R1}'\"\n    echo \"R2: '\\${R2}'\"\n    echo \"sid: '${sid}'\"\n    echo \"padding: '${params.padding}'\"\n    echo \"outdir: '\\${outdir}'\"\n    echo \"targetfile: '\\${targetfile}'\"\n    echo \"species: '\\${species}'\"   \n\n    /opt/edico/bin/dragen -f -r /staging/\\$species/reference/$ref \\\\\n        -1 \\${R1} \\\\\n        -2 \\${R2} \\\\\n        --RGID ${projid}_${sid} \\\\\n        --RGSM $sid \\\\\n        --intermediate-results-dir /staging/tmp/ \\\\\n        --enable-map-align true \\\\\n        --enable-map-align-output true \\\\\n        --vc-target-bed \\$targetfile \\\\\n        --vc-target-bed-padding ${params.padding} \\\\\n        --output-format bam \\\\\n        --output-directory \\$outdir \\\\\n        --enable-variant-caller true \\\\\n        --enable-sv true \\\\\n        --output-file-prefix $sid \\\\\n        --remove-duplicates true \\\\\n        --qc-coverage-region-1 \\$targetfile \\\\\n\t--qc-coverage-region-padding-1 ${params.padding} \\\\\n        --qc-coverage-ignore-overlaps true \\\\\n        --qc-coverage-filters-1 \"mapq<20,bq<20\"\n\t\n    # move vcfs to vcf dir     \n    # SNV\n    mv \\${outdir}/${sid}.hard-filtered.vcf.gz \\${dragendir}/vcf/${sid}/\n    # SV\n    mv \\${outdir}/sv/results/variants/diploidSV.vcf.gz \\${dragendir}/vcf/${sid}/${sid}.diploidSV.vcf.gz\n    \"\"\"\n}"], "list_proc": ["perllb/ctg-exome/dragen_align_vc"], "list_wf_names": ["perllb/ctg-exome"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["perllb"], "nb_wf": 1, "list_wf": ["ctg-exome"], "list_contrib": ["perllb"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n\n\ttag \"${sid}_${projid}\"\n\t\n\tinput:\n\tset sid, projid from fastqc_go\n\n        output:\n        val projid into multiqc_fastqc\n\n\t\n\t\"\"\"\n\n        qcdir=${OUTDIR}/${projid}/qc\n        fqcdir=${OUTDIR}/${projid}/qc/fastqc\n        mkdir -p \\$qcdir\n        mkdir -p \\$fqcdir \n\n\tread1=\\$(echo ${OUTDIR}/$projid/fastq/$sid/$sid*R1*fastq.gz)\n   \tread2=\\$(echo ${OUTDIR}/$projid/fastq/$sid/$sid*R2*fastq.gz)\n\n    \t# Check if fastq is not found with pattern above (due to sample fastq are not put in sample id folder\n    \tif [[ \\$read1 == *\"*R1*\"* ]]; then\n       \t   read1=\\$(echo ${FQDIR}/${projid}/${sid}/${sid}*R1*fastq.gz)\n       \t   read2=\\$(echo ${FQDIR}/${projid}/${sid}/${sid}*R2*fastq.gz)\n    \tfi\n\n\tfastqc -t $task.cpus --outdir \\$fqcdir \\$read1\n\tfastqc -t $task.cpus --outdir \\$fqcdir \\$read2\n \t\n\t\"\"\"\n    \n}"], "list_proc": ["perllb/ctg-exome/fastqc"], "list_wf_names": ["perllb/ctg-exome"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["perllb"], "nb_wf": 1, "list_wf": ["ctg-exome"], "list_contrib": ["perllb"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n\n    tag \"${projid}\"\n\n    input:\n    set projid, projid2 from multiqc_fastqc.unique().phase(multiqc_dragen.unique())\n\n\n    \"\"\"\n    \n    cd $OUTDIR\n    multiqc -f ${OUTDIR}/$projid/ --outdir ${OUTDIR}/$projid/qc/multiqc/ -n ${projid}_exome_dragen_report.html\n\n    mkdir -p ${CTGQC}\n    mkdir -p ${CTGQC}/$projid\n\n    cp -r ${OUTDIR}/$projid/qc ${CTGQC}/$projid/\n\n    \"\"\"\n\n}"], "list_proc": ["perllb/ctg-exome/multiqc"], "list_wf_names": ["perllb/ctg-exome"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["perllb"], "nb_wf": 1, "list_wf": ["ctg-exome"], "list_contrib": ["perllb"], "nb_contrib": 1, "codes": ["\nprocess multiqc_run {\n\n\ttag \"$metaID\"\n\n\tinput:\n\tval x from completed.collect()\n\n\t\"\"\"\n\n\tcd $OUTDIR\n\tmkdir -p ${OUTDIR}/qc\n\tmultiqc -f ${OUTDIR} $params.runfolder/ctg-interop --outdir ${OUTDIR}/qc/ -n exome_dragen_run_${metaID}_multiqc_report.html\n\n\tmkdir -p ${CTGQC}\n\n\tcp -r ${OUTDIR}/qc ${CTGQC}\n\t\"\"\"\n}"], "list_proc": ["perllb/ctg-exome/multiqc_run"], "list_wf_names": ["perllb/ctg-exome"]}, {"nb_reuse": 1, "tools": ["snippy"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-biohansel-snippy-comparison"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess SNIPPY {\n  tag \"$accession\"\n  publishDir \"$outdir/snippy\", mode: 'symlink'\n\n  input:\n    val mincov\n    tuple val(scheme),\n          path(scheme_fasta),\n          val(accession),\n          path(reads1),\n          path(reads2),\n          path(ref_genbank)\n  output:\n    tuple val(scheme),\n          val(accession),\n          path(\"$accession\"), emit: 'results'\n    tuple val(scheme),\n          val(1),\n          val(task.cpus),\n          val('snippy'),\n          path('.command.trace'),\n          path(reads1),\n          path(reads2), emit: 'trace'\n\n  script:\n  \"\"\"\n  snippy --prefix $accession \\\\\n    --outdir $accession \\\\\n    --cpus ${task.cpus} \\\\\n    --ram ${task.memory.toGiga()} \\\\\n    --mincov $mincov \\\\\n    --R1 $reads1 \\\\\n    --R2 $reads2 \\\\\n    --ref $ref_genbank \\\\\n    --tmpdir ./\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-biohansel-snippy-comparison/SNIPPY"], "list_wf_names": ["peterk87/nf-biohansel-snippy-comparison"]}, {"nb_reuse": 1, "tools": ["snippy"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-biohansel-sra-benchmark"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess snippy {\n  tag \"CPU=$ncpus|$accession\"\n                                                                       \n  cpus { ncpus }\n\n  input:\n    each ncpus from thread_combos\n    set val(scheme), file(scheme_fasta), val(accession), file(reads1), file(reads2), file(metadata), file(ref_genbank) from ch_fastqs_snippy\n  output:\n    set val(scheme), val(1), val(ncpus), val('snippy'), file('.command.trace'), val(1), file(reads1), file(reads2) into ch_snippy_trace\n\n  script:\n  \"\"\"\n  snippy --prefix $accession \\\\\n    --outdir out \\\\\n    --cpus ${task.cpus} \\\\\n    --ram ${task.memory.toGiga()} \\\\\n    --R1 $reads1 \\\\\n    --R2 $reads2 \\\\\n    --ref $ref_genbank \\\\\n    --cleanup \\\\\n    --tmpdir ./\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-biohansel-sra-benchmark/snippy"], "list_wf_names": ["peterk87/nf-biohansel-sra-benchmark"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-cleanplex-preprocess"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess FASTP {\n  label \"process_medium\"\n  publishDir \"${params.outdir}/preprocess/fastp\",\n             saveAs: { filename ->\n                          if (filename.endsWith(\".json\")) filename\n                          else if (filename.endsWith(\".fastp.html\")) filename\n                          else if (filename.endsWith(\"_fastqc.html\")) \"fastqc/$filename\"\n                          else if (filename.endsWith(\".zip\")) \"fastqc/zips/$filename\"\n                          else if (filename.endsWith(\".log\")) \"log/$filename\"\n                          else params.save_fastp_trimmed ? filename : null\n                    },\n             mode: 'copy'\n\n  input:\n  tuple val(sample), path(reads)\n\n  output:\n  tuple val(sample), path(reads), emit: reads\n  path \"*.{log,fastp,html}\"\n\n  script:\n  \"\"\"\n  fastp \\\\\n    --in1 ${reads[0]} \\\\\n    --in2 ${reads[1]} \\\\\n    --out1 ${sample}_1.trim.fastq.gz \\\\\n    --out2 ${sample}_2.trim.fastq.gz \\\\\n    --detect_adapter_for_pe \\\\\n    --cut_front \\\\\n    --cut_tail \\\\\n    --cut_mean_quality 30 \\\\\n    --qualified_quality_phred 30 \\\\\n    --unqualified_percent_limit 10 \\\\\n    --length_required 50 \\\\\n    --trim_poly_x \\\\\n    --thread ${task.cpus} \\\\\n    --json ${sample}.fastp.json \\\\\n    --html ${sample}.fastp.html\n  cp .command.log ${sample}.fastp.log\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-cleanplex-preprocess/FASTP"], "list_wf_names": ["peterk87/nf-cleanplex-preprocess"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-cleanplex-preprocess"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess BWA_MEM2_MAP {\n  label \"process_medium\"\n  publishDir \"${params.outdir}/mapping/untrimmed\",\n             pattern: \"*.sorted.{bam,bam.bai}\",\n             mode: 'copy'\n\n  input:\n  tuple val(sample), path(reads), path(ref_fasta), path(ref_index)\n\n  output:\n  tuple val(sample), path(\"*.sorted.{bam,bam.bai}\")\n\n  script:\n  \"\"\"\n  bwa-mem2 mem -t ${task.cpus} $ref_fasta $reads \\\\\n  | samtools sort -@ ${task.cpus} -T $sample \\\\\n  > ${sample}.sorted.bam\n  samtools index ${sample}.sorted.bam\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-cleanplex-preprocess/BWA_MEM2_MAP"], "list_wf_names": ["peterk87/nf-cleanplex-preprocess"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-cleanplex-preprocess"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess FGBIO_TRIM_PRIMERS {\n  label \"process_low\"\n  publishDir \"${params.outdir}/mapping/trimmed\",\n             pattern: \"*.trim.{bam,bam.bai}\",\n             mode: 'copy'\n\n  input:\n  tuple val(sample), path(bam), path(primers_table)\n\n  output:\n  tuple val(sample), path(\"*.trim.{bam,bam.bai}\")\n\n  script:\n  \"\"\"\n  fgbio \\\\\n    --sam-validation-stringency=LENIENT \\\\\n    TrimPrimers \\\\\n      -i ${bam[0]} \\\\\n      -o ${sample}.sorted.fgbio.trim.bam \\\\\n      -p $primers_table \\\\\n      -H true\n  samtools index ${sample}.sorted.fgbio.trim.bam\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-cleanplex-preprocess/FGBIO_TRIM_PRIMERS"], "list_wf_names": ["peterk87/nf-cleanplex-preprocess"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-cleanplex-preprocess"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess PRIMER_TRIMMED_BAM_TO_FASTQ {\n  publishDir \"${params.outdir}/fastq/primer_trimmed\",\n             pattern: \"*.fastq.gz\",\n             mode: 'copy'\n  input:\n  tuple val(sample), path(bam_and_index)\n\n  output:\n  tuple val(sample), path(\"*.fastq.gz\")\n\n  script:\n  \"\"\"\n  samtools fastq \\\\\n    -1 ${sample}_1.fastq.gz \\\\\n    -2 ${sample}_2.fastq.gz \\\\\n    ${bam_and_index[0]}\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-cleanplex-preprocess/PRIMER_TRIMMED_BAM_TO_FASTQ"], "list_wf_names": ["peterk87/nf-cleanplex-preprocess"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["process MULTIQC {\n  publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n  input:\n  path(multiqc_config)\n  path(mqc_custom_config)\n  path('fastqc/*')\n  path('samtools/*')\n  path('mosdepth/*')\n  path('mash_screen/*')\n  path('bcftools/*')\n  path('edlib/*')\n  path('consensus/*')\n  path('software_versions/*')\n  path(workflow_summary)\n\n  output:\n  path \"*multiqc_report.html\", emit: multiqc_report\n  path \"*_data\"\n  path \"multiqc_plots\"\n\n  script:\n  custom_runName = params.name\n  if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n    custom_runName = workflow.runName\n  }\n  rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n  rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n  custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                     \n  \"\"\"\n  multiqc -f $rtitle $rfilename $custom_config_file .\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/MULTIQC"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["BCFtools", "SAMtools", "MultiQC", "TMAP", "FastQC", "Mash"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["process SOFTWARE_VERSIONS {\n  tag \"Parse software version numbers\"\n  publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n      saveAs: { filename ->\n                  if (filename.indexOf(\".csv\") > 0) filename\n                  else null\n              }\n\n  output:\n  path 'software_versions_mqc.yaml', emit: software_versions_yaml\n  path \"software_versions.csv\"\n\n  script:\n  \"\"\"\n  echo $workflow.manifest.version > v_pipeline.txt\n  echo $workflow.nextflow.version > v_nextflow.txt\n  fastqc --version > v_fastqc.txt\n  multiqc --version > v_multiqc.txt\n  tmap --version 2> v_tmap.txt\n  tvc --version > v_tvc.txt\n  samtools --version > v_samtools.txt\n  bcftools --version > v_bcftools.txt\n  mash --version > v_mash.txt\n  pigz --version 2> v_pigz.txt\n  scrape_software_versions.py &> software_versions_mqc.yaml\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/SOFTWARE_VERSIONS"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["process SAMTOOLS_STATS {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/samtools/$sample\",\n             mode: params.publish_dir_mode\n  \n  input:\n  tuple val(sample), path(bam), path(ref_fasta)\n\n  output:\n  path('*.{flagstat,idxstats,stats}')\n\n  script:\n  \"\"\"\n  samtools flagstat *.bam > ${sample}.flagstat\n  samtools idxstats *.bam > ${sample}.idxstats\n  samtools stats *.bam > ${sample}.stats\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/SAMTOOLS_STATS"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess BCFTOOLS_VCF_FILTER {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/variants/vcf\",\n             pattern: \"*.filt.vcf\",\n             mode: params.publish_dir_mode\n\n  input:\n  tuple val(sample), path(bam), path(vcf), path(ref_fasta)\n\n  output:\n  tuple val(sample), path(bam), path('*.filt.vcf'), path(ref_fasta)\n\n  script:\n  \"\"\"\n  bcftools filter -e 'INFO/AF<0.5' -Ov $vcf > ${sample}.norm.filt.vcf\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/BCFTOOLS_VCF_FILTER"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["process FASTQC {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n      saveAs: { filename ->\n                    filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n              }\n\n  input:\n  tuple val(sample), path(reads)\n\n  output:\n  path \"*_fastqc.{zip,html}\"\n\n  script:\n  \"\"\"\n  fastqc --quiet --threads $task.cpus $reads\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/FASTQC"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["mosdepth"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["process MOSDEPTH_GENOME {\n  tag \"$sample\"\n  label 'process_medium'\n  publishDir \"${params.outdir}/mosdepth\", \n             mode: params.publish_dir_mode,\n             saveAs: { filename ->\n               if (filename.endsWith(\".pdf\")) \"plots/$filename\"\n               else if (filename.endsWith(\".tsv\")) \"plots/$filename\"\n               else filename\n             }\n\n  input:\n  tuple val(sample), path(bam), path(ref_fasta)\n\n  output:\n  path \"*.global.dist.txt\", emit: mqc\n  path \"*.{txt,gz,csi,tsv}\"\n\n  script:\n  \"\"\"\n  mosdepth \\\\\n      --by 200 \\\\\n      --fast-mode \\\\\n      $sample \\\\\n      ${bam[0]}\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/MOSDEPTH_GENOME"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["seqtk", "Mash"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-ionampliseq"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["process MASH_SCREEN {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/mash_screen\",\n             pattern: \"*-mash_screen.tsv\",\n             mode: params.publish_dir_mode\n  publishDir \"${params.outdir}/mash_screen\",\n             pattern: \"*-top_ref.fasta\",\n             mode: params.publish_dir_mode\n\n  input:\n  tuple val(sample), path(reads), path(ref_fasta)\n\n  output:\n  tuple val(sample), path('*-mash_screen.tsv'), emit: results\n  tuple val(sample), path('*-mash_screen-top_ref.fasta'), emit: top_ref\n\n  script:\n  \"\"\"\n  mash sketch -i -k ${params.mash_k} -s ${params.mash_s} $ref_fasta\n  mash screen -p ${task.cpus} -w ${ref_fasta}.msh $reads | sort -grk1 > ${sample}-mash_screen.tsv\n  cat ${sample}-mash_screen.tsv | head -n1 | awk '{print \\$5}' > ${sample}-mash_screen-top_ref.txt\n  seqtk subseq $ref_fasta ${sample}-mash_screen-top_ref.txt > ${sample}-mash_screen-top_ref.fasta\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-ionampliseq/MASH_SCREEN"], "list_wf_names": ["peterk87/nf-ionampliseq"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-villumina"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess KRAKEN2 {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/kraken2/results\", pattern: \"*-kraken2_results.tsv\", mode: 'copy'\n  publishDir \"${params.outdir}/kraken2/reports\", pattern: \"*-kraken2_report.tsv\", mode: 'copy'\n\n  input:\n    path(kraken2_db_dir)\n    tuple sample_id,\n          path(reads1),\n          path(reads2)\n  output:\n    tuple sample_id,\n          path(reads1),\n          path(reads2),\n          path(results),\n          path(report)\n\n  script:\n  results = \"${sample_id}-kraken2_results.tsv\"\n  report = \"${sample_id}-kraken2_report.tsv\"\n  \"\"\"\n  kraken2 --memory-mapping --threads ${task.cpus} \\\\\n    --db ./${kraken2_db_dir}/ \\\\\n    --output ${results} \\\\\n    --report ${report} \\\\\n    $reads1 $reads2\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-villumina/KRAKEN2"], "list_wf_names": ["peterk87/nf-villumina"]}, {"nb_reuse": 1, "tools": ["Centrifuge"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-villumina"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess CENTRIFUGE {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/centrifuge/results\", pattern: \"*-centrifuge_results.tsv\", mode: 'copy'\n  publishDir \"${params.outdir}/centrifuge/reports\", pattern: \"*-centrifuge_kreport.tsv\", mode: 'copy'\n                                                                                \n  memory {\n    file_sizes = file(centrifuge_db_dir).listFiles()\n      .findAll { it.isFile() && file(it).getExtension() == 'cf' }\n      .collect { it.size() }\n      .inject(0, { r, i -> r + i })\n      .toLong()\n    (file_sizes * 1.25).toLong()\n  }\n\n  input:\n    tuple db_name, \n          path(centrifuge_db_dir)\n    tuple sample_id,\n          path(reads1),\n          path(reads2)\n  output:\n    tuple sample_id,\n          path(reads1),\n          path(reads2),\n          path(results),\n          path(kreport)\n\n  script:\n  results = \"${sample_id}-centrifuge_results.tsv\"\n  kreport = \"${sample_id}-centrifuge_kreport.tsv\"\n  \"\"\"\n  centrifuge -x ${centrifuge_db_dir}/${db_name} \\\\\n    -1 $reads1 -2 $reads2 \\\\\n    -S $results -p ${task.cpus} \n  centrifuge-kreport -x ${centrifuge_db_dir}/${db_name} $results > $kreport\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-villumina/CENTRIFUGE"], "list_wf_names": ["peterk87/nf-villumina"]}, {"nb_reuse": 1, "tools": ["Unicycler"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-villumina"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess UNICYCLER_ASSEMBLY {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/assemblies/unicycler/$sample_id\", mode: 'copy'\n\n  input:\n    tuple(val(sample_id), path(reads1), path(reads2))\n  output:\n    tuple sample_id, val('unicycler'), path(output_contigs, optional: true), emit: 'contigs'\n    tuple sample_id, path(output_unicycler_log, optional: true), emit: 'log'\n    tuple sample_id, path(output_gfa, optional: true), emit: 'gfa'\n\n  script:\n  output_contigs = \"${sample_id}-assembly.fasta\"\n  output_gfa = \"${sample_id}-assembly.gfa\"\n  output_unicycler_log = \"${sample_id}-unicycler.log\"\n  \"\"\"\n  unicycler -t ${task.cpus} --mode ${params.unicycler_mode} -o $sample_id -1 $reads1 -2 $reads2\n  ln -s ${sample_id}/assembly.fasta $output_contigs\n  ln -s ${sample_id}/assembly.gfa $output_gfa\n  ln -s ${sample_id}/unicycler.log $output_unicycler_log\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-villumina/UNICYCLER_ASSEMBLY"], "list_wf_names": ["peterk87/nf-villumina"]}, {"nb_reuse": 1, "tools": ["shovill"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-villumina"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess SHOVILL_ASSEMBLY {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/assemblies/shovill/$sample_id\", mode: 'copy'\n\n  input:\n    tuple val(sample_id), path(reads1), path(reads2)\n  output:\n    tuple sample_id, val('shovill'), path(output_contigs, optional: true), emit: 'contigs'\n    tuple sample_id, path(output_shovill_log, optional: true), emit: 'log'\n    tuple sample_id, path(output_gfa, optional: true), emit: 'gfa'\n\n  script:\n  output_contigs = \"${sample_id}-contigs.fasta\"\n  output_gfa = \"${sample_id}-contigs.gfa\"\n  output_shovill_log = \"${sample_id}-shovill.log\"\n  shovill_trim = (params.shovill_trim) ? \"--trim\" : \"\"\n  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory.toGiga()} \\\\\n    --R1 $reads1 --R2 $reads2 \\\\\n    --mincov 0.1 --depth 0 $shovill_trim \\\\\n    --outdir $sample_id \n  ln -s ${sample_id}/contigs.fa $output_contigs\n  ln -s ${sample_id}/contigs.gfa $output_gfa\n  ln -s ${sample_id}/shovill.log $output_shovill_log\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-villumina/SHOVILL_ASSEMBLY"], "list_wf_names": ["peterk87/nf-villumina"]}, {"nb_reuse": 1, "tools": ["MEGAHIT"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-villumina"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess MEGAHIT_ASSEMBLY {\n  publishDir \"${params.outdir}/assemblies/megahit/$sample_id\", mode: 'copy'\n\n  input:\n    tuple val(sample_id), path(reads1), path(reads2)\n  output:\n    tuple sample_id, val('megahit'), path(output_contigs, optional: true), emit: 'contigs'\n    tuple sample_id, path(output_log, optional: true), emit: 'log'\n\n  script:\n  output_contigs = \"${sample_id}-contigs.fasta\"\n  output_log = \"${sample_id}-megahit.log\"\n  megahit_preset = (params.megahit_preset.toString() == '') ? '' : \"--presets ${params.megahit_preset}\"\n  \"\"\"\n  megahit \\\\\n    -t ${task.cpus} \\\\\n    -m ${task.memory.toBytes()} \\\\\n    $megahit_preset \\\\\n    -1 $reads1 \\\\\n    -2 $reads2 \\\\\n    -o out \\\\\n    --out-prefix out\n  ln -s out/out.contigs.fa $output_contigs\n  ln -s out/out.log $output_log\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-villumina/MEGAHIT_ASSEMBLY"], "list_wf_names": ["peterk87/nf-villumina"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-villumina"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess BLASTN {\n  publishDir \"${params.outdir}/blastn/$assembler\", pattern: \"*.tsv\", mode: 'copy'\n  tag \"$sample_id|$dbname|$assembler|$txids\"\n\n  input:\n    tuple dbname, path(dbdir)\n    path(txids)\n    tuple val(sample_id), val(assembler), path(contigs)\n  output:\n    tuple val(sample_id), val(assembler), path(contigs), path(blast_out)\n\n  script:\n  taxidlist_opt = (taxids == 'EMPTY') ? '' : \"-taxidlist $txids\"\n  blast_out = \"blastn-${sample_id}-VS-${dbname}.tsv\"\n  blast_tab_columns = \"qaccver saccver pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen stitle staxid ssciname\"\n  \"\"\"\n  blastn $taxidlist_opt \\\\\n    -num_threads ${task.cpus} \\\\\n    -db $dbdir/$dbname \\\\\n    -query $contigs \\\\\n    -outfmt \"6 $blast_tab_columns\" \\\\\n    -evalue 1e-6 \\\\\n    -out $blast_out\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-villumina/BLASTN"], "list_wf_names": ["peterk87/nf-villumina"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess MAP_STATS {\n  tag \"$sample VS $ref_name\"\n  publishDir \"${params.outdir}/mapping/$sample\", mode: 'copy', pattern: \"*.{tsv,flagstat,idxstats}\"\n\n  input:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam)\n\n  output:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(flagstat),\n          path(idxstats)\n  script:\n  ref_name = ref_fasta.getBaseName()\n  depths = \"${sample}-${ref_name}-depths.tsv\"\n  flagstat = \"${sample}-${ref_name}.flagstat\"\n  idxstats = \"${sample}-${ref_name}.idxstats\"\n  \"\"\"\n  samtools flagstat $bam > $flagstat\n  samtools depth -a -d 0 $bam | perl -ne 'chomp \\$_; print \"${sample}\\t\\$_\\n\"' > $depths\n  samtools idxstats $bam | head -n1 | perl -ne 'chomp \\$_; print \"${sample}\\t\\$_\\n\"' > $idxstats\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/MAP_STATS"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess MEDAKA {\n  tag \"$sample - $ref_name\"\n  publishDir \"${params.outdir}/vcf\", mode: 'copy', pattern: '*.vcf'\n\n  input:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(flagstat),\n          path(idxstats)\n  output:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(vcf)\n  script:\n  ref_name = ref_fasta.getBaseName()\n  vcf = \"${sample}-${ref_name}.medaka.vcf\"\n  \"\"\"\n  samtools index $bam\n  medaka consensus --chunk_len 800 --chunk_ovlp 400 $bam ${bam}.hdf\n  medaka variant $ref_fasta ${bam}.hdf $vcf\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/MEDAKA"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess LONGSHOT {\n  tag \"$sample - $ref_name\"\n  publishDir \"${params.outdir}/vcf\", mode: 'copy', pattern: '*.vcf'\n\n  input:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(medaka_vcf)\n  output:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(longshot_vcf)\n  script:\n  ref_name = ref_fasta.getBaseName()\n  longshot_vcf = \"${sample}-${ref_name}.longshot.vcf\"\n  script:\n  \"\"\"\n  samtools faidx $ref_fasta\n  samtools index $bam\n  longshot -P 0 -F -A --no_haps \\\\\n    --potential_variants $medaka_vcf \\\\\n    --bam $bam \\\\\n    --ref $ref_fasta \\\\\n    --out $longshot_vcf\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/LONGSHOT"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess BCF_FILTER {\n  tag \"$sample - $ref_name\"\n  publishDir \"${params.outdir}/vcf\",\n    pattern: \"*.filt.vcf\",\n    mode: 'copy'\n\n  input:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(vcf)\n  output:\n    tuple val(sample),\n          path(ref_fasta),\n          path(bam),\n          path(depths),\n          path(filt_vcf)\n  script:\n  ref_name = ref_fasta.getBaseName()\n  filt_vcf = \"${file(vcf).getBaseName()}.filt.vcf\"\n  \"\"\"\n  bcftools filter \\\\\n    -e 'AC[0] >= AC[1] || AC[1]<=2' \\\\\n    $vcf \\\\\n    -Ov \\\\\n    -o $filt_vcf\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/BCF_FILTER"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess KRAKEN2 {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/kraken2/results\",\n    pattern: \"*-kraken2_results.tsv\",\n    mode: 'copy'\n  publishDir \"${params.outdir}/kraken2/reports\",\n    pattern: \"*-kraken2_report.tsv\",\n    mode: 'copy'\n\n  input:\n    path(db)\n    tuple val(sample), \n          path(reads)\n  output:\n    tuple val(sample),\n          path(reads),\n          path(results),\n          path(report)\n\n  script:\n  results = \"${sample}-kraken2_results.tsv\"\n  report = \"${sample}-kraken2_report.tsv\"\n  \"\"\"\n  kraken2 \\\\\n    --threads ${task.cpus} \\\\\n    --memory-mapping \\\\\n    --db ./${db}/ \\\\\n    --report ${report} \\\\\n    --output ${results} \\\\\n    $reads\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/KRAKEN2"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["Centrifuge"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess CENTRIFUGE {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/centrifuge/$sample\",\n    pattern: \"*.tsv\",\n    mode: 'copy'\n  publishDir \"${params.outdir}/centrifuge\",\n    pattern: \"*-kreport.tsv\",\n    mode: 'copy'\n\n  input:\n    tuple db_name, \n          path(db)\n    tuple val(sample),\n          path(reads)\n  output:\n    tuple val(sample),\n          path(reads),\n          path(results),\n          path(kreport)\n\n  script:\n  results = \"${sample}-centrifuge_results.tsv\"\n  kreport = \"${sample}-kreport.tsv\"\n  \"\"\"\n  centrifuge \\\\\n    -x ${db}/${db_name} \\\\\n    -U $reads \\\\\n    -S $results \\\\\n    -p ${task.cpus}\n  centrifuge-kreport -x ${db}/${db_name} $results > $kreport\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/CENTRIFUGE"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["Unicycler"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["nf-virontus"], "list_contrib": ["peterk87"], "nb_contrib": 1, "codes": ["\nprocess UNICYCLER_ASSEMBLY {\n  tag \"$sample\"\n  publishDir \"${params.outdir}/assemblies/unicycler/$sample\", mode: 'copy'\n  errorStrategy 'ignore'\n\n  input:\n    tuple val(sample),\n          path(reads)\n  output:\n    tuple val(sample),\n          val('unicycler'),\n          path(\"${sample}/\")\n\n  script:\n  output_contigs = \"${sample}-assembly.fasta\"\n  output_gfa = \"${sample}-assembly.gfa\"\n  output_unicycler_log = \"${sample}-unicycler.log\"\n  \"\"\"\n  unicycler -t ${task.cpus} --mode ${params.unicycler_mode} -o $sample -l $reads\n  ln -s ${sample}/assembly.fasta $output_contigs\n  ln -s ${sample}/assembly.gfa $output_gfa\n  ln -s ${sample}/unicycler.log $output_unicycler_log\n  \"\"\"\n}"], "list_proc": ["peterk87/nf-virontus/UNICYCLER_ASSEMBLY"], "list_wf_names": ["peterk87/nf-virontus"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["peterk87"], "nb_wf": 1, "list_wf": ["phylogenetictree"], "list_contrib": ["nhhaidee"], "nb_contrib": 1, "codes": ["\nprocess MSA_MAFFT {\n    \n    tag { params.prefix }\n\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\"\n\n    input:\n    file(cat_consensus_seqeuences)\n    file(ch_reference_fasta)\n\n    output:\n    path \"msa.aln\", emit: ch_msa_mafft\n\n    script:\n    \"\"\"\n    mafft \\\\\n        --thread ${task.cpus} \\\\\n        --6merpair \\\\\n        --addfragments ${cat_consensus_seqeuences} \\\\\n        $ch_reference_fasta > msa.aln\n    \"\"\"\n}"], "list_proc": ["peterk87/phylogenetictree/MSA_MAFFT"], "list_wf_names": ["peterk87/phylogenetictree"]}, {"nb_reuse": 1, "tools": ["Jellyfish"], "nb_own": 1, "list_own": ["pgonzale60"], "nb_wf": 1, "list_wf": ["genokmer_hifi"], "list_contrib": ["pgonzale60"], "nb_contrib": 1, "codes": ["\nprocess jellyfish {\n    tag \"${sample}_${kmer}\"\n\n    input:\n      tuple val(sample), path(reads), val(kmer)\n    output:\n      tuple val(sample), path(\"${sample}.histo\"), val(kmer)\n    script:\n      \"\"\"\n      if [ -f *.gz ]; then\n            jellyfish count -C -m $kmer -s 200M \\\n            -t ${task.cpus} \\\n            <(zcat $reads)\n        else\n            jellyfish count -C -m $kmer -s 200M \\\n            -t ${task.cpus} \\\n            $reads\n        fi\n      jellyfish histo -t ${task.cpus} mer_counts.jf > ${sample}.histo\n      rm mer_counts.jf\n      \"\"\"\n}"], "list_proc": ["pgonzale60/genokmer_hifi/jellyfish"], "list_wf_names": ["pgonzale60/genokmer_hifi"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["process generateMinimap2Index {\n    publishDir \"${params.outdir}/compositeMinimapIndex\", pattern: \"composite_ref.mmi\", mode: \"symlink\"\n\n    label 'indexResources'\n\n    input:\n    path(human_ref)\n    path(viral_ref)\n\n    output:\n    file \"composite_ref.mmi\"\n\n    script:\n    \"\"\"\n    cat $human_ref $viral_ref > composite_reference.fa\n    minimap2 -d composite_ref.mmi composite_reference.fa\n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/generateMinimap2Index"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["\nprocess removeHumanReads {\n    label 'smallCPU'\n    tag { sampleName }\n\n    input:\n    tuple val(sampleName), path(sorted_bam)\n\n    output:\n    tuple val(sampleName), path(\"${sampleName}.host_removed.sorted.bam\"), optional: true, emit: bam\n    path \"${sampleName}*.csv\", emit: csv\n\n    script:\n\n    def rev = workflow.commitId ?: workflow.revision ?: workflow.scriptId\n\n    \"\"\"\n    samtools index $sorted_bam\n    dehost_nanopore.py --file $sorted_bam --min_reads ${params.min_read_count} --output ${sampleName}.host_removed.sorted.bam --revision ${rev}\n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/removeHumanReads"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["\nprocess regenerateFastqFilesFlat {\n    publishDir \"${params.outdir}/${params.run_name}/run/fastq_pass/\", pattern: \"*.host_removed.fastq\", mode: \"copy\"\n\n    label 'smallCPU'\n    tag { sampleName }\n\n    input:\n    tuple val(sampleName), path(dehosted_bam)\n\n    output:\n    tuple val(sampleName), file(\"${sampleName}.host_removed.fastq\")\n\n    script:\n    \"\"\"\n    samtools fastq $dehosted_bam > ${sampleName}.host_removed.fastq\n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/regenerateFastqFilesFlat"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["\nprocess indexCompositeReference {\n    publishDir \"${params.outdir}/humanBWAIndex\", pattern: \"*.fa*\", mode: \"symlink\"\n\n    label 'indexResources'\n\n    input:\n    path(composite_ref)\n\n    output:\n    file(\"*.fa*\")\n\n    script:\n    \"\"\"\n    bwa index -a bwtsw $composite_ref\n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/indexCompositeReference"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["\nprocess compositeMappingBWA {\n    publishDir \"${params.outdir}/compositeMAPs\", pattern: \"${sampleName}.*\", mode: \"copy\"\n\n    input:\n    tuple( val(sampleName), path(forward), path(reverse), path(composite_reference))\n    path(indexed_reference)\n\n    output:\n    tuple val(sampleName), path(\"${sampleName}.sorted.bam\"), emit: bam\n    path(\"${sampleName}.flagstats.txt\")\n\n    script:\n    \"\"\"\n    bwa mem -t ${params.illumina_threads} ${composite_reference} ${forward} ${reverse} | samtools sort --threads ${params.illumina_threads} -T \"temp\" -O BAM -o ${sampleName}.sorted.bam\n    samtools flagstat ${sampleName}.sorted.bam > ${sampleName}.flagstats.txt\n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/compositeMappingBWA"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["\nprocess dehostBamFiles {\n    publishDir \"${params.outdir}/dehostedBAMs\", pattern: \"${sampleName}.dehosted.bam\", mode: \"copy\"\n\n    label 'smallCPU'\n    tag { sampleName }\n\n    input:\n    tuple( val(sampleName), path(composite_bam))\n\n    output:\n    tuple val(sampleName), path(\"${sampleName}.dehosted.bam\"), emit: bam\n    path \"${sampleName}*.csv\", emit: csv\n\n    script:\n\n    def rev = workflow.commitId ?: workflow.revision ?: workflow.scriptId\n\n    \"\"\"\n    samtools index ${composite_bam}\n    dehost_illumina.py --file ${composite_bam} \\\n    --keep_id ${params.covid_ref_id} \\\n    -q ${params.keep_min_map_quality} \\\n    -Q ${params.remove_min_map_quality} \\\n    -o ${sampleName}.dehosted.bam \\\n    -R ${rev} \n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/dehostBamFiles"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["phac-nml"], "nb_wf": 1, "list_wf": ["ncov-dehoster"], "list_contrib": ["DarianHole", "Takadonet"], "nb_contrib": 2, "codes": ["\nprocess generateDehostedReads {\n    publishDir \"${params.outdir}/dehosted_paired_fastqs\", pattern: \"${sampleName}_dehosted_R*\", mode: \"copy\"\n\n    label 'mediumMem'\n    tag { sampleName }\n\n    input:\n    tuple( val(sampleName), path(dehosted_bam))\n\n    output:\n    path(\"${sampleName}_dehosted_R*\")\n\n    script:\n    \"\"\"\n    samtools fastq -1 ${sampleName}_dehosted_R1.fastq.gz -2 ${sampleName}_dehosted_R2.fastq.gz ${dehosted_bam}\n    \"\"\"\n}"], "list_proc": ["phac-nml/ncov-dehoster/generateDehostedReads"], "list_wf_names": ["phac-nml/ncov-dehoster"]}, {"nb_reuse": 2, "tools": ["STAR", "IRProfiler", "Jellyfish"], "nb_own": 2, "list_own": ["photocyte", "phelelani"], "nb_wf": 2, "list_wf": ["nf-rnaSeqCount", "genomeScope_nf"], "list_contrib": ["photocyte", "phelelani"], "nb_contrib": 2, "codes": [" process run_GenerateSTARIndex {\n            label 'maxi'\n            tag { \"Generate Star Index\" }\n            publishDir \"$index_dir\", mode: 'copy', overwrite: true\n            \n            output:\n            set val(\"starIndex\"), file(\"*\") into star_index\n            \n            \"\"\"\n            STAR --runThreadN ${task.cpus} \\\n                --runMode genomeGenerate \\\n                --genomeDir . \\\n                --genomeFastaFiles ${genome} \\\n                --sjdbGTFfile ${genes} \\\n                --sjdbOverhang 99\n            \"\"\"\n        }", "\nprocess smudgeplotHetkmers {\nexecutor \"local\"\ncpus 1\nconda \"bioconda::kmer-jellyfish r::r r::r-devtools r::r-roxygen2\"\ninput:\n val theK\n val maxKmerCov\n path \"kmer_k21.hist\"\n val theK2\n val maxKmerCov2\n path \"kmer_counts.jf\"\noutput:\n val theK\n val maxKmerCov\n path \"kmer_pairs_coverages_2.tsv\"\nshell:\n'''\ngit clone https://github.com/KamilSJaron/smudgeplot\n\n##Set CRAN mirror for R\nif grep -Fxq 'CRAN=\"http://cran.us.r-project.org\"' ${CONDA_PREFIX}/lib/R/library/base/R/Rprofile\nthen\n    ##Do nothing\n    echo \"\"\nelse\n    echo 'options(repos=structure(c(CRAN=\"http://cran.us.r-project.org\")))' >> ${CONDA_PREFIX}/lib/R/library/base/R/Rprofile\nfi\n\ncd smudgeplot\nRscript install.R\ninstall -C exec/smudgeplot.py ${CONDA_PREFIX}/bin\ninstall -C exec/smudgeplot_plot.R ${CONDA_PREFIX}/bin\n\nL=$(smudgeplot.py cutoff kmer_k21.hist L)\nU=$(smudgeplot.py cutoff kmer_k21.hist U)\necho $L $U # these need to be sane values like 30 800 or so\njellyfish dump -c -L $L -U $U kmer_counts.jf | smudgeplot.py hetkmers -o kmer_pairs\n'''\n}"], "list_proc": ["phelelani/nf-rnaSeqCount/run_GenerateSTARIndex", "photocyte/genomeScope_nf/smudgeplotHetkmers"], "list_wf_names": ["phelelani/nf-rnaSeqCount", "photocyte/genomeScope_nf"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqCount"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_GenerateBowtie2Index {\n            label 'maxi'\n            tag { \"Generate Bowtie2 Index\" }\n            publishDir \"$index_dir\", mode: 'copy', overwrite: true\n        \n            output:\n            set val(\"bowtieIndex\"), file(\"*\") into bowtie_index\n            \n            \"\"\"\n            bowtie2-build --threads ${task.cpus} ${genome} genome\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqCount/run_GenerateBowtie2Index"], "list_wf_names": ["phelelani/nf-rnaSeqCount"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqCount"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_QualityChecks {\n            label 'midi'\n            tag { samples }\n            publishDir \"${qc_dir}\", mode: 'copy', overwrite: true\n            \n            input:\n            set val(samples), file(reads) from read_pairs\n            \n            output:\n            set val(samples), file(\"*.{html,zip}\") into qc_html\n\n            \"\"\"\n            fastqc ${reads.findAll().join(' ') } \\\n                --threads ${task.cpus} \\\n                --noextract\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqCount/run_QualityChecks"], "list_wf_names": ["phelelani/nf-rnaSeqCount"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqCount"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_STAR {\n            label 'maxi'\n            tag { sample }\n            publishDir \"${align_dir}\", mode: 'copy', overwrite: true\n\n            input:\n            set sample, file(reads) from read_pairs\n    \n            output:\n            set sample, file(\"${sample}*.{out,tab}\"), file(\"${sample}_Aligned.out.bam\") into star_alignments\n            \n            \"\"\"\n            STAR --runMode alignReads \\\n                --genomeDir ${index_dir} \\\n                --readFilesCommand gunzip -c \\\n                --readFilesIn ${reads.findAll().join(' ')} \\\n                --runThreadN ${task.cpus} \\\n                --outSAMtype BAM Unsorted \\\n                --outSAMunmapped Within \\\n                --outSAMattributes Standard \\\n                --outFileNamePrefix ${sample}_\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqCount/run_STAR"], "list_wf_names": ["phelelani/nf-rnaSeqCount"]}, {"nb_reuse": 1, "tools": ["htseqcount"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqCount"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_HTSeqCount {\n            label 'mini'\n            tag { sample }\n            publishDir \"${counts_dir}/htseqCounts\", mode: 'copy', overwrite: true\n\n            input:\n            set sample, file(bam) from bams_htseqCounts\n\n            output:\n            set sample, \"${sample}.txt\" into htseqCounts\n            set sample, \"${sample}.txt\" into htseqCounts_cleanup\n    \n            \"\"\"\n            htseq-count -f bam \\\n                -r name \\\n                -i gene_id \\\n                -a 10 \\\n                -s reverse \\\n                -m union \\\n                -t exon \\\n                $bam $genes > ${sample}.txt\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqCount/run_HTSeqCount"], "list_wf_names": ["phelelani/nf-rnaSeqCount"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqCount"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_FeatureCounts {\n            label 'maxi'\n            tag { 'featureCounts - ALL' }\n            publishDir \"${counts_dir}/featureCounts\", mode: 'copy', overwrite: false\n\n            input:\n            file(samples) from sample_bams\n            \n            output:\n            file('gene_counts*') into featureCounts\n            file('gene_counts.txt') into featureCounts_raw\n            \n            \"\"\"\n            featureCounts -p -B -C -P -J -s 2 \\\n                -G $genome -J \\\n                -t exon \\\n                -d 40 \\\n                -g gene_id \\\n                -a $genes \\\n                -T ${task.cpus} \\\n                -o gene_counts.txt \\\n                `< ${samples}`\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqCount/run_FeatureCounts"], "list_wf_names": ["phelelani/nf-rnaSeqCount"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqCount"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_MultiQC {\n            label 'mini'\n            tag { 'MultiQC - ALL' }\n            publishDir \"${multiqc_dir}\", mode: 'copy', overwrite: false\n            \n            output:\n            file('*') into multiQC\n            \n            \"\"\"\n            multiqc ${out_dir} --force\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqCount/run_MultiQC"], "list_wf_names": ["phelelani/nf-rnaSeqCount"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqMetagen"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_GenerateSTARIndex {\n            label 'maxi'\n            tag { \"Generate Star Index\" }\n            publishDir \"$index_dir\", mode: 'copy', overwrite: true\n            \n            output:\n            set val(\"starIndex\"), file(\"*\") into star_index\n            \n            \"\"\"\n            STAR --runThreadN ${task.cpus} \\\n                --runMode genomeGenerate \\\n                --genomeDir . \\\n                --genomeFastaFiles ${genome} \\\n                --sjdbGTFfile ${genes} \\\n                --sjdbOverhang 99\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqMetagen/run_GenerateSTARIndex"], "list_wf_names": ["phelelani/nf-rnaSeqMetagen"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqMetagen"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_STAR {\n            label 'maxi'\n            tag { sample }\n                                                                                                              \n    \n            input:\n            set sample, file(reads) from read_pairs\n    \n            output:\n            set sample, file(\"${sample}*.{out,tab}\") into star_results\n            set sample, file(\"${sample}_Unmapped*\") into unmapped_reads\n\n            \"\"\"\n            STAR --runMode alignReads \\\n                --genomeDir ${index_dir} \\\n                --readFilesCommand gunzip -c \\\n                --readFilesIn ${reads.findAll().join(' ')} \\\n                --runThreadN ${task.cpus} \\\n                --outSAMtype BAM Unsorted \\\n                --outReadsUnmapped Fastx \\\n                --outFileNamePrefix ${sample}_\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqMetagen/run_STAR"], "list_wf_names": ["phelelani/nf-rnaSeqMetagen"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqMetagen"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_KrakenClassifyReads {\n            label 'maxi'\n            tag { sample }\n                                                                             \n\n            input:\n            set sample, file(reads) from unmapped_kraken\n\n            output:\n            set sample, file(\"${sample}_reads.krak\") into kraken_reads_report\n            set sample, file(\"${sample}_classified_*.fastq\") into kraken_classified_reads\n            set sample, file(\"${sample}_unclassified_*.fastq\") into kraken_unclassified_reads\n\n            \"\"\"\t\n            kraken2 --db ${db} \\\n                --paired ${reads.findAll().join(' ')} \\\n                --threads ${task.cpus} \\\n                --classified-out ${sample}_classified#.fastq \\\n                --unclassified-out ${sample}_unclassified#.fastq \\\n                --output ${sample}_reads.krak\n            \"\"\" \n        }"], "list_proc": ["phelelani/nf-rnaSeqMetagen/run_KrakenClassifyReads"], "list_wf_names": ["phelelani/nf-rnaSeqMetagen"]}, {"nb_reuse": 1, "tools": ["Trinity"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqMetagen"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_TrinityAssemble {\n            label 'maxi'\n            tag { sample }\n            publishDir \"$out_dir/${sample}\", mode: 'copy', overwrite: true\n\n            input:\n            set sample, file(reads) from unmapped_trinity\n\n            output:\n            set sample, \"trinity_${sample}/Trinity.fasta\" into trinity_assembled_reads\n\n            \"\"\"\n            /bin/hostname\n            Trinity --seqType fq \\\n               --max_memory 150G \\\n               --left ${reads.find { it =~ 'R1' } } \\\n               --right ${reads.find { it =~ 'R2' } } \\\n               --SS_lib_type RF \\\n               --CPU ${task.cpus} \\\n               --output trinity_${sample}\n            \"\"\"\n         }"], "list_proc": ["phelelani/nf-rnaSeqMetagen/run_TrinityAssemble"], "list_wf_names": ["phelelani/nf-rnaSeqMetagen"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqMetagen"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_KrakenClassifyFasta {\n            label 'maxi'\n            tag { sample }\n            publishDir \"$out_dir/${sample}\", mode: 'copy', overwrite: true, pattern: \"*{_fasta.krak,_classified.fasta}\"\n\n            input:\n            set sample, file(fasta) from trinity_assembled_reads\n\n            output:\n            set sample, file(\"${sample}_fasta.krak\") into kraken_fasta_report\n            set sample, file(\"${sample}_classified.fasta\") into kraken_classified_fasta\n            set sample, file(\"${sample}_unclassified.fasta\") into kraken_unclassified_fasta\n\n            \"\"\"\t\n            kraken2 --db ${db} \\\n                ${fasta} \\\n                --threads ${task.cpus} \\\n                --classified-out ${sample}_classified.fasta \\\n                --unclassified-out ${sample}_unclassified.fasta \\\n                --output ${sample}_fasta.krak\n            \"\"\" \n        }"], "list_proc": ["phelelani/nf-rnaSeqMetagen/run_KrakenClassifyFasta"], "list_wf_names": ["phelelani/nf-rnaSeqMetagen"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["phelelani"], "nb_wf": 1, "list_wf": ["nf-rnaSeqMetagen"], "list_contrib": ["phelelani"], "nb_contrib": 1, "codes": [" process run_MultiQC {\n            label 'mini'\n            tag { \"Get QC Information\" }\n            publishDir \"$out_dir/MultiQC\", mode: 'copy', overwrite: true\n\n            input:\n            file(star) from qc_star\n\n            output:\n            file('*') into multiQC\n\n            \"\"\"\n            multiqc `< ${star}` --force\n            \"\"\"\n        }"], "list_proc": ["phelelani/nf-rnaSeqMetagen/run_MultiQC"], "list_wf_names": ["phelelani/nf-rnaSeqMetagen"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["philDTU"], "nb_wf": 1, "list_wf": ["plasmidPublication"], "list_contrib": ["philDTU", "palatinate"], "nb_contrib": 2, "codes": ["\nprocess run_assembly {\n    input:\n    tuple read_id, id, path(\"input.fastq\") from split_ch\n \n    output:\n    tuple read_id, path(\"${id}.fasta\") into assembly_ch\n    tuple read_id, id, path(\"${id}.fasta\") into assembly_results_ch\n \n    \"\"\"\n    minimap2 -x ava-ont input.fastq input.fastq > ${id}.paf\n    miniasm -s 800 -f input.fastq ${id}.paf > ${id}.gfa\n    awk '/^S/{print \">\"\\$2\"\\\\n\"\\$3}' ${id}.gfa  > ${id}.fasta\n    sed -i \"s/>.*/&_${read_id}/\" ${id}.fasta\n    \"\"\"\n}"], "list_proc": ["philDTU/plasmidPublication/run_assembly"], "list_wf_names": ["philDTU/plasmidPublication"]}, {"nb_reuse": 1, "tools": ["Minimap2", "BEDTools"], "nb_own": 1, "list_own": ["philDTU"], "nb_wf": 1, "list_wf": ["plasmidPublication"], "list_contrib": ["philDTU", "palatinate"], "nb_contrib": 2, "codes": ["\nprocess run_map_back {\n    input:\n    tuple read_id, id, path(\"split.fastq\"), path(\"plasmid_candidat.fasta\") from fastq_ch2.join(assembly_ch)\n \n    output:\n    tuple read_id, id, path(\"${id}.tsv\") into remap_ch\n \n    \"\"\"\n    minimap2 -ax map-ont split.fastq plasmid_candidat.fasta -p 0.0 > ${id}.sam\n    bedtools bamtobed -i ${id}.sam | sort -nk 2 > ${id}.bed\n    Rscript ${baseDir}/scripts/filter_it.R ${id}.bed ${id}.tsv 50\n    \"\"\"\n}"], "list_proc": ["philDTU/plasmidPublication/run_map_back"], "list_wf_names": ["philDTU/plasmidPublication"]}, {"nb_reuse": 1, "tools": ["shovill"], "nb_own": 1, "list_own": ["phiweger"], "nb_wf": 1, "list_wf": ["isolate_pe"], "list_contrib": ["hoelzer", "phiweger"], "nb_contrib": 2, "codes": ["\nprocess assembly {\n                                                 \n    label 'shovill'\n    publishDir \"${params.results}\", mode: 'copy', overwrite: true\n               \n                     \n\n    input:\n        tuple(val(name), path(genomes))\n\n    output:\n        tuple(val(name), path(\"*.fasta\"))\n\n    \"\"\"\n    shovill --R1 ${genomes[0]} --R2 ${genomes[1]} --gsize 5M --assembler megahit --trim --outdir assembly --minlen ${params.minlen} --cpus ${task.cpus} --force\n    mv assembly/contigs.fa ${name}.fasta\n    \"\"\"\n}"], "list_proc": ["phiweger/isolate_pe/assembly"], "list_wf_names": ["phiweger/isolate_pe"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["phiweger"], "nb_wf": 1, "list_wf": ["isolate_pe"], "list_contrib": ["hoelzer", "phiweger"], "nb_contrib": 2, "codes": ["\nprocess annotate {\n                                                 \n    label 'prokka'\n    publishDir \"${params.results}\", mode: 'copy', overwrite: true\n               \n\n    input:\n        tuple(val(name), path(genomes))\n\n    output:\n        tuple(val(name), path(\"${name}.gff\"))\n        tuple(val(name), path(\"${name}.faa\"), emit: 'proteins')\n\n    \"\"\"\n    prokka --mincontiglen ${params.minlen} --cpus ${task.cpus} --outdir anno --prefix ${name} ${genomes}\n    cp anno/${name}.gff ${name}.gff\n    cp anno/${name}.faa ${name}.faa\n    \"\"\"\n}"], "list_proc": ["phiweger/isolate_pe/annotate"], "list_wf_names": ["phiweger/isolate_pe"]}, {"nb_reuse": 1, "tools": ["BLAT"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["doSameSpeciesLiftOver_nextflow"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess constructOocFile {\n    conda \"blat\"\n                                    \n    tag \"$old_2bit\"\n    input:\n      file old_2bit from old_2bit_2\n    output:\n      file \"${old_2bit}.ooc\" into ooc\n    script:\n    \"\"\"\n    ##TODO Should follow protocol for repMatch using the \"Construct ooc file\" instructions from http://genomewiki.ucsc.edu/index.php/DoSameSpeciesLiftOver.pl\n    blat ${old_2bit} /dev/null /dev/null -stepSize=1 -tileSize=11 -makeOoc=${old_2bit}.ooc -repMatch=4096\n    \"\"\"\n}"], "list_proc": ["photocyte/doSameSpeciesLiftOver_nextflow/constructOocFile"], "list_wf_names": ["photocyte/doSameSpeciesLiftOver_nextflow"]}, {"nb_reuse": 1, "tools": ["BLAT"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["doSameSpeciesLiftOver_nextflow"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess blat_align {\nconda \"blat ucsc-fasplit ucsc-liftup\"\n                                \nmemory '4 GB'\ninput:\n set file(originalFasta),file(liftupFile),file(fastaSubChunk),file(old_2bit),file(ooc) from blatCmds\noutput:\n file \"${fastaSubChunk}.lifted.psl\" into axtChainCmds\ntag \"${fastaSubChunk}\"\nscript:\n\"\"\"\nif [ \"${params.splitSize}\" -lt \"4000\" ]; then\n  blat ${old_2bit} ${fastaSubChunk} -ooc=${ooc} -maxIntron=0 -stepSize=1 -tileSize=11 -minIdentity=98 -noHead -minScore=100 -fastMap -extendThroughN ${fastaSubChunk}.subsplit.psl\nelse\n  blat ${old_2bit} ${fastaSubChunk} -ooc=${ooc} -maxIntron=0 -stepSize=1 -tileSize=11 -minIdentity=98 -noHead -minScore=100 -extendThroughN ${fastaSubChunk}.subsplit.psl\nfi\n\nliftUp -pslQ ${fastaSubChunk}.lifted.psl ${liftupFile} warn ${fastaSubChunk}.subsplit.psl\n\"\"\"\n}"], "list_proc": ["photocyte/doSameSpeciesLiftOver_nextflow/blat_align"], "list_wf_names": ["photocyte/doSameSpeciesLiftOver_nextflow"]}, {"nb_reuse": 1, "tools": ["GTM"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["doSameSpeciesLiftOver_nextflow"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess normalizeGff {\npublishDir './liftover_output/',mode:'copy',overwrite:true\ntag \"$gff by $fasta\"\ninput:\n set file(gff),file(fasta) from normalizeCmds\noutput:\n file \"target.${gff}.gff3\" optional true into normalizedGff\n file \"ignored.${gff}.gff3\"\nscript:\n\"\"\"\nseqkit fx2tab --only-id -n ${fasta} | tr -s \"\\t\" > target_scaffolds.txt\necho \"##gff-version 3\" >> target_scaffolds.txt\ngt gff3 -tidy -sort -retainids -fixregionboundaries ${gff} > normalized.${gff}.gff3 \ngrep -f target_scaffolds.txt normalized.${gff}.gff3 > target.${gff}.gff3\ngrep -v -f target_scaffolds.txt normalized.${gff}.gff3 > ignored.${gff}.gff3\n\nif [[ \\$(wc -l <target.${gff}.gff3) -le 1 ]]\nthen\n    echo \"No targets were found. Deleting target gff3 file.\"\n    rm -f target.${gff}.gff3\nfi\n\"\"\"\n}"], "list_proc": ["photocyte/doSameSpeciesLiftOver_nextflow/normalizeGff"], "list_wf_names": ["photocyte/doSameSpeciesLiftOver_nextflow"]}, {"nb_reuse": 1, "tools": ["LiftOver"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["doSameSpeciesLiftOver_nextflow"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess ucsc_liftover {\nconda \"ucsc-liftover\"\n                                \ntag \"$gffFile & liftOverFile\"\ninput:\n set file(gffFile),file(liftOverFile) from liftoverCmds\noutput:\n file \"original_${gffFile}\" into gffOriginal\n file \"ucsc-lifted_${gffFile}\" into ucsc_lifted_gff, ucsc_lifted_gff_ch2\n file \"unmapped_${gffFile}\" into unmapped_gff, unmapped_gff_ch2\nscript:\n\"\"\"\nliftOver -gff ${gffFile} ${liftOverFile} ucsc-lifted_${gffFile} unmapped_${gffFile}\nln -s ${gffFile} original_${gffFile}\n\"\"\"\n}"], "list_proc": ["photocyte/doSameSpeciesLiftOver_nextflow/ucsc_liftover"], "list_wf_names": ["photocyte/doSameSpeciesLiftOver_nextflow"]}, {"nb_reuse": 1, "tools": ["GTM"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["doSameSpeciesLiftOver_nextflow"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess sort_gff {\npublishDir './liftover_output/',mode:'copy',overwrite:true\nconda \"genometools-genometools\"\n                                \ninput:\n file gff from ucsc_lifted_gff\n file unmapped from unmapped_gff\n file rescued from rescuedGff\noutput:\n file \"srt_${gff}\" into final_gff\n file \"${unmapped}\"\ntag \"${gff}\" \nscript:\n\"\"\"\n THENAME=${gff}\n NEWNAME=lifted_\\${THENAME#lifted_unsorted_}\n cat ${gff} ${rescued} | grep -v \"#\" | gt gff3 -tidy -sort -retainids > srt_${gff}\n\"\"\"\n}"], "list_proc": ["photocyte/doSameSpeciesLiftOver_nextflow/sort_gff"], "list_wf_names": ["photocyte/doSameSpeciesLiftOver_nextflow"]}, {"nb_reuse": 1, "tools": ["Jellyfish"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["genomeScope_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess jellyfishCount {\ncpus 10 \nmemory \"100GB\"\nconda \"bioconda::kmer-jellyfish\"\ninput:\n val theK\n val maxKmerCov\n path reads\noutput:\n path \"*.jf\", emit:jellyfishDbs\ntag \"${reads[0]}\"\nshell:\n'''\n##Decompress files in parallel using named pipes.\nfor f in !{reads}\ndo\n mkfifo ${f%.gz}\n gzip -cdqf -- $f > ${f%.gz} &\n echo ${f%.gz} >> fifos.txt\ndone\n\njellyfish count -C -m !{theK} -s 10000000000 -t !{task.cpus} --upper-count=!{maxKmerCov} $(cat fifos.txt | tr '\\n' ' ') -o !{reads[0]}.jf\n\n##Cleanup temporary files. The named pipes won't take up any space.\n##rm -f reads.jf\n'''\n}"], "list_proc": ["photocyte/genomeScope_nf/jellyfishCount"], "list_wf_names": ["photocyte/genomeScope_nf"]}, {"nb_reuse": 1, "tools": ["Jellyfish"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["genomeScope_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess jellyfishMerge {\nexecutor \"local\"\ncpus 1\nmemory \"100GB\"\nconda \"bioconda::kmer-jellyfish\"\ninput:\n val theK\n val maxKmerCov\n path jellyfishDbs\noutput:\n val theK\n val maxKmerCov\n path \"merged.jf\"\nshell:\n'''\njellyfish merge --upper-count=!{maxKmerCov} !{jellyfishDbs} -o merged.jf\n'''\n}"], "list_proc": ["photocyte/genomeScope_nf/jellyfishMerge"], "list_wf_names": ["photocyte/genomeScope_nf"]}, {"nb_reuse": 1, "tools": ["Jellyfish"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["genomeScope_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess jellyfishHisto {\ncpus 10 \nmemory \"100GB\"\nconda \"bioconda::kmer-jellyfish\"\ninput:\n val theK\n val maxKmerCov\n path jellyfishDbs\noutput:\n val theK\n val maxKmerCov\n path \"merged.histo\"\nshell:\n'''\njellyfish histo -t !{task.cpus} --high=!{maxKmerCov} !{jellyfishDbs} -o merged.histo\n'''\n}"], "list_proc": ["photocyte/genomeScope_nf/jellyfishHisto"], "list_wf_names": ["photocyte/genomeScope_nf"]}, {"nb_reuse": 1, "tools": ["IRProfiler"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["genomeScope_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess genomeScope2 {\nexecutor \"local\"\npublishDir './results/' , mode:'link'\nconda \"bioconda::kmer-jellyfish r::r r::r-devtools r::r-roxygen2\"\ninput:\n val theK\n val maxKmerCov\n path kmerCounts\noutput:\n path \"output_dir/*\"\nshell:\n'''\ngit clone https://github.com/tbenavi1/genomescope2.0.git\n\nif [[ ! -d \"$CONDA_PREFIX/lib/R_libs\" ]]\nthen\n mkdir \"$CONDA_PREFIX/lib/R_libs\"\nfi\n\n##Set CRAN mirror for R\nif grep -Fxq 'CRAN=\"http://cran.us.r-project.org\"' ${CONDA_PREFIX}/lib/R/library/base/R/Rprofile\nthen\n    ##Do nothing\n    echo \"\"\nelse\n    echo 'options(repos=structure(c(CRAN=\"http://cran.us.r-project.org\")))' >> ${CONDA_PREFIX}/lib/R/library/base/R/Rprofile\nfi\n\n\nsed -i \"s^\\\"~/R_libs/\\\"^\\\"$CONDA_PREFIX/lib/R_libs\\\"^g\" ./genomescope2.0/install.R\nsed -i \"s^, lib=local_lib_path^^g\" ./genomescope2.0/install.R\n##cat ./genomescope2.0/install.R | grep -v \"minpack.lm\" | grep -v \"argparse\" > tmp.R ##installs handled by bioconda\n##mv -f tmp.R ./genomescope2.0/install.R\ncd ./genomescope2.0/\nRscript install.R\ncd ../\n\n##echo \"$(head -n 1 install.R)\" >> $CONDA_PREFIX/lib/R/etc/Renviron\n\necho \"Now running GenomeScope 2.0\"\n./genomescope2.0/genomescope.R -i !{kmerCounts} -m !{maxKmerCov} -o output_dir -k !{theK}\n'''\n}"], "list_proc": ["photocyte/genomeScope_nf/genomeScope2"], "list_wf_names": ["photocyte/genomeScope_nf"]}, {"nb_reuse": 1, "tools": ["IRProfiler"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["genomeScope_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess smudgeplot {\nexecutor \"local\"\npublishDir './results/' , mode:'link'\nconda \"bioconda::kmer-jellyfish r::r r::r-devtools r::r-roxygen2\"\ninput:\n val theK\n val maxKmerCov\n path kmerPairsCov\noutput:\n path \"output_dir/*\"\nshell:\n'''\ngit clone https://github.com/KamilSJaron/smudgeplot\n\n##Set CRAN mirror for R\nif grep -Fxq 'CRAN=\"http://cran.us.r-project.org\"' ${CONDA_PREFIX}/lib/R/library/base/R/Rprofile\nthen\n    ##Do nothing\n    echo \"\"\nelse\n    echo 'options(repos=structure(c(CRAN=\"http://cran.us.r-project.org\")))' >> ${CONDA_PREFIX}/lib/R/library/base/R/Rprofile\nfi\n\ncd smudgeplot\nRscript install.R\ninstall -C exec/smudgeplot.py ${CONDA_PREFIX}/bin\ninstall -C exec/smudgeplot_plot.R ${CONDA_PREFIX}/bin\n\n\nsmudgeplot.py plot kmer_pairs_coverages_2.tsv -o my_genome\n'''\n}"], "list_proc": ["photocyte/genomeScope_nf/smudgeplot"], "list_wf_names": ["photocyte/genomeScope_nf"]}, {"nb_reuse": 1, "tools": ["AUGUSTUS"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["luciferase-PPX-predictor-nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess augustus_ppx_run {\npublishDir \"results\",mode:\"link\",overwrite:\"true\"\ninput:\n path genome_fasta \n path prfl_file\noutput:\n path \"augustus_out_${genome_fasta}.gff3\"\nshell:\n'''\naugustus --species=fly --proteinprofile=!{prfl_file} --UTR=off --gff3=on !{genome_fasta} > augustus_out_!{genome_fasta}.gff3\n'''\n}"], "list_proc": ["photocyte/luciferase-PPX-predictor-nf/augustus_ppx_run"], "list_wf_names": ["photocyte/luciferase-PPX-predictor-nf"]}, {"nb_reuse": 1, "tools": ["RepeatModeler"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["repeatModeler2_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess RepeatModeler_modelRepeatLibrary {\n  publishDir \"results\", mode:\"copy\",overwrite:\"true\"\n                                        \n                      \n                   \n                      \n  cpus params.cpuNum\n              \n  input:\n     path db_translate\n     path db_blastdb\n  tag \"$db_translate\"\n  output:\n                                                                 \n     path \"*-families.fa\", emit: repeat_library_ch\n     path \"*-families.stk\", emit: repeat_msa_ch    \n     path \"unaligned.fa\" optional true\n  shell:\n  '''\n  ##From database  \n  THENAME=$(basename !{db_translate})\n  THENAME=${THENAME%.translation}\n  ##Print the path and/or version into the stdout\n  ##conda list > conda-env.txt\n  which RepeatModeler\n  ##\n  RepeatModeler -engine ncbi -pa !{task.cpus} -LTRStruct -database \\$THENAME \n  sleep 10 ##Helps with rare filesystem latency issues\n  '''\n}"], "list_proc": ["photocyte/repeatModeler2_nf/RepeatModeler_modelRepeatLibrary"], "list_wf_names": ["photocyte/repeatModeler2_nf"]}, {"nb_reuse": 1, "tools": ["RepeatMasker"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["repeatModeler2_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess RepeatMasker_parallel_exec {\ncpus params.cpuNum\n                        \ninput:\n tuple path(genome), path(repeat_lib_chunk)\noutput:\n path \"*.out\"\ntag \"${repeat_lib_chunk}, ${genome.baseName}\"\nshell:\n'''\n  ##Print the path and/or version into the stdout\n  ##conda list > conda-env.txt\n  which RepeatMasker\n  RepeatMasker -v\n  ##\n  RepeatMasker -nolow -no_is -norna -pa !{task.cpus} -gff -q -lib !{repeat_lib_chunk} !{genome}\n'''\n}"], "list_proc": ["photocyte/repeatModeler2_nf/RepeatMasker_parallel_exec"], "list_wf_names": ["photocyte/repeatModeler2_nf"]}, {"nb_reuse": 1, "tools": ["RepeatMasker"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["repeatModeler2_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess RepeatMasker_simple_exec {\ncpus params.cpuNum\ninput:\n path genome\noutput: \n path \"*.out\", emit: rm_simple_out\ntag \"$genome\"\nscript:\n\"\"\"\nRepeatMasker -noint -pa ${task.cpus} -gff -q ${genome}\n\"\"\"\n}"], "list_proc": ["photocyte/repeatModeler2_nf/RepeatMasker_simple_exec"], "list_wf_names": ["photocyte/repeatModeler2_nf"]}, {"nb_reuse": 1, "tools": ["GTM"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["repeatModeler2_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess tidy_to_gff3 {\nexecutor 'local'\npublishDir \"results\", mode:\"copy\",overwrite:\"true\"\n                    \nconda \"genometools-genometools\"\ninput:\n path genome\n path \"tmp.gff\"\noutput:\n path \"${genome}.repeats.gff3.gz\", emit: repeats_gff_ch\ntag \"${genome}.repeats.gff3.gz\"\nshell:\n'''\nset -o pipefail\nconda list > conda-env.txt\ncat tmp.gff | grep -vP \"^#\" | gt gff3 -tidy -sort -retainids | uniq | gzip > !{genome}.repeats.gff3.gz\n'''\n}"], "list_proc": ["photocyte/repeatModeler2_nf/tidy_to_gff3"], "list_wf_names": ["photocyte/repeatModeler2_nf"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["photocyte"], "nb_wf": 1, "list_wf": ["repeatModeler2_nf"], "list_contrib": ["photocyte"], "nb_contrib": 1, "codes": ["\nprocess soft_mask {\nexecutor 'local'\npublishDir \"results\", mode:\"copy\",overwrite:\"true\"\n                    \nconda \"bedtools seqkit\"\ntag \"${genome}\"\ninput:\n path genome\n path repeats_gff\noutput:\n  path \"softmasked.${genome}\"\nshell:\n\"\"\"\nset -o pipefail\nbedtools maskfasta -soft -fi <(seqkit seq -u !{genome}) -bed !{repeats_gff} -fo softmasked.!{genome}\nsleep 10 ##Helps with rare filesystem latency issues\n\"\"\"\n}"], "list_proc": ["photocyte/repeatModeler2_nf/soft_mask"], "list_wf_names": ["photocyte/repeatModeler2_nf"]}, {"nb_reuse": 1, "tools": ["Bagphenotype"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["GWAS-nf"], "list_contrib": ["phue"], "nb_contrib": 1, "codes": ["\nprocess scatterPhenotypes {\n    tag \"$env\"\n\n    echo true\n    publishDir \"${params.outdir}/traits\", mode: 'copy'\n    input:\n        tuple val(env), path(pheno) from ch_pheno\n    output:\n        tuple val(env), path('*.csv') into traits mode flatten optional true\n\n    script:\n        def selection = params.trait ? \"['${params.trait.tokenize(',').join(\"','\")}']\" : \"phenotype.columns\"\n        \"\"\"\n        #!/usr/bin/env python\n\n        import pandas as pd\n\n        phenotype = pd.read_csv(\"${pheno}\", index_col=[0])\n        for trait in ${selection}: \n            try:\n                slice = phenotype[trait].dropna()\n                assert slice.nunique() > 1\n            except KeyError:\n                print(f'Trait {trait} not found in ${pheno.name}. Skipping.')\n            except AssertionError:\n                print(f'Trait values for {trait} are all equal in ${pheno.name}. Skipping.')\n            else:\n                slice.sort_index().to_csv(f'{trait}.csv', header=False)\n        \"\"\"\n}"], "list_proc": ["phue/GWAS-nf/scatterPhenotypes"], "list_wf_names": ["phue/GWAS-nf"]}, {"nb_reuse": 1, "tools": ["MAFsnp", "PHENO", "MaCS", "IBS"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["GWAS-nf"], "list_contrib": ["phue"], "nb_contrib": 1, "codes": ["\nprocess filterGenotypes {\n    tag \"$traitname\"\n\n    input:\n        path(geno) from ch_geno.collect()\n        tuple val(env), val(traitname), path(traitfile, stageAs: 'trait*.csv') from ch_traitsplit\n    output:\n        tuple val(env), val(traitname), path('pheno.csv'), path('geno.pkl.xz'), path('kinship.pkl.xz') optional true into ch_filtered\n\n    script:\n        def kinship_mode = params.kinship_from_all_markers ? 'all' : 'filtered'\n        \"\"\"\n        #!/usr/bin/env python\n\n        import h5py\n        import pandas as pd\n        import numpy as np\n        import logging\n\n        from limix.stats import linear_kinship\n\n        logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n        logger = logging.getLogger()\n\n        pheno = pd.concat([pd.read_csv(trait, index_col=[0], header=None) for trait in ['${traitfile.join(\"','\")}']], axis=1).dropna()\n        \n        pheno_acc_ids = np.array(pheno.index, dtype=np.uint32)\n\n        # read SNP matrix\n        with h5py.File('${geno}', 'r') as genofile:\n            geno_acc_ids = np.array(genofile['accessions'][:], dtype=np.uint32)\n            snps = np.array(genofile['snps'][:], dtype=bool)\n\n            chr_names = genofile['positions'].attrs.get('chrs')\n            chr_regions = np.array(genofile['positions'].attrs.get('chr_regions'))\n            geno_chroms = []\n            for i, reg in enumerate(chr_regions):\n                geno_chroms.extend(np.repeat(chr_names[i].decode('utf-8'), reg[1]-reg[0]))\n            pos = np.array(genofile['positions'][()], dtype=np.uint32)\n            geno_chroms = np.array(geno_chroms, dtype=np.uint8)\n\n        def get_kinship(snpmat, gower_norm):\n            ibs = linear_kinship(snpmat.to_numpy().T)\n            if gower_norm:\n                from limix.qc import normalise_covariance\n                ibs = normalise_covariance(ibs @ ibs.T)\n            return pd.DataFrame(ibs, index=snpmat.columns, columns=snpmat.columns)\n        \n        genotypes = pd.DataFrame(snps,\n                                 index=pd.MultiIndex.from_arrays([geno_chroms, pos]),\n                                 columns=geno_acc_ids)\n\n        if '${kinship_mode}' == 'all':\n            kinship = get_kinship(genotypes, ${params.normalise_covariance.toString().capitalize()})\n\n        pheno_geno_intersect = np.intersect1d(geno_acc_ids, pheno_acc_ids)\n        \n        phenotypes = pheno.loc[pheno_geno_intersect, :]\n        genotypes = genotypes.loc[:, pheno_geno_intersect]\n        \n        logger.info('%i accessions with both genotype and phenotype. Removed %i accessions because of missing genotype.', len(phenotypes), len(pheno) - len(phenotypes))\n\n        acs = genotypes.sum(axis=1)\n        macs = np.minimum(acs, genotypes.shape[1]-acs)\n        mafs = macs/genotypes.shape[1]\n\n        genotypes = genotypes[mafs >= ${params.maf}]\n\n        logger.info('Removed SNPs not satisfying MAF threshold %d%% (MAC %i). (Remaining SNPs: %i across %i accessions)', ${params.maf}*100, ${params.maf}*genotypes.shape[1], genotypes.shape[0], genotypes.shape[1])\n\n        if '${kinship_mode}' == 'filtered':\n            kinship = get_kinship(genotypes, ${params.normalise_covariance.toString().capitalize()})\n        else:\n            kinship = kinship.loc[genotypes.columns, genotypes.columns]\n\n        if genotypes.shape[0] > 0:\n            phenotypes.to_csv(\"pheno.csv\", header=False)\n            genotypes.to_pickle(\"geno.pkl.xz\")\n            kinship.to_pickle(\"kinship.pkl.xz\")\n        \"\"\"\n}"], "list_proc": ["phue/GWAS-nf/filterGenotypes"], "list_wf_names": ["phue/GWAS-nf"]}, {"nb_reuse": 1, "tools": ["PHENO", "MAFsnp", "MaCS", "Genoma", "EvoFreq", "MCRestimate"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["GWAS-nf"], "list_contrib": ["phue"], "nb_contrib": 1, "codes": ["\nprocess runGWAS {\n    tag \"$traitname\"\n\n    publishDir \"${params.outdir}/pvals\", mode: 'copy'\n    input:\n        tuple val(env), val(traitname), path(pheno), path(geno), path(kinship) from ch_filtered\n\n    output:\n        tuple val(env), val(traitname), path('*.csv.gz') into ch_pvals mode flatten optional true\n\n    script:\n        def pheno_transform = params.transform == 'no_transformation' ? \"\" : \".apply(${params.transform}, raw=True)\"\n        def locus_fixed = params.locus ? \"genotypes.xs((${params.locus.tokenize(',')[0]},${params.locus.tokenize(',')[1]}), axis=0).to_numpy().ravel()\" : \"None\"\n        if (!params.multitrait)\n            \"\"\"\n            #!/usr/bin/env python\n            \n            import pandas as pd\n            import numpy as np\n\n            from limix.qtl import scan\n            from limix.qc import mean_standardize, quantile_gaussianize, boxcox\n\n            phenotypes = pd.read_csv('${pheno}', index_col=[0], dtype=np.float32, header=None)${pheno_transform}\n\n            pheno = phenotypes.to_numpy(dtype=np.float32)\n\n            genotypes = pd.read_pickle('${geno}')\n            \n            chromosomes = np.array(genotypes.index.get_level_values(0))\n            positions = np.array(genotypes.index.get_level_values(1))\n\n            geno = genotypes.to_numpy().T\n\n            kinship = pd.read_pickle('${kinship}').to_numpy()\n\n            # calculate maf and mac\n            acs = geno.sum(axis=0)\n            macs = np.minimum(acs, geno.shape[0]-acs)\n            mafs = macs/geno.shape[0]\n\n            freq = pd.DataFrame(data={'maf': np.array(mafs), 'mac': np.array(macs)},\n                                index=pd.MultiIndex.from_arrays([chromosomes, positions]))\n\n            st = scan(G=geno,\n                    Y=pheno,\n                    M=${locus_fixed},\n                    K=kinship,\n                    verbose=True)\n\n            effsize = st.effsizes['h2'].loc[st.effsizes['h2']['effect_type'] == 'candidate']\n\n            def phenotypic_variance_explained(beta, beta_se, mafs, n):\n                '''Estimate phenotypic variance explained following Shim et al. (2015) https://doi.org/10.1371/journal.pone.0120758'''\n                return (2 * beta**2 * mafs * (1 - mafs)) / (2 * beta**2 * mafs * (1 - mafs) + beta_se**2 * 2 * n * mafs * (1 - mafs))\n\n            pve = phenotypic_variance_explained(effsize['effsize'].to_numpy(), effsize['effsize_se'].to_numpy(), mafs, pheno.shape[0])\n\n            result = pd.DataFrame(data={'pv': st.stats['pv20'].to_numpy(), 'gve': effsize['effsize'].to_numpy(), 'pve': np.array(pve)},\n                                  index=pd.MultiIndex.from_arrays([chromosomes, positions]))\n\n            if result['pv'].min() < ${params.pthresh}: \n                #result['-log10pv'] = -np.log10(result['pv'])\n                result = result.join(freq)\n                result.to_csv(f'${env}_${traitname}_mac{round(${params.maf}*pheno.shape[0])}.csv.gz', index_label=['chrom', 'pos'], compression='gzip')\n            \"\"\"\n        else\n            \"\"\"\n            #!/usr/bin/env python\n\n            import pandas as pd\n            import numpy as np\n            \n            from limix.qtl import scan\n            from limix.qc import mean_standardize, quantile_gaussianize, boxcox\n\n            phenotypes = pd.read_csv('${pheno}', index_col=[0], dtype=np.float32, header=None)${pheno_transform}\n            \n            pheno = phenotypes.to_numpy(dtype=np.float32)\n            \n            genotypes = pd.read_pickle('${geno}')\n\n            geno = genotypes.to_numpy().T\n\n            kinship = pd.read_pickle('${kinship}')\n            \n            # calculate maf and mac\n            acs = geno.sum(axis=0)\n            macs = np.minimum(acs, geno.shape[0]-acs)\n            mafs = macs/geno.shape[0]\n\n            chromosomes = np.array(genotypes.index.get_level_values(0))\n            positions = np.array(genotypes.index.get_level_values(1))\n\n            freq = pd.DataFrame(data={'maf': np.array(mafs), 'mac': np.array(macs)},\n                                index=pd.MultiIndex.from_arrays([chromosomes, positions]))\n\n            n_pheno = pheno.shape[1]  # number of traits\n\n            A = np.eye(n_pheno)  # p x p matrix of fixed effect sizes\n            # common effects: 1 DoF\n            Asnps0 = np.ones((n_pheno, 1))\n            Asnps = np.eye(n_pheno)\n\n            mtlmm = scan(G=geno,\n                        Y=pheno,\n                        K=kinship,\n                        A=A,\n                        A0=Asnps0,\n                        A1=Asnps,\n                        verbose=True)\n\n            # specific (GxE)\n            specific = pd.DataFrame(mtlmm.stats['pv21'].to_numpy(),\n                                    index=pd.MultiIndex.from_arrays([chromosomes, positions]),\n                                    columns=['pv'])                      \n\n            # common (G)\n            common = pd.DataFrame(mtlmm.stats['pv10'].to_numpy(),\n                                  index=pd.MultiIndex.from_arrays([chromosomes, positions]),\n                                  columns=['pv'])\n\n            # common (G + GxE)\n            any = pd.DataFrame(mtlmm.stats['pv20'].to_numpy(),\n                               index=pd.MultiIndex.from_arrays([chromosomes, positions]),\n                               columns=['pv'])\n\n            results =  {'specific': specific, 'common': common, 'any': any}\n\n            for name, result in results.items():\n                if result['pv'].min() < ${params.pthresh}:\n                    #result['-log10pv'] = -np.log10(result['pv'])\n                    result = result.join(freq)\n                    result.to_csv(f'${traitname}_mac{round(${params.maf}*pheno.shape[0])}_{name}.csv.gz', index_label=['chrom', 'pos'], compression='gzip')\n            \"\"\"\n}"], "list_proc": ["phue/GWAS-nf/runGWAS"], "list_wf_names": ["phue/GWAS-nf"]}, {"nb_reuse": 1, "tools": ["Count"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["aradeepopsis"], "list_contrib": ["nschan", "phue"], "nb_contrib": 2, "codes": ["\nprocess build_records {\n    stageInMode 'copy'\n    input:\n        tuple val(index), path('images/*') from ch_images\n    output:\n        tuple val(index), path('*.tfrecord') into ch_shards\n        tuple val(index), path('images/*', includeInputs: true) into ch_originals\n        tuple val(index), path('ratios.p') into ch_ratios\n        path('*.txt') into invalid_images optional true\n    when:\n        !params.masks\n    script:\n        \"\"\"\n        #!/usr/bin/env python\n\n        import logging\n        import os\n        import pickle\n\n        import tensorflow as tf\n\n        from data_record import create_record\n\n        logger = tf.get_logger()\n        logger.propagate = False\n        logger.setLevel('INFO')\n\n        images = tf.io.gfile.glob('images/*')\n\n        count = len(images)\n        invalid = 0\n        scale_factors = {}\n\n        with tf.io.TFRecordWriter('chunk.tfrecord') as writer:\n            for i in range(count):\n                filename = os.path.basename(images[i])\n                image_data = tf.io.gfile.GFile(images[i], 'rb').read()\n                try:\n                    image = tf.io.decode_image(image_data, channels=3)\n                except tf.errors.InvalidArgumentError:\n                    logger.info(\"%s is either corrupted or not a supported image format\" % filename)\n                    invalid += 1\n                    with open(\"invalid.txt\", \"a\") as broken:\n                        broken.write(f'{filename}\\\\n')\n                    continue\n\n                height, width = image.shape[:2]\n                max_dimension = 602\n                ratio = 1.0\n\n                if height * width > max_dimension**2:\n                    logger.info('%s: dimensions %d x %d are too large,' % (filename, height, width))\n                    ratio = max(height,width)/max_dimension\n                    new_height = int(height/ratio)\n                    new_width = int(width/ratio)\n                    logger.info('%s: resized to %d x %d (scale factor:%f)' % (filename, new_height, new_width, ratio))\n                    image = tf.image.resize(image, size=[new_height,new_width], preserve_aspect_ratio=False, antialias=True)\n                    image_data = tf.image.encode_png(tf.cast(image, tf.uint8)).numpy()\n                    tf.io.write_file(os.path.join(f'images/{filename}'), image_data)\n\n                scale_factors[filename] = ratio\n                record = create_record(image_data=image_data,\n                                    filename=filename,\n                                    height=height,\n                                    width=width,\n                                    ratio=ratio)\n\n                writer.write(record.SerializeToString())\n\n        pickle.dump(scale_factors, open(\"ratios.p\", \"wb\"))\n        \"\"\"\n}"], "list_proc": ["phue/aradeepopsis/build_records"], "list_wf_names": ["phue/aradeepopsis"]}, {"nb_reuse": 1, "tools": ["MultiDataSet", "maskBAD", "SAVER-X", "STRAW", "Rseg"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["aradeepopsis"], "list_contrib": ["nschan", "phue"], "nb_contrib": 2, "codes": [" process run_predictions_DPP {\n        input:\n            path(\"vegetation-segmentation/*\") from ch_model.collect()\n            tuple val(index), path(shard) from ch_shards\n        output:\n            tuple val(index), path('*.png') into ch_predictions\n        when:\n            !params.masks\n        script:\n            \"\"\"\n            #!/usr/bin/env python\n\n            import logging\n\n            import numpy as np\n            import tensorflow as tf\n            import deepplantphenomics as dpp\n\n            from cv2 import imwrite\n            from data_record import parse_record\n\n            logger = tf.get_logger()\n            logger.propagate = False\n            logger.setLevel('INFO')\n\n            pretrainedDPP = dpp.networks.vegetationSegmentationNetwork(8)\n\n            def checkpoint_override(net, checkpoint_path, num_classes):\n                if num_classes != 2:\n                    net.model.set_num_segmentation_classes(num_classes)\n                net.model._add_layers_to_graph()\n                saver = tf.compat.v1.train.Saver()\n                saver.restore(net.model._session, tf.train.latest_checkpoint(checkpoint_path))\n\n            with pretrainedDPP.model._graph.as_default():\n                checkpoint_override(pretrainedDPP,'vegetation-segmentation/', 2)\n                dataset = (\n                tf.data.TFRecordDataset('${shard}')\n                .map(parse_record)\n                .batch(1)\n                .prefetch(1))\n\n                samples = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n                for i in samples:\n                    img, filename = tf.cast(samples['original'],tf.float32),  samples['filename']\n                    raw = pretrainedDPP.model.forward_pass(img, deterministic=True)\n                    try:\n                        while True:\n                            prediction, name = pretrainedDPP.model._session.run([raw,filename])\n                            logger.info(\"Running prediction on image %s\" % name)\n                            seg = np.interp(prediction, (prediction.min(), prediction.max()), (0, 1))\n                            mask = (np.squeeze(seg) > 0.5).astype(np.uint8)\n                            name = name[0].decode('utf-8').rsplit('.', 1)[0]\n                            imwrite(f'{name}.png', mask)\n                    except tf.errors.OutOfRangeError:\n                        pass\n            \"\"\"\n    }"], "list_proc": ["phue/aradeepopsis/run_predictions_DPP"], "list_wf_names": ["phue/aradeepopsis"]}, {"nb_reuse": 1, "tools": ["MultiDataSet", "PredictPA"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["aradeepopsis"], "list_contrib": ["nschan", "phue"], "nb_contrib": 2, "codes": [" process run_predictions {\n        input:\n            path(model) from ch_model.collect()\n            tuple val(index), path(shard) from ch_shards\n        output:\n            tuple val(index), path('*.png') into ch_predictions\n        when:\n            !params.masks\n        script:\n            \"\"\"\n            #!/usr/bin/env python\n\n            import logging\n\n            import tensorflow as tf\n\n            from data_record import parse_record\n            from frozen_graph import wrap_frozen_graph\n\n            logger = tf.get_logger()\n            logger.propagate = False\n            logger.setLevel('INFO')\n\n            with tf.io.gfile.GFile('${model}', \"rb\") as f:\n                graph_def = tf.compat.v1.GraphDef()\n                graph_def.ParseFromString(f.read())\n\n            predict = wrap_frozen_graph(\n                graph_def,\n                inputs='ImageTensor:0',\n                outputs='SemanticPredictions:0')\n\n            dataset = (\n                tf.data.TFRecordDataset('${shard}')\n                .map(parse_record)\n                .batch(1)\n                .prefetch(1)\n                .enumerate(start=1))\n\n            size = len(list(dataset))\n\n            for index, sample in dataset:\n                filename = sample['filename'].numpy()[0].decode('utf-8')\n                logger.info(\"Running prediction on image %s (%d/%d)\" % (filename,index,size))\n                raw_segmentation = predict(sample['original'])[0][:, :, None]\n                output = tf.image.encode_png(tf.cast(raw_segmentation, tf.uint8))\n                tf.io.write_file(filename.rsplit('.', 1)[0] + '.png',output)\n            \"\"\"\n    }"], "list_proc": ["phue/aradeepopsis/run_predictions"], "list_wf_names": ["phue/aradeepopsis"]}, {"nb_reuse": 1, "tools": ["TraceMontage"], "nb_own": 1, "list_own": ["phue"], "nb_wf": 1, "list_wf": ["aradeepopsis"], "list_contrib": ["nschan", "phue"], "nb_contrib": 2, "codes": ["\nprocess draw_diagnostics {\n    publishDir \"${params.outdir}/diagnostics\", mode: 'copy',\n        saveAs: { filename ->\n                    if (filename.startsWith(\"mask_\")) \"summary/mask/$filename\"\n                    else if (filename.startsWith(\"overlay_\")) \"summary/overlay/$filename\"\n                    else if (filename.startsWith(\"crop_\")) \"summary/crop/$filename\"\n                    else null\n                }\n    input:\n        tuple val(index), val(type), path(image) from ch_masks.concat(ch_overlays,ch_crops)\n    output:\n        path('*.jpeg')\n    when:\n        params.summary_diagnostics\n\n    script:\n        def polaroid = params.polaroid ? '+polaroid' : ''\n        \"\"\"\n        #!/usr/bin/env bash\n\n        montage * -background 'black' -font Ubuntu-Condensed -geometry 200x200 -set label '%t' -fill white ${polaroid} \"${type}_chunk_${index}.jpeg\"\n        \"\"\"\n}"], "list_proc": ["phue/aradeepopsis/draw_diagnostics"], "list_wf_names": ["phue/aradeepopsis"]}, {"nb_reuse": 1, "tools": ["STAR", "FastQC", "SAMtools", "MultiQC"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-circtools"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n    saveAs: {filename ->\n        if (filename.indexOf(\".csv\") > 0) filename\n        else null\n    }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version &> v_pipeline.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    fastqc --version &> v_fastqc.txt  || true\n    STAR --version &> v_star.txt  || true\n    samtools --version &> v_samtools.txt  || true\n    multiqc --version &> v_multiqc.txt  || true\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-circtools/get_software_versions"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-circtools"]}, {"nb_reuse": 2, "tools": ["SAMtools", "STAR"], "nb_own": 2, "list_own": ["pilm-bioinformatics", "plateau-gao"], "nb_wf": 2, "list_wf": ["pipelines-nf-circtools", "CUTnTag"], "list_contrib": ["plateau-gao", "lpantano"], "nb_contrib": 2, "codes": [" process bam2bigwig {\n        publishDir \"${params.result}/alignment/bigwig\", mode:\"copy\"\n\n        input:\n        tuple val(group), val(cond), val(id), path(sort_bam) from exp_bam_ch\n\n        output:\n        tuple val(group), val(id), path(\"${group}_${id}_raw.bw\") into (bw4trans_ch, bw4seacr_ch, bw4macs2_ch)\n\n        script:\n        \"\"\"\n        samtools index ${sort_bam}\n        bamCoverage -b ${sort_bam} -o ${group}_${id}_raw.bw\n        \"\"\"  \n    }", " process makeSTARindex {\n        label 'high_memory'\n        tag \"$fasta\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/reference_genome\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta from ch_fasta_for_star_index\n        file gtf from gtf_makeSTARindex\n\n        output:\n        file \"star\" into star_index\n\n        script:\n        def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --runThreadN ${task.cpus} \\\\\n            --sjdbGTFfile $gtf \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            $avail_mem\n        \"\"\"\n    }"], "list_proc": ["plateau-gao/CUTnTag/bam2bigwig", "pilm-bioinformatics/pipelines-nf-circtools/makeSTARindex"], "list_wf_names": ["plateau-gao/CUTnTag", "pilm-bioinformatics/pipelines-nf-circtools"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-circtools"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": ["\nprocess makeRRNAindex {\n    tag \"$rrna.simpleName\"\n    publishDir path: { \"${params.outdir}\" },\n               mode: 'copy'\n\n    input:\n    file rrna from tx_rrna_ch\n\n    output:\n    file 'rrna_index' into rrna_index_ch\n\n    script:\n    \"\"\"\n    mkdir rrna_index\n    bowtie2-build --threads $task.cpus $rrna rrna_index/rrna\n    \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-circtools/makeRRNAindex"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-circtools"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-circtools"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": ["\nprocess rrna {\n    tag \"$sample\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/rrna\", mode: 'copy'\n\n    input:\n    file rrna_index from rrna_index_ch.collect()\n    file reads from trimmed_reads\n\n    output:\n    file \"*_clean.fastq.gz\" into clean_ch\n    \n    script:\n    prefix = reads[0].toString() - ~/(_trimmed)?(\\.fq)?(\\.gz)?$/\n    if (params.singleEnd){\n        \"\"\"\n        bowtie2 -x $rrna_index/rrna -U $reads  --no-unal --omit-sec-seq --threads ${task.cpus} --mm --seed 1337 --un-gz ${prefix}_clean.fastq.gz --time -S ${prefix}.sam 2> ${prefix}.log\n        \"\"\"\n    }else{\n        \"\"\"\n        bowtie2 -x $rrna_index -1 $reads[0] -2 $reads[1] -S ${reads.baseName}.sam --no-unal --omit-sec-seq --threads ${task.cpus} --mm --seed 1337 --time --un-conc-gz ${reads.baseName}.fastq.gz 2> ${reads.baseName}.log\n\n        \"\"\"\n    }\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-circtools/rrna"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-circtools"]}, {"nb_reuse": 1, "tools": ["circtools"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-circtools"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": ["\nprocess circtools {\n    tag \"$sample\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/circtools\", mode: 'copy'\n\n    input:\n    file samples from circtools_ch.collect()\n    file bams from bam_ch.collect()\n    file gtf from gtf_circtools.collect()\n    file repeat from repeat_ch.collect()\n    file genome from ch_fasta_for_circtools.collect()\n\n    output:\n    file \"*.circRNA\" into circtools_out\n    file \"*.circRNAmapped\" into circtools_mapped_out\n    file \"CircRNACount\" into rnacount\n    file \"CircCoordinates\" into coordinates\n    \n    script:\n    \"\"\"\n    circtools detect $samples -B $bams -D -F -M -Nr 2 1 -fg -G -R $repeat -an $gtf -A $genome\n  \n    # circtools detect $samples -D -Pi -F -M -Nr 5 1 -fg -G -R $repeat -an $gtf -A $genome\n    \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-circtools/circtools"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-circtools"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-circtools"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    when:\n    !params.skip_multiqc\n\n    input:\n    file multiqc_config from ch_multiqc_config\n    file (fastqc:'fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('trim/*') from trimmed_logs.collect()\n    file ('alignment/*') from alignment_logs.collect()\n    file ('software_versions/*') from software_versions_yaml\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename --config $multiqc_config \\\\\n        -m custom_content -m star -m cutadapt -m fastqc\n    \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-circtools/multiqc"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-circtools"]}, {"nb_reuse": 3, "tools": ["Picard", "Salmon", "MultiQC"], "nb_own": 3, "list_own": ["sebastianlzy", "pilm-bioinformatics", "tobsecret"], "nb_wf": 3, "list_wf": ["nextflow-demo", "BADAS_Nextflow_Tutorial", "pipelines-nf-fastrnaseq"], "list_contrib": ["pditommaso", "KevinSayers", "evanfloden", "tobsecret", "stevekm", "lpantano"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n    conda \"bioconda::multiqc\"\n       \n    input:\n    file('*') from quant_ch.mix(fastqc_ch).collect()\n    \n    output:\n    file('multiqc_report.html')  \n     \n    script:\n    \"\"\"\n    multiqc . \n    \"\"\"\n}", "\nprocess coverage {\n\n\n    tag \"$sample_id\"\n    echo true\n\n    publishDir \"${params.outdir}/coverage_files_picard\", mode:\"copy\", overwrite: true\n\n    input:\n    tuple val(sample_id), path(sorted_deduplicated_bam) from ch_bismark_sorted_deduplicated_bam1\n    file GENOME_FASTA\n\n    output:\n\n    tuple val(sample_id),file(\"*_coverage_picard.txt\")\n\n    script:\n    \"\"\"\n   ## \t  module load java\n   ## \t  module load samtools/1.3\n\n\tpicard CollectWgsMetrics REFERENCE_SEQUENCE=hg38.fa \\\n\t    MINIMUM_MAPPING_QUALITY=0 \\\n\t\tINPUT=$sorted_deduplicated_bam \\\n\t\tOUTPUT=${sample_id}_coverage_picard.txt\n\n\n    \"\"\"\n\n}", "\nprocess index {\n    tag \"$transcriptome.simpleName\"\n\n    input:\n    file transcriptome from tx_fasta_ch\n\n    output:\n    file 'index' into index_ch\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["tobsecret/BADAS_Nextflow_Tutorial/multiqc", "sebastianlzy/nextflow-demo/coverage", "pilm-bioinformatics/pipelines-nf-fastrnaseq/index"], "list_wf_names": ["sebastianlzy/nextflow-demo", "tobsecret/BADAS_Nextflow_Tutorial", "pilm-bioinformatics/pipelines-nf-fastrnaseq"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-fastrnaseq"], "list_contrib": ["evanfloden", "pditommaso", "lpantano"], "nb_contrib": 3, "codes": ["\nprocess quant {\n    tag \"$pair_id\"\n    publishDir params.outdir, mode: 'copy'\n\n    input:\n    file index from index_ch.collect()\n    set pair_id, file(reads) from read_ch\n\n    output:\n    file(pair_id) into quant_ch\n\n    log.info \"{pair_id}\"\n    \n    script:\n    \"\"\"\n    salmon quant --validateMappings --threads $task.cpus --libType=U -i $index -r ${reads[0]} -o $pair_id\n    \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-fastrnaseq/quant"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-fastrnaseq"]}, {"nb_reuse": 3, "tools": ["BamTools", "FastQC", "Minimap2", "SAMtools"], "nb_own": 3, "list_own": ["sguizard", "tolkit", "pilm-bioinformatics"], "nb_wf": 3, "list_wf": ["nemADSQ", "isoseq", "pipelines-nf-fastrnaseq"], "list_contrib": ["pgonzale60", "lstevens17", "pditommaso", "evanfloden", "sguizard", "peterwharrison", "Alexey-ebi", "lpantano"], "nb_contrib": 8, "codes": ["\nprocess BAMTOOLS_SPLIT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bamtools=2.5.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bamtools:2.5.1--h9a82719_9\"\n    } else {\n        container \"quay.io/biocontainers/bamtools:2.5.1--h9a82719_9\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bamtools \\\\\n        split \\\\\n        -in $bam \\\\\n        $options.args\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( bamtools --version | grep -e 'bamtools' | sed 's/^.*bamtools //' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess remove_ecoli {\n    tag \"${strain}_${fileNum}\"\n    label 'btk'\n\n    input:\n        tuple val(strain), val(fileNum), val(reads)\n        path(assembly)\n\n    output:\n        tuple val(strain), val(fileNum), path(\"${strain}_${fileNum}.fasta.gz\")\n\n    script:\n        \"\"\"\n        iget $reads - | minimap2 -ax map-ont -t ${task.cpus} $assembly - \\\n            | samtools fasta -f 4 - \\\n            | gzip -c > ${strain}_${fileNum}.fasta.gz\n        \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    set sample_id, file(reads) from read_qc_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}"], "list_proc": ["sguizard/isoseq/BAMTOOLS_SPLIT", "tolkit/nemADSQ/remove_ecoli", "pilm-bioinformatics/pipelines-nf-fastrnaseq/fastqc"], "list_wf_names": ["tolkit/nemADSQ", "pilm-bioinformatics/pipelines-nf-fastrnaseq", "sguizard/isoseq"]}, {"nb_reuse": 2, "tools": ["SAMtools", "MultiQC"], "nb_own": 2, "list_own": ["pilm-bioinformatics", "tamara-hodgetts"], "nb_wf": 2, "list_wf": ["nf-atac-seq", "pipelines-nf-fastrnaseq"], "list_contrib": ["evanfloden", "lpantano", "pditommaso", "tamara-hodgetts"], "nb_contrib": 4, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    file('*') from quant_ch.mix(fastqc_ch).collect()\n\n    output:\n    file('multiqc_report.html')\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n                                                    \n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-fastrnaseq/multiqc", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_SORT"], "list_wf_names": ["tamara-hodgetts/nf-atac-seq", "pilm-bioinformatics/pipelines-nf-fastrnaseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-genomes"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": ["\nprocess copy_fasta {\n  publishDir path: { \"${outdir}/seq\"},\n  mode: 'copy'\n\n  input:\n  file fasta from ch_fasta_for_cp\n\n  output:\n  file \"${params.genome}.${params.release}.fa\" into ch_fasta_for_star_index, ch_fasta_for_hisat_index, ch_fasta_for_txome, ch_fasta_for_gentrome, ch_fasta_for_config\n  file \"${params.genome}.${params.release}.fa.fai\"\n  \n  script:\n  \"\"\"\n  cp ${fasta} ${params.genome}.${params.release}.fa\n  samtools faidx ${params.genome}.${params.release}.fa\n\"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-genomes/copy_fasta"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-genomes"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-genomes"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": [" process makeSTARindex {\n        label 'high_memory'\n        tag \"$fasta\"\n        publishDir path: { \"${outdir}\" },\n                   mode: 'copy'\n\n        input:\n        file fasta from ch_fasta_for_star_index\n        file gtf from gtf_makeSTARindex\n\n        output:\n        file \"star\" into star_index\n\n        script:\n        def avail_mem = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --runThreadN ${task.cpus} \\\\\n            --sjdbGTFfile $gtf \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            $avail_mem\n        \"\"\"\n    }"], "list_proc": ["pilm-bioinformatics/pipelines-nf-genomes/makeSTARindex"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-genomes"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["pilm-bioinformatics"], "nb_wf": 1, "list_wf": ["pipelines-nf-genomes"], "list_contrib": ["lpantano"], "nb_contrib": 1, "codes": [" process makeTxome {\n  publishDir path: { \"${outdir}/rnaseq\"},\n  mode: 'copy'\n  \n  input:\n  file fasta from ch_fasta_for_txome\n  file gtf from ch_gtf_for_txome\n  file pre_gtf from ch_pre_gtf\n\n  output:\n  file \"tx_${gtf.baseName}.fa\" into ch_txfasta_for_gentrome, ch_txfasta_for_config\n  file \"tx_${pre_gtf.baseName}.fa\" into ch_pre_txfasta_for_config\n\n  script:\n  \"\"\"\n  gffread -w tx_${gtf.baseName}.fa -g $fasta $gtf\n  gffread -w tx_${pre_gtf.baseName}.fa -g $fasta $gtf\n  \"\"\"\n}"], "list_proc": ["pilm-bioinformatics/pipelines-nf-genomes/makeTxome"], "list_wf_names": ["pilm-bioinformatics/pipelines-nf-genomes"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": [" process fastqc {\n        publishDir \"${params.result}/fastqc\", mode:\"copy\"\n\n        input:\n        tuple val(group), val(cond), val(id), path(read1), path(read2) from fastqc_ch\n\n        output:\n        path \"*\"\n\n        script:\n        \"\"\"\n        fastqc -f fastq ${read1}\n        fastqc -f fastq ${read2}\n        \"\"\"\n    }"], "list_proc": ["plateau-gao/CUTnTag/fastqc"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 2, "tools": ["Bowtie", "MultiQC"], "nb_own": 2, "list_own": ["propan2one", "plateau-gao"], "nb_wf": 2, "list_wf": ["variantcallerbench", "CUTnTag"], "list_contrib": ["propan2one", "plateau-gao"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", " process build_index_hg38{\n        publishDir \"${params.result}/index_hg38\", mode:\"copy\"\n\n        input:\n        path ref_hg38 from ref_hg38_ch\n\n        output:\n        path \"hg38.*\" into index_hg38_ch\n\n        script:\n        \"\"\"\n        bowtie2-build --threads ${params.threads} ${ref_hg38} hg38\n        \"\"\"\n    }"], "list_proc": ["propan2one/variantcallerbench/multiqc", "plateau-gao/CUTnTag/build_index_hg38"], "list_wf_names": ["propan2one/variantcallerbench", "plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": [" process build_index_ecoli{\n        publishDir \"${params.result}/index_ecoli\", mode:\"copy\"\n\n        input:\n        path ref_ecoli from ref_ecoli_ch\n\n        output:\n        path \"ecoli.*\" into index_ecoli_ch\n\n        script:\n        \"\"\"\n        bowtie2-build --threads ${params.threads} ${ref_ecoli} ecoli\n        \"\"\"    \n    }"], "list_proc": ["plateau-gao/CUTnTag/build_index_ecoli"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess align_hg38 {\n    publishDir \"${params.result}/alignment/hg38_sam\", mode:\"copy\"\n\n    input:\n    tuple val(group), val(cond), val(id), path(read1), path(read2), path(index) from align_hg38_ch\n\n    output:\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_bowtie2.sam\") into (sam_hg38_picard_ch, sam_hg38_nopicard_ch)\n    path \"${group}_${id}_bowtie2_hg38.txt\" into bowtie2_report_hg38_ch\n\n    script:\n    index_base = index[0].toString() - ~/.rev.\\d.bt2?/ - ~/.\\d.bt2?/\n    \"\"\"\n    bowtie2 ${params.args.bowtie2_align_target} -p ${params.threads} \\\n    -x $index_base -1 ${read1} -2 ${read2} \\\n    -S ${group}_${id}_bowtie2.sam &>${group}_${id}_bowtie2_hg38.txt\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/align_hg38"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess align_ecoli {\n    publishDir \"${params.result}/alignment/spikeIn_sam\", mode:\"copy\"\n\n    input:\n    tuple val(group), val(cond), val(id), path(read1), path(read2), path(index) from align_ecoli_ch\n\n    output:\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_bowtie2_spikeIn.sam\")\n    path \"${group}_${id}_bowtie2_spikeIn.txt\" into bowtie2_report_spikeIn_ch\n    tuple val(group), val(cond), val(id), path (\"${group}_${id}_spikeIn.seqDepth\") into (seqdepth_4_bedgraph_ch, seqdepth_4_R_ch)\n\n    script:\n    index_base = index[0].toString() - ~/.rev.\\d.bt2?/ - ~/.\\d.bt2?/\n    \"\"\"\n    # align to ecoli\n    bowtie2 ${params.args.bowtie2_align_spikein} -p ${params.threads} \\\n    -x $index_base -1 ${read1} -2 ${read2} \\\n    -S ${group}_${id}_bowtie2_spikeIn.sam &> ${group}_${id}_bowtie2_spikeIn.txt\n\n    # count seqDepth\n    seqDepthDouble=`samtools view ${params.args.samtool_seqDepth_view} ${group}_${id}_bowtie2_spikeIn.sam | wc -l`\n    seqDepth=\\$((seqDepthDouble/2))\n    echo \\$seqDepth > ${group}_${id}_spikeIn.seqDepth\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/align_ecoli"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess picard_rmDup {\n    publishDir \"${params.result}/alignment/hg38_sam\", pattern:\"*_sorted.sam\", mode:\"copy\"\n    publishDir \"${params.result}/alignment/removeDuplicate\", pattern:\"*_*Dup.sam\", mode:\"copy\"\n    publishDir \"${params.result}/alignment/removeDuplicate/picard_summary\", pattern:\"*_*Dup.txt\", mode:\"copy\"\n\n    input:\n    tuple val(group), val(cond), val(id), path(ori_sam) from sam_hg38_picard_ch\n\n    output:\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_sorted.sam\")\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_markDup.sam\")\n    path (\"${group}_${id}_markDup.txt\")\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_rmDup.sam\") into rmdup_sam_ch\n    path \"${group}_${id}_rmDup.txt\" into rmdup_report_ch\n\n    script:\n    \"\"\"\n    picard SortSam -I ${ori_sam} -O ${group}_${id}_sorted.sam ${params.args.picard_sort}\n    picard MarkDuplicates -I ${group}_${id}_sorted.sam -O ${group}_${id}_markDup.sam --METRICS_FILE ${group}_${id}_markDup.txt\n    picard MarkDuplicates -I ${group}_${id}_sorted.sam -O ${group}_${id}_rmDup.sam --REMOVE_DUPLICATES true -METRICS_FILE ${group}_${id}_rmDup.txt\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/picard_rmDup"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess assess_frag_size_distribution {\n    publishDir \"${params.result}/alignment/sam/fragmentLen\", mode: 'copy'\n\n    input:\n    tuple val(group), val(cond), val(id), path(sam) from sam4assess_frag\n\n    output:\n    path(\"${group}_${id}_fragmentLen.txt\") into fragmentLen_ch\n\n    script:\n    \"\"\"\n    samtools view ${params.args.samtool_assess_frag} ${sam} | ${params.args.assess_frag} > ${group}_${id}_fragmentLen.txt\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/assess_frag_size_distribution"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": [" process quality_filter {\n        publishDir \"${params.result}/alignment/hg38_filterScore_${params.agrs.filter_miniQualityScore}_sam\", mode:\"copy\"\n\n        input:\n        tuple val(group), val(cond), val(id), path(sam) from sam4filter_ch\n\n        output:\n        tuple val(group), val(cond), val(id), path(\"${group}_${id}_*.sam\") into sam2bam_ch\n\n        script:\n        \"\"\"\n        samtools view -q ${params.agrs.filter_miniQualityScore} ${sam} >${group}_${id}_qualityScore${params.agrs.filter_miniQualityScore}.sam\n        \"\"\"\n    }"], "list_proc": ["plateau-gao/CUTnTag/quality_filter"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess sam2bam {\n    publishDir \"${params.result}/alignment/bam\", mode: \"copy\"\n\n    input:\n    tuple val(group), val(cond), val(id), path(sam) from sam2bam_ch\n\n    output:\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_mapped.bam\") into (bam2bed_ch, bam2bigwig_ch, bam4macs2_ch, bam4seacr_report_ch, bam4macs2_report_ch)\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_sorted.bam\") into sort_bam_ch\n\n    script:\n    \"\"\"\n    samtools view ${params.args.samtool_sam2bam_view} ${sam} > ${group}_${id}_mapped.bam\n    samtools sort ${group}_${id}_mapped.bam -o ${group}_${id}_sorted.bam\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/sam2bam"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess bam2bed {\n    publishDir \"${params.result}/alignment/bed\", mode: \"copy\"\n\n    input:\n    tuple val(group), val(cond), val(id), path(mapped_bam) from bam2bed_ch\n\n    output:\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}.bed\") into bed_ch\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_clean.bed\") into clean_bed_ch\n    tuple val(group), val(cond), val(id), path(\"${group}_${id}_fragments.bed\") into (fragBed_assess_ch, fragBed_convert_ch)\n    \n    script:\n    \"\"\"\n    bedtools bamtobed -i ${mapped_bam} ${params.args.bedtool_bam2bed} > ${group}_${id}.bed\n    awk ${params.args.clean_bed} ${group}_${id}.bed > ${group}_${id}_clean.bed\n    cut ${params.args.frag_bed_cut} ${group}_${id}_clean.bed | sort ${params.args.frag_bed_sort} > ${group}_${id}_fragments.bed\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/bam2bed"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["plateau-gao"], "nb_wf": 1, "list_wf": ["CUTnTag"], "list_contrib": ["plateau-gao"], "nb_contrib": 1, "codes": ["\nprocess bed2bedgraph {\n    echo true\n    publishDir \"${params.result}/alignment/bedgraph\", mode: \"copy\"\n\n    input:\n    tuple val(group), val(cond), val(id), path(seq_Depth), path(fragbed) from bed2bedgraph_ch\n\n    output:\n    tuple val(group), val(cond), val(id), path (\"${group}_${id}_*.bedgraph\") into bedgraph_ch\n\n    script:\n    \"\"\"\n    seqDepth=`cat ${seq_Depth}`\n    if [[ \"\\$seqDepth\" -gt \"1\" ]]; then\n        scale_factor=`echo \"${params.args.constant_C} / \\$seqDepth\"| bc -l`\n        bedtools genomecov -bg -scale \\$scale_factor -i ${fragbed} -g ${params.genome_size} > ${group}_${id}_fragments.normalized.bedgraph\n    fi\n    \"\"\"\n}"], "list_proc": ["plateau-gao/CUTnTag/bed2bedgraph"], "list_wf_names": ["plateau-gao/CUTnTag"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n\n    publishDir \"${params.results}/fastqc\", mode: 'rellink'\n    container 'library://porchard/default/general:20220107'\n    errorStrategy 'retry'\n    maxRetries 1\n    maxForks 10\n\n    input:\n    path(fastq)\n\n    output:\n    path(fastqc_out)\n\n    script:\n    fastqc_out = fastq.getName().replaceAll('.fastq.gz', '_fastqc.zip')\n\n    \"\"\"\n    fastqc $fastq -o .\n    \"\"\"\n\n}"], "list_proc": ["porchard/ATACseq-NextFlow/fastqc"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n\n    publishDir \"${params.results}/multiqc/${before_or_after_trim}\", mode: 'rellink', overwrite: true\n    container 'library://porchard/default/general:20220107'\n    time '24h'\n\n    input:\n    tuple val(before_or_after_trim), path(x)\n\n    output:\n    path('multiqc_data')\n    path('multiqc_report.html')\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n\n}"], "list_proc": ["porchard/ATACseq-NextFlow/multiqc"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess bwa {\n\n    publishDir \"${params.results}/bwa\", mode: 'rellink'\n    container 'library://porchard/default/bwa:0.7.15'\n    memory '50 GB'\n    cpus 12\n    errorStrategy 'retry'\n    maxRetries 1\n    time '48h'\n\n    input:\n    tuple val(library), val(readgroup), path(fastq_1), path(fastq_2)\n\n    output:\n    tuple val(library), path(\"${library}-${readgroup}.bam\")\n\n    \"\"\"\n    bwa mem -I 200,200,5000 -M -t 12 ${get_bwa_index(get_genome(library))} ${fastq_1} ${fastq_2} | samtools sort -m 1g -@ 11 -O bam -T sort_tmp -o ${library}-${readgroup}.bam -\n    \"\"\"\n\n}"], "list_proc": ["porchard/ATACseq-NextFlow/bwa"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess mark_duplicates {\n\n    publishDir \"${params.results}/mark_duplicates\", mode: 'rellink'\n    container 'library://porchard/default/general:20220107'\n    errorStrategy 'retry'\n    maxRetries 1\n    time '5h'\n    maxForks 15\n\n    input:\n    tuple val(library), path(bam)\n\n    output:\n    tuple val(library), path(\"${library}.md.bam\"), path(\"${library}.md.bam.bai\")\n\n    \"\"\"\n    java -Xmx4g -Xms4g -jar \\$PICARD_JAR MarkDuplicates I=$bam O=${library}.md.bam ASSUME_SORTED=true METRICS_FILE=${library}.metrics VALIDATION_STRINGENCY=LENIENT\n    samtools index ${library}.md.bam\n    \"\"\"\n}"], "list_proc": ["porchard/ATACseq-NextFlow/mark_duplicates"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess bamtobed {\n\n    container 'library://porchard/default/general:20220107'\n    time '4h'\n    maxForks 10\n\n    input:\n    tuple val(library), path(bam)\n\n    output:\n    tuple val(library), path(\"${library}.bed\")\n\n    \"\"\"\n    bedtools bamtobed -i $bam > ${library}.bed\n    \"\"\"\n\n}"], "list_proc": ["porchard/ATACseq-NextFlow/bamtobed"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess blacklist_filter_peaks {\n\n    publishDir \"${params.results}/macs2\", mode: 'rellink'\n    container 'library://porchard/default/general:20220107'\n    time '1h'\n\n    input:\n    tuple val(library), path(peaks)\n\n    output:\n    path(\"${library}_peaks.broadPeak.noblacklist\")\n\n    when:\n    has_blacklist(get_genome(library))\n\n    \"\"\"\n    bedtools intersect -a $peaks -b ${get_blacklists(get_genome(library)).join(' ')} -v > ${library}_peaks.broadPeak.noblacklist\n    \"\"\"\n\n}"], "list_proc": ["porchard/ATACseq-NextFlow/blacklist_filter_peaks"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["bedGraphToBigWig"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["ATACseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess bigwig {\n\n    publishDir \"${params.results}/bigwig\", mode: 'rellink'\n    container 'library://porchard/default/general:20220107'\n    time '5h'\n\n    input:\n    tuple val(library), path(bedgraph)\n\n    output:\n    tuple val(genome), path(\"${library}.bw\")\n\n    script:\n    genome = get_genome(library)\n\n    \"\"\"\n    LC_COLLATE=C sort -k1,1 -k2n,2 $bedgraph > sorted.bedgraph\n    bedClip sorted.bedgraph ${get_chrom_sizes(genome)} clipped.bedgraph\n    bedGraphToBigWig clipped.bedgraph ${get_chrom_sizes(genome)} ${library}.bw\n    rm sorted.bedgraph clipped.bedgraph\n    \"\"\"\n\n}"], "list_proc": ["porchard/ATACseq-NextFlow/bigwig"], "list_wf_names": ["porchard/ATACseq-NextFlow"]}, {"nb_reuse": 2, "tools": ["MultiQC", "BEDTools"], "nb_own": 2, "list_own": ["porchard", "ralsallaq"], "nb_wf": 2, "list_wf": ["RNAseq-NextFlow", "metaGx_nf"], "list_contrib": ["porchard", "ralsallaq"], "nb_contrib": 2, "codes": [" process quantifyCDSCov {\n        container \"${container_bedtools}\"\n        label 'multithread'\n        publishDir \"${params.outD}/assembly-based/readsOntoCDS/\", mode: 'copy'\n        \n        input:\n        set sname, file(unsortedBam), file(sortedBam) from readsOntoCDS_bam_ch\n        \n        output:\n        set sname, file(\"${sname}.cds_coverage.hist\") into cdsCovHisto_ch\n        \n        \"\"\"\n        bedtools genomecov -ibam ${sortedBam}  > ${sname}.cds_coverage.hist\n        \"\"\"\n    }", "\nprocess star_multiqc {\n\n    publishDir \"${params.results}/multiqc/star\", mode: 'rellink', overwrite: true\n    container 'library://porchard/default/general:20220107'\n\n    input:\n    path(x)\n\n    output:\n    path('multiqc_data')\n    path('multiqc_report.html')\n\n    \"\"\"\n    multiqc .\n    \"\"\"\n\n}"], "list_proc": ["ralsallaq/metaGx_nf/quantifyCDSCov", "porchard/RNAseq-NextFlow/star_multiqc"], "list_wf_names": ["ralsallaq/metaGx_nf", "porchard/RNAseq-NextFlow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["porchard"], "nb_wf": 1, "list_wf": ["RNAseq-NextFlow"], "list_contrib": ["porchard"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n\n    publishDir \"${params.results}/fastqc\", mode: 'rellink', overwrite: true\n    container 'library://porchard/default/general:20220107'\n    maxForks 6\n    tag \"${library} ${readgroup}\"\n\n    input:\n    tuple val(library), val(readgroup), path(fastq)\n\n    output:\n    tuple path(outfile_1), path(outfile_2)\n\n    script:\n    outfile_1 = fastq.getName().replaceAll('.fastq.gz', '_fastqc.html')\n    outfile_2 = fastq.getName().replaceAll('.fastq.gz', '_fastqc.zip')\n\n    \"\"\"\n    fastqc $fastq\n    \"\"\"\n\n}"], "list_proc": ["porchard/RNAseq-NextFlow/fastqc"], "list_wf_names": ["porchard/RNAseq-NextFlow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "MultiQC", "GATK"], "nb_own": 3, "list_own": ["sanger-tol", "vincenthhu", "priyanka-surana"], "nb_wf": 2, "list_wf": ["readmapping", "nf-core-westest", "hic"], "list_contrib": ["vincenthhu", "priyanka-surana"], "nb_contrib": 2, "codes": ["process SAMTOOLS_COLLATE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1':\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        collate \\\\\n        $args \\\\\n        -@ ${task.cpus-1} \\\\\n        -o ${prefix}.bam \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' )\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GETPILEUPSUMMARIES {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n    path variants\n    path variants_tbi\n    path sites\n\n    output:\n    tuple val(meta), path('*.pileups.table'), emit: table\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def sitesCommand = ''\n\n    sitesCommand = sites ? \" -L ${sites} \" : \" -L ${variants} \"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GetPileupSummaries] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GetPileupSummaries \\\\\n        -I $bam \\\\\n        -V $variants \\\\\n        $sitesCommand \\\\\n        -O ${prefix}.pileups.table \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["sanger-tol/readmapping/SAMTOOLS_COLLATE", "priyanka-surana/hic/MULTIQC"], "list_wf_names": ["priyanka-surana/hic", "sanger-tol/readmapping"]}, {"nb_reuse": 1, "tools": ["FastQC", "MultiQC"], "nb_own": 1, "list_own": ["qbic-pipelines"], "nb_wf": 1, "list_wf": ["cellranger"], "list_contrib": ["ggabernet"], "nb_contrib": 1, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                        if (filename.indexOf('.csv') > 0) filename\n                        else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    cellranger --version > v_cellranger.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["qbic-pipelines/cellranger/get_software_versions"], "list_wf_names": ["qbic-pipelines/cellranger"]}, {"nb_reuse": 2, "tools": ["BWA", "MultiQC"], "nb_own": 2, "list_own": ["qbic-pipelines", "tamara-hodgetts"], "nb_wf": 2, "list_wf": ["cellranger", "nf-atac-seq"], "list_contrib": ["ggabernet", "tamara-hodgetts"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"          , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bwa\n    bwa index $options.args $fasta -p bwa/${fasta.baseName}\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["qbic-pipelines/cellranger/multiqc", "tamara-hodgetts/nf-atac-seq/BWA_INDEX"], "list_wf_names": ["tamara-hodgetts/nf-atac-seq", "qbic-pipelines/cellranger"]}, {"nb_reuse": 9, "tools": ["BCFtools", "Flye", "blima", "SAMtools", "STAR", "ULTRA", "RNASEQR", "FastQC", "QualiMap"], "nb_own": 8, "list_own": ["ray1919", "sheynkman-lab", "replikation", "tdelhomme", "vibbits", "sguizard", "qbic-pipelines", "wtsi-hgi"], "nb_wf": 8, "list_wf": ["IsoSeq-Nextflow", "vcf_ancestry-nf", "nextflow-pipelines", "isoseq", "rnaseq-editing", "docker_pipelines", "vcreport", "lRNA-Seq"], "list_contrib": ["ray1919", "replikation", "alex-botzki", "ggabernet", "LaurenceKuhl", "abotzki", "tdelhomme", "sguizard", "wtsi-mercury", "bj8th", "mult1fractal", "peterwharrison", "Alexey-ebi", "gn5"], "nb_contrib": 14, "codes": ["\nprocess samtools_index_idxstats {\n    tag \"${samplename}\"\n    container \"nfcore-rnaseq\"\n    memory = '8G'\n    cpus 1\n    time '300m'\n    errorStrategy { task.attempt <= 5 ? 'retry' : 'ignore' }\n    maxRetries 5\n    publishDir \"${params.outdir}/idxstats/\", mode: 'symlink', pattern: \"*.idxstats\"\n\n    when:\n    params.run\n\n    input:\n    set val(aligner), val(samplename), file(thebam)                   \n\n    output:\n    set val(samplename), file(\"*.idxstats\")                     \n\n    script:\n    \"\"\"\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:\\$PATH\n\n    samtools index $thebam\n    samtools idxstats $thebam > ${samplename}.idxstats\n    rm ${thebam}.bai\n    \"\"\"\n}", "\nprocess index_cram {\n    memory '3G'\n    tag \"$cram_file\"\n    cpus 1\n                   \n    time '100m'\n    queue 'normal'\n    container \"graphtyper\"\n    containerOptions = \"--bind /lustre\"\n                                \n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    publishDir \"${params.outdir}/cram_index/\", mode: 'symlink', overwrite: true, pattern: \"${cram_file}.crai\"\n    maxRetries 3\n\n    when:\n    params.run\n     \n    input:\n    file(cram_file)\n\n    output: \n    tuple file(\"${cram_file}.crai\"), emit: indexes\n\n    script:\n\"\"\" \nsamtools index $cram_file\n\"\"\"\n}", "\nprocess lima{\n    tag \"${ccs_read}\"\n    label \"isoseq3\"\n    publishDir \"${params.outdir}/isoseq3/lima/${ccs_read.name.split('\\\\.')[0]}\", mode: \"copy\"\n    input:\n        file(ccs_read) from ch_ccs_reads.flatten()\n        file(barcodes) from ch_barcodes\n\n    output:\n        file(\"*.bam\") into ch_individual_lima_bam \n        file(\"*\")\n\n    script:\n        sample_name = ccs_read.name.split('\\\\.')[0]\n        \"\"\"\n        lima $ccs_read $barcodes ${sample_name}.bam --split-bam-named --isoseq --peek-guess\n        \"\"\"\n}", "\nprocess merge_imputed_vcf {\n\n  input:\n  file fasta_ref\n  file all_chr from imp_res.collect()\n\n  output:\n  file(\"*.vcf.gz\") into final_vcf\n\n  shell:\n  '''\n  for f in `ls *vcf.gz`\n  do\n    filename=`basename $f`\n    tabix -p vcf $f\n    bcftools filter -i 'INFO/DR2>0.3' $f | bgzip -c > ${filename}_DR2filter.vcf.gz\n  done\n  bcftools concat *{1..22}_DR2filter.vcf.gz | bcftools norm -m - -Oz -f !{fasta_ref} > !{params.cohort}_plinkfilt_imputed_allchr_DR2filter_norm.vcf.gz\n  '''\n\n}", "\nprocess ULTRA_PIPELINE {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::ultra_bioinformatics=0.0.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ultra_bioinformatics:0.0.4--pyh5e36f6f_1\"\n    } else {\n        container \"quay.io/biocontainers/ultra_bioinformatics:0.0.4--pyh5e36f6f_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path genome\n    path gtf\n\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    uLTRA \\\\\n        pipeline \\\\\n        --t $task.cpus \\\\\n        --prefix $prefix \\\\\n        $options.args \\\\\n        \\$(pwd)/$genome \\\\\n        \\$(pwd)/$gtf \\\\\n        \\$(pwd)/$reads \\\\\n        ./\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( uLTRA --version|sed 's/uLTRA //g' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n    }\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    def args     = options.args.tokenize()\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n    scratch false\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1\"\n    } else {\n        container \"quay.io/biocontainers/qualimap:2.2.2d--1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $options.args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "process flye {\n    label 'flye'  \n    errorStrategy 'ignore'\n  input:\n    tuple val(name), path(read)\n  output:\n    tuple val(name), path(read), path(\"${name}.fasta\")\n  script:\n    \"\"\"\n    flye --plasmids --meta -t ${task.cpus} --nano-raw ${read} -o assembly \n    mv assembly/assembly.fasta ${name}.fasta \n    \"\"\"\n  }"], "list_proc": ["wtsi-hgi/nextflow-pipelines/samtools_index_idxstats", "wtsi-hgi/nextflow-pipelines/index_cram", "sheynkman-lab/IsoSeq-Nextflow/lima", "tdelhomme/vcf_ancestry-nf/merge_imputed_vcf", "sguizard/isoseq/ULTRA_PIPELINE", "qbic-pipelines/vcreport/FASTQC", "vibbits/rnaseq-editing/STAR_GENOMEGENERATE", "ray1919/lRNA-Seq/QUALIMAP_RNASEQ", "replikation/docker_pipelines/flye"], "list_wf_names": ["replikation/docker_pipelines", "vibbits/rnaseq-editing", "wtsi-hgi/nextflow-pipelines", "sheynkman-lab/IsoSeq-Nextflow", "tdelhomme/vcf_ancestry-nf", "ray1919/lRNA-Seq", "sguizard/isoseq", "qbic-pipelines/vcreport"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["quinlan-lab"], "nb_wf": 1, "list_wf": ["preon"], "list_contrib": ["brwnj"], "nb_contrib": 1, "codes": ["\nprocess mark_duplicates {\n    tag \"${sample_id}\"\n    publishDir path: \"$outdir/alignments\"\n\n    input:\n    set sample_id, file(bam) from bwa_ch.groupTuple()\n\n    output:\n    file(\"${sample_id}.md.bam\") into (alignments_ch, alignments_upload_ch, create_call_bams_ch)\n    file(\"${sample_id}.md.bam.bai\") into (alignment_indexes_ch, alignment_indexes_upload_ch, indexcov_input_ch)\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g MarkDuplicatesSpark \\\n        ${bam.collect { \"--input $it\" }.join(\" \")} \\\n        --output ${sample_id}.md.bam \\\n        --tmp-dir . \\\n        --spark-master \\'local[*]\\'\n    gatk --java-options -Xmx${task.memory.toGiga()}g BuildBamIndex \\\n        --INPUT ${sample_id}.md.bam \\\n        --OUTPUT ${sample_id}.md.bam.bai \\\n        --TMP_DIR .\n    \"\"\"\n}"], "list_proc": ["quinlan-lab/preon/mark_duplicates"], "list_wf_names": ["quinlan-lab/preon"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["quinlan-lab"], "nb_wf": 1, "list_wf": ["preon"], "list_contrib": ["brwnj"], "nb_contrib": 1, "codes": ["\nprocess smoove_call {\n    label 'smoove'\n    publishDir path: \"$outdir/sv/smoove/called\", pattern: \"*.vcf.gz*\"\n    publishDir path: \"$outdir/sv/logs\", pattern: \"*-stats.txt\"\n    publishDir path: \"$outdir/sv/logs\", pattern: \"*-smoove-call.log\"\n\n    input:\n    env SMOOVE_KEEP_ALL from params.sensitive\n    set sample, file(bam), file(bai) from call_bams\n    file fasta\n    file faidx\n    file bed\n\n    output:\n    file(\"${sample}-smoove.genotyped.vcf.gz\") into call_vcfs\n    file(\"${sample}-smoove.genotyped.vcf.gz.csi\") into call_idxs\n    file(\"${sample}-stats.txt\") into variant_counts\n    file(\"${sample}-smoove-call.log\") into sequence_counts\n\n    script:\n    excludechroms = params.exclude ? \"--excludechroms \\\"${params.exclude}\\\"\" : \"\"\n    filters = params.sensitive ? \"--noextrafilters\" : \"\"\n    \"\"\"\n    smoove call --genotype --name $sample --processes ${task.cpus} \\\n        --fasta $fasta --exclude $bed $excludechroms $filters \\\n        $bam 2> >(tee -a ${sample}-smoove-call.log >&2)\n    bcftools stats ${sample}-smoove.genotyped.vcf.gz > ${sample}-stats.txt\n    \"\"\"\n}"], "list_proc": ["quinlan-lab/preon/smoove_call"], "list_wf_names": ["quinlan-lab/preon"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["quinlan-lab"], "nb_wf": 1, "list_wf": ["preon"], "list_contrib": ["brwnj"], "nb_contrib": 1, "codes": ["\nprocess smoove_genotype {\n    label 'smoove'\n    publishDir path: \"$outdir/sv/smoove/genotyped\"\n\n    input:\n    env SMOOVE_KEEP_ALL from params.sensitive\n    set sample, file(bam), file(bai) from genotype_bams\n    file sites\n    file fasta\n    file faidx\n\n    output:\n    file(\"${sample}-smoove.genotyped.vcf.gz.csi\") into genotyped_idxs\n    file(\"${sample}-smoove.genotyped.vcf.gz\") into genotyped_vcfs\n\n    script:\n    \"\"\"\n    wget -q https://raw.githubusercontent.com/samtools/samtools/develop/misc/seq_cache_populate.pl\n    perl seq_cache_populate.pl -root \\$(pwd)/cache $fasta 1> /dev/null 2> err || (cat err; exit 2)\n    export REF_PATH=\\$(pwd)/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s\n    export REF_CACHE=xx\n\n    samtools quickcheck -v $bam\n    smoove genotype --duphold --processes ${task.cpus} --removepr --outdir ./ --name ${sample} --fasta $fasta --vcf $sites $bam\n    \"\"\"\n}"], "list_proc": ["quinlan-lab/preon/smoove_genotype"], "list_wf_names": ["quinlan-lab/preon"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["ralsallaq"], "nb_wf": 1, "list_wf": ["metaGx_nf"], "list_contrib": ["ralsallaq"], "nb_contrib": 1, "codes": [" process removeAdapters {\n        container \"${container_cutadapt}\"\n        label 'multithread'\n        publishDir \"${params.outD}/noAdpt/\", mode: 'copy'\n        \n        input:\n        set sname, file(intrlF) from vpair_ch2\n        file adaptersF_path\n    \n        output:\n        set sname, file(\"${sname}.noAdpt.fq.gz\") into noAdpt_ch\n    \n        \"\"\"\n        cutadapt --interleaved -j ${task.cpus}  -a file:$adaptersF_path/$adaptersF_name -A file:$adaptersF_path/$adaptersF_name -o ${sname}.noAdpt.fq.gz ${intrlF}\n        \"\"\"\n    }"], "list_proc": ["ralsallaq/metaGx_nf/removeAdapters"], "list_wf_names": ["ralsallaq/metaGx_nf"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["ralsallaq"], "nb_wf": 1, "list_wf": ["metaGx_nf"], "list_contrib": ["ralsallaq"], "nb_contrib": 1, "codes": [" process decontByMap1 {\n        container \"${container_bwa}\"\n        label 'multithread'\n                                                                    \n        \n        input:\n        set sname, file(noArtiF) from noArtiF_ch1\n        path idxbase_parent\n    \n        output:\n        set sname, file(\"${sname}.alignment.sam\") into alnToHuman_ch\n    \n                 \n                                                                    \n        \n        \"\"\"\n        #align noArtiFile to human genome\n        bwa mem -t ${task.cpus} -o ${sname}.alignment.sam -p ${idxbase_parent}/human_genome_bwa ${noArtiF} \n        \"\"\"\n    }"], "list_proc": ["ralsallaq/metaGx_nf/decontByMap1"], "list_wf_names": ["ralsallaq/metaGx_nf"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["ralsallaq"], "nb_wf": 1, "list_wf": ["metaGx_nf"], "list_contrib": ["ralsallaq"], "nb_contrib": 1, "codes": [" process qualityTrim {\n        container \"${container_cutadapt}\"\n        label 'multithread'\n        publishDir \"${params.outD}/qtrimmed/\", mode: 'copy'\n        \n        input:\n        set sname, file(decontF) from noHuman_ch1\n    \n        output:\n        set sname, file(\"${sname}.qtrimmed.fq.gz\") into qtrimmed_ch1, qtrimmed_ch2, qtrimmed_ch3, qtrimmed_ch4, qtrimmed_ch5, qtrimmed_ch6, qtrimmed_ch7, qtrimmed_ch8\n    \n        \"\"\"\n        cutadapt --interleaved -j ${task.cpus} -q 10 -m 50 -A XXX -o ${sname}.qtrimmed.fq.gz ${decontF}\n        \"\"\"\n    }"], "list_proc": ["ralsallaq/metaGx_nf/qualityTrim"], "list_wf_names": ["ralsallaq/metaGx_nf"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["ralsallaq"], "nb_wf": 1, "list_wf": ["metaGx_nf"], "list_contrib": ["ralsallaq"], "nb_contrib": 1, "codes": [" process alnReadsToMegaRes {\n        container \"${container_bwa}\"\n        label 'multithread'\n\n        input:\n        set sname, file(qtrimmedF) from qtrimmed_ch3\n        path megaResIdxbase_parent\n\n        output:\n        set sname, file(\"${sname}.megaResalgn.sam\") into alnToMegaRes_ch1, alnToMegaRes_ch2, alnToMegaRes_ch3\n        \n        \"\"\"\n        bwa mem -t ${task.cpus} -o ${sname}.megaResalgn.sam -p ${megaResIdxbase_parent}/megaRes_bwa ${qtrimmedF} \n        \"\"\"\n\n    }"], "list_proc": ["ralsallaq/metaGx_nf/alnReadsToMegaRes"], "list_wf_names": ["ralsallaq/metaGx_nf"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["ralsallaq"], "nb_wf": 1, "list_wf": ["metaGx_nf"], "list_contrib": ["ralsallaq"], "nb_contrib": 1, "codes": [" process getExtendedFAA {\n        container \"${container_seqtk}\"\n        label 'io_mem'\n        publishDir \"${params.outD}/gene_discover/${geneName}/\", mode: 'copy'\n        \n        input:\n        set geneName, file(blastLikeAlnF) from extDBAln_ch\n        set geneName, file(seedF) from refu_ch\n        file geneExDBFAA\n        \n        output:\n        set geneName, file(\"${geneName}.extended.faa\") into geneExtFAA_ch\n        \n        \"\"\"\n        # get target ids\n        cat \"${blastLikeAlnF}\" | awk '{print \\$2}' > target_ids\n        # remove duplicates\n        awk '!seen[\\$1]++'  target_ids >  target_ids_nodup\n        # use seqtk to subseq the database\n        seqtk subseq ${geneExDBFAA} target_ids_nodup > ${geneName}.extended.faa \n\t\t\n        \"\"\"\n    }"], "list_proc": ["ralsallaq/metaGx_nf/getExtendedFAA"], "list_wf_names": ["ralsallaq/metaGx_nf"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["raonyguimaraes"], "nb_wf": 1, "list_wf": ["fast-ngs-admix"], "list_contrib": ["PhilPalmer"], "nb_contrib": 1, "codes": ["\nprocess plink {\n  tag \"${name}\"\n  container 'alliecreason/plink:1.90'\n  publishDir \"${params.outdir}/plink\", mode: 'copy'\n\n  input:\n  set val(name), file(map), file(ped) from plinkGenotypes\n\n  output:\n  set val(name), file(\"${name}.map\"), file(\"${name}.ped\"), file(\"${name}.bed\"), file(\"${name}.bim\"), file(\"${name}.fam\") into plink, plink2\n\n  script:\n  \"\"\"\n  plink --file ${name} --list-duplicate-vars ids-only suppress-first\n  plink --file ${name} --recode -exclude plink.dupvar --out ${name}\n  plink --file ${name} --out ${name}\n  \"\"\"\n}"], "list_proc": ["raonyguimaraes/fast-ngs-admix/plink"], "list_wf_names": ["raonyguimaraes/fast-ngs-admix"]}, {"nb_reuse": 1, "tools": ["PncStress"], "nb_own": 1, "list_own": ["raonyguimaraes"], "nb_wf": 1, "list_wf": ["nextflow-hello"], "list_contrib": ["raonyguimaraes"], "nb_contrib": 1, "codes": ["\nprocess sayHello {\n  container \"progrium/stress\"\n  input: \n    val x from cheers\n\n  output:\n    file 'temp' into cheers2\n\n  script:\n    \"\"\"\n    echo '$x world!' > temp\n    stress -c 5 -t 10\n    \"\"\"\n\n\n\n}"], "list_proc": ["raonyguimaraes/nextflow-hello/sayHello"], "list_wf_names": ["raonyguimaraes/nextflow-hello"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess FASTQC {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/preprocess/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.endsWith(\".zip\") ? \"zips/$filename\" : filename\n                }\n\n    when:\n    !params.skip_fastqc\n\n    input:\n    tuple val(sample), val(single_end), path(reads) from ch_cat_fastqc\n\n    output:\n    path \"*.{zip,html}\" into ch_fastqc_raw_reports_mqc\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus *.fastq.gz\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/FASTQC"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess BOWTIE2 {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/bam\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".log\")) \"log/$filename\"\n                      else params.save_align_intermeds ? filename : null\n                }\n\n    when:\n    !params.skip_variants\n\n    input:\n    tuple val(sample), val(single_end), path(reads) from ch_fastp_bowtie2\n    path index from ch_index\n\n    output:\n    tuple val(sample), val(single_end), path(\"*.bam\") into ch_bowtie2_bam\n    path \"*.log\" into ch_bowtie2_mqc\n\n    script:\n    input_reads = single_end ? \"-U $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    filter = params.filter_unmapped ? \"-F4\" : \"\"\n    \"\"\"\n    bowtie2 \\\\\n        --threads $task.cpus \\\\\n        --local \\\\\n        --very-sensitive-local \\\\\n        -x ${index}/${index_base} \\\\\n        $input_reads \\\\\n        2> ${sample}.bowtie2.log \\\\\n        | samtools view -@ $task.cpus -b -h -O BAM -o ${sample}.bam $filter -\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/BOWTIE2"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": [" process PICARD_MARKDUPLICATES {\n        tag \"$sample\"\n        label 'process_medium'\n        publishDir \"${params.outdir}/variants/bam\", mode: params.publish_dir_mode,\n            saveAs: { filename ->\n                          if (filename.endsWith(\".flagstat\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".idxstats\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".stats\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".metrics.txt\")) \"picard_metrics/$filename\"\n                          else filename\n                    }\n\n        when:\n        !params.skip_variants\n\n        input:\n        tuple val(sample), val(single_end), path(bam) from ch_ivar_trim_bam\n        path fasta from ch_fasta\n\n        output:\n        tuple val(sample), val(single_end), path(\"*.sorted.{bam,bam.bai}\") into ch_markdup_bam_metrics,\n                                                                                ch_markdup_bam_mosdepth_genome,\n                                                                                ch_markdup_bam_mosdepth_amplicon,\n                                                                                ch_markdup_bam_mpileup,\n                                                                                ch_markdup_bam_varscan2_consensus,\n                                                                                ch_markdup_bam_bcftools,\n                                                                                ch_markdup_bam_bcftools_consensus,\n                                                                                ch_markdup_bam_freebayes,\n                                                                                ch_markdup_bam_freebayes_consensus\n        path \"*.{flagstat,idxstats,stats}\" into ch_markdup_bam_flagstat_mqc\n        path \"*.txt\" into ch_markdup_bam_metrics_mqc\n\n        script:\n        def avail_mem = 3\n        if (!task.memory) {\n            log.info \"[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.\"\n        } else {\n            avail_mem = task.memory.toGiga()\n        }\n        prefix = params.protocol == 'amplicon' ? \"${sample}.trim.mkD\" : \"${sample}.mkD\"\n        keep_dup = params.filter_dups ? \"true\" : \"false\"\n        \"\"\"\n        picard -Xmx${avail_mem}g MarkDuplicates \\\\\n            INPUT=${bam[0]} \\\\\n            OUTPUT=${prefix}.sorted.bam \\\\\n            ASSUME_SORTED=true \\\\\n            REMOVE_DUPLICATES=$keep_dup \\\\\n            METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt \\\\\n            VALIDATION_STRINGENCY=LENIENT \\\\\n            TMP_DIR=tmp\n        samtools index ${prefix}.sorted.bam\n        samtools idxstats ${prefix}.sorted.bam > ${prefix}.sorted.bam.idxstats\n        samtools flagstat ${prefix}.sorted.bam > ${prefix}.sorted.bam.flagstat\n        samtools stats ${prefix}.sorted.bam > ${prefix}.sorted.bam.stats\n        \"\"\"\n    }"], "list_proc": ["rastiks/viralrecon/PICARD_MARKDUPLICATES"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["mosdepth"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess MOSDEPTH_GENOME {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/bam/mosdepth/genome\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".pdf\")) \"plots/$filename\"\n                      else if (filename.endsWith(\".tsv\")) \"plots/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && !params.skip_mosdepth\n\n    input:\n    tuple val(sample), val(single_end), path(bam) from ch_markdup_bam_mosdepth_genome\n\n    output:\n    path \"*.global.dist.txt\" into ch_mosdepth_genome_mqc\n    path \"*.{txt,gz,csi,tsv,pdf}\"\n\n    script:\n    suffix = params.skip_markduplicates ? \"\" : \".mkD\"\n    prefix = params.protocol == 'amplicon' ? \"${sample}.trim${suffix}.genome\" : \"${sample}${suffix}.genome\"\n    plot_suffix = params.protocol == 'amplicon' ? \".trim${suffix}.genome\" : \"${suffix}.genome\"\n    \"\"\"\n    mosdepth \\\\\n        --by 200 \\\\\n        --fast-mode \\\\\n        $prefix \\\\\n        ${bam[0]}\n\n    plot_mosdepth_regions.r \\\\\n        --input_files ${prefix}.regions.bed.gz \\\\\n        --input_suffix ${plot_suffix}.regions.bed.gz \\\\\n        --output_dir ./ \\\\\n        --output_suffix ${plot_suffix}.regions\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/MOSDEPTH_GENOME"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["mosdepth"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": [" process MOSDEPTH_AMPLICON {\n        tag \"$sample\"\n        label 'process_medium'\n        publishDir \"${params.outdir}/variants/bam/mosdepth/amplicon\", mode: params.publish_dir_mode\n\n        when:\n        !params.skip_variants && !params.skip_mosdepth\n\n        input:\n        tuple val(sample), val(single_end), path(bam) from ch_markdup_bam_mosdepth_amplicon\n        path bed from ch_amplicon_bed\n\n        output:\n        path \"*.regions.bed.gz\" into ch_mosdepth_amplicon_region_bed\n        path \"*.{txt,gz,csi}\"\n\n        script:\n        suffix = params.skip_markduplicates ? \"\" : \".mkD\"\n        prefix = \"${sample}.trim${suffix}.amplicon\"\n        \"\"\"\n        collapse_amplicon_bed.py \\\\\n            --left_primer_suffix $params.amplicon_left_suffix \\\\\n            --right_primer_suffix $params.amplicon_right_suffix \\\\\n            $bed \\\\\n            amplicon.collapsed.bed\n\n        mosdepth \\\\\n            --by amplicon.collapsed.bed \\\\\n            --fast-mode \\\\\n            --use-median \\\\\n            --thresholds 0,1,10,50,100,500 \\\\\n            ${prefix} \\\\\n            ${bam[0]}\n        \"\"\"\n    }"], "list_proc": ["rastiks/viralrecon/MOSDEPTH_AMPLICON"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess SAMTOOLS_MPILEUP {\n    tag \"$sample\"\n    label 'process_medium'\n    if (params.save_mpileup) {\n        publishDir \"${params.outdir}/variants/bam/mpileup\", mode: params.publish_dir_mode\n    }\n\n    when:\n    !params.skip_variants\n\n    input:\n    tuple val(sample), val(single_end), path(bam) from ch_markdup_bam_mpileup\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"*.mpileup\") into ch_mpileup_varscan2,\n                                                               ch_mpileup_ivar_variants,\n                                                               ch_mpileup_ivar_consensus,\n                                                               ch_mpileup_ivar_bcftools\n\n    script:\n    suffix = params.skip_markduplicates ? \"\" : \".mkD\"\n    prefix = params.protocol == 'amplicon' ? \"${sample}.trim${suffix}\" : \"${sample}${suffix}\"\n    \"\"\"\n    samtools mpileup \\\\\n        --count-orphans \\\\\n        --ignore-overlaps \\\\\n        --no-BAQ \\\\\n        --max-depth $params.mpileup_depth \\\\\n        --fasta-ref $fasta \\\\\n        --min-BQ $params.min_base_qual \\\\\n        --output ${prefix}.mpileup \\\\\n        ${bam[0]}\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/SAMTOOLS_MPILEUP"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["BCFtools", "VarScan"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess VARSCAN2 {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/varscan2\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".log\")) \"log/$filename\"\n                      else if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'varscan2' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(mpileup) from ch_mpileup_varscan2\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"${prefix}.vcf.gz*\") into ch_varscan2_highfreq_consensus,\n                                                                       ch_varscan2_highfreq_snpeff,\n                                                                       ch_varscan2_highfreq_intersect\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_varscan2_lowfreq_snpeff\n    path \"${prefix}.bcftools_stats.txt\" into ch_varscan2_bcftools_highfreq_mqc\n    path \"*.varscan2.log\" into ch_varscan2_log_mqc\n    path \"${sample}.bcftools_stats.txt\"\n\n    script:\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    strand = params.protocol != 'amplicon' && params.varscan2_strand_filter ? \"--strand-filter 1\" : \"--strand-filter 0\"\n    \"\"\"\n    echo \"$sample\" > sample_name.list\n    varscan mpileup2cns \\\\\n        $mpileup \\\\\n        --min-coverage $params.min_coverage \\\\\n        --min-reads2 5 \\\\\n        --min-avg-qual $params.min_base_qual \\\\\n        --min-var-freq $params.min_allele_freq \\\\\n        --p-value 0.99 \\\\\n        --output-vcf 1 \\\\\n        --vcf-sample-list sample_name.list \\\\\n        --variants \\\\\n        $strand \\\\\n        2> ${sample}.varscan2.log \\\\\n        | bgzip -c > ${sample}.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n    sed -i.bak '/LC_ALL/d' ${sample}.varscan2.log\n\n    bcftools filter \\\\\n        -i 'FORMAT/AD / (FORMAT/AD + FORMAT/RD) >= $params.max_allele_freq' \\\\\n        --output-type z \\\\\n        --output ${prefix}.vcf.gz \\\\\n        ${sample}.vcf.gz\n    tabix -p vcf -f ${prefix}.vcf.gz\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/VARSCAN2"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["BCFtools", "BEDTools"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess VARSCAN2_CONSENSUS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/varscan2/consensus\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".tsv\")) \"base_qc/$filename\"\n                      else if (filename.endsWith(\".pdf\")) \"base_qc/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'varscan2' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(bam), path(vcf) from ch_markdup_bam_varscan2_consensus.join(ch_varscan2_highfreq_consensus, by: [0,1])\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"*consensus.masked.fa\") into ch_varscan2_consensus\n    path \"*.{consensus.fa,tsv,pdf}\"\n\n    script:\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    \"\"\"\n\n    bedtools genomecov \\\\\n        -bga \\\\\n        -ibam ${bam[0]} \\\\\n        -g $fasta \\\\\n        | awk '\\$4 < $params.min_coverage' > ${prefix}.lowcov.bed\n    parse_mask_bed.py ${vcf[0]} ${prefix}.lowcov.bed ${prefix}.lowcov.fix.bed\n    bedtools merge -i ${prefix}.lowcov.fix.bed > ${prefix}.mask.bed\n\t\n    bedtools maskfasta \\\\\n        -fi $fasta \\\\\n        -bed ${prefix}.mask.bed \\\\\n        -fo ${index_base}.ref.masked.fa\n    cat ${index_base}.ref.masked.fa | bcftools consensus ${vcf[0]} > ${prefix}.consensus.masked.fa\n    header=\\$(head -n 1 ${prefix}.consensus.masked.fa | sed 's/>//g')\n    sed -i \"s/\\${header}/${sample}/g\" ${prefix}.consensus.masked.fa\n\n    plot_base_density.r --fasta_files ${prefix}.consensus.masked.fa --prefixes $prefix --output_dir ./\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/VARSCAN2_CONSENSUS"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 2, "tools": ["BCFtools", "SAMtools", "SnpSift", "Minimap2", "Bandage", "VGE", "snpEff"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess METASPADES_VG {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/metaspades/variants\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                      else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_assembly && 'metaspades' in assemblers && !single_end && !params.skip_vg\n\n    input:\n    tuple val(sample), val(single_end), path(scaffolds) from ch_metaspades_vg\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_metaspades_vg_vcf\n    path \"*.bcftools_stats.txt\" into ch_metaspades_vg_bcftools_mqc\n    path \"*.{gfa,png,svg}\"\n\n    script:\n    \"\"\"\n    minimap2 -c -t $task.cpus -x asm20 $fasta $scaffolds > ${sample}.paf\n\n    cat $scaffolds $fasta > ${sample}.withRef.fasta\n    seqwish --paf-alns ${sample}.paf --seqs ${sample}.withRef.fasta --gfa ${sample}.gfa --threads $task.cpus\n\n    vg view -Fv ${sample}.gfa --threads $task.cpus > ${sample}.vg\n    vg convert -x ${sample}.vg > ${sample}.xg\n\n    samtools faidx $fasta\n    vg snarls ${sample}.xg > ${sample}.snarls\n    for chrom in `cat ${fasta}.fai | cut -f1`\n    do\n        vg deconstruct -p \\$chrom ${sample}.xg -r ${sample}.snarls --threads $task.cpus \\\\\n            | bcftools sort -O v -T ./ \\\\\n            | bgzip -c > ${sample}.\\$chrom.vcf.gz\n    done\n    bcftools concat --output-type z --output ${sample}.vcf.gz *.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n\n    if [ -s ${sample}.gfa ]\n    then\n        Bandage image ${sample}.gfa ${sample}.png --height 1000\n        Bandage image ${sample}.gfa ${sample}.svg --height 1000\n    fi\n    \"\"\"\n}", "\nprocess VARSCAN2_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/varscan2/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_variants && 'varscan2' in callers && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(highfreq_vcf), path(lowfreq_vcf) from ch_varscan2_highfreq_snpeff.join(ch_varscan2_lowfreq_snpeff, by: [0,1])\n    tuple file(db), file(config) from ch_snpeff_db_varscan2\n\n    output:\n    path \"${prefix}.snpEff.csv\" into ch_varscan2_snpeff_highfreq_mqc\n    path \"${sample}.snpEff.csv\"\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${lowfreq_vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${highfreq_vcf[0]} \\\\\n        -csvStats ${prefix}.snpEff.csv \\\\\n        | bgzip -c > ${prefix}.snpEff.vcf.gz\n    tabix -p vcf -f ${prefix}.snpEff.vcf.gz\n    mv snpEff_summary.html ${prefix}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${prefix}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${prefix}.snpSift.table.txt\n    \t\"\"\"\n}"], "list_proc": ["rastiks/viralrecon/METASPADES_VG", "rastiks/viralrecon/VARSCAN2_SNPEFF"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["BCFtools", "AIVAR"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess IVAR_VARIANTS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/ivar\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".bcftools_stats.txt\")) \"bcftools_stats/$filename\"\n                      else if (filename.endsWith(\".log\")) \"log/$filename\"\n                      else if (filename.endsWith(\"_mqc.tsv\")) null\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'ivar' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(mpileup) from ch_mpileup_ivar_variants\n    path header from ch_ivar_variants_header_mqc\n    path fasta from ch_fasta\n    path gff from ch_gff\n\n    output:\n    tuple val(sample), val(single_end), path(\"${prefix}.vcf.gz*\") into ch_ivar_highfreq_snpeff,\n                                                                       ch_ivar_highfreq_intersect\n    path \"${prefix}.bcftools_stats.txt\" into ch_ivar_bcftools_highfreq_mqc\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_ivar_lowfreq_snpeff\n    path \"${sample}.variant.counts_mqc.tsv\" into ch_ivar_count_mqc\n    path \"${sample}.bcftools_stats.txt\"\n    path \"${sample}.tsv\"\n    path \"*.log\"\n\n    script:\n    features = params.gff ? \"-g $gff\" : \"\"\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    \"\"\"\n    cat $mpileup | ivar variants -q $params.min_base_qual -t $params.min_allele_freq -m $params.min_coverage -r $fasta $features -p $sample\n\n    ivar_variants_to_vcf.py ${sample}.tsv ${sample}.vcf > ${sample}.variant.counts.log\n    bgzip -c ${sample}.vcf > ${sample}.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n    cat $header ${sample}.variant.counts.log > ${sample}.variant.counts_mqc.tsv\n\n    ivar_variants_to_vcf.py ${sample}.tsv ${prefix}.vcf --pass_only --allele_freq_thresh $params.max_allele_freq > ${prefix}.variant.counts.log\n    bgzip -c ${prefix}.vcf > ${prefix}.vcf.gz\n    tabix -p vcf -f ${prefix}.vcf.gz\n    bcftools stats ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/IVAR_VARIANTS"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["AIVAR"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess IVAR_CONSENSUS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/ivar/consensus\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".tsv\")) \"base_qc/$filename\"\n                      else if (filename.endsWith(\".pdf\")) \"base_qc/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'ivar' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(mpileup) from ch_mpileup_ivar_consensus\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"*.fa\") into ch_ivar_consensus\n    path \"*.{txt,tsv,pdf}\"\n\n    script:\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    \"\"\"\n    cat $mpileup | ivar consensus -q $params.min_base_qual -t $params.max_allele_freq -m $params.min_coverage -n N -p ${prefix}.consensus\n    header=\\$(head -n1 ${prefix}.consensus.fa | sed 's/>//g')\n    sed -i \"s/\\${header}/${sample}/g\" ${prefix}.consensus.fa\n\n    plot_base_density.r --fasta_files ${prefix}.consensus.fa --prefixes $prefix --output_dir ./\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/IVAR_CONSENSUS"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["SnpSift", "snpEff"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess IVAR_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/ivar/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_variants && 'ivar' in callers && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(highfreq_vcf), path(lowfreq_vcf) from ch_ivar_highfreq_snpeff.join(ch_ivar_lowfreq_snpeff, by: [0,1])\n    tuple file(db), file(config) from ch_snpeff_db_ivar\n\n    output:\n    path \"${prefix}.snpEff.csv\" into ch_ivar_snpeff_highfreq_mqc\n    path \"${sample}.snpEff.csv\"\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${lowfreq_vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${highfreq_vcf[0]} \\\\\n        -csvStats ${prefix}.snpEff.csv \\\\\n        | bgzip -c > ${prefix}.snpEff.vcf.gz\n    tabix -p vcf -f ${prefix}.snpEff.vcf.gz\n    mv snpEff_summary.html ${prefix}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${prefix}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${prefix}.snpSift.table.txt\n   \t\"\"\"\n}"], "list_proc": ["rastiks/viralrecon/IVAR_SNPEFF"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess BCFTOOLS_VARIANTS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/bcftools\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'bcftools' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(bam) from ch_markdup_bam_bcftools\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"*.vcf.gz*\") into ch_bcftools_variants_consensus,\n                                                               ch_bcftools_variants_snpeff,\n                                                               ch_bcftools_variants_intersect\n    path \"*.bcftools_stats.txt\" into ch_bcftools_variants_mqc\n\n    script:\n    \"\"\"\n    echo \"$sample\" > sample_name.list\n    bcftools mpileup \\\\\n        --count-orphans \\\\\n        --no-BAQ \\\\\n        --max-depth $params.mpileup_depth \\\\\n        --fasta-ref $fasta \\\\\n        --min-BQ $params.min_base_qual \\\\\n        --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR \\\\\n        ${bam[0]} \\\\\n        | bcftools call --output-type v --ploidy 1 --keep-alts --keep-masked-ref --multiallelic-caller --variants-only \\\\\n        | bcftools reheader --samples sample_name.list \\\\\n        | bcftools view --output-file ${sample}.vcf.gz --output-type z --include 'INFO/DP>=$params.min_coverage'\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/BCFTOOLS_VARIANTS"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 2, "tools": ["Cutadapt", "Bowtie", "AIVAR", "VGE", "Picard", "FastQC", "snpEff", "mosdepth", "kraken2", "FreeBayes", "G-BLASTN", "MultiQC", "BEDTools", "BCFtools", "Minia", "SAMtools", "Minimap2", "Bandage", "Unicycler", "fastPHASE"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess BCFTOOLS_CONSENSUS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/bcftools/consensus\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".tsv\")) \"base_qc/$filename\"\n                      else if (filename.endsWith(\".pdf\")) \"base_qc/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'bcftools' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(bam), path(vcf) from ch_markdup_bam_bcftools_consensus.join(ch_bcftools_variants_consensus, by: [0,1])\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"*consensus.masked.fa\") into ch_bcftools_consensus_masked\n    path \"*.{consensus.fa,tsv,pdf}\"\n\n    script:\n    \"\"\"\n\n    bedtools genomecov \\\\\n        -bga \\\\\n        -ibam ${bam[0]} \\\\\n        -g $fasta \\\\\n        | awk '\\$4 < $params.min_coverage' > ${sample}.lowcov.bed\n\n    parse_mask_bed.py ${vcf[0]} ${sample}.lowcov.bed ${sample}.lowcov.fix.bed\n\n    bedtools merge -i ${sample}.lowcov.fix.bed > ${sample}.mask.bed\n\n    bedtools maskfasta \\\\\n        -fi $fasta \\\\\n        -bed ${sample}.mask.bed \\\\\n        -fo ${index_base}.ref.masked.fa\n    cat ${index_base}.ref.masked.fa | bcftools consensus ${vcf[0]} > ${sample}.consensus.masked.fa\n    sed -i 's/${index_base}/${sample}/g' ${sample}.consensus.masked.fa\n    header=\\$(head -n1 ${sample}.consensus.masked.fa | sed 's/>//g')\n    sed -i \"s/\\${header}/${sample}/g\" ${sample}.consensus.masked.fa\n\n    plot_base_density.r --fasta_files ${sample}.consensus.masked.fa --prefixes $sample --output_dir ./\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".csv\")) filename\n                      else null\n                }\n\n    output:\n    path \"software_versions_mqc.yaml\" into ch_software_versions_yaml\n    path \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    parallel-fastq-dump --version > v_parallel_fastq_dump.txt\n    fastqc --version > v_fastqc.txt\n    fastp --version 2> v_fastp.txt\n    bowtie2 --version > v_bowtie2.txt\n    samtools --version > v_samtools.txt\n    bedtools --version > v_bedtools.txt\n    mosdepth --version > v_mosdepth.txt\n    picard CollectMultipleMetrics --version &> v_picard.txt || true\n    ivar -v > v_ivar.txt\n    echo \\$(varscan 2>&1) > v_varscan.txt\n    bcftools -v > v_bcftools.txt\n    freebayes --version > v_freebayes.txt\n    snpEff -version > v_snpeff.txt\n    echo \\$(SnpSift 2>&1) > v_snpsift.txt\n    quast.py --version > v_quast.txt\n    cutadapt --version > v_cutadapt.txt\n    kraken2 --version > v_kraken2.txt\n    spades.py --version > v_spades.txt\n    unicycler --version > v_unicycler.txt\n    minia --version > v_minia.txt\n    blastn -version > v_blast.txt\n    abacas.pl -v &> v_abacas.txt || true\n    plasmidID -v > v_plasmidid.txt  || true\n    Bandage --version > v_bandage.txt\n    minimap2 --version > v_minimap2.txt\n    vg version > v_vg.txt\n    echo \\$(R --version 2>&1) > v_R.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/BCFTOOLS_CONSENSUS", "rastiks/viralrecon/get_software_versions"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 6, "tools": ["Salmon", "SAMtools", "SnpSift", "BEDTools", "STAR", "snpEff"], "nb_own": 2, "list_own": ["ray1919", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "lRNA-Seq"], "list_contrib": ["jcurado-flomics", "ray1919", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.5.1--h84f40af_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.5.1--h84f40af_0\"\n    }\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $options.args \\\\\n        -i salmon\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bai\"), optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\"), optional:true, emit: csi\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools index $options.args $bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_high_memory'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"*.version.txt\"                       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $options.args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $options.args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    bedtools --version | sed -e \"s/bedtools v//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BCFTOOLS_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/bcftools/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_variants && 'bcftools' in callers && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(vcf) from ch_bcftools_variants_snpeff\n    tuple file(db), file(config) from ch_snpeff_db_bcftools\n\n    output:\n    path \"*.snpEff.csv\" into ch_bcftools_snpeff_mqc\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n    \t\"\"\"\n}", "\nprocess STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n                     \"https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0\"\n                     'https://depot.galaxyproject.org/singularity/star:2.7.9a--h9ee0642_0'\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:a7908dfb0485a80ca94e4d17b0ac991532e4e989-0\"\n    } else {\n                                                                                                                                          \n        container \"quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:a7908dfb0485a80ca94e4d17b0ac991532e4e989-0\"\n                                                                    \n    }\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"         , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def memory   = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    def args     = options.args.tokenize()\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n                                                                        \n                                                                                                                                                                                                            \n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            $options.args\n\n        STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/SALMON_INDEX", "ray1919/lRNA-Seq/SAMTOOLS_INDEX", "ray1919/lRNA-Seq/BEDTOOLS_GENOMECOV", "rastiks/viralrecon/BCFTOOLS_SNPEFF", "ray1919/lRNA-Seq/STAR_GENOMEGENERATE", "ray1919/lRNA-Seq/SAMTOOLS_FLAGSTAT"], "list_wf_names": ["rastiks/viralrecon", "ray1919/lRNA-Seq"]}, {"nb_reuse": 1, "tools": ["BCFtools", "FreeBayes"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess FREEBAYES_VARIANTS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/freebayes\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'freebayes' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(bam) from ch_markdup_bam_freebayes\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"${prefix}.vcf.gz*\") into ch_freebayes_variants_consensus,\n                                                                       ch_freebayes_variants_snpeff,\n                                                                       ch_freebayes_variants_intersect\n    path \"*.bcftools_stats.txt\" into ch_freebayes_variants_mqc\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_freebayes_lowfreq_snpeff\n\n    script:\n    prefix = \"${sample}.AF${params.max_allele_freq}\"\n    \"\"\"\n    #freebayes-parallel <(fasta_generate_regions.py ${fasta}.fai 100000) $task.cpus \\\\\n    freebayes \\\\\n        --min-mapping-quality $params.min_base_qual \\\\\n        --min-alternate-count $params.min_coverage \\\\\n        --min-alternate-fraction $params.min_allele_freq \\\\\n        -f $fasta \\\\\n        ${bam[0]} \\\\\n        | vcfallelicprimitives -k -g \\\\\n        | bgzip -c > ${sample}.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats --af-tag AF ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n\n    bcftools filter \\\\\n        -i 'INFO/AO /(INFO/AO + INFO/RO) >= $params.max_allele_freq' \\\\\n        --output-type z \\\\\n        --output ${prefix}.vcf.gz \\\\\n        ${sample}.vcf.gz\n    tabix -p vcf -f ${prefix}.vcf.gz\n    bcftools stats --af-tag AF ${prefix}.vcf.gz > ${prefix}.bcftools_stats.txt\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/FREEBAYES_VARIANTS"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["BCFtools", "BEDTools"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess FREEBAYES_CONSENSUS {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/freebayes/consensus\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".tsv\")) \"base_qc/$filename\"\n                      else if (filename.endsWith(\".pdf\")) \"base_qc/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_variants && 'freebayes' in callers\n\n    input:\n    tuple val(sample), val(single_end), path(bam), path(vcf) from ch_markdup_bam_freebayes_consensus.join(ch_freebayes_variants_consensus, by: [0,1])\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"*consensus.masked.fa\") into ch_freebayes_consensus_masked\n    path \"*.{consensus.fa,tsv,pdf}\"\n\n    script:\n    \"\"\"\n    bedtools genomecov \\\\\n        -bga \\\\\n        -ibam ${bam[0]} \\\\\n        -g $fasta \\\\\n        | awk '\\$4 < $params.min_coverage' > ${sample}.lowcov.bed\n\n    parse_mask_bed.py ${vcf[0]} ${sample}.lowcov.bed ${sample}.lowcov.fix.bed\n\n    bedtools merge -i ${sample}.lowcov.fix.bed > ${sample}.mask.bed\n\n    bedtools maskfasta \\\\\n        -fi $fasta \\\\\n        -bed ${sample}.mask.bed \\\\\n        -fo ${index_base}.ref.masked.fa\n    cat ${index_base}.ref.masked.fa | bcftools consensus ${vcf[0]} > ${sample}.consensus.masked.fa\n    sed -i 's/${index_base}/${sample}/g' ${sample}.consensus.masked.fa\n    header=\\$(head -n1 ${sample}.consensus.masked.fa | sed 's/>//g')\n    sed -i \"s/\\${header}/${sample}/g\" ${sample}.consensus.masked.fa\n\n    plot_base_density.r --fasta_files ${sample}.consensus.masked.fa --prefixes $sample --output_dir ./\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/FREEBAYES_CONSENSUS"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 5, "tools": ["SortMeRna", "SnpSift", "fastPHASE", "preseq", "snpEff", "gffread"], "nb_own": 2, "list_own": ["ray1919", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "lRNA-Seq"], "list_contrib": ["jcurado-flomics", "ray1919", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_16'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0\"\n    } else {\n        container \"quay.io/biocontainers/gffread:0.12.1--h8b12597_0\"\n    }\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    gffread $gff $options.args -o ${gff.baseName}.gtf\n    echo \\$(gffread --version 2>&1) > ${software}.version.txt\n    \"\"\"\n}", "\nprocess FREEBAYES_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/variants/freebayes/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_variants && 'freebayes' in callers && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(vcf) from ch_freebayes_variants_snpeff\n    tuple file(db), file(config) from ch_snpeff_db_freebayes\n\n    output:\n    path \"*.snpEff.csv\" into ch_freebayes_snpeff_mqc\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n      \"\"\"\n}", "\nprocess SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'data/sortmerna', meta:meta, publish_by_meta:['id']) },\n        pattern: \"*.{log,txt}\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'output/cleandata', meta:meta, publish_by_meta:['id']) },\n        pattern: \"*.fastq.gz\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/sortmerna:4.2.0--0\"\n    } else {\n        container \"quay.io/biocontainers/sortmerna:4.2.0--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def Refs = \"\"\n    for (i=0; i<fasta.size(); i++) { Refs+= \" --ref ${fasta[i]}\" }\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $options.args\n\n        # gzip -f < non_rRNA_reads.fq > ${prefix}.fastq.gz\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            -m 30720 \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $options.args\n\n        # gzip -f < non_rRNA_reads_fwd.fq > ${prefix}_1.fastq.gz\n        # gzip -f < non_rRNA_reads_rev.fq > ${prefix}_2.fastq.gz\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1\"\n    } else {\n        container \"quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"*.version.txt\"                , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/FASTP", "ray1919/lRNA-Seq/GFFREAD", "rastiks/viralrecon/FREEBAYES_SNPEFF", "ray1919/lRNA-Seq/SORTMERNA", "ray1919/lRNA-Seq/PRESEQ_LCEXTRAP"], "list_wf_names": ["rastiks/viralrecon", "ray1919/lRNA-Seq"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": [" process BCFTOOLS_ISEC {\n        tag \"$sample\"\n        label 'process_medium'\n        label 'error_ignore'\n        publishDir \"${params.outdir}/variants/intersect\", mode: params.publish_dir_mode\n\n        input:\n        tuple val(sample), val(single_end), path('varscan2/*'), path('ivar/*'), path('bcftools/*'), path('freebayes/*') from ch_varscan2_highfreq_intersect\n\n        output:\n        path \"$sample\"\n\n        script:\n        \"\"\"\n        bcftools isec  \\\\\n            --nfiles +2 \\\\\n            --output-type z -c indels \\\\\n            -p $sample \\\\\n            */*.vcf.gz\n        \"\"\"\n    }"], "list_proc": ["rastiks/viralrecon/BCFTOOLS_ISEC"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["Cutadapt", "FastQC"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": [" process CUTADAPT {\n        tag \"$sample\"\n        label 'process_medium'\n        publishDir \"${params.outdir}/assembly/cutadapt\", mode: params.publish_dir_mode,\n            saveAs: { filename ->\n                          if (filename.endsWith(\".html\")) \"fastqc/$filename\"\n                          else if (filename.endsWith(\".zip\")) \"fastqc/zips/$filename\"\n                          else if (filename.endsWith(\".log\")) \"log/$filename\"\n                          else params.save_trimmed ? filename : null\n                    }\n\n        input:\n        tuple val(sample), val(single_end), path(reads) from ch_fastp_cutadapt\n        path amplicons from ch_amplicon_fasta\n\n        output:\n        tuple val(sample), val(single_end), path(\"*.ptrim.fastq.gz\") into ch_cutadapt_kraken2\n        path \"*.{zip,html}\" into ch_cutadapt_fastqc_mqc\n        path \"*.log\" into ch_cutadapt_mqc\n\n        script:\n        adapters = single_end ? \"-a file:primers.fasta\" : \"-a file:primers.fasta -A file:primers.fasta\"\n        out_reads = single_end ? \"-o ${sample}.ptrim.fastq.gz\" : \"-o ${sample}_1.ptrim.fastq.gz -p ${sample}_2.ptrim.fastq.gz\"\n        \"\"\"\n        sed -r '/^[ACTGactg]+\\$/ s/\\$/X/g' $amplicons > primers.fasta\n\n        cutadapt \\\\\n            --cores $task.cpus \\\\\n            --overlap 5 \\\\\n            --minimum-length 30 \\\\\n            --error-rate 0.1 \\\\\n            $adapters \\\\\n            $out_reads \\\\\n            $reads \\\\\n            > ${sample}.cutadapt.log\n\n        fastqc --quiet --threads $task.cpus *.ptrim.fastq.gz\n        \"\"\"\n    }"], "list_proc": ["rastiks/viralrecon/CUTADAPT"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": [" process KRAKEN2 {\n        tag \"$db\"\n        label 'process_high'\n        publishDir \"${params.outdir}/assembly/kraken2\", mode: params.publish_dir_mode,\n            saveAs: { filename ->\n                          if (filename.endsWith(\".txt\")) filename\n                          else params.save_kraken2_fastq ? filename : null\n                    }\n\n        input:\n        tuple val(sample), val(single_end), path(reads) from ch_fastp_kraken2\n        path db from ch_kraken2_db\n\n        output:\n        tuple val(sample), val(single_end), path(\"*.viral*\") into ch_kraken2_spades,\n                                                                  ch_kraken2_metaspades,\n                                                                  ch_kraken2_unicycler,\n                                                                  ch_kraken2_minia\n        path \"*.report.txt\" into ch_kraken2_report_mqc\n        path \"*.host*\"\n\n\n        script:\n        pe = single_end ? \"\" : \"--paired\"\n        classified = single_end ? \"${sample}.host.fastq\" : \"${sample}.host#.fastq\"\n        unclassified = single_end ? \"${sample}.viral.fastq\" : \"${sample}.viral#.fastq\"\n        \"\"\"\n        kraken2 \\\\\n            --db $db \\\\\n            --threads $task.cpus \\\\\n            --unclassified-out $unclassified \\\\\n            --classified-out $classified \\\\\n            --report ${sample}.kraken2.report.txt \\\\\n            --report-zero-counts \\\\\n            $pe \\\\\n            --gzip-compressed \\\\\n            $reads\n        pigz -p $task.cpus *.fastq\n        \"\"\"\n    }"], "list_proc": ["rastiks/viralrecon/KRAKEN2"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Salmon", "Bandage"], "nb_own": 2, "list_own": ["ray1919", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "lRNA-Seq"], "list_contrib": ["jcurado-flomics", "ray1919", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.5.1--h84f40af_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.5.1--h84f40af_0\"\n    }\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $options.args \\\\\n        -i salmon\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SPADES {\n    tag \"$sample\"\n    label 'process_high'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/spades\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                      else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_assembly && 'spades' in assemblers\n\n    input:\n    tuple val(sample), val(single_end), path(reads) from ch_kraken2_spades\n\n    output:\n    tuple val(sample), val(single_end), path(\"*scaffolds.fa\") into ch_spades_blast,\n                                                                   ch_spades_abacas,\n                                                                   ch_spades_plasmidid,\n                                                                   ch_spades_quast,\n                                                                   ch_spades_vg\n    path \"*assembly.{gfa,png,svg}\"\n\n\n    script:\n    input_reads = single_end ? \"-s $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    spades.py \\\\\n        --threads $task.cpus \\\\\n        $input_reads \\\\\n        -o ./\n    mv scaffolds.fasta ${sample}.scaffolds.fa\n    mv assembly_graph_with_scaffolds.gfa ${sample}.assembly.gfa\n\n    if [ -s ${sample}.assembly.gfa ]\n    then\n        Bandage image ${sample}.assembly.gfa ${sample}.assembly.png --height 1000\n        Bandage image ${sample}.assembly.gfa ${sample}.assembly.svg --height 1000\n    fi\n    \"\"\"\n}", "\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/SALMON_INDEX", "rastiks/viralrecon/SPADES", "ray1919/lRNA-Seq/SAMTOOLS_FLAGSTAT"], "list_wf_names": ["rastiks/viralrecon", "ray1919/lRNA-Seq"]}, {"nb_reuse": 4, "tools": ["SAMtools", "QualiMap", "RNASEQR", "G-BLASTN"], "nb_own": 3, "list_own": ["raygozag", "ray1919", "rastiks"], "nb_wf": 3, "list_wf": ["viralrecon", "lRNA-Seq", "rnaseq"], "list_contrib": ["jcurado-flomics", "ray1919", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "raygozag", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 16, "codes": ["\nprocess GET_CHROM_SIZES {\n    tag \"$fasta\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'genome', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = 'samtools'\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SPADES_BLAST {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/spades/blast\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'spades' in assemblers && !params.skip_blast\n\n    input:\n    tuple val(sample), val(single_end), path(scaffold) from ch_spades_blast\n    path db from ch_blast_db\n    path header from ch_blast_outfmt6_header\n\n    output:\n    path \"*.blast*\"\n\n    script:\n    \"\"\"\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db $db/$fasta_base \\\\\n        -query $scaffold \\\\\n        -outfmt \\'6 stitle std slen qlen qcovs\\' \\\\\n        -out ${sample}.blast.txt\n\n    awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"}{print \\$0,\\$5/\\$15,\\$5/\\$14}' ${sample}.blast.txt | awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"} \\$15 > 200 && \\$17 > 0.7 && \\$1 !~ /phage/ {print \\$0}' > ${sample}.blast.filt.txt\n    cat $header ${sample}.blast.filt.txt > ${sample}.blast.filt.header.txt\n    \"\"\"\n}", "\nprocess QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n    scratch false\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1\"\n    } else {\n        container \"quay.io/biocontainers/qualimap:2.2.2d--1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $options.args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/GET_CHROM_SIZES", "rastiks/viralrecon/SPADES_BLAST", "ray1919/lRNA-Seq/QUALIMAP_RNASEQ", "raygozag/rnaseq/SAMTOOLS_SORT"], "list_wf_names": ["rastiks/viralrecon", "ray1919/lRNA-Seq", "raygozag/rnaseq"]}, {"nb_reuse": 4, "tools": ["Salmon", "BCFtools", "SAMtools", "Minimap2", "Bandage", "VGE", "Mgenome", "STAR"], "nb_own": 3, "list_own": ["raygozag", "ray1919", "rastiks"], "nb_wf": 3, "list_wf": ["viralrecon", "lRNA-Seq", "rnaseq"], "list_contrib": ["jcurado-flomics", "ray1919", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "raygozag", "MiguelJulia", "rastiks", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 16, "codes": ["\nprocess SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.5.1--h84f40af_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.5.1--h84f40af_0\"\n    }\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $options.args \\\\\n        -i salmon\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}", "process SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $args \\\\\n        -i salmon\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0\"\n    }\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"                , emit: index\n    path \"rsem/*transcripts.fa\", emit: transcript_fasta\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def args     = options.args.tokenize()\n    if (args.contains('--star')) {\n        args.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $options.args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess SPADES_VG {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/spades/variants\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                      else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_assembly && 'spades' in assemblers && !params.skip_vg\n\n    input:\n    tuple val(sample), val(single_end), path(scaffolds) from ch_spades_vg\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_spades_vg_vcf\n    path \"*.bcftools_stats.txt\" into ch_spades_vg_bcftools_mqc\n    path \"*.{gfa,png,svg}\"\n\n    script:\n    \"\"\"\n    minimap2 -c -t $task.cpus -x asm20 $fasta $scaffolds > ${sample}.paf\n\n    cat $scaffolds $fasta > ${sample}.withRef.fasta\n    seqwish --paf-alns ${sample}.paf --seqs ${sample}.withRef.fasta --gfa ${sample}.gfa --threads $task.cpus\n\n    vg view -Fv ${sample}.gfa --threads $task.cpus > ${sample}.vg\n    vg convert -x ${sample}.vg > ${sample}.xg\n\n    samtools faidx $fasta\n    vg snarls ${sample}.xg > ${sample}.snarls\n    for chrom in `cat ${fasta}.fai | cut -f1`\n    do\n        vg deconstruct -p \\$chrom ${sample}.xg -r ${sample}.snarls --threads $task.cpus \\\\\n            | bcftools sort -O v -T ./ \\\\\n            | bgzip -c > ${sample}.\\$chrom.vcf.gz\n    done\n    bcftools concat --output-type z --output ${sample}.vcf.gz *.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n\n    if [ -s ${sample}.gfa ]\n    then\n        Bandage image ${sample}.gfa ${sample}.png --height 1000\n        Bandage image ${sample}.gfa ${sample}.svg --height 1000\n    fi\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/SALMON_INDEX", "raygozag/rnaseq/SALMON_INDEX", "ray1919/lRNA-Seq/RSEM_PREPAREREFERENCE", "rastiks/viralrecon/SPADES_VG"], "list_wf_names": ["rastiks/viralrecon", "ray1919/lRNA-Seq", "raygozag/rnaseq"]}, {"nb_reuse": 4, "tools": ["SAMtools", "SnpSift", "snpEff"], "nb_own": 3, "list_own": ["raygozag", "ray1919", "rastiks"], "nb_wf": 3, "list_wf": ["viralrecon", "lRNA-Seq", "rnaseq"], "list_contrib": ["jcurado-flomics", "ray1919", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "raygozag", "MiguelJulia", "rastiks", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 16, "codes": ["\nprocess GET_CHROM_SIZES {\n    tag \"$fasta\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'genome', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = 'samtools'\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SPADES_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/spades/variants/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'spades' in assemblers && !params.skip_vg && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(vcf) from ch_spades_vg_vcf\n    tuple file(db), file(config) from ch_snpeff_db_spades\n\n    output:\n    path \"*.snpEff.csv\" into ch_spades_snpeff_mqc\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n    \t\"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/GET_CHROM_SIZES", "raygozag/rnaseq/SAMTOOLS_STATS", "rastiks/viralrecon/SPADES_SNPEFF", "raygozag/rnaseq/SAMTOOLS_SORT"], "list_wf_names": ["rastiks/viralrecon", "ray1919/lRNA-Seq", "raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["Bandage"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess METASPADES {\n    tag \"$sample\"\n    label 'process_high'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/metaspades\", mode: params.publish_dir_mode,\n    saveAs: { filename ->\n                  if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                  else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                  else filename\n            }\n\n    when:\n    !params.skip_assembly && 'metaspades' in assemblers && !single_end\n\n    input:\n    tuple val(sample), val(single_end), path(reads) from ch_kraken2_metaspades\n\n    output:\n    tuple val(sample), val(single_end), path(\"*scaffolds.fa\") into ch_metaspades_blast,\n                                                                   ch_metaspades_abacas,\n                                                                   ch_metaspades_plasmidid,\n                                                                   ch_metaspades_quast,\n                                                                   ch_metaspades_vg\n    path \"*assembly.{gfa,png,svg}\"\n\n\n    script:\n    \"\"\"\n    spades.py \\\\\n        --meta \\\\\n        --threads $task.cpus \\\\\n        -1 ${reads[0]} \\\\\n        -2 ${reads[1]} \\\\\n        -o ./\n    mv scaffolds.fasta ${sample}.scaffolds.fa\n    mv assembly_graph_with_scaffolds.gfa ${sample}.assembly.gfa\n\n    if [ -s ${sample}.assembly.gfa ]\n    then\n        Bandage image ${sample}.assembly.gfa ${sample}.assembly.png --height 1000\n        Bandage image ${sample}.assembly.gfa ${sample}.assembly.svg --height 1000\n    fi\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/METASPADES"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 3, "tools": ["SAMtools", "G-BLASTN"], "nb_own": 2, "list_own": ["raygozag", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "rnaseq"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "raygozag", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess METASPADES_BLAST {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/metaspades/blast\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'metaspades' in assemblers && !single_end && !params.skip_blast\n\n    input:\n    tuple val(sample), val(single_end), path(scaffold) from ch_metaspades_blast\n    path db from ch_blast_db\n    path header from ch_blast_outfmt6_header\n\n    output:\n    path \"*.blast*\"\n\n    script:\n    \"\"\"\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db $db/$fasta_base \\\\\n        -query $scaffold \\\\\n        -outfmt \\'6 stitle std slen qlen qcovs\\' \\\\\n        -out ${sample}.blast.txt\n\n    awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"}{print \\$0,\\$5/\\$15,\\$5/\\$14}' ${sample}.blast.txt | awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"} \\$15 > 200 && \\$17 > 0.7 && \\$1 !~ /phage/ {print \\$0}' > ${sample}.blast.filt.txt\n    cat $header ${sample}.blast.filt.txt > ${sample}.blast.filt.header.txt\n    \"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/METASPADES_BLAST", "raygozag/rnaseq/SAMTOOLS_STATS", "raygozag/rnaseq/SAMTOOLS_SORT"], "list_wf_names": ["rastiks/viralrecon", "raygozag/rnaseq"]}, {"nb_reuse": 3, "tools": ["SAMtools", "SnpSift", "Salmon", "snpEff"], "nb_own": 2, "list_own": ["raygozag", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "rnaseq"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "raygozag", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess METASPADES_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/metaspades/variants/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'metaspades' in assemblers && !single_end && !params.skip_vg && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(vcf) from ch_metaspades_vg_vcf\n    tuple file(db), file(config) from ch_snpeff_db_metaspades\n\n    output:\n    path \"*.snpEff.csv\" into ch_metaspades_snpeff_mqc\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n    \t\"\"\"\n}", "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/METASPADES_SNPEFF", "raygozag/rnaseq/SAMTOOLS_STATS", "raygozag/rnaseq/SALMON_QUANT"], "list_wf_names": ["rastiks/viralrecon", "raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["Unicycler", "Bandage"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess UNICYCLER {\n    tag \"$sample\"\n    label 'process_high'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/unicycler\", mode: params.publish_dir_mode,\n    saveAs: { filename ->\n                  if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                  else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                  else filename\n            }\n\n    when:\n    !params.skip_assembly && 'unicycler' in assemblers\n\n    input:\n    tuple val(sample), val(single_end), path(reads) from ch_kraken2_unicycler\n\n    output:\n    tuple val(sample), val(single_end), path(\"*scaffolds.fa\") into ch_unicycler_blast,\n                                                                   ch_unicycler_abacas,\n                                                                   ch_unicycler_plasmidid,\n                                                                   ch_unicycler_quast,\n                                                                   ch_unicycler_vg\n    path \"*assembly.{gfa,png,svg}\"\n\n    script:\n    input_reads = single_end ? \"-s $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    \"\"\"\n    unicycler \\\\\n        --threads $task.cpus \\\\\n        $input_reads \\\\\n        --out ./\n    mv assembly.fasta ${sample}.scaffolds.fa\n    mv assembly.gfa ${sample}.assembly.gfa\n\n    if [ -s ${sample}.assembly.gfa ]\n    then\n        Bandage image ${sample}.assembly.gfa ${sample}.assembly.png --height 1000\n        Bandage image ${sample}.assembly.gfa ${sample}.assembly.svg --height 1000\n    fi\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/UNICYCLER"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 2, "tools": ["Salmon", "G-BLASTN"], "nb_own": 2, "list_own": ["raygozag", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "rnaseq"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "raygozag", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["\nprocess UNICYCLER_BLAST {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/unicycler/blast\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'unicycler' in assemblers && !params.skip_blast\n\n    input:\n    tuple val(sample), val(single_end), path(scaffold) from ch_unicycler_blast\n    path db from ch_blast_db\n    path header from ch_blast_outfmt6_header\n\n    output:\n    path \"*.blast*\"\n\n    script:\n    \"\"\"\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db $db/$fasta_base \\\\\n        -query $scaffold \\\\\n        -outfmt \\'6 stitle std slen qlen qcovs\\' \\\\\n        -out ${sample}.blast.txt\n\n    awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"}{print \\$0,\\$5/\\$15,\\$5/\\$14}' ${sample}.blast.txt | awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"} \\$15 > 200 && \\$17 > 0.7 && \\$1 !~ /phage/ {print \\$0}' > ${sample}.blast.filt.txt\n    cat $header ${sample}.blast.filt.txt > ${sample}.blast.filt.header.txt\n    \"\"\"\n}", "process SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n\n    conda (params.enable_conda ? 'bioconda::salmon=1.5.2' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/salmon:1.5.2--h84f40af_0' :\n        'quay.io/biocontainers/salmon:1.5.2--h84f40af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        salmon: \\$(echo \\$(salmon --version) | sed -e \"s/salmon //g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/UNICYCLER_BLAST", "raygozag/rnaseq/SALMON_QUANT"], "list_wf_names": ["rastiks/viralrecon", "raygozag/rnaseq"]}, {"nb_reuse": 2, "tools": ["BCFtools", "SAMtools", "Minimap2", "Bandage", "VGE", "Mgenome", "STAR"], "nb_own": 2, "list_own": ["raygozag", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "rnaseq"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "raygozag", "MiguelJulia", "rastiks", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["process RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' :\n        'quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0' }\"\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"           , emit: index\n    path \"*transcripts.fa\", emit: transcript_fasta\n    path \"versions.yml\"   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def args_list = args.tokenize()\n    if (args_list.contains('--star')) {\n        args_list.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args_list.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        cp rsem/genome.transcripts.fa .\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            rsem: \\$(rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\")\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess UNICYCLER_VG {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/unicycler/variants\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                      else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_assembly && 'unicycler' in assemblers && !params.skip_vg\n\n    input:\n    tuple val(sample), val(single_end), path(scaffolds) from ch_unicycler_vg\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_unicycler_vg_vcf\n    path \"*.bcftools_stats.txt\" into ch_unicycler_vg_bcftools_mqc\n    path \"*.{gfa,png,svg}\"\n\n    script:\n    \"\"\"\n    minimap2 -c -t $task.cpus -x asm20 $fasta $scaffolds > ${sample}.paf\n\n    cat $scaffolds $fasta > ${sample}.withRef.fasta\n    seqwish --paf-alns ${sample}.paf --seqs ${sample}.withRef.fasta --gfa ${sample}.gfa --threads $task.cpus\n\n    vg view -Fv ${sample}.gfa --threads $task.cpus > ${sample}.vg\n    vg convert -x ${sample}.vg > ${sample}.xg\n\n    samtools faidx $fasta\n    vg snarls ${sample}.xg > ${sample}.snarls\n    for chrom in `cat ${fasta}.fai | cut -f1`\n    do\n        vg deconstruct -p \\$chrom ${sample}.xg -r ${sample}.snarls --threads $task.cpus \\\\\n            | bcftools sort -O v -T ./ \\\\\n            | bgzip -c > ${sample}.\\$chrom.vcf.gz\n    done\n    bcftools concat --output-type z --output ${sample}.vcf.gz *.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n\n    if [ -s ${sample}.gfa ]\n    then\n        Bandage image ${sample}.gfa ${sample}.png --height 1000\n        Bandage image ${sample}.gfa ${sample}.svg --height 1000\n    fi\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/RSEM_PREPAREREFERENCE", "rastiks/viralrecon/UNICYCLER_VG"], "list_wf_names": ["rastiks/viralrecon", "raygozag/rnaseq"]}, {"nb_reuse": 2, "tools": ["FastQC", "SnpSift", "snpEff"], "nb_own": 2, "list_own": ["raygozag", "rastiks"], "nb_wf": 2, "list_wf": ["viralrecon", "rnaseq"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "raygozag", "MiguelJulia", "rastiks", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 15, "codes": ["process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}", "\nprocess UNICYCLER_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/unicycler/variants/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'unicycler' in assemblers && !params.skip_vg && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(vcf) from ch_unicycler_vg_vcf\n    tuple file(db), file(config) from ch_snpeff_db_unicycler\n\n    output:\n    path \"*.snpEff.csv\" into ch_unicycler_snpeff_mqc\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n    \t\"\"\"\n}"], "list_proc": ["raygozag/rnaseq/FASTQC", "rastiks/viralrecon/UNICYCLER_SNPEFF"], "list_wf_names": ["rastiks/viralrecon", "raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["Minia"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess MINIA {\n    tag \"$sample\"\n    label 'process_high'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/minia/${params.minia_kmer}\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'minia' in assemblers\n\n    input:\n    tuple val(sample), val(single_end), path(reads) from ch_kraken2_minia\n\n    output:\n    tuple val(sample), val(single_end), path(\"*scaffolds.fa\") into ch_minia_vg,\n                                                                   ch_minia_blast,\n                                                                   ch_minia_abacas,\n                                                                   ch_minia_plasmidid,\n                                                                   ch_minia_quast\n\n    script:\n    \"\"\"\n    echo \"${reads.join(\"\\n\")}\" > input_files.txt\n    minia \\\\\n        -kmer-size $params.minia_kmer \\\\\n        -abundance-min 20 \\\\\n        -nb-cores $task.cpus \\\\\n        -in input_files.txt \\\\\n        -out ${sample}.k${params.minia_kmer}.a20\n    mv ${sample}.k${params.minia_kmer}.a20.contigs.fa ${sample}.k${params.minia_kmer}.scaffolds.fa\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/MINIA"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["G-BLASTN"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess MINIA_BLAST {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/minia/${params.minia_kmer}/blast\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'minia' in assemblers && !params.skip_blast\n\n    input:\n    tuple val(sample), val(single_end), path(scaffold) from ch_minia_blast\n    path db from ch_blast_db\n    path header from ch_blast_outfmt6_header\n\n    output:\n    path \"*.blast*\"\n\n    script:\n    \"\"\"\n    blastn \\\\\n        -num_threads $task.cpus \\\\\n        -db $db/$fasta_base \\\\\n        -query $scaffold \\\\\n        -outfmt \\'6 stitle std slen qlen qcovs\\' \\\\\n        -out ${sample}.blast.txt\n\n    awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"}{print \\$0,\\$5/\\$15,\\$5/\\$14}' ${sample}.blast.txt | awk 'BEGIN{OFS=\\\"\\\\t\\\";FS=\\\"\\\\t\\\"} \\$15 > 200 && \\$17 > 0.7 && \\$1 !~ /phage/ {print \\$0}' > ${sample}.blast.filt.txt\n    cat $header ${sample}.blast.filt.txt > ${sample}.blast.filt.header.txt\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/MINIA_BLAST"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["BCFtools", "SAMtools", "Minimap2", "Bandage", "VGE"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess MINIA_VG {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/minia/${params.minia_kmer}/variants\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\".txt\")) \"bcftools_stats/$filename\"\n                      else if (filename.endsWith(\".png\")) \"bandage/$filename\"\n                      else if (filename.endsWith(\".svg\")) \"bandage/$filename\"\n                      else filename\n                }\n\n    when:\n    !params.skip_assembly && 'minia' in assemblers && !params.skip_vg\n\n    input:\n    tuple val(sample), val(single_end), path(scaffolds) from ch_minia_vg\n    path fasta from ch_fasta\n\n    output:\n    tuple val(sample), val(single_end), path(\"${sample}.vcf.gz*\") into ch_minia_vg_vcf\n    path \"*.bcftools_stats.txt\" into ch_minia_vg_bcftools_mqc\n    path \"*.{gfa,png,svg}\"\n\n    script:\n    \"\"\"\n    minimap2 -c -t $task.cpus -x asm20 $fasta $scaffolds > ${sample}.paf\n\n    cat $scaffolds $fasta > ${sample}.withRef.fasta\n    seqwish --paf-alns ${sample}.paf --seqs ${sample}.withRef.fasta --gfa ${sample}.gfa --threads $task.cpus\n\n    vg view -Fv ${sample}.gfa --threads $task.cpus > ${sample}.vg\n    vg convert -x ${sample}.vg > ${sample}.xg\n\n    samtools faidx $fasta\n    vg snarls ${sample}.xg > ${sample}.snarls\n    for chrom in `cat ${fasta}.fai | cut -f1`\n    do\n        vg deconstruct -p \\$chrom ${sample}.xg -r ${sample}.snarls --threads $task.cpus \\\\\n            | bcftools sort -O v -T ./ \\\\\n            | bgzip -c > ${sample}.\\$chrom.vcf.gz\n    done\n    bcftools concat --output-type z --output ${sample}.vcf.gz *.vcf.gz\n    tabix -p vcf -f ${sample}.vcf.gz\n    bcftools stats ${sample}.vcf.gz > ${sample}.bcftools_stats.txt\n\n    if [ -s ${sample}.gfa ]\n    then\n        Bandage image ${sample}.gfa ${sample}.png --height 1000\n        Bandage image ${sample}.gfa ${sample}.svg --height 1000\n    fi\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/MINIA_VG"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["SnpSift", "snpEff"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess MINIA_SNPEFF {\n    tag \"$sample\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}/assembly/minia/${params.minia_kmer}/variants/snpeff\", mode: params.publish_dir_mode\n\n    when:\n    !params.skip_assembly && 'minia' in assemblers && !params.skip_vg && params.gff && !params.skip_snpeff\n\n    input:\n    tuple val(sample), val(single_end), path(vcf) from ch_minia_vg_vcf\n    tuple file(db), file(config) from ch_snpeff_db_minia\n\n    output:\n    path \"*.snpEff.csv\" into ch_minia_snpeff_mqc\n    path \"*.vcf.gz*\"\n    path \"*.{txt,html}\"\n\n    script:\n    \"\"\"\n    snpEff ${index_base} \\\\\n        -config $config \\\\\n        -dataDir $db \\\\\n        ${vcf[0]} \\\\\n        -csvStats ${sample}.snpEff.csv \\\\\n        | bgzip -c > ${sample}.snpEff.vcf.gz\n    tabix -p vcf -f ${sample}.snpEff.vcf.gz\n    mv snpEff_summary.html ${sample}.snpEff.summary.html\n\n    SnpSift extractFields -s \",\" \\\\\n        -e \".\" \\\\\n        ${sample}.snpEff.vcf.gz \\\\\n        CHROM POS REF ALT \\\\\n        \"ANN[*].GENE\" \"ANN[*].GENEID\" \\\\\n        \"ANN[*].IMPACT\" \"ANN[*].EFFECT\" \\\\\n        \"ANN[*].FEATURE\" \"ANN[*].FEATUREID\" \\\\\n        \"ANN[*].BIOTYPE\" \"ANN[*].RANK\" \"ANN[*].HGVS_C\" \\\\\n        \"ANN[*].HGVS_P\" \"ANN[*].CDNA_POS\" \"ANN[*].CDNA_LEN\" \\\\\n        \"ANN[*].CDS_POS\" \"ANN[*].CDS_LEN\" \"ANN[*].AA_POS\" \\\\\n        \"ANN[*].AA_LEN\" \"ANN[*].DISTANCE\" \"EFF[*].EFFECT\" \\\\\n        \"EFF[*].FUNCLASS\" \"EFF[*].CODON\" \"EFF[*].AA\" \"EFF[*].AA_LEN\" \\\\\n        > ${sample}.snpSift.table.txt\n    \t\"\"\"\n}"], "list_proc": ["rastiks/viralrecon/MINIA_SNPEFF"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["rastiks"], "nb_wf": 1, "list_wf": ["viralrecon"], "list_contrib": ["jcurado-flomics", "JoseEspinosa", "morpholino", "ggabernet", "ewels", "maxulysse", "svarona", "rastiks", "MiguelJulia", "heuermh", "ktrns", "stevekm", "drpatelh", "saramonzon"], "nb_contrib": 14, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.endsWith(\"assembly_metrics_mqc.tsv\")) \"assembly/$filename\"\n                      else if (filename.endsWith(\"variants_metrics_mqc.tsv\")) \"variants/$filename\"\n                      else \"multiqc/$filename\"\n                }\n\n    when:\n    !params.skip_multiqc\n\n    input:\n    path (multiqc_config) from ch_multiqc_config\n    path (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    path ('fastqc/*') from ch_fastqc_raw_reports_mqc.collect().ifEmpty([])\n    path ('fastp/log/*') from ch_fastp_mqc.collect().ifEmpty([])\n    path ('fastp/fastqc/*') from ch_fastp_fastqc_mqc.collect().ifEmpty([])\n    path ('bowtie2/log/*') from ch_bowtie2_mqc.collect().ifEmpty([])\n    path ('bowtie2/flagstat/*') from ch_sort_bam_flagstat_mqc.collect().ifEmpty([])\n    path ('ivar/trim/flagstat/*') from ch_ivar_trim_flagstat_mqc.collect().ifEmpty([])\n    path ('ivar/trim/log/*') from ch_ivar_trim_log_mqc.collect().ifEmpty([])\n    path ('picard/markdup/*') from ch_markdup_bam_flagstat_mqc.collect().ifEmpty([])\n    path ('picard/metrics/*') from ch_markdup_bam_metrics_mqc.collect().ifEmpty([])\n    path ('picard/metrics/*') from ch_picard_metrics_mqc.collect().ifEmpty([])\n    path ('mosdepth/genome/*') from ch_mosdepth_genome_mqc.collect().ifEmpty([])\n    path ('varscan2/counts/lowfreq/*') from ch_varscan2_log_mqc.collect().ifEmpty([])\n    path ('varscan2/bcftools/highfreq/*') from ch_varscan2_bcftools_highfreq_mqc.collect().ifEmpty([])\n    path ('varscan2/snpeff/highfreq/*') from ch_varscan2_snpeff_highfreq_mqc.collect().ifEmpty([])\n    path ('varscan2/quast/highfreq/*') from ch_varscan2_quast_mqc.collect().ifEmpty([])\n    path ('ivar/variants/counts/lowfreq/*') from ch_ivar_count_mqc.collect().ifEmpty([])\n    path ('ivar/variants/bcftools/highfreq/*') from ch_ivar_bcftools_highfreq_mqc.collect().ifEmpty([])\n    path ('ivar/variants/snpeff/highfreq/*') from ch_ivar_snpeff_highfreq_mqc.collect().ifEmpty([])\n    path ('ivar/consensus/quast/highfreq/*') from ch_ivar_quast_mqc.collect().ifEmpty([])\n    path ('bcftools/variants/bcftools/*') from ch_bcftools_variants_mqc.collect().ifEmpty([])\n    path ('bcftools/variants/snpeff/*') from ch_bcftools_snpeff_mqc.collect().ifEmpty([])\n    path ('bcftools/consensus/quast/*') from ch_bcftools_quast_mqc.collect().ifEmpty([])\n    path ('freebayes/variants/bcftools/*') from ch_freebayes_variants_mqc.collect().ifEmpty([])\n    path ('freebayes/variants/snpeff/*') from ch_freebayes_snpeff_mqc.collect().ifEmpty([])\n    path ('freebayes/consensus/quast/*') from ch_freebayes_quast_mqc.collect().ifEmpty([])\n    path ('cutadapt/log/*') from ch_cutadapt_mqc.collect().ifEmpty([])\n    path ('cutadapt/fastqc/*') from ch_cutadapt_fastqc_mqc.collect().ifEmpty([])\n    path ('kraken2amp/*') from ch_kraken2amp_report_mqc.collect().ifEmpty([])\n\tpath ('kraken2/*') from ch_kraken2_report_mqc.collect().ifEmpty([])\n    path ('spades/bcftools/*') from ch_spades_vg_bcftools_mqc.collect().ifEmpty([])\n    path ('spades/snpeff/*') from ch_spades_snpeff_mqc.collect().ifEmpty([])\n    path ('spades/quast/*') from ch_quast_spades_mqc.collect().ifEmpty([])\n    path ('metaspades/bcftools/*') from ch_metaspades_vg_bcftools_mqc.collect().ifEmpty([])\n    path ('metaspades/snpeff/*') from ch_metaspades_snpeff_mqc.collect().ifEmpty([])\n    path ('metaspades/quast/*') from ch_quast_metaspades_mqc.collect().ifEmpty([])\n    path ('unicycler/bcftools/*') from ch_unicycler_vg_bcftools_mqc.collect().ifEmpty([])\n    path ('unicycler/snpeff/*') from ch_unicycler_snpeff_mqc.collect().ifEmpty([])\n    path ('unicycler/quast/*') from ch_quast_unicycler_mqc.collect().ifEmpty([])\n    path ('minia/bcftools/*') from ch_minia_vg_bcftools_mqc.collect().ifEmpty([])\n    path ('minia/snpeff/*') from ch_minia_snpeff_mqc.collect().ifEmpty([])\n    path ('minia/quast/*') from ch_quast_minia_mqc.collect().ifEmpty([])\n    path ('software_versions/*') from ch_software_versions_yaml.collect()\n    path workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    path \"*multiqc_report.html\" into ch_multiqc_report\n    path \"*_data\"\n    path \"*.tsv\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc . -f $rtitle $rfilename $custom_config_file\n    multiqc_to_custom_tsv.py\n    multiqc . -f $rtitle $rfilename $custom_config_file\n    \"\"\"\n}"], "list_proc": ["rastiks/viralrecon/MULTIQC"], "list_wf_names": ["rastiks/viralrecon"]}, {"nb_reuse": 1, "tools": ["StringTie", "GATK"], "nb_own": 2, "list_own": ["ray1919", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "lRNA-Seq"], "list_contrib": ["ray1919", "vincenthhu"], "nb_contrib": 2, "codes": ["process GATK4_FILTERMUTECTCALLS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(stats), path(orientationbias), path(segmentation), path(contaminationfile), val(contaminationest)\n    path fasta\n    path fai\n    path dict\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\")            , emit: vcf\n    tuple val(meta), path(\"*.vcf.gz.tbi\")        , emit: tbi\n    tuple val(meta), path(\"*.filteringStats.tsv\"), emit: stats\n    path \"versions.yml\"                          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def orientationbias_options = ''\n    if (orientationbias) {\n        orientationbias_options = '--orientation-bias-artifact-priors ' + orientationbias.join(' --orientation-bias-artifact-priors ')\n    }\n\n    def segmentation_options = ''\n    if (segmentation) {\n        segmentation_options = '--tumor-segmentation ' + segmentation.join(' --tumor-segmentation ')\n    }\n\n    def contamination_options = contaminationest ? \" --contamination-estimate ${contaminationest} \" : ''\n    if (contaminationfile) {\n        contamination_options = '--contamination-table ' + contaminationfile.join(' --contamination-table ')\n    }\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK FilterMutectCalls] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" FilterMutectCalls \\\\\n        -R $fasta \\\\\n        -V $vcf \\\\\n        $orientationbias_options \\\\\n        $segmentation_options \\\\\n        $contamination_options \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess STRINGTIE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::stringtie=2.1.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/stringtie:2.1.4--h7e0af3c_0\"\n    } else {\n        container \"quay.io/biocontainers/stringtie:2.1.4--h7e0af3c_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"*.coverage.gtf\")   , emit: coverage_gtf\n    tuple val(meta), path(\"*.transcripts.gtf\"), emit: transcript_gtf\n    tuple val(meta), path(\"*.abundance.txt\")  , emit: abundance\n    tuple val(meta), path(\"*.ballgown\")       , emit: ballgown\n    path  \"*.version.txt\"                     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = '--fr'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = '--rf'\n    }\n    \"\"\"\n    stringtie \\\\\n        $bam \\\\\n        $strandedness \\\\\n        -G $gtf \\\\\n        -o ${prefix}.transcripts.gtf \\\\\n        -A ${prefix}.gene.abundance.txt \\\\\n        -C ${prefix}.coverage.gtf \\\\\n        -b ${prefix}.ballgown \\\\\n        $options.args\n\n    stringtie --version > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/STRINGTIE"], "list_wf_names": ["ray1919/lRNA-Seq"]}, {"nb_reuse": 1, "tools": ["Picard", "MarkDuplicates (IP)", "GATK"], "nb_own": 2, "list_own": ["ray1919", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "lRNA-Seq"], "list_contrib": ["ray1919", "vincenthhu"], "nb_contrib": 2, "codes": ["process GATK4_CALCULATECONTAMINATION {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(pileup), path(matched)\n    val segmentout\n\n    output:\n    tuple val(meta), path('*.contamination.table'), emit: contamination\n    tuple val(meta), path('*.segmentation.table') , emit: segmentation, optional:true\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def matched_command = matched ? \" -matched ${matched} \" : ''\n    def segment_command = segmentout ? \" -segments ${prefix}.segmentation.table\" : ''\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CalculateContamination] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CalculateContamination \\\\\n        -I $pileup \\\\\n        $matched_command \\\\\n        -O ${prefix}.contamination.table \\\\\n        $segment_command \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::picard=2.23.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/picard:2.23.9--0\"\n    } else {\n        container \"quay.io/biocontainers/picard:2.23.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"*.version.txt\"                 , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $options.args \\\\\n        INPUT=$bam \\\\\n        OUTPUT=${prefix}.bam \\\\\n        METRICS_FILE=${prefix}.MarkDuplicates.metrics.txt\n\n    echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d: > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/PICARD_MARKDUPLICATES"], "list_wf_names": ["ray1919/lRNA-Seq"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["ray1919"], "nb_wf": 1, "list_wf": ["lRNA-Seq"], "list_contrib": ["ray1919"], "nb_contrib": 1, "codes": ["\nprocess SALMON_QUANT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'data/' + getSoftwareName(task.process) + '/alignment', meta:meta, publish_by_meta:['id']) },\n        enabled: \"$alignment_mode\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'data/' + getSoftwareName(task.process) + '/mapping', meta:meta, publish_by_meta:['id']) },\n        enabled: \"$alignment_mode\"\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.5.1--h84f40af_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.5.1--h84f40af_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n    path  transcript_fasta\n    val   alignment_mode\n    val   lib_type\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    prefix          = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def reference   = \"--index $index\"\n    def input_reads = meta.single_end ? \"-r $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    def gencode     = ''\n    if (alignment_mode) {\n        reference   = \"-t $transcript_fasta\"\n        input_reads = \"-a $reads\"\n        gencode     = params.gencode ? '--gencode' : ''\n    }\n\n    def strandedness_opts = [\n        'A', 'U', 'SF', 'SR',\n        'IS', 'IU' , 'ISF', 'ISR',\n        'OS', 'OU' , 'OSF', 'OSR',\n        'MS', 'MU' , 'MSF', 'MSR'\n    ]\n    def strandedness =  'A'\n    if (lib_type) {\n        if (strandedness_opts.contains(lib_type)) {\n            strandedness = lib_type\n        } else {\n            log.info \"[Salmon Quant] Invalid library type specified '--libType=${lib_type}', defaulting to auto-detection with '--libType=A'.\"\n        }\n    } else {\n        strandedness = meta.single_end ? 'U' : 'IU'\n        if (meta.strandedness == 'forward') {\n            strandedness = meta.single_end ? 'SF' : 'ISF'\n        } else if (meta.strandedness == 'reverse') {\n            strandedness = meta.single_end ? 'SR' : 'ISR'\n        }\n    }\n    \"\"\"\n    salmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $gencode \\\\\n        $options.args \\\\\n        -o $prefix\n\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/SALMON_QUANT"], "list_wf_names": ["ray1919/lRNA-Seq"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FastQC", "bedGraphToBigWig"], "nb_own": 3, "list_own": ["ray1919", "viktorlj", "tamara-hodgetts"], "nb_wf": 3, "list_wf": ["SarGet", "nf-atac-seq", "lRNA-Seq"], "list_contrib": ["ray1919", "viktorlj", "tamara-hodgetts"], "nb_contrib": 3, "codes": ["\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    } else {\n        container \"quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    }\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"*.version.txt\"             , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig $bedgraph $sizes ${prefix}.bigWig\n    echo $VERSION > ${software}.version.txt\n    \"\"\"\n}", "\nprocess RunFastQC {\n  tag {idPatient}\n\n  publishDir \"${directoryMap.fastQC}/${idSample}\", mode: 'link'\n\n  input:\n    set idPatient, idSample, file(fastqFile1), file(fastqFile2) from fastqFilesforFastQC\n\n  output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  script:\n  \"\"\"\n  fastqc -q ${fastqFile1} ${fastqFile2}\n  \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/UCSC_BEDGRAPHTOBIGWIG", "viktorlj/SarGet/RunFastQC", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_IDXSTATS"], "list_wf_names": ["viktorlj/SarGet", "tamara-hodgetts/nf-atac-seq", "ray1919/lRNA-Seq"]}, {"nb_reuse": 9, "tools": ["BWA", "SortMeRna", "SAMtools", "bedGraphToBigWig", "BEDTools", "RNASEQR", "QualiMap"], "nb_own": 4, "list_own": ["ray1919", "vibbits", "remiolsen", "tamara-hodgetts"], "nb_wf": 4, "list_wf": ["nf-atac-seq", "rnaseq-editing", "hicscaff", "lRNA-Seq"], "list_contrib": ["ray1919", "alex-botzki", "remiolsen", "abotzki", "tamara-hodgetts"], "nb_contrib": 5, "codes": ["\nprocess SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools flagstat $bam > ${bam}.flagstat\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    } else {\n        container \"quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    }\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"*.version.txt\"             , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig $bedgraph $sizes ${prefix}.bigWig\n    echo $VERSION > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"*.version.txt\"                       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $options.args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $options.args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    bedtools --version | sed -e \"s/bedtools v//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', publish_id:'') }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7\"\n    } else {\n        container \"quay.io/biocontainers/bwa:0.7.17--hed695b0_7\"\n    }\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"          , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bwa\n    bwa index $options.args $fasta -p bwa/${fasta.baseName}\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.12\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:66ed1b38d280722529bb8a0167b0cf02f8a0b488-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $options.args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools view $options.args2 -@ $task.cpus -bhS -o ${prefix}.bam -\n\n    echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'data/sortmerna', meta:meta, publish_by_meta:['id']) },\n        pattern: \"*.{log,txt}\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'output/cleandata', meta:meta, publish_by_meta:['id']) },\n        pattern: \"*.fastq.gz\"\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/sortmerna:4.2.0--0\"\n    } else {\n        container \"quay.io/biocontainers/sortmerna:4.2.0--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def Refs = \"\"\n    for (i=0; i<fasta.size(); i++) { Refs+= \" --ref ${fasta[i]}\" }\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $options.args\n\n        # gzip -f < non_rRNA_reads.fq > ${prefix}.fastq.gz\n        mv non_rRNA_reads.fq.gz ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            -m 30720 \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $options.args\n\n        # gzip -f < non_rRNA_reads_fwd.fq > ${prefix}_1.fastq.gz\n        # gzip -f < non_rRNA_reads_rev.fq > ${prefix}_2.fastq.gz\n        mv non_rRNA_reads_fwd.fq.gz ${prefix}_1.fastq.gz\n        mv non_rRNA_reads_rev.fq.gz ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n    scratch false\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1\"\n    } else {\n        container \"quay.io/biocontainers/qualimap:2.2.2d--1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $options.args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["tamara-hodgetts/nf-atac-seq/SAMTOOLS_FLAGSTAT", "tamara-hodgetts/nf-atac-seq/UCSC_BEDGRAPHTOBIGWIG", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_VIEW", "vibbits/rnaseq-editing/BEDTOOLS_GENOMECOV", "remiolsen/hicscaff/BWA_INDEX", "ray1919/lRNA-Seq/SAMTOOLS_IDXSTATS", "remiolsen/hicscaff/BWA_MEM", "ray1919/lRNA-Seq/SORTMERNA", "ray1919/lRNA-Seq/QUALIMAP_RNASEQ"], "list_wf_names": ["tamara-hodgetts/nf-atac-seq", "vibbits/rnaseq-editing", "ray1919/lRNA-Seq", "remiolsen/hicscaff"]}, {"nb_reuse": 7, "tools": ["gffread", "BCFtools", "Match", "MultiQC", "Bismark", "bedGraphToBigWig", "FastQC", "Trimmomatic"], "nb_own": 7, "list_own": ["ray1919", "remiolsen", "vibbits", "sguizard", "sebastianlzy", "wtsi-hgi", "tamara-hodgetts"], "nb_wf": 6, "list_wf": ["nextflow-demo", "eqtl", "nf-core-radseq", "lRNA-Seq", "rnaseq-editing", "nf-atac-seq", "nf-feelnc"], "list_contrib": ["ray1919", "alex-botzki", "Sanger-ad7", "remiolsen", "abotzki", "sguizard", "tamara-hodgetts"], "nb_contrib": 7, "codes": ["\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    } else {\n        container \"quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    }\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"*.version.txt\"             , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig $bedgraph $sizes ${prefix}.bigWig\n    echo $VERSION > ${software}.version.txt\n    \"\"\"\n}", "\nprocess bismark_align_lambda {\n\n    tag \"$sample_id\"\n    echo true\n\n    publishDir \"${params.outdir}/wgbs/$sample_id/bam_files_lambda\"\n\n    input:\n    tuple val(sample_id), val(sub_ID), file(trimmed_reads_r1),file(trimmed_reads_r2) from ch_trimmed_reads_for_align_lambda\n    file LAMBDA_PATH\n\n    output:\n\n    tuple val(sample_id),file(\"*_bismark_bt2_PE_report.txt\")\n\n    script:\n    \"\"\"\n    ## --path_to_bowtie2 /usr/local/bin\n    bismark --bowtie2 -p 2 --bam --score_min L,0,-0.2 lambda -1 ${trimmed_reads_r1} -2 ${trimmed_reads_r2}\n\n    ## remove the bam files\n    rm ${sample_id}*_bismark_bt2_pe.bam\n\n\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0\"\n    } else {\n        container \"quay.io/biocontainers/gffread:0.12.1--h8b12597_0\"\n    }\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    gffread $gff $options.args -o ${gff.baseName}.gtf\n    echo \\$(gffread --version 2>&1) > ${software}.version.txt\n    \"\"\"\n}", "\nprocess trimmomatic {\n    tag \"$name\"\n    publishDir \"${params.outdir}/trimmed_reads\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from read_files_trim\n\n    output:\n    set val(name), file(\"trim_*.fastq.gz\") into trimP_files\n    file \"*_trim.out\" into trim_logs\n\n    script:\n    head_string = \"\"\n    trunc_string = \"MINLEN:30 CROP:30\"\n    if(params.trim_truncate > 30){\n      trunc_string = \"MINLEN:${params.trim_truncate} CROP:${params.trim_truncate}\"\n    }\n    if(params.trim_head > 0){\n      head_string = \"HEADCROP:${params.trim_head}\"\n    }\n    \"\"\"\n    trimmomatic PE \\\n    -threads ${task.cpus} \\\n    -trimlog ${name}_trim.log \\\n    -phred33 \\\n    ${reads} trim_${reads[0]} U_${reads[0]} trim_${reads[1]} U_${reads[1]} \\\n    ${head_string} ILLUMINACLIP:${params.trim_adapters}:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 \\\n    ${trunc_string} 2> ${name}_trim.out\n    \"\"\"\n}", "\nprocess SUBSET_GENOTYPE {\n    tag \"${samplename}.${sample_subset_file}\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/subset_genotype/\", mode: \"${params.copy_mode}\", pattern: \"${samplename}.${sample_subset_file}.subset.vcf.gz\"\n    \n    \n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"/software/hgi/containers/eqtl.img\"\n    } else {\n        log.info 'change the docker container - this is not the right one'\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n\n    input:\n        path(donor_vcf)\n        val(file__reduced_dims)\n\n\n    output:\n    \n        path(\"${samplename}.subset.vcf.gz\"), emit: samplename_subsetvcf\n\n    script:\n        file__reduced_dims = file__reduced_dims.join(\",\")\n        samplename='subset'\n        \"\"\"\n            tabix -p vcf ${donor_vcf} || echo 'not typical VCF'\n            bcftools view ${donor_vcf} -s ${file__reduced_dims} -Oz -o ${samplename}.subset.vcf.gz\n            rm ${donor_vcf}.tbi || echo 'not typical VCF'\n        \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "process GZIP_decompress {\n\n  input:\n    path(compressed)\n\n  output:\n    path(decompressed)\n\n  shell:\n    match = (compressed.getName() =~ /^(.+?)((\\.tar)?(\\.gz)?)$/)\n    decompressed = match[0][1]\n    compression = match[0][2]\n    if (compression.startsWith('.tar'))\n      '''\n      tar -xf !{compressed} -C '!{decompressed}'\n      '''\n    else if (compression.startsWith('.gz'))\n      '''\n      gzip -c -d !{compressed} > '!{decompressed}'\n      '''\n    else\n      '''\n      '''\n}"], "list_proc": ["tamara-hodgetts/nf-atac-seq/UCSC_BEDGRAPHTOBIGWIG", "sebastianlzy/nextflow-demo/bismark_align_lambda", "ray1919/lRNA-Seq/FASTQC", "vibbits/rnaseq-editing/GFFREAD", "wtsi-hgi/eqtl/SUBSET_GENOTYPE", "wtsi-hgi/eqtl/MULTIQC", "sguizard/nf-feelnc/GZIP_decompress"], "list_wf_names": ["wtsi-hgi/eqtl", "sebastianlzy/nextflow-demo", "sguizard/nf-feelnc", "vibbits/rnaseq-editing", "ray1919/lRNA-Seq", "tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 8, "tools": ["ABRicate", "MarkDuplicates (IP)", "BCFtools", "PLINK", "SAMtools", "MultiQC", "FastQC", "G-BLASTN", "Picard", "Trimmomatic"], "nb_own": 8, "list_own": ["ray1919", "replikation", "remiolsen", "tdelhomme", "raygozag", "vibbits", "telatin", "tamara-hodgetts"], "nb_wf": 7, "list_wf": ["vcf_ancestry-nf", "nextflow-example", "blastn_so_hot", "nf-core-radseq", "rnaseq-editing", "nf-atac-seq", "lRNA-Seq", "rnaseq"], "list_contrib": ["ray1919", "replikation", "alex-botzki", "remiolsen", "abotzki", "tdelhomme", "raygozag", "vmikk", "telatin", "tamara-hodgetts"], "nb_contrib": 10, "codes": ["process PICARD_MARKDUPLICATES {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.7--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.7--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n    path  \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        -Xmx${avail_mem}g \\\\\n        MarkDuplicates \\\\\n        $args \\\\\n        I=$bam \\\\\n        O=${prefix}.bam \\\\\n        M=${prefix}.MarkDuplicates.metrics.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools idxstats $bam > ${bam}.idxstats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess filter_VCF {\n\n  input:\n  file vcf\n  file fasta_ref\n\n  output:\n  file \"tmp.vcf.gz\" into filt_vcf\n\n  shell:\n  '''\n  # DR2 filtering\n  # bcftools norm -m - -Oz -f !{fasta_ref} !{vcf} | bcftools filter -i 'INFO/DR2>0.3' | bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' > tmp0.vcf.gz\n  tabix -p vcf !{vcf}\n  bcftools norm -m - -Oz -f !{fasta_ref} !{vcf} | bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' > tmp0.vcf.gz\n\n  # filtering on HWE and MAF\n  plink --vcf tmp0.vcf.gz --maf 0.1 --hwe 1e-6 --make-bed --out filtered_vcf --recode vcf\n\n  # LD pruning\n  plink --vcf filtered_vcf.vcf --indep-pairwise 50 5 0.5 --out filtered_vcf_LD_prun\n  plink --vcf filtered_vcf.vcf --extract filtered_vcf_LD_prun.prune.in --recode vcf --out filtered_vcf_prun\n  bgzip -c filtered_vcf_prun.vcf > tmp.vcf.gz\n  '''\n}", "\nprocess PCA {\n\n  input:\n  file common_snps from common_snps\n\n  output:\n  file \"plink_eigenvec\" into eigenvec\n\n  shell:\n  '''\n  mkdir -p merge\n  echo \"common_snps/1KG_intersection_chr1\" > merge/list\n  for chr in {2..22}; do echo \"common_snps/1KG_intersection_chr${chr}\" >> merge/list; done\n  echo \"common_snps/input_VCF_intersection\" >> merge/list\n\n  plink --memory 4000 --merge-list merge/list --out merge/1KG_with_input_VCF\n\n  mkdir -p pca && cd pca\n  plink --bfile ../merge/1KG_with_input_VCF --pca\n  cd .. && cp pca/plink.eigenvec plink_eigenvec\n  '''\n}", "\nprocess ABRICATE_SUMMARY {\n    tag { sample_id }\n    \n    publishDir \"$params.outdir/abricate/\", \n        mode: 'copy'\n\n    input:\n    path(\"*\")\n\n    output:\n    path(\"summary.tsv\")\n    path(\"abricate_mqc.tsv\"), emit: multiqc\n\n    script:\n    \"\"\"\n    abricate --summary *.tab > summary.tsv\n    abricateToMqc.py -i summary.tsv -o abricate_mqc.tsv\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $params.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    trimmomatic -version > v_trimmomatic.txt\n    ustacks -v 2> v_stacks.txt || true\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.10--h9402c20_2\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess blastn_local {\n        label 'blast'\n        publishDir \"${params.output}/${name}/\", mode: 'copy'\n    input:\n        tuple val(name), path(fasta)\n        path(database)\n    output:\n\t    tuple val(name), path(\"${name}.xml\") \n    script:\n    \"\"\"\n    blastn -query ${fasta} -db ${database}/${database} -out ${name}.xml -outfmt 5 -num_threads ${task.cpus} -evalue 10E-120 -qcov_hsp_perc 10 -max_hsps 10\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/PICARD_MARKDUPLICATES", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_IDXSTATS", "tdelhomme/vcf_ancestry-nf/filter_VCF", "tdelhomme/vcf_ancestry-nf/PCA", "telatin/nextflow-example/ABRICATE_SUMMARY", "vibbits/rnaseq-editing/FASTQC", "ray1919/lRNA-Seq/SAMTOOLS_STATS", "replikation/blastn_so_hot/blastn_local"], "list_wf_names": ["vibbits/rnaseq-editing", "tdelhomme/vcf_ancestry-nf", "telatin/nextflow-example", "raygozag/rnaseq", "ray1919/lRNA-Seq", "replikation/blastn_so_hot", "tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["ray1919"], "nb_wf": 1, "list_wf": ["lRNA-Seq"], "list_contrib": ["ray1919"], "nb_contrib": 1, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--pyhdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--pyhdfd78af_1\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n             'fastqc/*' \n             'trimgalore/fastqc/*' \n             'trimgalore/*' \n    path ('data/fastp/*')\n    path ('sortmerna/*')\n    path ('star/*')\n    path ('hisat2/*')\n    path ('rsem/*')\n    path ('salmon/*')\n    path ('samtools/stats/*')\n    path ('samtools/flagstat/*')\n    path ('samtools/idxstats/*')\n    path ('picard/markduplicates/*')\n    path ('featurecounts/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/aligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('deseq2/pseudoaligner/*')\n    path ('preseq/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}"], "list_proc": ["ray1919/lRNA-Seq/MULTIQC"], "list_wf_names": ["ray1919/lRNA-Seq"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["raygozag"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["raygozag"], "nb_contrib": 1, "codes": ["process STAR_GENOMEGENERATE {\n    tag \"$fasta\"\n    label 'process_high'\n\n                                                         \n    conda (params.enable_conda ? \"bioconda::star=2.6.1d bioconda::samtools=1.10 conda-forge::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' :\n        'quay.io/biocontainers/mulled-v2-1fa26d1ce03c295fe2fdcf85831a92fbcbd7e8c2:59cdd445419f14abac76b31dd0d71217994cbcc9-0' }\"\n\n    input:\n    path fasta\n    path gtf\n\n    output:\n    path \"star\"        , emit: index\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = (task.ext.args ?: '').tokenize()\n    def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n    if (args.contains('--genomeSAindexNbases')) {\n        \"\"\"\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        samtools faidx $fasta\n        NUM_BASES=`gawk '{sum = sum + \\$2}END{if ((log(sum)/log(2))/2 - 1 > 14) {printf \"%.0f\", 14} else {printf \"%.0f\", (log(sum)/log(2))/2 - 1}}' ${fasta}.fai`\n\n        mkdir star\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir star/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            --genomeSAindexNbases \\$NUM_BASES \\\\\n            $memory \\\\\n            ${args.join(' ')}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            star: \\$(STAR --version | sed -e \"s/STAR_//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["raygozag/rnaseq/STAR_GENOMEGENERATE"], "list_wf_names": ["raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["raygozag"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["raygozag"], "nb_contrib": 1, "codes": ["process GET_CHROM_SIZES {\n    tag \"$fasta\"\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'     , emit: sizes\n    path '*.fai'       , emit: fai\n    path \"versions.yml\", emit: versions\n\n    script:\n    \"\"\"\n    samtools \\\\\n        faidx \\\\\n        $fasta\n\n    cut -f 1,2 ${fasta}.fai > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/GET_CHROM_SIZES"], "list_wf_names": ["raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["StringTie"], "nb_own": 1, "list_own": ["raygozag"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["raygozag"], "nb_contrib": 1, "codes": ["process STRINGTIE {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::stringtie=2.1.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/stringtie:2.1.7--h978d192_0' :\n        'quay.io/biocontainers/stringtie:2.1.7--h978d192_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"*.coverage.gtf\")   , emit: coverage_gtf\n    tuple val(meta), path(\"*.transcripts.gtf\"), emit: transcript_gtf\n    tuple val(meta), path(\"*.abundance.txt\")  , emit: abundance\n    tuple val(meta), path(\"*.ballgown\")       , emit: ballgown\n    path  \"versions.yml\"                      , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = '--fr'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = '--rf'\n    }\n    \"\"\"\n    stringtie \\\\\n        $bam \\\\\n        $strandedness \\\\\n        -G $gtf \\\\\n        -o ${prefix}.transcripts.gtf \\\\\n        -A ${prefix}.gene.abundance.txt \\\\\n        -C ${prefix}.coverage.gtf \\\\\n        -b ${prefix}.ballgown \\\\\n        -p $task.cpus \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        stringtie: \\$(stringtie --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["raygozag/rnaseq/STRINGTIE"], "list_wf_names": ["raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["raygozag"], "nb_wf": 1, "list_wf": ["rnaseq"], "list_contrib": ["raygozag"], "nb_contrib": 1, "codes": ["\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' :\n        'quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\")                   , emit: bam\n    tuple val(meta), path(\"*.log\")                   , emit: summary\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n    path  \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            hisat2: $VERSION\n            samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["raygozag/rnaseq/HISAT2_ALIGN"], "list_wf_names": ["raygozag/rnaseq"]}, {"nb_reuse": 1, "tools": ["BUStools"], "nb_own": 1, "list_own": ["redst4r"], "nb_wf": 1, "list_wf": ["nf-10x-kallisto"], "list_contrib": ["redst4r"], "nb_contrib": 1, "codes": [" process bustools_correct_sort{\n                  \n     publishDir \"${params.outdir}/kallisto/sort_bus\", mode: 'copy'\n\n     input:\n     file bus from kallisto_bus_to_sort\n     file whitelist from barcode_whitelist_kallisto\n\n     output:\n     file bus into (kallisto_corrected_sort_to_count, kallisto_corrected_sort_to_metrics)\n\n\n     script:\n     if(params.bustools_correct) {\n       correct = \"bustools correct -w $whitelist -o ${bus}/output.corrected.bus ${bus}/output.bus\"\n       sort_file = \"${bus}/output.corrected.bus\"\n                                                                    \n                                   \n       cleanup = \"rm ${bus}/output.corrected.bus && rm ${bus}/output.bus\"\n     } else {\n       correct = \"\"\n       sort_file = \"${bus}/output.bus\"\n       cleanup = \"rm ${bus}/output.bus\"\n\n     }\n     \"\"\"\n     $correct\n     mkdir -p tmp\n     bustools sort -T tmp/ -t ${params.cpus} -m ${params.mem} -o ${bus}/output.corrected.sort.bus $sort_file\n     $cleanup\n     \"\"\"\n }"], "list_proc": ["redst4r/nf-10x-kallisto/bustools_correct_sort"], "list_wf_names": ["redst4r/nf-10x-kallisto"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process mafft {\n    label 'mafft'\n  input:\n    tuple val(name), path(dir) \n  output:\n\t  tuple val(name), path(\"protein_alignments.msa\")\n  script:\n    if (!params.filenames)\n    \"\"\"\n    cat ${dir}/* > all_proteins.aa\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n    else if (params.filenames)\n    \"\"\"\n    for file in ${dir}/* ; do\n      filename=\\$(basename \\${file})\n      cat \\${file} | sed \"1s/.*/>\\${filename%.*}/\"   >> all_proteins.aa\n    done\n\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/mafft"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["seqret"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process fastqTofasta {\n                                                                                             \n      label 'emboss'\n    input:\n      tuple val(name), file(fastq)\n    output:\n      tuple val(name), file(\"filtered_reads.fasta\")\n    script:\n      \"\"\"\n      seqret -sequence ${fastq} -outseq filtered_reads.fasta\n      \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/fastqTofasta"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["ABRicate"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["\nprocess abricate_compare {\n    label 'abricate'\n                                                                              \n  input:\n    tuple val(name), val(splitname), val(type), path(fasta) \n    each method\n  output:\n\t  tuple val(name), path(\"${method}_${type}_${name}_*.abricate\")\n  script:\n    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 85 --db ${method} > ${method}_${type}_${name}_\\${PWD##*/}.abricate\n    \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/abricate_compare"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["ABRicate"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["\nprocess abricate_transposon {\n    label 'abricate'\n    publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"*.tab\"\n  input:\n    tuple val(name), path(fasta), path(database)\n  output:\n\t  tuple val(name), path(\"*.tab\")\n  script:\n    \"\"\"\n    mkdir -p db/mobile_elements\n    cp ${database} db/mobile_elements/sequences\n    abricate --setupdb --datadir \\$PWD/db\n\n  \tabricate ${fasta} --datadir \\$PWD/db --nopath --quiet --mincov 85 --db mobile_elements > mobile_elements_\\${PWD##*/}.tab\n    \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/abricate_transposon"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["Racon"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process racon {\n      label 'racon'\n   input:\n      tuple val(name), path(read), path(assembly), path(mapping) \n   output:\n   \ttuple val(name), file(read), file(\"${name}_consensus.fasta\") \n   shell:\n      \"\"\"\n      racon -t ${task.cpus} ${read} ${mapping} ${assembly} > ${name}_consensus.fasta\n      \"\"\"\n  }"], "list_proc": ["replikation/docker_pipelines/racon"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["Centrifuge"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process centrifuge_illumina {\n      publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_filtered.csv\"\n      publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}.out\"\n      label 'centrifuge'\n\n                                                                             \n                                                                  \n                                  \n                                       \n                      \n\n    input:\n      tuple val(name), file(fastq) \n      path(database) \n    output:\n      tuple val(name), file(\"${name}.out\"), file(\"${name}_pavian_report_filtered.csv\")\n    shell:\n      \"\"\"\n      case \"!{database}\" in\n      *.tar.gz)\n        tar xzf !{database}\n        ;;\n      *.gz | *.tgz ) \n        gzip -d !{database}\n        ;;\n      *.tar)\n        tar xf !{database}\n        ;;\n      esac\n      \n      DBname=\\$(ls *.[1-9].cf | head -1 | cut -f1 -d\".\")\n\n      centrifuge -p ${task.cpus} -x \\${DBname} -k 5 \\\n        -1 ${fastq[0]} -2 ${fastq[0]} -S centrifuge_results.out --report-file centrifuge_out.log\n\n      # filter based on score\n      < centrifuge_results.out awk '{if(NR < 2 || \\$4 >= 250) {print}}' | awk '{if(NR < 2 || \\$6 >= 150) {print}}' > centrifuge_filtered.out\n\n      centrifuge-kreport -x \\${DBname} centrifuge_filtered.out > ${name}_pavian_report_filtered.csv\n\n      mv centrifuge_filtered.out ${name}.out\n\n      \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/centrifuge_illumina"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process sourmashclassification {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"taxonomic-classification.txt\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(fasta) \n      file(database) \n    output:\n      tuple val(name), file(\"taxonomic-classification.txt\")\n    script:\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o ${name}.sig\n      sourmash lca classify --db ${database} --query ${name}.sig -o taxonomic-classification.txt\n      \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/sourmashclassification"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["sourmash"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process sourmashclusterdir {\n      publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"*.pdf\"\n      publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"results.csv\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(dir) \n    output:\n      tuple val(name), file(\"*.pdf\")\n      tuple val(name), file(\"results.csv\")\n    script:\n      \"\"\"\n      cp ${dir}/*.* .\n      sourmash sketch dna -p scaled=10000,k=31 *.fa*\n      sourmash compare *.sig -o results_sig --csv results.csv\n      sourmash plot --pdf --subsample=250 --labels results_sig\n      \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/sourmashclusterdir"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["TAP"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["docker_pipelines"], "list_contrib": ["mult1fractal", "replikation"], "nb_contrib": 2, "codes": ["process filter_fasta_by_length {\n    label 'ubuntu'\n  input:\n    tuple val(name), path(fasta)\n  output:\n\t  tuple val(name), path(\"${name}_filtered.fasta\")\n  script:\n    \"\"\"\n  \t# make fasta files to one liner\n    sed ':a;N;/^>/M!s/\\n//;ta;P;D' ${fasta} |\\\n    awk '/^>/ { getline seq } length(seq) > 1500 { print \\$0 \"\\\\n\" seq }' |\\\n    awk '/^>/ { getline seq } length(seq) < 1000000 { print \\$0 \"\\\\n\" seq }' > ${name}_filtered.fasta \n    \"\"\"\n}"], "list_proc": ["replikation/docker_pipelines/filter_fasta_by_length"], "list_wf_names": ["replikation/docker_pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools", "ARTIC"], "nb_own": 1, "list_own": ["replikation"], "nb_wf": 1, "list_wf": ["poreCov"], "list_contrib": ["replikation", "DataSpott", "bwlang", "angelovangel", "MarieLataretu", "RaverJay", "hoelzer"], "nb_contrib": 7, "codes": ["\nprocess artic_nanopolish {\n        label 'artic'\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"        \n        publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n\n    input:\n        tuple val(name), path(reads), path(external_scheme), path(fast5_dir), path(txt_files)\n    output:\n        tuple val(name), path(\"*.consensus.fasta\"), emit: fasta\n        tuple val(name), path(\"${name}_mapped_*.primertrimmed.sorted.bam\"), path(\"${name}_mapped_*.primertrimmed.sorted.bam.bai\"), emit: reference_bam\n        tuple val(name), path(\"SNP_${name}.pass.vcf\"), emit: vcf\n        tuple val(name), path(\"${name}.pass.vcf.gz\"), path(\"${name}.coverage_mask.txt.*1.depths\"), path(\"${name}.coverage_mask.txt.*2.depths\"), emit: covarplot\n        tuple val(name), path(\"${name}.trimmed.rg.sorted.bam\"), emit: fullbam\n    script:   \n        \"\"\"\n        artic minion --minimap2 --normalise 500 \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${external_scheme} \\\n            --read-file ${reads} \\\n            --fast5-directory ${fast5_dir} \\\n            --sequencing-summary sequencing_summary*.txt \\\n            nCoV-2019/${params.primerV} ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths ${external_scheme}/nCoV-2019/${params.primerV}/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"\n        stub:\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\"\n}"], "list_proc": ["replikation/poreCov/artic_nanopolish"], "list_wf_names": ["replikation/poreCov"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["rikenbit"], "nb_wf": 1, "list_wf": ["ramdaq"], "list_contrib": ["yuifu", "myoshimura080822"], "nb_contrib": 2, "codes": ["\nprocess FASTQC {\n    label 'process_low'\n    tag \"$name\"\n    \n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy',\n        saveAs: { filename ->\n                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n                }\n\n    input:\n    tuple val(name), file(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\", emit: fastqc_results\n\n    script:\n    def prefix = options.suffix ? \"${name}${options.suffix}\" : \"${name}\"\n    def prefix_1 = options.suffix ? \"${name}_1${options.suffix}\" : \"${name}_1\"\n    def prefix_2 = options.suffix ? \"${name}_2${options.suffix}\" : \"${name}_2\"\n\n    if (params.single_end) {\n        newfastq = (reads.getName() =~ /\\.gz$/) ? \"${prefix}.fastq.gz\" : \"${prefix}.fastq\"\n        \"\"\"\n        if [ ! -f $newfastq ]; then\n            ln -s $reads $newfastq\n        fi\n        fastqc $options.args --threads $task.cpus $newfastq\n        \"\"\"\n    } else {\n        newfastq1 = (reads[0].getName() =~ /\\.gz$/) ? \"${prefix_1}.fastq.gz\" : \"${prefix_1}}.fastq\"\n        newfastq2 = (reads[1].getName() =~ /\\.gz$/) ? \"${prefix_2}.fastq.gz\" : \"${prefix_2}.fastq\"\n        \"\"\"\n        if [ ! -f $newfastq1 ]; then\n            ln -s ${reads[0]} $newfastq1\n            ln -s ${reads[1]} $newfastq2\n        fi\n        fastqc $options.args --threads $task.cpus $newfastq1\n        fastqc $options.args --threads $task.cpus $newfastq2\n        \"\"\"\n    }"], "list_proc": ["rikenbit/ramdaq/FASTQC"], "list_wf_names": ["rikenbit/ramdaq"]}, {"nb_reuse": 1, "tools": ["HISAT2", "SAMtools", "MultiQC", "BamTools", "FeatureCounts", "FastQC"], "nb_own": 1, "list_own": ["rikenbit"], "nb_wf": 1, "list_wf": ["ramdaq"], "list_contrib": ["yuifu", "myoshimura080822"], "nb_contrib": 2, "codes": ["\nprocess GET_SOFTWARE_VERSIONS {\n\n    label 'process_low'\n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy', overwrite: true,\n        saveAs: { filename ->\n                  if (filename.indexOf(\".csv\") > 0) filename\n                  else null\n            }\n    \n    output:\n    path \"software_versions_mqc.yaml\", emit: software_versions_yaml\n    path \"software_versions.csv\"\n    \n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    fastq-mcf -V > v_fastqmcf.txt\n    hisat2 --version > v_hisat2.txt\n    samtools --version > v_samtools.txt\n    bam2wig.py --version > v_bam2wig.txt\n    bamtools --version > v_bamtools.txt\n    read_distribution.py --version > v_read_distribution.txt\n    infer_experiment.py --version > v_infer_experiment.txt\n    inner_distance.py --version > v_inner_distance.txt\n    junction_annotation.py --version > v_junction_annotation.txt\n    featureCounts -v > v_featurecounts.txt 2>&1\n    rsem-calculate-expression --version > v_rsem.txt\n    Rscript -e \"write(x=as.character(R.version.string), file='v_R.txt')\"\n    Rscript -e \"library(edgeR); write(x=as.character(packageVersion('edgeR')), file='v_edgeR.txt')\"\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["rikenbit/ramdaq/GET_SOFTWARE_VERSIONS"], "list_wf_names": ["rikenbit/ramdaq"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["rikenbit"], "nb_wf": 1, "list_wf": ["ramdaq"], "list_contrib": ["yuifu", "myoshimura080822"], "nb_contrib": 2, "codes": ["\nprocess RSEM_BOWTIE2  {\n    tag \"$name\"\n    label 'process_high'\n    \n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy', overwrite: true,\n        saveAs: { filename ->\n                    filename.indexOf(\".log\") > 0 ? \"logs/$filename\" : \"$filename\"\n                }\n\n    input:\n    tuple val(name), file(bam), file(bai)                  \n    tuple val(name), file(reads)\n    path rsem_indices\n\n    output:\n    path \"*.isoforms.results\", emit: rsem_isoforms_to_merge\n    path \"*.genes.results\", emit: rsem_genes_to_merge\n    path \"*.stat/*.cnt\", emit: rsem_results_stat\n    path \"*.{results,log}\"\n\n    script:\n    def prefix = options.suffix ? \"${name}${options.suffix}\" : \"${name}\"\n\n    def strandness = ''\n    if (params.stranded == 'fr-firststrand') {\n        strandness = \"--strandedness reverse\"\n    } else if (params.stranded == 'fr-secondstrand'){\n        strandness = \"--strandedness forward\"\n    }\n    threads_num = params.rsem_threads_num > 0 ? \"-p ${params.rsem_threads_num}\" : ''\n    index_base = rsem_indices[0].toString().split('\\\\.')[0]\n\n    if (params.single_end) {\n        if (params.stranded && params.stranded != 'unstranded') {\n            \"\"\"\n            rsem-calculate-expression $threads_num $strandness $reads --bowtie2 --bowtie2-path /opt/conda/envs/ramdaq-1.0dev/bin/ $index_base ${prefix}\n            samtools sort ${prefix}.transcript.bam -o ${prefix}.rsem.bam\n            samtools index ${prefix}.rsem.bam\n            samtools flagstat ${prefix}.rsem.bam > ${prefix}.rsem.bam.flagstat\n            rm ${prefix}.transcript.bam\n            \"\"\"\n        } else {\n            \"\"\"\n            rsem-calculate-expression $threads_num $reads --bowtie2 --bowtie2-path /opt/conda/envs/ramdaq-1.0dev/bin/ $index_base ${prefix}\n            samtools sort ${prefix}.transcript.bam -o ${prefix}.rsem.bam\n            samtools index ${prefix}.rsem.bam\n            samtools flagstat ${prefix}.rsem.bam > ${prefix}.rsem.bam.flagstat\n            rm ${prefix}.transcript.bam\n            \"\"\"\n        }\n    } else {\n        if (params.stranded && params.stranded != 'unstranded') {\n            \"\"\"\n            rsem-calculate-expression $threads_num $strandness --paired-end ${reads[0]} ${reads[1]} --bowtie2 --bowtie2-path /opt/conda/envs/ramdaq-1.0dev/bin/ \\\\\n            $index_base ${prefix}\n            samtools sort ${prefix}.transcript.bam -o ${prefix}.rsem.bam\n            samtools index ${prefix}.rsem.bam\n            samtools flagstat ${prefix}.rsem.bam > ${prefix}.rsem.bam.flagstat\n            rm ${prefix}.transcript.bam\n            \"\"\"\n        } else {\n            \"\"\"\n            rsem-calculate-expression $threads_num --paired-end ${reads[0]} ${reads[1]} --bowtie2 --bowtie2-path /opt/conda/envs/ramdaq-1.0dev/bin/ \\\\\n            $index_base ${prefix}\n            samtools sort ${prefix}.transcript.bam -o ${name}.rsem.bam\n            samtools index ${prefix}.rsem.bam\n            samtools flagstat ${prefix}.rsem.bam > ${prefix}.rsem.bam.flagstat\n            rm ${prefix}.transcript.bam\n            \"\"\"\n        }\n    }\n}"], "list_proc": ["rikenbit/ramdaq/RSEM_BOWTIE2"], "list_wf_names": ["rikenbit/ramdaq"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["rikenbit"], "nb_wf": 1, "list_wf": ["ramdaq"], "list_contrib": ["yuifu", "myoshimura080822"], "nb_contrib": 2, "codes": ["\nprocess FEATURECOUNTS {\n    tag \"$name\"\n    \n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy', overwrite: true,\n        saveAs: {filename ->\n            if (filename.indexOf(\"biotype_counts\") > 0) \"biotype_counts/$filename\"\n            else if (filename.indexOf(\".featureCounts.txt.summary\") > 0) \"count_summaries/$filename\"\n            else if (filename.indexOf(\".featureCounts.txt\") > 0) \"counts/$filename\"\n            else \"$filename\"\n        }\n    \n    input:\n    tuple val(name), file(bam), file(bai)\n    file gtf\n    file biotypes_header\n\n    output:\n    path \"*.featureCounts.txt\", emit: counts_to_merge\n    path \"*.featureCounts.txt.summary\", emit: counts_summary\n    path \"${name}.allgene.featureCounts.txt\", optional:true, emit: counts_to_plot_corr\n    path \"${name}_biotype_counts*mqc.{txt,tsv}\", optional:true, emit: counts_biotype\n\n    script:\n    def prefix = options.suffix ? \"${name}${options.suffix}\" : \"${name}\"\n\n    def is_pairedend = params.single_end ? '' : \"-p\"\n    def strandspecific = ''\n    if (params.stranded && params.stranded == 'fr-firststrand') {\n        strandspecific = \"-s 2\"\n    } else if (params.stranded && params.stranded == 'fr-secondstrand'){\n        strandspecific = \"-s 1\"\n    }\n    def extra_attributes = params.extra_attributes ? \"--extraAttributes ${params.extra_attributes}\" : ''\n    def allow_multimap = params.allow_multimap ? \"-M\" : ''\n    def allow_overlap = params.allow_overlap ? \"-O\" : ''\n    def count_fractionally = params.count_fractionally ? \"--fraction\" : ''\n    def threads_num = params.fc_threads_num > 0 ? \"-T ${params.fc_threads_num}\" : ''\n    def biotype = params.group_features_type\n\n    if (options.suffix != '.allgene') {\n        \"\"\"\n        featureCounts -a $gtf -g ${params.group_features} -t ${params.count_type} -o ${prefix}.featureCounts.txt  \\\\\n        $is_pairedend $strandspecific $extra_attributes $allow_multimap $allow_overlap $count_fractionally $threads_num ${bam}\n        \"\"\"\n    } else {\n        \"\"\"\n        featureCounts -a $gtf -g ${params.group_features} -t ${params.count_type} -o ${prefix}.featureCounts.txt  \\\\\n        $is_pairedend $strandspecific $extra_attributes $allow_multimap $allow_overlap $count_fractionally $threads_num ${bam}\n\n        featureCounts -a $gtf -g $biotype -o ${name}_biotype_featureCounts.txt $is_pairedend $strandspecific ${bam}\n        cut -f 1,7 ${name}_biotype_featureCounts.txt | tail -n +3 | cat $biotypes_header - >> ${name}_biotype_counts_mqc.txt\n        \"\"\"\n\n    }\n\n\n\n\n}"], "list_proc": ["rikenbit/ramdaq/FEATURECOUNTS"], "list_wf_names": ["rikenbit/ramdaq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["rikenbit"], "nb_wf": 1, "list_wf": ["ramdaq"], "list_contrib": ["yuifu", "myoshimura080822"], "nb_contrib": 2, "codes": ["\nprocess MULTIQC {\n\n    label 'process_medium'\n    publishDir \"${params.outdir}/${options.publish_dir}\", mode: 'copy', overwrite: true\n    container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path ('fastqc/*')\n    path ('fastqc/*')\n    path ('hisat2_genome/*')\n    path ('hisat2_rrna/*')\n    path ('rseqc/*')\n    path ('featureCounts/biotype_counts/*')\n    path ('rsem_bowtie2_allgenes/*')\n    path ('plot_sample_correlation/*')\n    path ('plot_ercc_correlation/*')\n    path ('plot_ercc_correlation/*')\n    path ('featurecounts_all_gtf/*')\n    path ('featurecounts_mt_gtf/*')\n    path ('featurecounts_histone_gtf/*')\n    path ('plot_assignedgenome/*')\n    path ('plot_assignedgenome/*')\n    path ('plot_fcounts_maprate_allgene/*')\n    path ('plot_fcounts_maprate_allgene/*')\n    path ('plot_fcounts_maprate_mt/*')\n    path ('plot_fcounts_maprate_mt/*')\n    path ('plot_fcounts_maprate_histone/*')\n    path ('plot_fcounts_maprate_histone/*')\n    path ('plot_detectedgenes_dr/*')\n    path ('plot_detectedgenes_dr/*')\n    path ('plot_detectedgenes_dr/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_from_tpmcounts_rsem/*')\n    path ('plots_entropy_sirv/*')\n    path ('plots_entropy_sirv/*')\n    path ('software_versions/*')\n    path workflow_summary\n    \n    output:\n    path \"*.html\", emit: multiqc_report\n    path \"*_data\", emit: data\n    path \"*_plots\", optional:true, emit: plots\n    \n    script:\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $custom_config .\n    \"\"\"\n}"], "list_proc": ["rikenbit/ramdaq/MULTIQC"], "list_wf_names": ["rikenbit/ramdaq"]}, {"nb_reuse": 5, "tools": ["BCFtools", "SAMtools", "QualiMap", "FreeBayes", "MultiQC", "TIDDIT", "FREEC", "FastQC", "MSIsensor", "snpEff", "GATK", "VCFtools"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 5, "list_wf": ["saw.structural-variants", "custom_sarek", "dx_sarek", "germline_somatic", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "waffle-iron", "marcelm", "kusalananda", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "viklund", "glormph", "skrakau", "samuell", "adrlar", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "pallolason", "jhagberg"], "nb_contrib": 28, "codes": ["\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n                              \n    aligner=\"bwa-mem2\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idPatient}_${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess index_bamfile {\n    input:\n        set file(bamfile), val(uuid), val(dir) from ch_index_bam\n    output:\n        set file(bamfile), file('*.bam.bai'), val(uuid), val(dir) into ch_indexed_bam\n\n    tag \"$uuid\"\n\n    executor choose_executor()\n\n    when: 'manta' in workflowSteps\n\n    script:\n    \"\"\"\n    samtools index \"$bamfile\"\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/FreeBayes", "rmoran7/dx_sarek/FreeBayes", "rmoran7/custom_sarek/get_software_versions", "sickle-in-africa/saw.sarek/MergePileupSummaries", "sickle-in-africa/saw.structural-variants/index_bamfile"], "list_wf_names": ["sripaladugu/germline_somatic", "sickle-in-africa/saw.structural-variants", "rmoran7/custom_sarek", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess MergeMutect2Stats {\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSamplePair, file(\"${idSamplePair}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSamplePair}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess Mutect2Single {\n    tag \"${idSampleTumor}-${intervalBed.baseName}\"\n\n    label 'process_medium'\n\n    input:\n        set idPatient, idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from singleBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") into mutect2SingleOutput\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\") optional true into intervalStatsFilesSingle\n        set idPatient, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}.vcf\") optional true into mutect2StatsSingle\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}.vcf\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSamplePair, file(\"Mutect2_filtered_${idSamplePair}.vcf.gz\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSamplePair}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MergeMutect2Stats", "rmoran7/dx_sarek/Mutect2Single", "rmoran7/custom_sarek/BuildDict", "sickle-in-africa/saw.sarek/FilterMutect2Calls"], "list_wf_names": ["rmoran7/custom_sarek", "sripaladugu/germline_somatic", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "MSIsensor", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess MSIsensor_msi {\n    label 'cpus_4'\n    label 'memory_max'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/MSIsensor\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamMsisensor\n        file msiSites from msi_scan_ch\n\n    output:\n        set val(\"Msisensor\"), idPatient, file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor\"), file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor_dis\"), file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor_germline\"), file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor_somatic\") into msisensor_out_ch\n\n    when: 'msisensor' in tools\n\n    script:\n    \"\"\"\n    msisensor msi -d ${msiSites} \\\n                  -b 4 \\\n                  -n ${bamNormal} \\\n                  -t ${bamTumor} \\\n                  -o ${idSampleTumor}_vs_${idSampleNormal}_msisensor\n    \"\"\"\n}", "\nprocess MergePileupSummaries {\n    label 'cpus_1'\n\n    tag \"${idPatient}_${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(pileupSums) from pileupSummaries\n        file(dict) from ch_dict\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}_pileupsummaries.table\") into mergedPileupFile\n\n    when: 'mutect2' in tools\n\n    script:\n    allPileups = pileupSums.collect{ \"-I ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GatherPileupSummaries \\\n        --sequence-dictionary ${dict} \\\n        ${allPileups} \\\n        -O ${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'process_medium'\n\n    input:\n        set idPatient, idSample, file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from bamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? params.target_bed ? \"-L ${params.target_bed}\" : \"-L ${germlineResource}\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSample}_pileupsummaries.table\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/MSIsensor_msi", "sripaladugu/germline_somatic/MergePileupSummaries", "rmoran7/custom_sarek/BuildFastaFai", "rmoran7/dx_sarek/PileupSummariesForMutect2"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/custom_sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["FastQC", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess FilterMutect2Calls {\n    label 'cpus_1'\n\n    tag \"${idSamplePair}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSamplePair}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSamplePair, file(unfiltered), file(unfilteredIndex), file(stats), file(contaminationTable) from mutect2CallsToFilter\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set val(\"Mutect2\"), idPatient, idSamplePair, file(\"Mutect2_filtered_${idSamplePair}.vcf.gz\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.tbi\"), file(\"Mutect2_filtered_${idSamplePair}.vcf.gz.filteringStats.tsv\") into filteredMutect2Output\n\n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --contamination-table ${contaminationTable} \\\n        --stats ${stats} \\\n        -R ${fasta} \\\n        -O Mutect2_filtered_${idSamplePair}.vcf.gz\n    \"\"\"\n}", "\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_1'\n    disk '50 GB'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 4 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess CalculateContamination {\n    label 'process_medium'\n\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(mergedPileup) from mergedPileupFile\n\n     output:\n        set idPatient, val(\"${idSample}\"), file(\"${idSample}_contamination.table\") into contaminationTable\n\n    when: 'mutect2' in tools\n\n    script:   \n    \"\"\"\n    # calculate contamination\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CalculateContamination \\\n        -I ${idSample}_pileupsummaries.table \\\n        -O ${idSample}_contamination.table\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/BuildDict", "sripaladugu/germline_somatic/FilterMutect2Calls", "rmoran7/custom_sarek/FastQCFQ", "rmoran7/dx_sarek/CalculateContamination"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/custom_sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "FastQC", "MSIsensor"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess MSIsensor_msi {\n    label 'cpus_4'\n    label 'memory_max'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/MSIsensor\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from pairBamMsisensor\n        file msiSites from msi_scan_ch\n\n    output:\n        set val(\"Msisensor\"), idPatient, file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor\"), file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor_dis\"), file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor_germline\"), file(\"${idSampleTumor}_vs_${idSampleNormal}_msisensor_somatic\") into msisensor_out_ch\n\n    when: 'msisensor' in tools\n\n    script:\n    \"\"\"\n    msisensor msi -d ${msiSites} \\\n                  -b 4 \\\n                  -n ${bamNormal} \\\n                  -t ${bamTumor} \\\n                  -o ${idSampleTumor}_vs_${idSampleNormal}_msisensor\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MSIsensor_msi", "rmoran7/dx_sarek/Mpileup", "sickle-in-africa/saw.sarek/MapReads", "rmoran7/custom_sarek/FastQCBAM"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/custom_sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "FREEC"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/UMIMapBamFile", "sripaladugu/germline_somatic/ControlFREEC", "rmoran7/dx_sarek/ControlFREECSingle", "sickle-in-africa/saw.sarek/IndexBamMergedForSentieon"], "list_wf_names": ["rmoran7/custom_sarek", "sripaladugu/germline_somatic", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["BCFtools", "SAMtools", "SAMBLASTER", "GATK", "VCFtools"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/GroupReadsByUmi", "sripaladugu/germline_somatic/BcftoolsStats", "rmoran7/dx_sarek/Vcftools", "sickle-in-africa/saw.sarek/MarkDuplicates"], "list_wf_names": ["rmoran7/custom_sarek", "sripaladugu/germline_somatic", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "Picard", "BWA", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "sripaladugu", "rnharmening", "rmoran7"], "nb_wf": 4, "list_wf": ["custom_sarek", "saw.sarek", "nf-multipleReferenceMapper", "nextflow_align"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "rnharmening", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "sripaladugu", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess bamsorter {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(samfile) from sam_files\n    \n    output:\n    set sample_id, file(\"${sample_id}_sorted_aln.bam\") into sorted_bam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    picard -Xmx16g AddOrReplaceReadGroups \\\n           INPUT=${samfile} OUTPUT=${sample_id}_sorted_aln.bam SORT_ORDER=coordinate \\\n           RGID=${sample_id}-id RGLB=${sample_id}-lib RGPL=ILLUMINA RGPU=${sample_id}-01 RGSM=${sample_id}\n    \"\"\"\n}", "\nprocess bwamem {\n  tag \"$sample_id\"\n\n  publishDir \"${params.outdir}/BWA_mapping\", mode: 'copy'\n\n  input:\n    set read_id, file(reads), val(ref_id), file(bwa_index) from derep_reads_ch.combine(bwa_index_ch)  \n\n  output:\n    set sample_id, file(bam_file) into mapping_pair_ch \n    file bam_file\n    file \"*.bai\"\n  \n  script:\n    sample_id = \"${read_id}_v_${ref_id}\"\n    bam_file  = \"${sample_id}.sorted.bam\"\n    filter = params.keepOnlyMapped ? \"-F 4\" : \"\" \n  \"\"\"\n  bwa mem -t ${task.cpus} -o tmp.sam ${bwa_index}/${ref_id}.fa ${reads[0]} ${reads[1]} \n  samtools view $filter -b -@ ${task.cpus} -o tmp.bam tmp.sam\n  samtools sort -@ ${task.cpus} -o ${bam_file} tmp.bam\n  samtools index -@ ${task.cpus}  ${bam_file}\n  rm tmp.[sb]am\n  \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_4'\n    disk '120 GB'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}", "\nprocess RecalibrateIndelQualityScores {\n    label 'process_low'\n    tag \"${variantCaller}-${idSample}\"\n\n    publishDir \"${params.outdir}/recalibratedVariants/${idSample}/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(variantCaller), val(idSample), path(vcf), path(vcfIndex)\n        tuple path(recalTable), path(tranches)\n\n    script:\n        \"\"\"\n        gatk --java-options \"-Xmx5g -Xms5g\" \\\n            ApplyVQSR \\\n            -V ${vcf} \\\n            --recal-file ${recalTable} \\\n            --tranches-file ${tranches} \\\n            --truth-sensitivity-filter-level 99.7 \\\n            --create-output-variant-index true \\\n            -mode INDEL \\\n            -O ${idSample}_indel.recalibrated.vcf.gz\n        \"\"\"\n\n}"], "list_proc": ["sripaladugu/nextflow_align/bamsorter", "rnharmening/nf-multipleReferenceMapper/bwamem", "rmoran7/custom_sarek/MergeBamMapped", "sickle-in-africa/saw.sarek/RecalibrateIndelQualityScores"], "list_wf_names": ["sripaladugu/nextflow_align", "rmoran7/custom_sarek", "rnharmening/nf-multipleReferenceMapper", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "Bismark", "SIMULATE", "BWA"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "robinfchan", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "bisulfite_align_nf"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "robinfchan", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess mutateReference {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n\tcontainer params.simulatorImage\n\n\tinput:\n\tpath simulationInputs\n\n\toutput:\n\ttuple path(simulationInputs), path(\"${params.referenceSequence['label']}.mutated.fa\"), path(\"${params.cohortId}.truth.g.vcf\")\n\n\tscript:\n\t\"\"\"\n\tsimulate MutateReference \\\n\t\t--input_ref ${params.referenceSequence['path']} \\\n\t\t--output_ref ${params.referenceSequence['label']}.mutated.fa \\\n\t\t--input_json ${simulationInputs} \\\n\t\t--output_vcf ${params.cohortId}.truth.g.vcf\n\t\"\"\"\n}", " process MakeBWAIndex {\n        label 'build_index'\n        tag \"bwa_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n        input:\n        file fasta\n\n        output:\n        file \"BWAIndex/*\" into bwa_index\n\n        when:\n        aligner == \"bwa\"\n     \n        script:\n        \"\"\"\n        mkdir BWAIndex\n        cd BWAIndex/\n        bwa index -p ${fasta.baseName} -a bwtsw ../$fasta\n        cd ../\n        \"\"\"\n    }", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", " process bismark_align { \n    if (params.custom_container) container \"${params.custom_container}\"\n\n    if ( params.nugen ) {\n        ch_final_trimmed_reads_for_alignment = Channel.empty().mix(ch_nugen_trimmed_reads_for_alignment)\n        } else { \n            ch_final_trimmed_reads_for_alignment = Channel.empty().mix(ch_trimmed_reads_for_alignment) \n        }\n\n    tag \"$name\"\n    publishDir \"${params.outdir}/bismark_alignments\", mode: 'copy', overwrite: true,\n        saveAs: {filename ->\n            if( filename.indexOf(\".fq.gz\") > 0 ) \"unmapped/$filename\"\n            else if( filename.indexOf(\"report.txt\") > 0 ) \"reports/$filename\"\n            else if( filename.indexOf(\".bam\") > 0 ) \"$filename\"\n            else null\n        }\n\n    input:\n    set val(name), file(reads) from ch_final_trimmed_reads_for_alignment.mix(ch_trimmed_reads_for_alignment_preproc)\n    file(index) from ch_bismark_index_for_bismark_align.collect()\n\n    output:\n    set val(name), file(\"*.bam\") into ch_bam_for_bismark_deduplicate, ch_bam_for_bismark_summary, ch_bam_for_preseq\n    set val(name), file(\"*report.txt\") into ch_bismark_align_log_for_bismark_report, ch_bismark_align_log_for_bismark_summary, ch_bismark_align_log_for_multiqc\n    file \"*.fq.gz\" optional true\n\n    script:\n    reads_chunk = \"-1 ${reads[0]} -2 ${reads[1]}\"\n\n                                  \n    non_directional = params.non_directional ? \"--non_directional\" : ''\n    unmapped = params.unmapped ? \"--unmapped\" : ''\n    mismatches = params.num_mismatches ? \"--score_min L,0,-${params.num_mismatches}\" : ''\n    soft_clipping = params.local_alignment ? \"--local\" : ''\n\n                                                             \n    multicore = ''\n    if( task.cpus ){\n                                                                              \n        if( params.single_cell || params.non_directional ){\n            cpu_per_multicore = 5\n            mem_per_multicore = (21.GB).toBytes()\n        } else {\n            cpu_per_multicore = 3\n            mem_per_multicore = (15.GB).toBytes()\n        }\n                                                                   \n        if(params.bismark_align_cpu_per_multicore) {\n            cpu_per_multicore = (params.bismark_align_cpu_per_multicore as int)\n        }\n        if(params.bismark_align_mem_per_multicore) {\n            mem_per_multicore = (params.bismark_align_mem_per_multicore as nextflow.util.MemoryUnit).toBytes()\n        }\n                                                                         \n        ccore = ((task.cpus as int) / cpu_per_multicore ) as int\n                                                                              \n        try {\n            tmem = (task.memory as nextflow.util.MemoryUnit).toBytes()\n            mcore = (tmem / mem_per_multicore) as int\n            ccore = Math.min(ccore, mcore)\n        } catch (all) {\n            log.debug \"Warning: Not able to set bismark align multicore based on available resources\"\n        }\n        if( ccore > 1 ){\n            multicore = \"--multicore $ccore\"\n        }\n    }\n\n                   \n    \"\"\"\n    bismark $reads_chunk \\\n        --genome $index \\\n        $multicore \\\n        $non_directional \\\n        $unmapped \\\n        $mismatches \\\n        $soft_clipping\n    \"\"\"\n    }"], "list_proc": ["sickle-in-africa/saw.snv-indel/mutateReference", "ssun1116/meripseqpipe/MakeBWAIndex", "rmoran7/custom_sarek/IndexBamFile", "robinfchan/bisulfite_align_nf/bismark_align"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/custom_sarek", "robinfchan/bisulfite_align_nf"]}, {"nb_reuse": 4, "tools": ["SAMtools", "SIMULATE", "fastPHASE", "preseq", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "robinfchan", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "bisulfite_align_nf"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "robinfchan", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess generateReads {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n\tcontainer params.simulatorImage\n\n\tinput:\n\ttuple val(readsPrefix),  path(simulationInputs), path(mutatedReference)\n\n\tscript:\n\t\"\"\"\n\tmkdir -p ${params.rawReadsDir}\n\tsimulate GenerateReads \\\n\t\t--input_ref ${mutatedReference} \\\n\t\t--reads_prefix ${readsPrefix} \\\n\t\t--input_json ${simulationInputs}\n\t\"\"\"\n}", "\nprocess Fastp{\n    label 'fastp'\n    tag \"$sample_name\"\n                            \n    publishDir path: { params.skip_fastp ? params.outdir : \"${params.outdir}/QC/fastp\" },\n             saveAs: { params.skip_fastp ? null : it }, mode: 'link'\n        \n    input:\n    set val(sample_id), file(reads), val(reads_single_end), val(gzip), val(input), val(group) from raw_fastq\n\n    output:\n    set val(sample_name), file(\"*_aligners.fastq*\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into fastqc_reads, fastp_reads\n    file \"*\" into fastp_results\n\n    when:\n    aligner != \"none\"\n\n    shell:\n    skip_fastp = params.skip_fastp\n    if ( reads_single_end ){\n        filename = reads.toString() - ~/(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners = sample_name + \"_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then\n            fastp -i ${reads} -o ${add_aligners} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads} ${add_aligners}\n        fi\n        \"\"\"\n    } else {\n        filename = reads[0].toString() - ~/(_R[0-9])?(_[0-9])?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        sample_name = filename\n        add_aligners_1 = sample_name + \"_1_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        add_aligners_2 = sample_name + \"_2_aligners.fastq\" + (gzip ? \".gz\" : \"\")\n        \"\"\"\n        if [ $skip_fastp == \"false\" ]; then  \n            fastp -i ${reads[0]} -o ${add_aligners_1} -I ${reads[1]} -O ${add_aligners_2} -j ${sample_name}_fastp.json -h ${sample_name}_fastp.html -w ${task.cpus}\n        else\n            mv ${reads[0]} ${add_aligners_1}\n            mv ${reads[1]} ${add_aligners_2}\n        fi\n        \"\"\"\n    } \n}", "\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n\n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", " process preseq {\n        if (params.custom_container) container \"${params.custom_container}\"\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/preseq\", mode: 'copy', overwrite: true\n\n        input:\n        set val(name), file(bam) from ch_bam_for_preseq\n\n        output:\n        file \"${bam.baseName}.ccurve.txt\" into preseq_results\n\n        script:\n        def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n        def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n        \"\"\"\n        samtools sort $bam \\\\\n            -@ ${task.cpus} $sort_mem \\\\\n            -o ${bam.baseName}.sorted.bam\n        preseq lc_extrap -v -B ${bam.baseName}.sorted.bam -o ${bam.baseName}.ccurve.txt\n        \"\"\"\n    }"], "list_proc": ["sickle-in-africa/saw.snv-indel/generateReads", "ssun1116/meripseqpipe/Fastp", "rmoran7/custom_sarek/MarkDuplicates", "robinfchan/bisulfite_align_nf/preseq"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/custom_sarek", "robinfchan/bisulfite_align_nf"]}, {"nb_reuse": 4, "tools": ["SAMtools", "GATK", "HISAT2", "gffread"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "robinfchan", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "citeseq-nf"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "robinfchan", "juneb4869"], "nb_contrib": 6, "codes": [" process FilterrRNA {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/rRNA_dup\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from rRNA_reads\n    file index from rRNA_index.collect()\n\n    output:\n    set val(sample_name), file(\"*.fastq.gz\"), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) into tophat2_reads, hisat2_reads, bwa_reads, star_reads\n    file \"*_summary.txt\" into rRNA_log\n\n    when:\n    params.rRNA_fasta && !params.skip_filterrRNA\n\n    script:\n    gzip = true\n    index_base = index[0].toString() - ~/(\\.exon)?(\\.\\d)?(\\.fa)?(\\.gtf)?(\\.ht2)?$/\n    if (reads_single_end) {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-gz ${sample_id}.fastq.gz \\\n            -x $index_base \\\n            -U $reads | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        \"\"\"\n    } else {\n        \"\"\"\n        hisat2 --summary-file ${sample_name}_rRNA_summary.txt \\\n            --no-spliced-alignment --no-softclip --norc --no-unal \\\n            -p ${task.cpus} --dta --un-conc-gz ${sample_name}_fastq.gz \\\n            -x $index_base \\\n            -1 ${reads[0]} -2 ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -Shub - | \\\n            samtools sort -@ ${task.cpus} -o ${sample_name}_rRNA_sort.bam -\n        mv ${sample_name}_fastq.1.gz ${sample_name}_1.fastq.gz\n        mv ${sample_name}_fastq.2.gz ${sample_name}_2.fastq.gz\n        \"\"\"\n    }\n    }", "\nprocess extract_transcriptome {\n    if (params.custom_container) container \"${params.custom_container}\"\n    \n    tag \"${genome_fasta}\"\n    label 'midi_memory'\n    publishDir \"${params.outdir}/reference_data/extract_transcriptome\", mode: 'copy'\n\n    input:\n    file genome_fasta from genome_fasta_extract_transcriptome\n    file gtf from gtf_extract_transcriptome_trimmed\n\n    output:\n    file \"${genome_fasta}.transcriptome.fa\" into transcriptome_fasta_alevin_extracted\n\n    when:\n    !params.transcriptome_fasta && !params.skip_rna \n    \n    script:\n                                                        \n    \"\"\"\n    gffread -F $gtf -w \"${genome_fasta}.transcriptome.fa\" -g $genome_fasta\n    \"\"\"\n}", "\nprocess genotypeCombinedGvcfFile {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n    container params.gatk4Image\n\n\tinput:\n\tpath combinedGvcfFile\n\n\tscript:\n\t\"\"\"\n\tgatk GenotypeGVCFs \\\n\t\t-R ${params.referenceSequence['path']} \\\n\t\t-V ${params.cohortId}.g.vcf \\\n\t\t-O ${params.variantSetsDir}${params.cohortId}.genotyped.g.vcf\n\t\"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n    disk '65 GB'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["ssun1116/meripseqpipe/FilterrRNA", "robinfchan/citeseq-nf/extract_transcriptome", "sickle-in-africa/saw.snv-indel/genotypeCombinedGvcfFile", "rmoran7/custom_sarek/BaseRecalibrator"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/custom_sarek", "robinfchan/citeseq-nf"]}, {"nb_reuse": 4, "tools": ["Salmon", "HISAT2", "BWA", "SAMtools", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "robinfchan", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "citeseq-nf"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "robinfchan", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess alignReadsToReference {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.bwaImage\n\n    input:\n    tuple val(name), path(readsFilePair)\n\n    output:\n    tuple val(name), path(\"${name}.bam\")\n\n    script:\n    \"\"\"\n    bwa mem \\\n        -t ${task.cpus} \\\n        -R \\\"@RG\\\\tID:${params.runId}\\\\tPU:${params.runId}\\\\tSM:${name}\\\\tLB:${name}\\\\tPL:${params.sequencingPlatform}\\\" \\\n        ${params.referenceSequence['dir']}/bwa.${params.referenceSequence['label']} \\\n        ${readsFilePair[0]} ${readsFilePair[1]} | \\\n            samtools view \\\n                -b - | \\\n                samtools sort \\\n                    -@ ${task.cpus} \\\n                    -o ${name}.bam\n    \"\"\"\n}", "\nprocess Hisat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/hisat2\", mode: 'link', overwrite: true\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from hisat2_reads\n    file index from hisat2_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_hisat2.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into hisat2_bam\n    file \"*_summary.txt\" into hisat2_log\n\n    when:\n    aligner == \"hisat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.exon)?(\\.\\d)?(\\.fa)?(\\.gtf)?(\\.ht2)?$/\n    if (reads_single_end) {\n        \"\"\"\n        hisat2  --summary-file ${sample_name}_hisat2_summary.txt\\\n                -p ${task.cpus} --dta \\\n                -x $index_base \\\n                -U $reads | \\\n                samtools view -@ ${task.cpus} -hbS - > ${sample_name}_hisat2.bam \n        \"\"\"\n    } else {\n        \"\"\"\n        hisat2  --summary-file ${sample_name}_hisat2_summary.txt \\\n                -p ${task.cpus} --dta \\\n                -x $index_base \\\n                -1 ${reads[0]} -2 ${reads[1]} | \\\n                samtools view -@ ${task.cpus} -hbS - > ${sample_name}_hisat2.bam\n        \"\"\"\n    }\n}", "\nprocess alevin {\n    if (params.custom_container) container \"${params.custom_container}\"\n    \n    tag \"$name\"\n    label 'maxi_memory'\n    publishDir \"${params.outdir}/alevin/alevin\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from read_files_alevin\n    file index from salmon_index_alevin.collect()\n    file txp2gene from txp2gene.collect()\n\n    output:\n    file \"${name}_alevin_results\" into ( alevin_results, alevin_logs )\n    \n    when:\n    !params.skip_rna\n\n    script:\n    read1 = reads[0]\n    read2 = reads[1]\n    \"\"\"\n    salmon alevin -l ISR -1 ${read1} -2 ${read2} \\\n      ${scrna_config_string} \\\n      -i $index \\\n      -o ${name}_alevin_results \\\n      -p ${task.cpus} \\\n      --tgMap $txp2gene \\\n      --dumpFeatures \\\n      \u2013-dumpMtx\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/GatherBQSRReports", "sickle-in-africa/saw.snv-indel/alignReadsToReference", "ssun1116/meripseqpipe/Hisat2Align", "robinfchan/citeseq-nf/alevin"], "list_wf_names": ["rmoran7/custom_sarek", "sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "robinfchan/citeseq-nf"]}, {"nb_reuse": 4, "tools": ["SAMtools", "STAR", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "robsyme", "ssun1116", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "nf-nucmercoverage"], "list_contrib": ["robsyme", "ssun1116", "jackmo375", "rmoran7", "kingzhuky", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    disk '70 GB'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess addTagInfo {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.gatk4Image\n\n    input:\n    tuple val(name), path(bamFile)\n\n    output:\n    tuple val(name), path(\"${name}.fixed.bam\")\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        SetNmMdAndUqTags \\\n        -R ${params.referenceSequence['path']} \\\n        -I ${bamFile} \\\n        -O \"${name}.fixed.bam\"\n    \"\"\"\n\n}", "\nprocess StarAlign {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/star\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from star_reads\n    file star_index from star_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_star.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into star_bam\n    file \"*.final.out\" into star_log\n\n    when:\n    aligner == \"star\"\n\n    script:\n    gzip_cmd = gzip ? \"--readFilesCommand zcat\" : \"\"\n    if (reads_single_end) {\n        \"\"\"\n        STAR --runThreadN ${task.cpus} $gzip_cmd \\\n            --twopassMode Basic \\\n            --genomeDir $star_index \\\n            --readFilesIn $reads  \\\n            --outSAMtype BAM Unsorted \\\n            --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 \\\n            --outFilterIntronMotifs RemoveNoncanonical \\\n            --outFilterMultimapNmax 20 \\\n            --alignIntronMin 20 \\\n            --alignIntronMax 100000 \\\n            --alignMatesGapMax 1000000 \\\n            --outFileNamePrefix ${sample_name}  > ${sample_name}_log.txt\n        mv ${sample_name}Aligned.out.bam ${sample_name}_star.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        STAR --runThreadN ${task.cpus} $gzip_cmd \\\n            --twopassMode Basic \\\n            --genomeDir $star_index \\\n            --readFilesIn ${reads[0]} ${reads[1]}  \\\n            --outSAMtype BAM Unsorted \\\n            --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 \\\n            --outFilterIntronMotifs RemoveNoncanonical \\\n            --outFilterMultimapNmax 20 \\\n            --alignIntronMin 20 \\\n            --alignIntronMax 1000000 \\\n            --alignMatesGapMax 1000000 \\\n            --outFileNamePrefix ${sample_name} > ${sample_name}_log.txt\n        mv ${sample_name}Aligned.out.bam ${sample_name}_star.bam\n        \"\"\"\n    }\n}", "\nprocess makeGenomeFiles {\n  tag { id }\n\n  input:\n  set id, \"genome.fasta.gz\" from genomes1\n\n  output:\n  set id, \"genome.fasta.fai\" into genomeFiles\n\n  \"\"\"\nzcat genome.fasta.gz > genome.fasta\nsamtools faidx genome.fasta\n  \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/ApplyBQSR", "sickle-in-africa/saw.snv-indel/addTagInfo", "ssun1116/meripseqpipe/StarAlign", "robsyme/nf-nucmercoverage/makeGenomeFiles"], "list_wf_names": ["rmoran7/custom_sarek", "sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "robsyme/nf-nucmercoverage"]}, {"nb_reuse": 4, "tools": ["SAMtools", "BamTools", "MultiQC", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "rwtaylor", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "mpcr-analyses-pipelines"], "list_contrib": ["ssun1116", "rwtaylor", "jackmo375", "rmoran7", "kingzhuky", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess MergeBamRecal {\n    label 'cpus_4'\n     disk '120 GB'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess checkBamFile {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.gatk4Image\n\n    input:\n    tuple val(name), path(bamFile)\n\n    output:\n    tuple val(name), path(\"${bamFile}\")\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ValidateSamFile \\\n        -I ${bamFile} \\\n        -R ${params.referenceSequence['path']} \\\n        --TMP_DIR ${params.tempDir} \\\n        -M VERBOSE\n    \"\"\"\n}", "\nprocess multiqc{\n    publishDir \"${params.outdir}/Report/QCReadsReport\" , mode: 'link', overwrite: true\n    \n    when:\n    !params.skip_qc\n\n    input:\n    file arranged_qc from arranged_qc.collect()\n\n    output:\n    file \"multiqc*\" into multiqc_results\n\n    script:\n    \"\"\"\n    multiqc -n multiqc_$aligner .\n    \"\"\"\n}", "\nprocess Sample_bam_stats {\n  publishDir path:\"${params.publish_directory}/sample_bam_stats\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  file(bam) from bamFiles\n  file(bais) from baiFiles.toList()\n\n  output:\n  file(\"${bam.baseName}.stats\") into bam_stats\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bamtools stats -in ${bam} > ${bam.baseName}.stats\n  \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/MergeBamRecal", "sickle-in-africa/saw.snv-indel/checkBamFile", "ssun1116/meripseqpipe/multiqc", "rwtaylor/mpcr-analyses-pipelines/Sample_bam_stats"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "rmoran7/custom_sarek", "sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe"]}, {"nb_reuse": 4, "tools": ["BamTools", "SAMtools", "fastafrombed"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "rwtaylor", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "mpcr-analyses-pipelines"], "list_contrib": ["ssun1116", "rwtaylor", "jackmo375", "rmoran7", "kingzhuky", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess Concatenate_bams {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  file(bams) from bamFiles2.toList()\n  file(bais) from baiFiles2.toList()\n\n  output:\n  set file(\"all_samples.bam\"), file(\"all_samples.bam.bai\") into all_samples_bam\n\n  script:\n  input_bams = bams.collect{\"-in $it\"}.join(' ')\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bamtools merge ${input_bams} | bamtools sort -out all_samples.bam\n  bamtools index -in all_samples.bam\n  \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess indexInputBamFile {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n\tcontainer params.samtoolsImage\n\n\tinput:\n\ttuple val(bamId), path(bamFile)\n\n\toutput:\n\ttuple val(bamId), path(bamFile), path(\"${bamFile}.bai\")\n\n\tscript:\n\t\"\"\"\n\tsamtools index \\\n\t\t-b \\\n\t\t-@ ${task.cpus} \\\n\t\t${bamFile}\n\t\"\"\"\n}", "\nprocess MotifSearching {\n    label 'onecore_peak'\n    tag \"${bed_file.baseName}\"\n    publishDir \"${params.outdir}/m6AAnalysis/motif\", mode: 'link', overwrite: true\n    \n    input:\n    file bed_file from motif_collection\n    file chromsizesfile from chromsizesfile.collect()\n    file bed12 from bed12file.collect()\n    file fasta\n    file gtf\n\n    output:\n    file \"*_{dreme,homer}\" into motif_results, motif_results_for_report\n\n    when:\n    !params.skip_motif\n\n    script:\n    motif_file_dir = baseDir + \"/bin\"\n    bed_prefix = bed_file.baseName\n    println LikeletUtils.print_purple(\"Motif analysis is going on by DREME and Homer\")\n    \"\"\"\n    cp ${motif_file_dir}/m6A_motif.meme ./\n    sort -k5,5 -g ${bed_file} | awk 'FNR <= 2000{ print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$3}' > ${bed_prefix}.location\n    intersectBed -wo -a ${bed_prefix}.location -b $gtf | awk -v OFS=\"\\\\t\" '{print \\$1,\\$2,\\$3,\"*\",\"*\",\\$10}' | sort -k1,2 | uniq > ${bed_prefix}_bestpeaks.bed\n    fastaFromBed -name+ -split -s -fi $fasta -bed ${bed_prefix}_bestpeaks.bed > ${bed_prefix}_bestpeaks.fa\n    # ame -oc ${bed_prefix}_ame ${bed_prefix}_bestpeaks.fa m6A_motif.meme\n    shuffleBed -incl ${bed12} -seed 12345 -noOverlapping -i ${bed_prefix}_bestpeaks.bed -g ${chromsizesfile} > ${bed_prefix}_random_peak.bed\n    fastaFromBed -name+ -split -s -fi $fasta -bed ${bed_prefix}_random_peak.bed > ${bed_prefix}_random_peak.fa\n    findMotifs.pl ${bed_prefix}_bestpeaks.fa fasta ${bed_prefix}_homer -fasta ${bed_prefix}_random_peak.fa -p ${task.cpus} \\\n        -len 5,6,7,8 -S 10 -rna -dumpFasta > ${bed_prefix}_homer_run.log 2>&1\n    \"\"\"\n}"], "list_proc": ["rwtaylor/mpcr-analyses-pipelines/Concatenate_bams", "rmoran7/custom_sarek/IndexBamRecal", "sickle-in-africa/saw.snv-indel/indexInputBamFile", "ssun1116/meripseqpipe/MotifSearching"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "rmoran7/custom_sarek", "sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe"]}, {"nb_reuse": 3, "tools": ["BamTools", "BEDTools", "FastQC", "QualiMap", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "rwtaylor", "steepale", "rmoran7"], "nb_wf": 3, "list_wf": ["custom_sarek", "saw.snv-indel", "mpcr-analyses-pipelines", "nf-core-mutenrich"], "list_contrib": ["rmoran7", "rwtaylor", "steepale", "jackmo375"], "nb_contrib": 4, "codes": ["\nprocess buildGATKDictionary {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n    container params.gatk4Image\n\n\twhen:\n\t!file(params.referenceSequence['dir'] + params.referenceSequence['label'] + '.dict').exists()\n\n\tscript:\n\t\"\"\"\n\tgatk CreateSequenceDictionary \\\n\t\t\t-R ${params.referenceSequence['path']} \\\n\t\t\t-O ${params.referenceSequence['dir']}${params.referenceSequence['label']}'.dict'\n\t\"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}", "\nprocess BamQC {\n    machineType 'mem3_ssd1_x8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess Filter_mapping_quality {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam3\n  each filter from Channel.from([[\">=20\",\"overequal_mq20\"],[\"<20\",\"under_mq20\"]])\n\n  output:\n  set file(\"all_samples_${filter[1]}.bam\"), file(\"all_samples_${filter[1]}.targetcoverage.txt\") into filtered_bams\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bamtools filter -mapQuality \"${filter[0]}\" -in $allsamplesbam -out all_samples_${filter[1]}.bam\n  bedtools coverage -a ${params.mapping_targets_bed} -b all_samples_${filter[1]}.bam > all_samples_${filter[1]}.targetcoverage.txt\n  \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/buildGATKDictionary", "rmoran7/custom_sarek/BamQC", "rwtaylor/mpcr-analyses-pipelines/Filter_mapping_quality"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "sickle-in-africa/saw.snv-indel", "rmoran7/custom_sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "FastQC", "BCFtools", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "rwtaylor", "steepale", "rmoran7"], "nb_wf": 4, "list_wf": ["saw.structural-variants", "mpcr-analyses-pipelines", "custom_sarek", "wgsfastqtobam"], "list_contrib": ["rwtaylor", "jackmo375", "rmoran7", "waffle-iron", "marcelm", "pallolason", "kusalananda", "viklund", "glormph", "jhagberg", "steepale", "samuell"], "nb_contrib": 12, "codes": ["\nprocess create_fastq {\n    input:\n        set file(bamfile), val(uuid), val(dir) from ch_create_fastq\n    output:\n        set file('fastq.fq.gz'), val(uuid), val(dir) into ch_created_fastq\n\n    tag \"$uuid\"\n\n    executor choose_executor()\n\n    when: 'fermikit' in workflowSteps\n\n    script:\n    \"\"\"\n    samtools bam2fq \"$bamfile\" | gzip - > fastq.fq.gz\n    \"\"\"\n}", "\nprocess fastqc_1 {\n    cache true\n    container \"steepale/fastqc:1.0\"\n    publishDir \"${params.workdir}/results/fastqc_1\", mode: 'copy'\n    if (params.echo) {\n        echo true\n    }\n\n    input:\n    set pair_id, file(read1), file(read2) from read_pairs_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastQCreport\n\n                                            \n    when:\n    !params.skip_fastqc_1\n\n    script:\n    \"\"\"\n    ### Perform fastqc on all fastq files\n    fastqc \\\n    -t 2 \\\n    -q \\\n    ${read1} \\\n    ${read2}\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    memory '16 GB'\n    cpus '1'\n\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess Pileup_call_target {\n  container = '/zstor/containers/singularity/biobase.img'\n  publishDir path:\"${params.publish_directory}/vcfs\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}-${target_name}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set target_name, target_region from targetTasks\n  file(bams) from bwaMappedBams.toList()\n  file(bais) from bamIndexes.toList()\n\n  output:\n  file(\"${params.output_prefix}.${target_name}.target.vcf.gz\") into target_vcfs\n  file(\"${params.output_prefix}.${target_name}.target.vcf.gz.tbi\") into target_vcf_indexes\n\n  \"\"\"\n  set -e -o pipefail\n  mkdir -p temp\n  /usr/local/bin/bcftools mpileup -r ${target_region} -a INFO/AD,FORMAT/AD,FORMAT/DP -Ou --max-depth 100000 -f ${params.reference} ${bams} |\\\n   bcftools call -Ou -m | bcftools sort --temp-dir temp -Oz -o ${params.output_prefix}.${target_name}.target.vcf.gz\n   tabix -p vcf ${params.output_prefix}.${target_name}.target.vcf.gz\n  \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.structural-variants/create_fastq", "steepale/wgsfastqtobam/fastqc_1", "rmoran7/custom_sarek/HaplotypeCaller", "rwtaylor/mpcr-analyses-pipelines/Pileup_call_target"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "sickle-in-africa/saw.structural-variants", "steepale/wgsfastqtobam", "rmoran7/custom_sarek"]}, {"nb_reuse": 4, "tools": ["BCFtools", "FreeBayes", "MultiQC", "VCFtools"], "nb_own": 4, "list_own": ["rwtaylor", "silastittes", "steepale", "rmoran7"], "nb_wf": 4, "list_wf": ["custom_sarek", "panand_structure", "nextflow-pipelines", "wgsfastqtobam"], "list_contrib": ["silastittes", "rwtaylor", "steepale", "rmoran7"], "nb_contrib": 4, "codes": ["\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n\n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess filter {\n\n    publishDir \"$baseDir/data/vcf/filtered/\"\n\n    input:\n    tuple val(prefix), val(vcf) from vcf_ch\n\n    output:\n    tuple val(prefix), file(\"${prefix}_filtered.vcf.gz\") into filter_ch\n\n    \"\"\"\n    bcftools filter -i \"INFO/DP >= ${params.depth_low} && INFO/DP <= ${params.depth_high} && QUAL >= 100\" $baseDir/$vcf | gzip > ${prefix}_filtered.vcf.gz \n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess FilterVCF {\n  publishDir \"${params.publish_dir}/${prefix}\", mode: 'copy'\n  tag { prefix + \"-snp-\" + filterset.filter_name }\n  cpus 2\n  memory 8.GB\n  time 6.h\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 7\n  maxErrors '-1'\n\n  input:\n  set prefix, file(vcf) from input_vcfs\n  each filterset from params.filtersets\n  \n  output:\n  set val(\"${prefix}-snp-${filterset.filter_name}\"), file(\"*.vcf.gz\"), file(\"*.tbi\"), file(\"*.gbi\"), prefix into filtered_vcfs\n\n  script:\n  removeindividuals = params.excludesamples.collect{\"--remove-indv $it\"}.join(\" \")\n\n  \"\"\"\n  set -e -o pipefail\n  mkdir -p temp\n  \n  # Rename samples so that none have \"_\" because this borks PLINK\n  /usr/local/bin/vcftools --gzvcf ${vcf} --recode --recode-INFO-all --remove-indels ${removeindividuals}\\\n  --minQ ${filterset.minqual} --minGQ ${filterset.mingq} --hwe ${filterset.hwe} \\\n  --maf ${filterset.maf} --mac ${filterset.mac} --max-alleles ${filterset.maxa} \\\n  --stdout | vcf-sort --temporary-directory temp | bgzip -@ ${task.cpus} > ${prefix}-snp-${filterset.filter_name}.vcf.gz\n  tabix -p vcf ${prefix}-snp-${filterset.filter_name}.vcf.gz\n  grabix index $vcf\n  \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/FreebayesSingle", "silastittes/panand_structure/filter", "steepale/wgsfastqtobam/multiqc", "rwtaylor/nextflow-pipelines/FilterVCF"], "list_wf_names": ["rmoran7/custom_sarek", "steepale/wgsfastqtobam", "silastittes/panand_structure", "rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 4, "tools": ["BCFtools", "FreeBayes", "VCFtools"], "nb_own": 4, "list_own": ["stevekm", "rwtaylor", "silastittes", "rmoran7"], "nb_wf": 4, "list_wf": ["MuTect2_target_chunking", "panand_structure", "custom_sarek", "nextflow-pipelines"], "list_contrib": ["silastittes", "rwtaylor", "stevekm", "rmoran7"], "nb_contrib": 4, "codes": ["\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess beagle {\n\n    publishDir \"$baseDir/data/beagle/${prefix}_scaffolds\"\n\n    input:\n    tuple val(prefix), file(vcflist) from vcfind_ch\n    each scaffold from scaf_ch\n\n    output:\n    tuple val(prefix), file(\"${prefix}_${scaffold}_filtered_dropIND${params.indfilter}.beagle.gz\") into beagle_ch\n\n    \"\"\"\n    vcftools --gzvcf ${vcflist} --BEAGLE-PL --stdout --chr $scaffold | gzip -c > ${prefix}_${scaffold}_filtered_dropIND${params.indfilter}.beagle.gz\n    \"\"\"\n}", "\nprocess mutect2_chromChunk {\n    tag \"${prefix}\"\n    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n                \n\n    input:\n    set val(label), val(chrom), val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(\"targets.bed\"), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_chromChunk_ch\n\n    output:\n    set val(\"${label}\"), val(chrom), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_chromChunk\n    file(\"${output_vcf}\")\n    file(\"${multiallelics_stats}\")\n    file(\"${realign_stats}\")\n\n    when: params.disable != \"true\"\n\n    script:\n                                           \n    prefix = \"${comparisonID}.${label}.${chrom}\"\n    output_vcf = \"${prefix}.vcf\"\n    output_norm_vcf = \"${prefix}.norm.vcf\"\n    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n    \"\"\"\n    gatk.sh -T MuTect2 \\\n    -dt NONE \\\n    --logging_level WARN \\\n    --standard_min_confidence_threshold_for_calling 30 \\\n    --max_alt_alleles_in_normal_count 10 \\\n    --max_alt_allele_in_normal_fraction 0.05 \\\n    --max_alt_alleles_in_normal_qscore_sum 40 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --cosmic \"${cosmic_ref_vcf}\" \\\n    --intervals \"targets.bed\" \\\n    --interval_padding 10 \\\n    --input_file:tumor \"${tumorBam}\" \\\n    --input_file:normal \"${normalBam}\" \\\n    --out \"${output_vcf}\"\n\n    # normalize and split vcf entries\n    cat ${output_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n    \"${output_norm_vcf}\"\n    \"\"\"\n}", "\nprocess RenameChromosomes {\n  publishDir \"${params.publish_dir}/${output_folder}/chrenamed\", mode: 'copy'\n  tag { prefix }\n  cpus 2\n  memory 8.GB\n  time 6.h\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 7\n  maxErrors '-1'\n\n  input:\n  set prefix, file(vcf), file(tbindex), file(gbindex), output_folder from vcfs_to_rename\n  \n  output:\n  set val(\"${prefix}-chrenamed\"), file(\"*.vcf.gz\"), file(\"*.tbi\"), file(\"*.gbi\"), output_folder into renamed_vcfs\n\n  \"\"\"\n  set -e -o pipefail\n  mkdir -p temp\n  # 1) Get chromosome IDs, and output chromosome plus renamed chromosome to text file\n  #    This strips all non-numeric characters from the chromosome ID. May cause issues if non-numeric characters\n  #    are important for name uniquness...\n  set -e\n  zcat $vcf | grep -oP '^##contig=<ID=.*' | \\\n    sed -e 's/^##contig=<ID=\\\\(.*\\\\),.*/\\\\1/gm' | \\\n    awk '{ printf \\$1 \" \"; gsub(/[A-Z_.]/,\"\", \\$1); print 0\\$1}' \\\n    > scaffs.txt\n  # 2) Use BCFtools to rename chromosomes in VCF\n  bcftools annotate --rename-chrs scaffs.txt $vcf | vcf-sort --chromosomal-order --temporary-directory temp | bgzip -@ ${task.cpus} > ${prefix}-chrenamed.vcf.gz\n  tabix -p vcf ${prefix}-chrenamed.vcf.gz\n  grabix index ${prefix}-chrenamed.vcf.gz\n  \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/FreeBayes", "silastittes/panand_structure/beagle", "stevekm/MuTect2_target_chunking/mutect2_chromChunk", "rwtaylor/nextflow-pipelines/RenameChromosomes"], "list_wf_names": ["stevekm/MuTect2_target_chunking", "rmoran7/custom_sarek", "silastittes/panand_structure", "rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 4, "tools": ["SAMtools", "MultiQC", "Bowtie", "SyConn", "GATK"], "nb_own": 4, "list_own": ["rwtaylor", "stevekm", "simozhou", "rmoran7"], "nb_wf": 4, "list_wf": ["custom_sarek", "nextflow-demos", "nextflow-pipelines", "epi-awesome"], "list_contrib": ["rwtaylor", "simozhou", "rmoran7", "pditommaso", "stevekm"], "nb_contrib": 5, "codes": ["\nprocess concat_dbs {\n    publishDir \"${params.output_dir}\", overwrite: true\n\n    input:\n    file(all_dbs: \"db?\") from sample_dbs.collect()\n\n    output:\n    file(\"${output_sqlite}\") into plots_db\n\n    script:\n    output_sqlite = \"plots.sqlite\"\n    \"\"\"\n    python - ${all_dbs} <<E0F\nimport sys\nimport sqlite3\n\ndbs = sys.argv[1:]\n\n# setup new output db\noutput_db = \"${output_sqlite}\"\nconn = sqlite3.connect(output_db)\nconn.execute(\"CREATE TABLE TBL1(sampleID text, x text, y text, a text, baseplot blob, ggplot blob)\")\n\n# add each input db to the output db\nfor db in dbs:\n    conn.execute('''ATTACH '{0}' as dba'''.format(db))\n    conn.execute(\"BEGIN\")\n    for row in conn.execute('''SELECT * FROM dba.sqlite_master WHERE type=\"table\"'''):\n        combine_sql = \"INSERT INTO \"+ row[1] + \" SELECT * FROM dba.\" + row[1]\n        conn.execute(combine_sql)\n    conn.commit()\n    conn.execute(\"detach database dba\")\nE0F\n    \"\"\"\n}", "\nprocess bowtie2 {\n    tag \"$name\"\n    publishDir \"${params.outdir}/alignment\", mode: 'copy'\n\n    input:\n    file fasta from fasta\n    set val(name), file(reads) from read_files_trimming\n    file index from genome_index.collect()\n\n    output:\n    file '*.bam' into bowtie_output\n\n    script:\n    prefix=reads[0].toString() - ~/(.R1)?(_1)?(_R1)?(_trimmed)?(_val_1)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n    \"\"\"\n    bowtie2 -x genome.index -U $reads -p ${params.max_cpus} -S ${prefix}.sam\n    samtools view -bT ${fasta} -@ ${params.max_cpus} -o ${prefix}.bam ${prefix}.sam\n    rm ${prefix}.sam\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess SampleMultiQC {\n\n  publishDir \"${params.publish_directory}\", mode: 'copy', overwrite: true\n\n  cpus 1\n  memory 2.GB\n\n  input:\n  file ('fastqc/*') from fastqc_results\n\n  output:\n  file(qc_report) into multiqc_output\n\n  \"\"\"\n  /usr/local/bin/multiqc -f -o qc-report fastqc/.\n  \"\"\"\n}"], "list_proc": ["stevekm/nextflow-demos/concat_dbs", "simozhou/epi-awesome/bowtie2", "rmoran7/custom_sarek/MergeMutect2Stats", "rwtaylor/nextflow-pipelines/SampleMultiQC"], "list_wf_names": ["rmoran7/custom_sarek", "rwtaylor/nextflow-pipelines", "simozhou/epi-awesome", "stevekm/nextflow-demos"]}, {"nb_reuse": 4, "tools": ["SAMtools", "VCFtools"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess Vcftools {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfVCFtools\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\") into vcftoolsReport\n\n    when: !('vcftools' in skipQC)\n\n    script:\n    \"\"\"\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/Vcftools", "rmoran7/custom_sarek/Mpileup", "rmoran7/dx_sarek/Vcftools", "sickle-in-africa/saw.sarek/Vcftools"], "list_wf_names": ["rmoran7/custom_sarek", "sripaladugu/germline_somatic", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 7, "tools": ["BWA", "SAMtools", "FREEC", "Picard", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "sripaladugu", "rnharmening", "rmoran7"], "nb_wf": 4, "list_wf": ["custom_sarek", "saw.sarek", "nf-multipleReferenceMapper", "nextflow_align"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "rnharmening", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "sripaladugu", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess RecalibrateIndelQualityScores {\n    label 'process_low'\n    tag \"${variantCaller}-${idSample}\"\n\n    publishDir \"${params.outdir}/recalibratedVariants/${idSample}/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(variantCaller), val(idSample), path(vcf), path(vcfIndex)\n        tuple path(recalTable), path(tranches)\n\n    script:\n        \"\"\"\n        gatk --java-options \"-Xmx5g -Xms5g\" \\\n            ApplyVQSR \\\n            -V ${vcf} \\\n            --recal-file ${recalTable} \\\n            --tranches-file ${tranches} \\\n            --truth-sensitivity-filter-level 99.7 \\\n            --create-output-variant-index true \\\n            -mode INDEL \\\n            -O ${idSample}_indel.recalibrated.vcf.gz\n        \"\"\"\n\n}", "\nprocess GetVariantRecalibrationReport {\n    label 'cpus_1'\n                               \n\n    tag \"${variantCaller}-${idSample}\"\n\n    input:\n        tuple val(variantCaller), val(idSample), file(vcf)\n        file(fasta)\n        file(dict)\n        file(fastaFai)\n        file(dbsnp)\n        file(dbsnpIndex)\n        path hapmap\n        path hapmap_index\n        path onekgSnps\n        path onekgSnpsIndex\n        path onekgIndels\n        path onekgIndelsIndex\n        path onekgOmni\n        path onekgOmniIndex\n\n\n    output:\n        path \"${variantCaller}.${idSample}.recal\"\n\n    script:\n        \"\"\"\n        gatk IndexFeatureFile -I ${vcf}\n\n        gatk VariantRecalibrator \\\n            -R ${fasta} \\\n            -V ${vcf} \\\n            --resource:hapmap,known=false,training=true,truth=true,prior=15.0 ${hapmap} \\\n            --resource:1000G,known=false,training=true,truth=false,prior=10.0 ${onekgSnps} \\\n            --resource:1000G,known=false,training=true,truth=false,prior=10.0 ${onekgIndels} \\\n            --resource:omni,known=false,training=true,truth=false,prior=12.0 ${onekgOmni} \\\n            --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 ${dbsnp} \\\n            -an QD \\\n            -an MQ \\\n            -an MQRankSum \\\n            -an ReadPosRankSum \\\n            -an FS \\\n            -an SOR \\\n            -an InbreedingCoeff \\\n            -mode BOTH \\\n            -O ${variantCaller}.${idSample}.recal \\\n            --tranches-file ${variantCaller}.${idSample}.tranches \\\n            --rscript-file ${variantCaller}.${idSample}.plots.R\n        \"\"\"\n\n}", "\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n    contamination_adjustment = params.cf_contamination_adjustment ? \"contaminationAdjustment = TRUE\" : \"\"\n    contamination_value = params.cf_contamination ? \"contamination = ${params.cf_contamination}\" : \"\"\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"${contamination_adjustment}\" >> ${config}\n    echo \"${contamination_value}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess bamsorter {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(samfile) from sam_files\n    \n    output:\n    set sample_id, file(\"${sample_id}_sorted_aln.bam\") into sorted_bam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    picard -Xmx16g AddOrReplaceReadGroups \\\n           INPUT=${samfile} OUTPUT=${sample_id}_sorted_aln.bam SORT_ORDER=coordinate \\\n           RGID=${sample_id}-id RGLB=${sample_id}-lib RGPL=ILLUMINA RGPU=${sample_id}-01 RGSM=${sample_id}\n    \"\"\"\n}", "\nprocess bamindexer {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(sorted_bamfile) from sorted_bam_files\n    \n    output:\n    set sample_id, file(\"${sample_id}_sorted_aln.bam.bai\") into indexed_bam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    samtools index ${sorted_bamfile} \"${sample_id}_sorted_aln.bam.bai\"\n    \"\"\"\n}", "\nprocess bwa_index {\n  input:\n    file fasta from ref_ch\n  output:\n    set fasta_id, file(index) into bwa_index_ch \n  \n  script:\n  index = \"BWA_index\"\n  fasta_id = fasta.baseName\n  \"\"\"\n    bwa index $fasta\n    mkdir ${index} && mv ${fasta}* ${index}\n  \"\"\"\n}", "\nprocess bwamem {\n  tag \"$sample_id\"\n\n  publishDir \"${params.outdir}/BWA_mapping\", mode: 'copy'\n\n  input:\n    set read_id, file(reads), val(ref_id), file(bwa_index) from derep_reads_ch.combine(bwa_index_ch)  \n\n  output:\n    set sample_id, file(bam_file) into mapping_pair_ch \n    file bam_file\n    file \"*.bai\"\n  \n  script:\n    sample_id = \"${read_id}_v_${ref_id}\"\n    bam_file  = \"${sample_id}.sorted.bam\"\n    filter = params.keepOnlyMapped ? \"-F 4\" : \"\" \n  \"\"\"\n  bwa mem -t ${task.cpus} -o tmp.sam ${bwa_index}/${ref_id}.fa ${reads[0]} ${reads[1]} \n  samtools view $filter -b -@ ${task.cpus} -o tmp.bam tmp.sam\n  samtools sort -@ ${task.cpus} -o ${bam_file} tmp.bam\n  samtools index -@ ${task.cpus}  ${bam_file}\n  rm tmp.[sb]am\n  \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/RecalibrateIndelQualityScores", "sickle-in-africa/saw.sarek/GetVariantRecalibrationReport", "rmoran7/custom_sarek/ControlFREECSingle", "sripaladugu/nextflow_align/bamsorter", "sripaladugu/nextflow_align/bamindexer", "rnharmening/nf-multipleReferenceMapper/bwa_index", "rnharmening/nf-multipleReferenceMapper/bwamem"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/nextflow_align", "rmoran7/custom_sarek", "rnharmening/nf-multipleReferenceMapper"]}, {"nb_reuse": 4, "tools": ["GATK", "Bowtie", "MultiQC", "BCFtools"], "nb_own": 4, "list_own": ["sickle-in-africa", "rnharmening", "ssun1116", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "saw.snv-indel", "nf-multipleReferenceMapper"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "rnharmening", "kingzhuky", "juneb4869"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n  tag \"MultiQC\"\n\n  echo =false\n  publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n  input:\n    file fastqc from fastqc_results_ch.collect().ifEmpty([])\n    file qualimap from qualimap_results_ch.collect().ifEmpty([])\n  \n  output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*.html\"\n  \n  script:\n  \"\"\"\n    multiqc -f ./\n  \"\"\"\n}", "\nprocess getDepthOfCoverageReport {\n    beforeScript \"source ${params.processConfigFile}\"\n    container params.gatk4Image\n    clusterOptions = params.clusterOptions\n    queue = params.serverOptions['queue']\n    time =  params.serverOptions['time']\n\n    input:\n    tuple val(bamBase), path(bamFile)\n\n    script:\n    \"\"\"\n    mkdir -p ${params.outputDir}quality-reports/aligned/\n    gatk \\\n        DepthOfCoverage \\\n        -R ${params.referenceSequence['path']} \\\n        -O ${params.outputDir}quality-reports/alined/${bamBase}.depthofcov \\\n        -I ${bamFile} \\\n        -L []\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}", " process MakeTophat2Index {\n        label 'build_index'\n        tag \"tophat2_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\": params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n\n        output:\n        file \"Tophat2Index/*\" into tophat2_index\n\n        when:\n        aligner == \"tophat2\"\n\n        script:\n        tophat2_index = \"Tophat2Index/\" + fasta.baseName.toString()\n        \"\"\"\n        mkdir Tophat2Index\n        ln $fasta Tophat2Index\n        bowtie2-build -f $fasta $tophat2_index\n        \"\"\"\n    }"], "list_proc": ["rnharmening/nf-multipleReferenceMapper/multiqc", "sickle-in-africa/saw.snv-indel/getDepthOfCoverageReport", "rmoran7/custom_sarek/BcftoolsStats", "ssun1116/meripseqpipe/MakeTophat2Index"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/custom_sarek", "rnharmening/nf-multipleReferenceMapper"]}, {"nb_reuse": 4, "tools": ["SAMtools", "MultiQC", "FastQC", "preseq", "GATK"], "nb_own": 4, "list_own": ["sickle-in-africa", "ssun1116", "robinfchan", "rmoran7"], "nb_wf": 4, "list_wf": ["meripseqpipe", "custom_sarek", "bisulfite_align_nf", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "kingzhuky", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "ssun1116", "lescai", "szilvajuhos", "FriederikeHanssen", "juneb4869", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "robinfchan", "adrlar"], "nb_contrib": 24, "codes": [" process preseq {\n        if (params.custom_container) container \"${params.custom_container}\"\n\n        tag \"$name\"\n        publishDir \"${params.outdir}/preseq\", mode: 'copy', overwrite: true\n\n        input:\n        set val(name), file(bam) from ch_bam_for_preseq\n\n        output:\n        file \"${bam.baseName}.ccurve.txt\" into preseq_results\n\n        script:\n        def avail_mem = task.memory ? ((task.memory.toGiga() - 6) / task.cpus).trunc() : false\n        def sort_mem = avail_mem && avail_mem > 2 ? \"-m ${avail_mem}G\" : ''\n        \"\"\"\n        samtools sort $bam \\\\\n            -@ ${task.cpus} $sort_mem \\\\\n            -o ${bam.baseName}.sorted.bam\n        preseq lc_extrap -v -B ${bam.baseName}.sorted.bam -o ${bam.baseName}.ccurve.txt\n        \"\"\"\n    }", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess Fastqc{\n    tag \"$sample_name\"\n    publishDir path: { params.skip_fastqc ? params.outdir : \"${params.outdir}/QC\" },\n             saveAs: { params.skip_fastqc ? null : it }, mode: 'link'\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from fastqc_reads\n\n    output:\n    file \"fastqc/*\" into fastqc_results\n\n    when:\n    aligner != \"none\" && !params.skip_fastqc\n\n    shell:\n    skip_fastqc = params.skip_fastqc\n    if ( reads_single_end){\n        \"\"\"\n        mkdir fastqc\n        fastqc -o fastqc --noextract ${reads}\n        \"\"\"       \n    } else {\n        \"\"\"\n        mkdir fastqc   \n        fastqc -o fastqc --noextract ${reads[0]}\n        fastqc -o fastqc --noextract ${reads[1]}\n        \"\"\"      \n    }\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}"], "list_proc": ["robinfchan/bisulfite_align_nf/preseq", "sickle-in-africa/saw.sarek/BuildDict", "ssun1116/meripseqpipe/Fastqc", "rmoran7/custom_sarek/MultiQC"], "list_wf_names": ["sickle-in-africa/saw.sarek", "robinfchan/bisulfite_align_nf", "ssun1116/meripseqpipe", "rmoran7/custom_sarek"]}, {"nb_reuse": 4, "tools": ["SAMtools", "snpEff", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "custom_sarek", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}"], "list_proc": ["rmoran7/custom_sarek/BuildCache_snpEff", "sripaladugu/germline_somatic/IndexBamMergedForSentieon", "rmoran7/dx_sarek/IndexBamMergedForSentieon", "sickle-in-africa/saw.sarek/BuildDict"], "list_wf_names": ["rmoran7/custom_sarek", "sripaladugu/germline_somatic", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 4, "tools": ["BCFtools", "BWA", "SAMtools", "QualiMap", "FreeBayes", "MultiQC", "TIDDIT", "FREEC", "FastQC", "MSIsensor", "snpEff", "GATK", "VCFtools"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 4, "list_wf": ["germline_somatic", "saw.snv-indel", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess alignReadsToReference {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.bwaImage\n\n    input:\n    tuple val(name), path(readsFilePair)\n\n    output:\n    tuple val(name), path(\"${name}.bam\")\n\n    script:\n    \"\"\"\n    bwa mem \\\n        -t ${task.cpus} \\\n        -R \\\"@RG\\\\tID:${params.runId}\\\\tPU:${params.runId}\\\\tSM:${name}\\\\tLB:${name}\\\\tPL:${params.sequencingPlatform}\\\" \\\n        ${params.referenceSequence['dir']}/bwa.${params.referenceSequence['label']} \\\n        ${readsFilePair[0]} ${readsFilePair[1]} | \\\n            samtools view \\\n                -b - | \\\n                samtools sort \\\n                    -@ ${task.cpus} \\\n                    -o ${name}.bam\n    \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf('.csv') > 0) filename\n                      else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file 'software_versions.csv'\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/alignReadsToReference", "rmoran7/dx_sarek/get_software_versions", "sickle-in-africa/saw.sarek/IndexBamRecal", "sripaladugu/germline_somatic/BaseRecalibrator"], "list_wf_names": ["sripaladugu/germline_somatic", "sickle-in-africa/saw.snv-indel", "sickle-in-africa/saw.sarek", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["QualiMap", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 3, "list_wf": ["germline_somatic", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/ApplyBQSR", "rmoran7/dx_sarek/BuildDict", "sickle-in-africa/saw.sarek/BamQC"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 3, "list_wf": ["germline_somatic", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/IndexBamRecal", "rmoran7/dx_sarek/BuildFastaFai", "sickle-in-africa/saw.sarek/GenotypeGVCFs"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["FastQC", "MultiQC", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 3, "list_wf": ["germline_somatic", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess GenotypeGVCFs {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(intervalBed), file(gvcf) from gvcfGenotypeGVCFs\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n    set val(\"HaplotypeCaller\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        IndexFeatureFile \\\n        -I ${gvcf}\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -V ${gvcf} \\\n        -O ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/GenotypeGVCFs", "sickle-in-africa/saw.sarek/MultiQC", "rmoran7/dx_sarek/FastQCBAM"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["SAMtools", "SAMBLASTER", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 3, "list_wf": ["germline_somatic", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess MarkDuplicatesInSampleReadGroup {\n    label 'cpus_16'\n    label 'withGatkContainer'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.bam\")\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\")\n        tuple val(idPatient), val(idSample)\n        file (\"${idSample}.bam.metrics\") optional true\n\n    when: !(params.skip_markduplicates)\n\n    script:\n                                                                                                                                                                                    \n    markdup_java_options = params.markdup_java_options\n    metrics = 'markduplicates' in getInputSkipQC() ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}", "\nprocess Mutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamMutect2\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n        file(pon) from ch_pon\n        file(ponIndex) from ch_pon_tbi\n\n    output:\n        set val(\"Mutect2\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into mutect2Output\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\") optional true into intervalStatsFiles\n        set idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") optional true into mutect2Stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.pon ? \"--panel-of-normals ${pon}\" : \"\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    softClippedOption = params.ignore_soft_clipped_bases ? \"--dont-use-soft-clipped-bases true\" : \"\"\n    \"\"\"\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      ${intervalsOptions} \\\n      ${softClippedOption} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/GroupReadsByUmi", "sickle-in-africa/saw.sarek/MarkDuplicatesInSampleReadGroup", "sripaladugu/germline_somatic/Mutect2"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 3, "list_wf": ["germline_somatic", "saw.sarek", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}", "\nprocess PileupSummariesForMutect2 {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(bamNormal), file(baiNormal), file(bamTumor), file(baiTumor), file(intervalBed), file(statsFile) from pairBamPileupSummaries\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\") into pileupSummaries\n\n    when: 'mutect2' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        GetPileupSummaries \\\n        -I ${bamTumor} \\\n        -V ${germlineResource} \\\n        ${intervalsOptions} \\\n        -O ${intervalBed.baseName}_${idSampleTumor}_pileupsummaries.table\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/MapReads", "sickle-in-africa/saw.sarek/PileupSummariesForMutect2", "sripaladugu/germline_somatic/PileupSummariesForMutect2"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["SAMtools", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "sripaladugu", "rmoran7"], "nb_wf": 3, "list_wf": ["saw.sarek", "dx_sarek", "nextflow_align"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "sripaladugu", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 21, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_4'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess GetVariantRecalibrationReport {\n    label 'cpus_1'\n                               \n\n    tag \"${variantCaller}-${idSample}\"\n\n    input:\n        tuple val(variantCaller), val(idSample), file(vcf)\n        file(fasta)\n        file(dict)\n        file(fastaFai)\n        file(dbsnp)\n        file(dbsnpIndex)\n        path hapmap\n        path hapmap_index\n        path onekgSnps\n        path onekgSnpsIndex\n        path onekgIndels\n        path onekgIndelsIndex\n        path onekgOmni\n        path onekgOmniIndex\n\n\n    output:\n        path \"${variantCaller}.${idSample}.recal\"\n\n    script:\n        \"\"\"\n        gatk IndexFeatureFile -I ${vcf}\n\n        gatk VariantRecalibrator \\\n            -R ${fasta} \\\n            -V ${vcf} \\\n            --resource:hapmap,known=false,training=true,truth=true,prior=15.0 ${hapmap} \\\n            --resource:1000G,known=false,training=true,truth=false,prior=10.0 ${onekgSnps} \\\n            --resource:1000G,known=false,training=true,truth=false,prior=10.0 ${onekgIndels} \\\n            --resource:omni,known=false,training=true,truth=false,prior=12.0 ${onekgOmni} \\\n            --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 ${dbsnp} \\\n            -an QD \\\n            -an MQ \\\n            -an MQRankSum \\\n            -an ReadPosRankSum \\\n            -an FS \\\n            -an SOR \\\n            -an InbreedingCoeff \\\n            -mode BOTH \\\n            -O ${variantCaller}.${idSample}.recal \\\n            --tranches-file ${variantCaller}.${idSample}.tranches \\\n            --rscript-file ${variantCaller}.${idSample}.plots.R\n        \"\"\"\n\n}", "\nprocess bamindexer {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(sorted_bamfile) from sorted_bam_files\n    \n    output:\n    set sample_id, file(\"${sample_id}_sorted_aln.bam.bai\") into indexed_bam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    samtools index ${sorted_bamfile} \"${sample_id}_sorted_aln.bam.bai\"\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/ApplyBQSR", "sickle-in-africa/saw.sarek/GetVariantRecalibrationReport", "sripaladugu/nextflow_align/bamindexer"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/nextflow_align", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["SAMtools", "STAR", "BCFtools"], "nb_own": 3, "list_own": ["sickle-in-africa", "ssun1116", "rmoran7"], "nb_wf": 3, "list_wf": ["meripseqpipe", "saw.snv-indel", "dx_sarek"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "juneb4869"], "nb_contrib": 5, "codes": [" process MakeStarIndex {\n        label 'build_index'\n        tag \"star_index\"\n        publishDir path: { params.saveReference ? \"${params.outdir}/Genome/\" : params.outdir },\n                   saveAs: { params.saveReference ? it : null }, mode: 'copy'\n        input:\n        file fasta\n        file gtf\n\n        output:\n        file \"StarIndex\" into star_index\n\n        when:\n        aligner == \"star\"\n\n        script:\n        readLength = 101\n        overhang = readLength - 1\n        \"\"\"\n        mkdir StarIndex\n        STAR --runThreadN ${task.cpus} \\\n        --runMode genomeGenerate \\\n        --genomeDir StarIndex \\\n        --genomeFastaFiles $fasta \\\n        --sjdbGTFfile $gtf \\\n        --sjdbOverhang $overhang \n        \"\"\"\n    }", "\nprocess IndexBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") from bam_recalibrated_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated_indexed\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_no_int_qc\n        set idPatient, idSample into tsv_bam_recalibrated_no_int\n\n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess sortTruthVcfFile {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n\tcontainer params.bcftoolsImage\n\n\tinput:\n\ttuple path(simulationInputs), path(mutatedReference), path(truthVcfFile)\n\n\toutput:\n\ttuple path(simulationInputs), path(mutatedReference)\n\n\tscript:\n\t\"\"\"\n\tmkdir -p ${params.variantSetsDir}\n\tbcftools sort \\\n\t\t${truthVcfFile} \\\n\t\t-o ${params.variantSetsDir}/${params.cohortId}.truth.g.vcf\n\t\"\"\"\n}"], "list_proc": ["ssun1116/meripseqpipe/MakeStarIndex", "rmoran7/dx_sarek/IndexBamRecal", "sickle-in-africa/saw.snv-indel/sortTruthVcfFile"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["FastQC", "QualiMap", "TopHat"], "nb_own": 3, "list_own": ["sickle-in-africa", "ssun1116", "rmoran7"], "nb_wf": 3, "list_wf": ["meripseqpipe", "saw.snv-indel", "dx_sarek"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "juneb4869"], "nb_contrib": 5, "codes": ["\nprocess getFastaQualityReport {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.fastqcImage\n\n\tinput:\n\ttuple val(name), file(reads)\n\n    script:\n    \"\"\"\n    mkdir -p ${params.qualityReportsDir}/trimmed\n    fastqc \\\n    \t-t ${task.cpus} \\\n    \t-o ${params.qualityReportsDir}/trimmed \\\n    \t${reads[0]} ${reads[1]}\n    \"\"\"\n}", "\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}", "\nprocess Tophat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/tophat2\", mode: 'link', overwrite: true\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from tophat2_reads\n    file index from tophat2_index.collect()\n    file gtf\n\n    output:\n    set val(sample_id), file(\"*_tophat2.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into tophat2_bam\n    file \"*_log.txt\" into tophat2_log\n    \n    when:\n    aligner == \"tophat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.rev)?(\\.\\d)?(\\.fa)?(\\.bt2)?$/\n    strand_info = params.stranded == \"no\" ? \"fr-unstranded\" : params.stranded == \"reverse\" ? \"fr-secondstrand\" : \"fr-firststrand\"\n    if (reads_single_end) {\n        \"\"\"\n        tophat  -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                $reads > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        tophat -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                ${reads[0]} ${reads[1]} > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    }\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/getFastaQualityReport", "rmoran7/dx_sarek/BamQC", "ssun1116/meripseqpipe/Tophat2Align"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FreeBayes", "GATK"], "nb_own": 3, "list_own": ["sickle-in-africa", "ssun1116", "rmoran7"], "nb_wf": 3, "list_wf": ["meripseqpipe", "saw.snv-indel", "dx_sarek"], "list_contrib": ["ssun1116", "jackmo375", "rmoran7", "kingzhuky", "juneb4869"], "nb_contrib": 5, "codes": ["\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess callVariantsForEachSample {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.gatk4Image\n\n\tinput:\n\ttuple val(bamId), path(bamFile), path(bamIndex)\n\n\tscript:\n\t\"\"\"\n    mkdir -p ${params.variantSetsDir}\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n\t\tHaplotypeCaller \\\n\t\t-R ${params.referenceSequence['path']} \\\n\t\t-I ${bamFile} \\\n\t\t-O ${params.variantSetsDir}/${bamId}.g.vcf \\\n\t\t--lenient true \\\n\t\t-ERC GVCF\n\t\"\"\"\n}", "\nprocess CreateIGVjs {\n    publishDir \"${params.outdir}/Report\" , mode: 'link', overwrite: true,\n        saveAs: {filename ->\n                 if (filename.indexOf(\".html\") > 0)  \"Igv_js/$filename\"\n                 else if (filename.indexOf(\".pdf\") > 0)  \"Igv_js/$filename\"\n                 else \"Igv_js/$filename\"\n        }        \n    input:\n    file m6APipe_result from m6APipe_result\n    file fasta \n    file gtf\n    file formatted_designfile from formatted_designfile.collect()\n    file group_bed from group_merged_bed.collect()\n    file all_bed from all_merged_bed.collect()\n    file bedgraph from bedgraph_for_igv.collect()\n    \n    output:\n    file \"*\" into igv_js\n\n    script:    \n    igv_fasta = fasta.baseName.toString() + \".igv.fa\"\n    igv_gtf = gtf.baseName.toString() + \".igv.gtf\"\n    merged_allpeaks_igvfile = all_bed.baseName.toString() + \".igv.bed\"\n    \"\"\"\n    ls -l $fasta | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_fasta'\"}' | bash\n    ls -l $gtf | awk -F \"> \" '{print \"ln -s \"\\$2\" ./'$igv_gtf'\"}' | bash\n    ls -l $m6APipe_result | awk '{print \"ln -s \"\\$11\" initial.m6APipe\"}' | bash\n    ls -l $group_bed $all_bed | awk '{sub(\".bed\\$\",\".igv.bed\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    ls -l $bedgraph | awk '{sub(\".bedgraph\\$\",\".igv.bedgraph\",\\$9);print \"ln -s \"\\$11,\\$9}' | bash\n    samtools faidx $igv_fasta\n    bash $baseDir/bin/create_IGV_js.sh $igv_fasta $igv_gtf $merged_allpeaks_igvfile $formatted_designfile\n    \"\"\"\n}"], "list_proc": ["rmoran7/dx_sarek/FreebayesSingle", "sickle-in-africa/saw.snv-indel/callVariantsForEachSample", "ssun1116/meripseqpipe/CreateIGVjs"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "ssun1116/meripseqpipe", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["GATK", "BWA", "BEDTools"], "nb_own": 3, "list_own": ["sickle-in-africa", "steepale", "rmoran7"], "nb_wf": 3, "list_wf": ["saw.structural-variants", "dx_sarek", "wgsfastqtobam"], "list_contrib": ["jackmo375", "rmoran7", "waffle-iron", "marcelm", "pallolason", "kusalananda", "viklund", "glormph", "jhagberg", "steepale", "samuell"], "nb_contrib": 11, "codes": ["\nprocess cohort_mask_vcfs {\n    input:\n        set file(svfile), val(uuid), val(dir) from ch_cohort_mask_in\n    output:\n        set file('*_cohort_masked.vcf'), val(uuid), val(dir) into ch_cohort_masked_vcfs\n\n    tag \"$uuid $svfile\"\n\n    executor choose_executor()\n    when 'mask_cohort' in workflowSteps\n\n    \"\"\"\n    BNAME=\\$( echo $svfile | cut -d. -f1 )\n    MASK_FILE=\\${BNAME}_cohort_masked.vcf\n    MASK_DIR=$params.mask_cohort_dir\n\n    cp $svfile workfile\n    for mask in \\$MASK_DIR/*; do\n        if [[ ! -f \"\\$mask\" ]]; then\n            continue\n        fi\n        cat workfile \\\n            | bedtools intersect -header -v -a stdin -b \"\\$mask\" -f \"$params.sg_mask_ovlp\" \"$reciprocal\" \\\n            > tempfile\n        mv tempfile workfile\n    done\n    mv workfile \"\\$MASK_FILE\"\n    \"\"\"\n}", "\nprocess MergeMutect2Stats {\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(statsFiles), file(vcf) from mutect2Stats                                                   \n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(germlineResource) from ch_germline_resource\n        file(germlineResourceIndex) from ch_germline_resource_tbi\n        file(intervals) from ch_intervals\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.vcf.gz.stats\") into mergedStatsFile\n\n    when: 'mutect2' in tools\n\n    script:\n               stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess bwa {\n    cache true\n    container \"steepale/bwa:1.0\"\n    publishDir \"${params.workdir}/test\", mode: 'copy'\n    if (params.echo) {\n        echo true\n    }\n\n    input:\n    file(read1_paired) from sic_ch1.flatten().filter( ~/.*R1.*_paired_sickle.fastq/ )\n    file(read2_paired) from sic_ch2.flatten().filter( ~/.*R2.*_paired_sickle.fastq/ )\n                                                                    \n    file genome from ref_bwa_ch1\n    file genome_all from ref_bwa_ch2.collect()\n    set val(sample_id), val(barcode), val(lane), val(suffix), val(sample_label), val(type) from manifest_ch1\n\n    output:\n    file \"*.sam\" into bwa_out_ch\n\n                                            \n    when:\n    !params.skip_sickle\n\n    script:\n    \"\"\"\n    echo ${genome}\n    echo ${read1_paired}\n    echo ${read2_paired}\n    echo ${sample_id}\n    echo ${genome_all}\n    bwa mem \\\n    -t 2 \\\n    -T 20 \\\n    ${genome} \\\n    ${read1_paired} \\\n    ${read2_paired} \\\n    > ${sample_id}.sam\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.structural-variants/cohort_mask_vcfs", "rmoran7/dx_sarek/MergeMutect2Stats", "steepale/wgsfastqtobam/bwa"], "list_wf_names": ["sickle-in-africa/saw.structural-variants", "steepale/wgsfastqtobam", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["GATK", "BEDTools", "BCFtools"], "nb_own": 3, "list_own": ["sripaladugu", "stevekm", "rmoran7"], "nb_wf": 3, "list_wf": ["germline_somatic", "nextflow-pipeline-demo", "dx_sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "rmoran7", "lconde-ucl", "davidmasp", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "stevekm", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess pad_bed {\n    publishDir \"${params.output_dir}/targets\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'bedtools/2.26.0'\n\n    input:\n    set file(targets_bed_file), file(ref_chrom_sizes) from targets_bed3.combine(ref_chrom_sizes)\n\n    output:\n    file(\"targets.pad10.bed\") into targets_pad_bed\n\n    script:\n    \"\"\"\n    cat \"${targets_bed_file}\" | LC_ALL=C sort -k1,1 -k2,2n | bedtools slop -g \"${ref_chrom_sizes}\" -b 10 | bedtools merge -d 5 > targets.pad10.bed\n    \"\"\"\n}", "\nprocess BuildDict {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta.baseName}.dict\") into dictBuilt\n\n    when: !(params.dict) && params.fasta && !('annotate' in step) && !('controlfreec' in step)\n\n    script:\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'cpus_1'\n\n    tag \"${variantCaller} - ${vcf}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        set variantCaller, idSample, file(vcf) from vcfBCFtools\n\n    output:\n        file (\"*.bcf.tools.stats.out\") into bcftoolsReport\n\n    when: !('bcftools' in skipQC)\n\n    script:\n    \"\"\"\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/pad_bed", "sripaladugu/germline_somatic/BuildDict", "rmoran7/dx_sarek/BcftoolsStats"], "list_wf_names": ["stevekm/nextflow-pipeline-demo", "sripaladugu/germline_somatic", "rmoran7/dx_sarek"]}, {"nb_reuse": 3, "tools": ["BCFtools", "RTREE", "MultiQC"], "nb_own": 3, "list_own": ["sickle-in-africa", "stevekm", "rmoran7"], "nb_wf": 3, "list_wf": ["nextflow-samplesheet-demo", "saw.snv-indel", "dx_sarek"], "list_contrib": ["stevekm", "jackmo375", "rmoran7"], "nb_contrib": 3, "codes": ["\nprocess make_dir {\n    echo true\n    stageInMode \"copy\"\n\n    input:\n    file(\"*\") from samples_files.collect()\n\n    output:\n    file(\"samples_dir\") into samples_dir\n\n    script:\n    \"\"\"\n    echo \"[make_dir]\"\n    pwd\n    for item in *; do\n        mkdir -p samples_dir\n        mv \"\\${item}\" samples_dir/\n    done\n    tree\n    \"\"\"\n}", "\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}", "\nprocess sortTruthVcfFile {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n\tcontainer params.bcftoolsImage\n\n\tinput:\n\ttuple path(simulationInputs), path(mutatedReference), path(truthVcfFile)\n\n\toutput:\n\ttuple path(simulationInputs), path(mutatedReference)\n\n\tscript:\n\t\"\"\"\n\tmkdir -p ${params.variantSetsDir}\n\tbcftools sort \\\n\t\t${truthVcfFile} \\\n\t\t-o ${params.variantSetsDir}/${params.cohortId}.truth.g.vcf\n\t\"\"\"\n}"], "list_proc": ["stevekm/nextflow-samplesheet-demo/make_dir", "rmoran7/dx_sarek/MultiQC", "sickle-in-africa/saw.snv-indel/sortTruthVcfFile"], "list_wf_names": ["sickle-in-africa/saw.snv-indel", "stevekm/nextflow-samplesheet-demo", "rmoran7/dx_sarek"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["rnharmening"], "nb_wf": 1, "list_wf": ["nf-multipleReferenceMapper"], "list_contrib": ["rnharmening"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/FastQC\", mode: 'copy'\n\n  input:\n    set sample_id, file(reads) from reads_fastqc_ch\n\n  output:\n    file fastqc_out into fastqc_results_ch\n    file(fastqc_out)\n  \n  script:\n  fastqc_out = \"*_fastqc.{zip,html}\"\n  \"\"\"\n    fastqc -q $reads\n  \"\"\"\n}"], "list_proc": ["rnharmening/nf-multipleReferenceMapper/fastqc"], "list_wf_names": ["rnharmening/nf-multipleReferenceMapper"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["rnharmening"], "nb_wf": 1, "list_wf": ["nf-multipleReferenceMapper"], "list_contrib": ["rnharmening"], "nb_contrib": 1, "codes": ["\nprocess qualimap {\n  tag \"$sample_id\"\n  publishDir \"${params.outdir}/qualimap\", mode: 'copy'\n\n  echo=false\n\n  input:\n     set sample_id, file(bam_file) from mapping_pair_ch \n\n  output:\n    file \"$out\" into qualimap_results_ch\n    file \"$out\"\n\n  script:\n  out=\"${sample_id}/\"\n  \"\"\"\n    qualimap bamqc -bam $bam_file -nt ${task.cpus} -outdir $out\n  \"\"\"\n}"], "list_proc": ["rnharmening/nf-multipleReferenceMapper/qualimap"], "list_wf_names": ["rnharmening/nf-multipleReferenceMapper"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["robinfchan"], "nb_wf": 1, "list_wf": ["bisulfite_align_nf"], "list_contrib": ["robinfchan"], "nb_contrib": 1, "codes": [" process multiqc {\n        if (params.custom_container) container \"${params.custom_container}\"\n\n        publishDir \"${params.outdir}/MultiQC\", mode: 'copy', overwrite: true\n\n        input:\n        file ('fastqc/*') from ch_fastqc_results_for_multiqc.collect().ifEmpty([])\n        file ('trim_galore/*') from ch_trim_galore_results_for_multiqc.collect().ifEmpty([])\n        file ('bismark/*') from ch_bismark_align_log_for_multiqc.collect().ifEmpty([])\n        file ('bismark/*') from ch_bismark_dedup_log_for_multiqc.collect().ifEmpty([])\n        file ('bismark/*') from ch_bismark_splitting_report_for_multiqc.collect().ifEmpty([])\n        file ('bismark/*') from ch_bismark_mbias_for_multiqc.collect().ifEmpty([])\n        file ('bismark/*') from ch_bismark_reports_results_for_multiqc.collect().ifEmpty([])\n        file ('bismark/*') from ch_bismark_summary_results_for_multiqc.collect().ifEmpty([])\n        file ('qualimap/*') from ch_qualimap_results_for_multiqc.collect().ifEmpty([])\n        file ('preseq/*') from preseq_results.collect().ifEmpty([])\n\n        output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n\n        script:\n        \"\"\"\n        multiqc . -m picard -m qualimap -m bismark -m samtools -m preseq -m cutadapt -m fastqc\n        \"\"\"\n    }"], "list_proc": ["robinfchan/bisulfite_align_nf/multiqc"], "list_wf_names": ["robinfchan/bisulfite_align_nf"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["robinfchan"], "nb_wf": 1, "list_wf": ["citeseq-nf"], "list_contrib": ["robinfchan"], "nb_contrib": 1, "codes": ["\nprocess build_salmon_index {\n    if (params.custom_container) container \"${params.custom_container}\"\n    \n    tag \"$fasta\"\n    label 'mini_memory'\n    publishDir \"${params.outdir}/reference_genome/salmon_index\", mode: 'copy'\n\n    input:\n    file fasta from transcriptome_fasta_alevin.mix(transcriptome_fasta_alevin_extracted)\n\n    output:\n    file \"salmon_index\" into salmon_index_alevin\n\n    when:\n    !params.salmon_index && !params.skip_rna\n\n    script:\n    \"\"\"\n    salmon index -i salmon_index --gencode -k 31 -p 4 -t $fasta\n    \"\"\"\n}"], "list_proc": ["robinfchan/citeseq-nf/build_salmon_index"], "list_wf_names": ["robinfchan/citeseq-nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BEDTools"], "nb_own": 2, "list_own": ["rwtaylor", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["mpcr-analyses-pipelines", "layer_lab_caw"], "list_contrib": ["rwtaylor", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Target_coverage_across_samples {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam\n\n  output:\n  file(\"target_coverage.txt\") into target_coverage\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bedtools coverage -a ${params.mapping_targets_bed} -b $allsamplesbam > target_coverage.txt\n  \"\"\"\n}", "\nprocess BuildFastaFai {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.fai\")\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    samtools faidx ${fasta}\n    \"\"\"\n}"], "list_proc": ["rwtaylor/mpcr-analyses-pipelines/Target_coverage_across_samples", "ryanlayerlab/layer_lab_caw/BuildFastaFai"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 4, "tools": ["BamTools", "BEDTools", "BCFtools", "GATK"], "nb_own": 2, "list_own": ["rwtaylor", "ryanlayerlab"], "nb_wf": 3, "list_wf": ["nextflow-pipelines", "mpcr-analyses-pipelines", "layer_lab_caw"], "list_contrib": ["rwtaylor", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess IndividuallyGentoypeGVCF{\n    label 'container_llab'\n    label 'cpus_8'\n    tag {idSample + \"-\" + gvcf.baseName}\n                      \n                                                                                                                            \n    input:\n        tuple idPatient, idSample, file(intervalsBed), file(gvcf)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n    output:\n                                                                                                                                              \n        tuple idPatient, idSample, file(out_file), emit: vcf_HaplotypeCaller\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                       \n                                \n                                      \n    prefix=\"${gvcf.fileName}\" - \".g.vcf\"\n                                      \n                                          \n    out_file=\"${prefix}.vcf\"\n    \"\"\"\n    init.sh\n    bgzip  ${gvcf}\n    tabix  ${gvcf}.gz\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalsBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf}.gz \\\n        -O \"${out_file}\"\n    \"\"\"\n}", "\nprocess HaplotypeCaller {\n    label 'container_llab'\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_8'\n    \n    tag {idSample + \"-\" + intervalBed.baseName}\n                      \n                                                                                                              \n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed) \n                                                           \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n        tuple val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_HC\n                                                                                                                   \n        tuple idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\"), emit: gvcf_GenotypeGVCFs\n                                                                                                                                                                    \n        \n\n    when: 'haplotypecaller' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -L ${intervalBed} \\\n        -D ${dbsnp} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess Ontarget_hits {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam1\n\n  output:\n  set file(\"ontarget_reads.bam\"), file(\"ontarget_reads_stats.txt\") into ontarget_reads\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bedtools intersect -a $allsamplesbam -b ${params.mapping_targets_bed} > ontarget_reads.bam\n  bamtools stats -in ontarget_reads.bam > ontarget_reads_stats.txt\n  \"\"\"\n}", "\nprocess ConvertToTSV {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  container = '/zstor/containers/singularity/biobase.img'\n  publishDir \"${params.publish_directory}/tsvs\", mode: 'copy'\n\n  cpus 1\n\n  input:\n  set file(vcf), file(index) from vcfs\n\n  output:\n  file(\"*.tsv\") into tsv_files\n\n  script:\n  output_prefix = vcf.baseName - ~/\\.vcf*/\n\n  \"\"\"\n  bcftools query -H -f '%CHROM\\t%POS\\t%INDEL\\t%QUAL\\t%REF\\t%ALT{0}\\t%DP[\\t%PL:%GT:%AC]\\n' -o ${output_prefix}.tsv $vcf\n  \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/IndividuallyGentoypeGVCF", "ryanlayerlab/layer_lab_caw/HaplotypeCaller", "rwtaylor/mpcr-analyses-pipelines/Ontarget_hits", "rwtaylor/nextflow-pipelines/ConvertToTSV"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "ryanlayerlab/layer_lab_caw", "rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BWA", "GATK"], "nb_own": 2, "list_own": ["rwtaylor", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["mpcr-analyses-pipelines", "layer_lab_caw"], "list_contrib": ["rwtaylor", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SelectVariants {\n    label 'container_llab'\n    label 'cpus_8'\n    tag {interval_bed.baseName}\n    input:\n                                                                                                                                 \n        tuple val(caller), val(id_patient), val(id_sample), file(interval_bed), file (vcf), file (vcf_idx)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n\n    output:\n                                                                                                                                  \n                                                                                                                                                                                                                 \n    tuple val(\"HaplotypeCaller_Jointly_Genotyped\"), id_patient, id_sample, file(\"${interval_bed.baseName}_${id_sample}.vcf\"), emit: vcf_SelectVariants\n    \n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n            SelectVariants \\\n            -R ${fasta} \\\n            -L ${interval_bed} \\\n            -V ${vcf} \\\n            -O ${interval_bed.baseName}_${id_sample}.vcf \\\n            -sn ${id_sample}\n    \"\"\"\n}", " process Mapping_bwa {\n    publishDir path:\"${params.publish_directory}/bams\", mode: \"copy\", overwrite: true\n    tag \"${params.output_prefix}-${sampleID}\"\n\n    cpus 2\n    memory { task.cpus * 4.GB }\n\n    input:\n    set sampleID, file(fq1), file(fq2) from trimmedFastqs\n\n    output:\n    file(\"*.bam\") into bwaMappedBams\n    file(\"*.bam.bai\") into bamIndexes\n\n    script:\n    readGroupString=\"\\\"@RG\\\\tID:${sampleID}\\\\tSM:${sampleID}\\\\tLB:${sampleID}\\\\tPL:illumina\\\"\"\n\n    \"\"\"\n    set -eo pipefail\n    /usr/local/bin/bwa mem -M -R ${readGroupString} -B 3 -t ${task.cpus} ${params.reference} ${fq1} ${fq2} | \\\n    /usr/local/bin/samtools view -hu - | /usr/local/bin/samtools sort --threads ${task.cpus} -O bam - > ${sampleID}.bam\n    /usr/local/bin/samtools index ${sampleID}.bam\n    \"\"\"\n  }"], "list_proc": ["ryanlayerlab/layer_lab_caw/SelectVariants", "rwtaylor/mpcr-analyses-pipelines/Mapping_bwa"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["mpcr-analyses-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": [" process Pileup_call_regions {\n    tag \"${params.output_prefix}-${target_name}\"\n\n    cpus 2\n    memory { 8.GB }\n    time { 6.h }\n    errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n    maxRetries 5\n    maxErrors '-1'\n\n    input:\n    region from regions\n    file(bams) from wgBams.toList()\n    file(bais) from wgBamIndexes.toList()\n\n    output:\n    file(\"${params.output_prefix}.region.${region[0]}.vcf.gz\") into region_vcfs\n    file(\"${params.output_prefix}.region.${region[0]}.vcf.gz.tbi\") into region_vcf_indexes\n\n    \"\"\"\n    set -e -o pipefail\n    mkdir -p temp\n    bcftools mpileup -r ${region[1]} -a INFO/AD,FORMAT/AD,FORMAT/DP -Ou --max-depth ${params.maximum_depth} -f ${params.reference} ${bams} |\\\n     bcftools +fill-tags call -Ou -m | bcftools sort --temp-dir temp -Oz -o ${params.output_prefix}.region.${region[0]}.vcf.gz\n     tabix -p vcf ${params.output_prefix}.region.${region[0]}.vcf.gz\n    \"\"\"\n  }"], "list_proc": ["rwtaylor/mpcr-analyses-pipelines/Pileup_call_regions"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["mpcr-analyses-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": [" process ConcatenateVCFs {\n    publishDir path:\"${params.publish_directory}/vcfs\", mode: \"copy\", overwrite: true\n\n    cpus {  task.attempt == 1 ? 8: 16  }\n    memory { task.attempt == 1 ? 30.GB: 48.GB }\n    errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n    maxRetries 2\n    maxErrors '-1'\n\n    input:\n    file(vcfs) from region_vcfs\n    file(vcfindexes) from region_vcf_indexes\n\n    output:\n    set file(\"${params.output_prefix}.wg.vcf.gz\" ), file(\"${params.output_prefix}.wg.vcf.gz.tbi\") into vcf_wg\n\n    script:\n    input_vcfs = vcfs.collect{\"$it\"}.join(' ')\n\n    \"\"\"\n    set -e -o pipefail\n    bcftools concat --remove-duplicates -oz --threads ${task.cpus - 1} ${params.output_prefix}.region*.vcf.gz\n    tabix -p vcf ${params.output_prefix}.wg.vcf.gz\n    \"\"\"\n  }"], "list_proc": ["rwtaylor/mpcr-analyses-pipelines/ConcatenateVCFs"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools"], "nb_own": 2, "list_own": ["rwtaylor", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["mpcr-analyses-pipelines", "layer_lab_caw"], "list_contrib": ["rwtaylor", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SamtoolsStats {\n    label 'container_llab'\n    label 'cpus_2'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/SamToolsStats\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        file (\"${bam}.samtools.stats.out\")\n\n                                      \n\n    script:\n    \"\"\"\n    init.sh\n    samtools stats ${bam} > ${bam}.samtools.stats.out\n    \"\"\"\n}", "\nprocess ConvertToTSV {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  container = '/zstor/containers/singularity/biobase.img'\n  publishDir \"${params.publish_directory}/tsvs\", mode: 'copy'\n\n  cpus 1\n\n  input:\n  set file(vcf), file(index) from vcfs\n\n  output:\n  file(\"*.tsv\") into tsv_files\n\n  script:\n  output_prefix = vcf.baseName - ~/\\.vcf*/\n\n  \"\"\"\n  bcftools query -H -f '%CHROM\\t%POS\\t%INDEL\\t%QUAL\\t%REF\\t%ALT{0}\\t%DP[\\t%PL:%GT:%AC]\\n' -o ${output_prefix}.tsv $vcf\n  \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/SamtoolsStats", "rwtaylor/mpcr-analyses-pipelines/ConvertToTSV"], "list_wf_names": ["rwtaylor/mpcr-analyses-pipelines", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 1, "tools": ["FreeBayes"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Freebayes1 {\n  tag \"${params.pipeline_name}-${regionTask}\"\n\n  cpus { 1 }\n  memory { task.attempt == 1 ? 12.GB: task.attempt == 2 ? 24.GB: 64.GB }\n  time { 2.d }\n  errorStrategy { 'retry' }\n  maxRetries 2\n  maxErrors '-1'\n\n  input:\n  file(bams) from all_bam_files.first()\n  file(bais) from all_bai_files.first()\n  set regionTask, regions from regionTasks\n\n  output:\n  set regionTask, file(\"region_${regionTask}.vcf.bgz\"), file(\"*.tbi\") into region_VCFs\n\n  script:\n  input_bams = bams.collect{\"-b $it\"}.join(' ')\n\n  \"\"\"\n  set -e -o pipefail\n  mkdir -p temp\n  freebayes ${params.freebayes_options} \\\n    -f $params.genome \\\n    --gvcf \\\n    $input_bams \\\n    $regions | vcf-sort --temporary-directory temp | bgzip -@ ${task.cpus} > region_${regionTask}.vcf.bgz\n    tabix -p vcf region_${regionTask}.vcf.bgz\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Freebayes1"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess SampleVCF {\n  publishDir \"${params.publish_dir}/${output_folder}/subsampled\", mode: 'copy'\n  tag { prefix + \"-ss\" + subsamplerate }\n  cpus 2\n  memory 8.GB\n  time 6.h\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 7\n  maxErrors '-1'\n\n  input:\n  set prefix, file(vcf), file(tbindex), file(gbindex), output_folder from filtered_vcfs\n  each subn from params.subsample_ns\n\n  output:\n  set val(\"${prefix}-ss${subn[1]}\"), file(\"*.vcf.gz\"), file(\"*.tbi\"), file(\"*.gbi\"), output_folder into subsampled_vcfs\n\n  \"\"\"\n  set -e -o pipefail\n  mkdir -p temp\n  grabix index $vcf\n  grabix random $vcf ${subn[0]} | bcftools sort --temp-dir temp -O z -o ${prefix}-ss${subn[1]}.vcf.gz\n  tabix -p vcf ${prefix}-ss${subn[1]}.vcf.gz\n  grabix index ${prefix}-ss${subn[1]}.vcf.gz\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/SampleVCF"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": [" process Mapping_bwa {\n    publishDir path:\"${params.publish_directory}/bams\", mode: \"copy\", overwrite: true\n    tag \"${params.output_prefix}-${sampleID}\"\n\n    cpus 2\n    memory { task.cpus * 4.GB }\n\n    input:\n    set sampleID, file(fq1), file(fq2) from trimmedFastqs\n\n    output:\n    file(\"*.bam\") into bwaMappedBams\n    file(\"*.bam.bai\") into bamIndexes\n\n    script:\n    readGroupString=\"\\\"@RG\\\\tID:${sampleID}\\\\tSM:${sampleID}\\\\tLB:${sampleID}\\\\tPL:illumina\\\"\"\n\n    \"\"\"\n    set -eo pipefail\n    /usr/local/bin/bwa mem -M -R ${readGroupString} -B 3 -t ${task.cpus} ${params.reference} ${fq1} ${fq2} | \\\n    /usr/local/bin/samtools view -hu - | /usr/local/bin/samtools sort --threads ${task.cpus} -O bam - > ${sampleID}.bam\n    /usr/local/bin/samtools index ${sampleID}.bam\n    \"\"\"\n  }"], "list_proc": ["rwtaylor/nextflow-pipelines/Mapping_bwa"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess MakeSamBam {\n  tag { qID }\n  publishDir \"outputs\", mode: 'copy'\n\n  cpus 1\n  memory { task.exitStatus == 137 ? (task.attempt > 2 ? 64.GB: 32.GB) : 12.GB}\n  time { 2.h }\n  errorStrategy { 'retry' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set val(qID), file(maf_file) from to_sam\n  set val(prefix), file(ref_fasta) from ref_genome3.first()\n\n  output:\n  set qID, file(\"${qID}.sam\"), file(\"${qID}.bam\"), file(\"${qID}.bam.bai\") into aligned_sam_bam\n  file(\"${qID}.bam\") into aligned_bams\n  file(\"${qID}.bam.bai\") into aligned_bais\n\n  \"\"\"\n  set -e\n  /usr/local/bin/maf-convert -n sam ${maf_file} > temp.sam\n  /usr/local/bin/samtools faidx ${ref_fasta}\n  /usr/local/bin/samtools view -t ${ref_fasta}.fai temp.sam > ${qID}.sam\n  /usr/local/bin/samtools view -bu -t ${ref_fasta}.fai -T ${ref_fasta} ${qID}.sam | samtools addreplacerg -r ID:${qID} -r LB:${qID} -r SM:${qID} -o temp.bam\n  /usr/local/bin/samtools sort temp.bam > ${qID}.bam\n  /usr/local/bin/samtools index ${qID}.bam\n  rm temp.sam temp.bam\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/MakeSamBam"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess MergeBams {\n  publishDir \"outputs\", mode: 'copy'\n\n  cpus 1\n  memory { task.exitStatus == 137 ? (task.attempt > 2 ? 64.GB: 32.GB) : 12.GB}\n  time { 2.h }\n  errorStrategy { 'retry' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  file(bams) from aligned_bams.toList()\n  file(bais) from aligned_bais.toList()\n  \n\n  output:\n  set file(\"*.bam\"), file(\"*.bam.bai\") into merged_bam\n\n  \"\"\"\n  set -e\n  /usr/local/bin/samtools merge -r all_pseudohaps.bam $bams\n  /usr/local/bin/samtools index all_pseudohaps.bam\n  \n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/MergeBams"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BamTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Stats {\n\n    tag { qID }\n    publishDir \"outputs\", mode: 'copy'\n\n    cpus 1\n    memory { 12.GB}\n    time { 2.h }\n    errorStrategy { 'retry' }\n    maxRetries 5\n    maxErrors '-1'\n\n    input:\n    file(bam) from aligned_bams_for_stats\n\n    output:\n    file(\"*.stats\") into bam_stats\n\n    \"\"\"\n    /usr/bin/bamtools stats -in $bam > $bam.stats\n    \"\"\"\n\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Stats"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess FastQC {\n  tag \"${task.attempt}.${params.pipeline_name}-${sampleID}-${libID}-${laneID}\"\n\n  cpus 1\n  memory 2.GB\n\n  input:\n  set sampleID, libID, laneID, file(reads) from fastqFiles\n\n  output:\n  file('*_fastqc.{zip,html}') into fastqc_results\n\n  \"\"\"\n  /usr/local/bin/fastqc -q ${reads}\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/FastQC"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Pileup_call_target {\n  container = '/zstor/containers/singularity/biobase.img'\n  publishDir path:\"${params.publish_directory}/vcfs\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}-${target_name}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set target_name, target_region from targetTasks\n  file(bams) from bwaMappedBams.toList()\n  file(bais) from bamIndexes.toList()\n\n  output:\n  file(\"${params.output_prefix}.${target_name}.target.vcf.gz\") into target_vcfs\n  file(\"${params.output_prefix}.${target_name}.target.vcf.gz.tbi\") into target_vcf_indexes\n\n  \"\"\"\n  set -e -o pipefail\n  mkdir -p temp\n  /usr/local/bin/bcftools mpileup -r ${target_region} -a INFO/AD,FORMAT/AD,FORMAT/DP -Ou --max-depth 100000 -f ${params.reference} ${bams} |\\\n   bcftools call -Ou -m | bcftools sort --temp-dir temp -Oz -o ${params.output_prefix}.${target_name}.target.vcf.gz\n   tabix -p vcf ${params.output_prefix}.${target_name}.target.vcf.gz\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Pileup_call_target"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BamTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Sample_bam_stats {\n  publishDir path:\"${params.publish_directory}/sample_bam_stats\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  file(bam) from bamFiles\n  file(bais) from baiFiles.toList()\n\n  output:\n  file(\"${bam.baseName}.stats\") into bam_stats\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bamtools stats -in ${bam} > ${bam.baseName}.stats\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Sample_bam_stats"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Taget_coverage {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  file(bams) from bamFiles1.toList()\n  file(bais) from baiFiles1.toList()\n\n  output:\n  file(\"target_sample_coverage.txt\") into target_sample_coverage\n\n  script:\n  header = bams.collect{\"$it.baseName\"}.join('\\t')\n  header = \"chrom\\tstart\\tstop\\tsnp\\t\" + header\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  echo \"${header}\" > target_sample_coverage.txt\n  bedtools multicov -bed ${params.mapping_targets_bed} -bams ${bams} >> target_sample_coverage.txt\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Taget_coverage"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BamTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Concatenate_bams {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  file(bams) from bamFiles2.toList()\n  file(bais) from baiFiles2.toList()\n\n  output:\n  set file(\"all_samples.bam\"), file(\"all_samples.bam.bai\") into all_samples_bam\n\n  script:\n  input_bams = bams.collect{\"-in $it\"}.join(' ')\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bamtools merge ${input_bams} | bamtools sort -out all_samples.bam\n  bamtools index -in all_samples.bam\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Concatenate_bams"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Target_coverage_across_samples {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam\n\n  output:\n  file(\"target_coverage.txt\") into target_coverage\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bedtools coverage -a ${params.mapping_targets_bed} -b $allsamplesbam > target_coverage.txt\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Target_coverage_across_samples"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 2, "tools": ["BamTools", "BEDTools", "GATK"], "nb_own": 2, "list_own": ["rwtaylor", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["nextflow-pipelines", "layer_lab_caw"], "list_contrib": ["rwtaylor", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess ApplyBQSR {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n                      \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                        \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(fasta)\n        file(fastaFai) \n        file(dict)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\")\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess Ontarget_hits {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam1\n\n  output:\n  set file(\"ontarget_reads.bam\"), file(\"ontarget_reads_stats.txt\") into ontarget_reads\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bedtools intersect -a $allsamplesbam -b ${params.mapping_targets_bed} > ontarget_reads.bam\n  bamtools stats -in ontarget_reads.bam > ontarget_reads_stats.txt\n  \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/ApplyBQSR", "rwtaylor/nextflow-pipelines/Ontarget_hits"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BamTools", "BEDTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Offsite_hits {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam2\n\n  output:\n  set file(\"off_target_reads.bam\"), file(\"off_target_reads_stats.txt\") into offtarget_reads\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bedtools subtract -A -a $allsamplesbam -b ${params.mapping_targets_bed} > off_target_reads.bam\n  bamtools stats -in off_target_reads.bam > off_target_reads_stats.txt\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Offsite_hits"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["BamTools", "BEDTools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess Filter_mapping_quality {\n  publishDir path:\"${params.publish_directory}\", mode: \"copy\", overwrite: true\n  tag \"${params.output_prefix}\"\n\n  cpus 1\n  memory { 8.GB }\n  time { 6.h }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set file(allsamplesbam), file(allsamplesbai) from all_samples_bam3\n  each filter from Channel.from([[\">=20\",\"overequal_mq20\"],[\"<20\",\"under_mq20\"]])\n\n  output:\n  set file(\"all_samples_${filter[1]}.bam\"), file(\"all_samples_${filter[1]}.targetcoverage.txt\") into filtered_bams\n\n  \"\"\"  \n  set -e -o pipefail\n  mkdir -p temp\n  bamtools filter -mapQuality \"${filter[0]}\" -in $allsamplesbam -out all_samples_${filter[1]}.bam\n  bedtools coverage -a ${params.mapping_targets_bed} -b all_samples_${filter[1]}.bam > all_samples_${filter[1]}.targetcoverage.txt\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/Filter_mapping_quality"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess SplitFastq {\n  tag \"${task.attempt}.${params.pipeline_name}-${sampleID}-${libID}-${laneID}\"\n\n  cpus 4\n  time { 1.h * task.attempt }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n\n  input:\n  set sampleID, libID, laneID, file(fq1), file(fq2) from fastqFiles\n\n  output:\n  set sampleID, libID, laneID, file(\"*_R1.*.fq.gz\"), file(\"*_R2.*.fq.gz\") into splitFastqs\n\n  script:\n  nsplit = 4*params.fastq_chunksize\n\n  \"\"\"\n  zcat ${fq1} | seqtk seq -l0 - | split -d --additional-suffix=.fq -l $nsplit --filter='pigz -p${task.cpus} > \\$FILE.gz' - ${sampleID}_${libID}_${laneID}_R1.\n  zcat ${fq2} | seqtk seq -l0 - | split -d --additional-suffix=.fq -l $nsplit --filter='pigz -p${task.cpus} > \\$FILE.gz' - ${sampleID}_${libID}_${laneID}_R2.\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/SplitFastq"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 1, "tools": ["fqtools"], "nb_own": 1, "list_own": ["rwtaylor"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["rwtaylor"], "nb_contrib": 1, "codes": ["\nprocess FQ_validate {\n  publishDir path:\"${params.publish_directory}/${sampleID}\", mode: 'copy', overwrite: true\n\n  tag \"${task.attempt}.${params.pipeline_name}-${sampleID}-${libID}-${laneID}-s.${splitID}\"\n\n  cpus 1\n  time { 1.h * task.attempt }\n  errorStrategy { task.exitStatus == 143 ? 'retry' : 'finish' }\n  maxRetries 5\n  maxErrors '-1'\n  validExitStatus 0,1\n  \n  input:\n  set sampleID, libID, laneID, splitID, file(fq1), file(fq2) from trimmedFastqs\n\n  output:\n  set sampleID, libID, laneID, splitID, file(\"fqvalidate.txt\") into fastqvalidation\n\n  \"\"\"\n  /usr/local/bin/fqtools validate ${fq1} ${fq2} &> fqvalidate.txt\n  \"\"\"\n}"], "list_proc": ["rwtaylor/nextflow-pipelines/FQ_validate"], "list_wf_names": ["rwtaylor/nextflow-pipelines"]}, {"nb_reuse": 2, "tools": ["BCFtools", "BWA", "SAMtools", "FreeBayes", "MultiQC", "TIDDIT", "FastQC", "QualiMap", "QUAST", "GATK", "VCFtools"], "nb_own": 2, "list_own": ["salvadorlab", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["mbovpan", "layer_lab_caw"], "list_contrib": ["noahaus", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess GetSoftwareVersions {\n    label 'container_llab'\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode\n\n    output:\n                                                                       \n        file 'software_versions_mqc.yaml'\n\n                                    \n\n    script:\n    \"\"\"\n    init.sh\n    bcftools version > v_bcftools.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    configManta.py --version > v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version > v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    echo \"SNPEFF version\"\\$(snpEff -h 2>&1) > v_snpeff.txt\n    fastqc --version > v_fastqc.txt 2>&1 || true\n    freebayes --version > v_freebayes.txt 2>&1 || true\n    gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt  || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess quast {\n    publishDir = output\n\n    conda \"$workflow.projectDir/envs/quast.yaml\"\n    \n    cpus threads\n\n    input:\n    file(assemblies) from assembly_ch1.collect()\n    \n    output:\n    file(\"*\") into quast_ch\n    \n    script:\n    \"\"\"\n    quast -o assembly_stats ${assemblies} -r ${ref} -t ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/GetSoftwareVersions", "salvadorlab/mbovpan/quast"], "list_wf_names": ["salvadorlab/mbovpan", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SomaticPonGenomicsDBImport {\n    label 'cpus_32'\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/Preprocessing/Somatic_pon_db\", mode: params.publish_dir_mode\n\n    input:\n    file(\"vcfs/*\")\n    file(targetBED)\n\n    output:\n    file(\"somatic_pon.gdb\")\n\n    when: 'gen_somatic_pon' in tools\n\n    script:\n    sample_map=\"cohort_samples.map\"\n    \n                \n    \"\"\"\n    init.sh\n    vcfs=' '\n    for x in `ls vcfs/*.vcf.gz`\n    do\n        base_name=`basename \\${x}`\n        without_ext=\\${base_name%.vcf.gz}\n        sample_name=\\${without_ext##*_}\n        echo \"\\${sample_name}\\t\\$x\" >> $sample_map \n    done\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    GenomicsDBImport  \\\n    --genomicsdb-workspace-path somatic_pon.gdb \\\n    -L ${targetBED} \\\n    --sample-name-map $sample_map \\\n    --merge-input-intervals \\\n    --reader-threads ${task.cpus}\n    \"\"\"\n}", "\nprocess SomaticPonGenomicsDBImport {\n    label 'cpus_32'\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/Preprocessing/Somatic_pon_db\", mode: params.publish_dir_mode\n\n    input:\n    file(\"vcfs/*\")\n    file(targetBED)\n\n    output:\n    file(\"somatic_pon.gdb\")\n\n    when: 'gen_somatic_pon' in tools\n\n    script:\n    sample_map=\"cohort_samples.map\"\n    \n                \n    \"\"\"\n    init.sh\n    vcfs=' '\n    for x in `ls vcfs/*.vcf.gz`\n    do\n        base_name=`basename \\${x}`\n        without_ext=\\${base_name%.vcf.gz}\n        sample_name=\\${without_ext##*_}\n        echo \"\\${sample_name}\\t\\$x\" >> $sample_map \n    done\n\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    GenomicsDBImport  \\\n    --genomicsdb-workspace-path somatic_pon.gdb \\\n    -L ${targetBED} \\\n    --sample-name-map $sample_map \\\n    --merge-input-intervals \\\n    --reader-threads ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/SomaticPonGenomicsDBImport", "ryanlayerlab/layer_lab_chco/SomaticPonGenomicsDBImport"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CreateSomaticPON{\n    label 'container_llab'\n    label 'cpus_max'\n                         \n     publishDir \"${params.outdir}/Preprocessing/Somatic_pon\", mode: params.publish_dir_mode\n\n    input:\n    file(pon) \n    file(fasta)\n    file(fastaFai)\n    file(dict)\n    file(germlineResource)\n    file(germlineResourceIndex)\n    \n    output:\n    tuple file(out_file), file (\"${out_file}.tbi\")\n\n    when: 'gen_somatic_pon' in tools\n\n    script:\n    args_file = \"normals_for_pon_vcf.args\"\n    out_file = \"somatic_pon.vcf.gz\" \n    pon_db = \"gendb://${pon}\"\n    \n    \"\"\"\n    init.sh\n     gatk --java-options -Xmx${task.memory.toGiga()}g \\\n     CreateSomaticPanelOfNormals -R ${fasta} \\\n     --germline-resource ${germlineResource} \\\n    -V ${pon_db} \\\n    -O ${out_file}\n    \"\"\"\n}", "\nprocess IndividuallyGentoypeGVCF{\n    label 'container_llab'\n    label 'cpus_8'\n    tag {idSample + \"-\" + gvcf.baseName}\n                      \n                                                                                                                            \n    input:\n        tuple idPatient, idSample, file(intervalsBed), file(gvcf)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n    output:\n                                                                                                                                                                                 \n        tuple idPatient, idSample, file(out_file), emit: vcf_HaplotypeCaller\n\n    when: 'haplotypecaller' in tools\n\n    script:\n                       \n                                \n                                      \n    prefix=\"${gvcf.fileName}\" - \".g.vcf\"\n                                      \n                                          \n    out_file=\"${prefix}.vcf\"\n    \"\"\"\n    init.sh\n    bgzip  ${gvcf}\n    tabix  ${gvcf}.gz\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${intervalsBed} \\\n        -D ${dbsnp} \\\n        -V ${gvcf}.gz \\\n        -O \"${out_file}\"\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/CreateSomaticPON", "ryanlayerlab/layer_lab_chco/IndividuallyGentoypeGVCF"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_caw"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess GenomicsDBImport {\n    label 'container_llab'\n    label 'cpus_16'\n                \n    tag{interval_name}\n                                                                               \n\n    input:\n                                                                                                           \n    tuple val(interval_name), file(interval_bed), val(patientSampleIdMap), file(gvcfs)\n    \n    output:\n    tuple val(interval_name), file(interval_bed), val(patientSampleIdMap), file (\"${interval_name}.gdb\")\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    sample_map=\"cohort_samples.map\"\n    interval_name_with_underscore=\"${interval_name}_\"\n                \n    \"\"\"\n    init.sh\n    for x in *.g.vcf\n    do\n        bgzip \\$x\n        tabix \\${x}.gz\n    done\n\n    for x in *.g.vcf.gz\n    do\n        \n        base_name=`basename \\$x .g.vcf.gz`\n        sample=\\${base_name#$interval_name_with_underscore}\n        echo \"\\${sample}\\t\\${x}\" >> ${sample_map}\n    done\n    \n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n    GenomicsDBImport \\\n    --genomicsdb-workspace-path ${interval_name}.gdb \\\n    -L $interval_bed \\\n    --sample-name-map ${sample_map} \\\n    --reader-threads ${task.cpus}\n\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/GenomicsDBImport"], "list_wf_names": ["ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_caw"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess GenotypeGVCFs {\n    label 'container_llab'\n    label 'cpus_8'\n    tag {interval_bed.baseName}\n    input:\n                                                                                                             \n        tuple val(interval_name), file(interval_bed), val(patientSampleIdMap), file(gdb)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex)\n\n    output:\n                                                                                                                                                             \n    tuple val(\"HaplotypeCaller\"),  val(patientSampleIdMap), file(interval_bed), file(\"${interval_name}.vcf\"), file(\"${interval_name}.vcf.idx\"), emit: vcf_GenotypeGVCFs\n                                                                                                                                                                                                                                   \n    \n    when: 'haplotypecaller' in tools\n\n    script:\n                                                                                   \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GenotypeGVCFs \\\n        -R ${fasta} \\\n        -L ${interval_bed} \\\n        -D ${dbsnp} \\\n        -V gendb://${gdb} \\\n        --create-output-variant-index \\\n        -O \"${interval_name}.vcf\"\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/GenotypeGVCFs"], "list_wf_names": ["ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeBamMapped {\n    label 'cpus_16'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, out_suffix, file(bams)\n                                                                    \n\n    output:\n        tuple idPatient, idSample,  file(\"${idSample}${out_suffix}.bam\")\n\n    script:\n                                                          \n                                        \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} \"${idSample}${out_suffix}.bam\" ${bams}\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    tag {idSample + \"-\" + intervalBed.baseName}\n    \n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup.gz\" ? \"VariantCalling/${idSample}/mpileup/${it}\" : '' }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.pileup.gz\")\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n    \"\"\"\n    init.sh\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} \\\n    | bgzip --threads ${task.cpus} -c > ${prefix}${idSample}.pileup.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MergeBamMapped", "ryanlayerlab/layer_lab_caw/Mpileup"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n    label 'container_llab'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", \n    mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), \n        file(\"${idSample}_${idRun}_R2.fastq.gz\")\n\n    output:\n        file(\"*.{html,zip}\")\n\n                                                         \n    \n    script:\n    \"\"\"\n    init.sh\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_16'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Bams/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        tuple idPatient, idSample, file(bam), file(\"${bam.baseName}.bai\")\n\n                                \n\n    script:\n    \"\"\"\n    init.sh\n    samtools index ${bam}\n    mv ${bam}.bai ${bam.baseName}.bai\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/FastQCFQ", "ryanlayerlab/layer_lab_chco/IndexBamFile"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 4, "tools": ["SAMtools", "Sambamba", "BWA", "GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FilterBamRead1 {\n    label 'cpus_32'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample + \"_\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\")\n        \n    output:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_filtered_r1.bam\"), emit: filtered_reads\n\n    when: params.filter_bams\n    script:\n    if( params.remove_supplementary_reads)\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(first_of_pair and mapping_quality >=${params.bam_mapping_q} \\\n                and not ([XA] != null or [SA] != null)) \\\n                or second_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r1.bam\"\n        \"\"\"\n\n    else\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(first_of_pair and mapping_quality >=${params.bam_mapping_q}) \\\n                or second_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r1.bam\"\n        \"\"\"\n}", "\nprocess BuildBWAindexes {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/BWAIndex/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta}.*\")\n\n    when: !(params.bwa_index) && params.fasta && 'mapping' in step\n\n    script:\n    \"\"\"\n    init.sh\n    bwa index ${fasta}\n    \"\"\"\n}", "\nprocess MapReads {\n    label 'cpus_max'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(inputFile1), file(inputFile2)\n        file(fasta) \n        file(fastaFai)\n        file(bwaIndex) \n\n    output:\n                                                                             \n                                                                                         \n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\"), emit : bam_mapped\n                                                                                                                  \n    \n                                                                                \n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = status_map[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    \"\"\"\n        init.sh\n        ${convertToFastq}\n        bwa mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n        ${input} | \\\n        samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}", "\nprocess insertSize{\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/insertSize/\", mode: params.publish_dir_mode\n    publishDir \"${params.outdir}/QC/${idSample}/insertSize\", mode: params.publish_dir_mode\n\n    input:\n    tuple idPatient, idSample, file(bam), file(bai)\n\n\n    output:\n    path \"${idSample}_insert_size_metrics.txt\", emit: files\n    file(\"${idSample}_insert_size_histogram.pdf\")\n\n    when: ! ('chco_qc' in _skip_qc)\n\n    script:\n    \"\"\"\n        gatk --java-options -Xmx32G CollectInsertSizeMetrics \\\n        -I $bam \\\n        -O ${idSample}_insert_size_metrics.txt \\\n        -H ${idSample}_insert_size_histogram.pdf \\\n        -M 0.5\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/FilterBamRead1", "ryanlayerlab/layer_lab_caw/BuildBWAindexes", "ryanlayerlab/layer_lab_caw/MapReads", "ryanlayerlab/layer_lab_chco/insertSize"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["QualiMap", "GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess DenoiseReadCounts {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DenoisedReadCounts/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file( \"${idSample}.counts.hdf5\")\n        file(read_count_somatic_pon)\n\n    output:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio), emit: 'denoised_cr'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    std_copy_ratio = \"${idSample}.standardizedCR.tsv\"\n    denoised_copy_ratio = \"${idSample}.denoisedCR.tsv\"\n    pon_option = params.read_count_pon ? \"--count-panel-of-normals ${read_count_somatic_pon}\" : \"\"\n    \"\"\"\n    init.sh\n    gatk DenoiseReadCounts \\\n        -I ${idSample}.counts.hdf5 \\\n        ${pon_option} \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio}\n    \"\"\"\n}", "\nprocess BamQC {\n                         \n    label 'container_llab'\n    label 'cpus_16'\n                  \n\n    tag {idPatient + \"-\" + idSample}\n\n                                                                                             \n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam) \n        file(targetBED)\n\n    output:\n        file(\"${bam.baseName}\")\n\n                                 \n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    init.sh\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/DenoiseReadCounts", "ryanlayerlab/layer_lab_caw/BamQC"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["BEDTools", "GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CollectAlignmentSummaryMetrics{\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/alignment_summary/\", mode: params.publish_dir_mode\n    \n    input:\n    tuple idPatient, idSample, file(bam) \n    file(fasta) \n    file(fastaFai)\n    file(dict)\n\n    output:\n    file(\"${bam.baseName}_alignment_metrics.txt\")\n    \n                                                \n    \n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx32G CollectAlignmentSummaryMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}_alignment_metrics.txt \\\n    -R ${fasta}\n    \"\"\"\n}", "\nprocess gzip_probes{\n    label 'container_llab'\n    label 'cpus_1'\n\n    input:\n    file(probes)\n    \n    output:\n    tuple file(\"${probes}.sorted.gz\"), file(\"${probes}.sorted.gz.tbi\")\n    \n    \"\"\"\n    bedtools sort -i $probes > ${probes}.sorted\n    bgzip ${probes}.sorted\n    tabix ${probes}.sorted.gz -p bed\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/CollectAlignmentSummaryMetrics", "ryanlayerlab/layer_lab_chco/gzip_probes"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["GATK", "BEDTools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess merge_all_allele_counts {\n    label 'container_llab'\n    label 'cpus_1'\n\n    publishDir \"${params.outdir}/CNV_Plotting/merge_all_allele_counts\", mode: params.publish_dir_mode\n\n    input:\n    file(agged_allele_counts)                                 \n    file(probes)\n\n    output:\n    tuple file(\"ab.sorted.tsv.gz\"), file(\"ab.sorted.tsv.gz.tbi\")\n    \n    script:\n    \"\"\"\n    cat *.agg.allele_count.bed > all_samples.aggregate_probe_allele_counts.txt\n    create_all_sample_allele_count_bed.py all_samples.aggregate_probe_allele_counts.txt $probes> ab.bed\n    head -1 ab.bed > ab.header.tsv\n    tail -n +2 ab.bed > ab.tsv\n    bedtools sort -i ab.tsv > ab.sorted.tsv\n    bgzip ab.sorted.tsv\n    tabix -p bed ab.sorted.tsv.gz \n    \"\"\"\n}", "\nprocess CollectInsertSizeMetrics{\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/insert_size_metrics/\", mode: params.publish_dir_mode\n    \n    input:\n    tuple idPatient, idSample, file(bam)\n\n    output:\n    file(\"${bam.baseName}_insert_size_metrics.txt\")\n    file(\"${bam.baseName}_insert_size_histogram.pdf\")\n    \n    \n                                                 \n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx32G CollectInsertSizeMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}_insert_size_metrics.txt \\\n    -H ${bam.baseName}_insert_size_histogram.pdf \n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/merge_all_allele_counts", "ryanlayerlab/layer_lab_caw/CollectInsertSizeMetrics"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess DenoiseReadCounts {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DenoisedReadCounts/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file( \"${idSample}.counts.hdf5\")\n        file(read_count_somatic_pon)\n\n    output:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio), emit: 'denoised_cr'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    std_copy_ratio = \"${idSample}.standardizedCR.tsv\"\n    denoised_copy_ratio = \"${idSample}.denoisedCR.tsv\"\n    pon_option = params.read_count_pon ? \"--count-panel-of-normals ${read_count_somatic_pon}\" : \"\"\n    \"\"\"\n    init.sh\n    gatk DenoiseReadCounts \\\n        -I ${idSample}.counts.hdf5 \\\n        ${pon_option} \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio}\n    \"\"\"\n}", "\nprocess CollectHsMetrics{\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/hs_metrics/\", mode: params.publish_dir_mode\n    \n    input:\n                                                      \n    tuple idPatient, idSample, file(bam)\n    file(fasta) \n    file(fastaFai)\n    file(dict)\n    file(targetBED)\n    file(baitBED)\n                          \n\n    output:\n    file(\"${bam.baseName}.txt\")\n                                                   \n    \n    \n                                                           \n    script:\n    \"\"\"\n    init.sh\n    gatk BedToIntervalList -I ${targetBED} -O target.interval_list -SD ${dict}\n    gatk BedToIntervalList -I ${baitBED} -O bait.interval_list -SD ${dict}\n\n    gatk --java-options -Xmx32G CollectHsMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}.txt \\\n    -TI target.interval_list \\\n    -BI bait.interval_list \\\n    -R ${fasta}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/DenoiseReadCounts", "ryanlayerlab/layer_lab_caw/CollectHsMetrics"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeFilteredBamReads {\n    label 'cpus_32'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, file(partial_filtered_bams)\n\n    output:\n        tuple idPatient, idSample, idRun,  file(out_bam), file(\"${out_bam}.bai\"), emit: filtered_bam\n\n    when: (params.filter_bams)\n\n    script:\n     out_bam = \"${idSample}_${idRun}_filtered.bam\"\n                                  \n                            \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} -n -c -p merged.bam ${partial_filtered_bams}\n    samtools sort --threads ${task.cpus} merged.bam -o ${idSample}_${idRun}_filtered.bam\n    #samtools index  --threads ${task.cpus} ${idSample}_${idRun}_filtered.bam\n    samtools index   ${idSample}_${idRun}_filtered.bam\n    # cleaning\n    rm -f merged.bam\n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_16'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    input:\n        tuple idPatient, idSample, idRun, out_suffix, file(bams)\n                                                                    \n\n    output:\n        tuple idPatient, idSample,  file(\"${idSample}${out_suffix}.bam\")\n\n    script:\n                                                          \n                                        \n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} \"${idSample}${out_suffix}.bam\" ${bams}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MergeFilteredBamReads", "ryanlayerlab/layer_lab_caw/MergeBamMapped"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["SAMtools", "GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess IndexBamFile {\n    label 'cpus_16'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Bams/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        tuple idPatient, idSample, file(bam), file(\"${bam.baseName}.bai\")\n\n                                \n\n    script:\n    \"\"\"\n    init.sh\n    samtools index ${bam}\n    mv ${bam}.bai ${bam.baseName}.bai\n    \"\"\"\n}", "\nprocess CollectAlignmentSummaryMetrics{\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/alignment_summary/\", mode: params.publish_dir_mode\n    \n    input:\n    tuple idPatient, idSample, file(bam) \n    file(fasta) \n    file(fastaFai)\n    file(dict)\n\n    output:\n    file(\"${bam.baseName}_alignment_metrics.txt\")\n    \n                                                \n    \n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx32G CollectAlignmentSummaryMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}_alignment_metrics.txt \\\n    -R ${fasta}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/IndexBamFile", "ryanlayerlab/layer_lab_chco/CollectAlignmentSummaryMetrics"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BCFtools", "Bowtie"], "nb_own": 2, "list_own": ["ryanlayerlab", "rynge"], "nb_wf": 2, "list_wf": ["searchsra-nextflow", "layer_lab_caw"], "list_contrib": ["rynge", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess search {\n    input:\n    file reference from reference_file\n    file index from reference_index\n    val id from ids\n   \n    output:\n    set '*.bam', '*.bai' into bam_files\n    \n    shell:    \n    \"\"\"\n    pwd\n    ls -l\n\n    # check wrangler cache first\n    WRANGLER_LOC=/nas/wrangler/NCBI/SRA/Downloads/fastq/${id}.fastq.gz\n    if [ -e \\$WRANGLER_LOC ]; then\n        SRA_SOURCE=\"\\$WRANGLER_LOC\"\n        echo \"Will read ${id} from \\$WRANGLER_LOC\"\n    else\n        # not found - we should log this better\n        echo \"WARNING: ${id} not found on Wrangler - skipping...\"\n        continue \n    fi\n\n    bowtie2 -p 1 -q --no-unal -x ${reference}.index -U \\$SRA_SOURCE | samtools view -bS - | samtools sort - ${id}\n\n    samtools index ${id}.bam\n\n    rm -f \\$HOME/ncbi/public/sra/{id}.sra*\n\n    # need to wait a little bit as nextflow is not happy with the filesystem latency\n    sleep 2m\n\n    \"\"\"\n}", "\nprocess BcftoolsStats {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n                                                   \n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n                                                                 \n        file (\"*.bcf.tools.stats.out\")\n\n    when: !('bcftools' in skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}"], "list_proc": ["rynge/searchsra-nextflow/search", "ryanlayerlab/layer_lab_caw/BcftoolsStats"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "rynge/searchsra-nextflow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "VCFtools"], "nb_own": 2, "list_own": ["s-andrews", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["nextflow_pipelines", "layer_lab_caw"], "list_contrib": ["FelixKrueger", "laurabiggins", "s-andrews", "javaidm"], "nb_contrib": 4, "codes": ["\nprocess Vcftools {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\")\n\n    when: !('vcftools' in skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}", "\nprocess SAMTOOLS_SORT{\t\n    \n\ttag \"$bam\"                                                        \n\tlabel 'bigMem'        \n\n\tinput:\n\t\tpath(bam)\n\t\tval (outputdir)\n\t\tval (samtools_sort_args)\n\t\tval (verbose)\n\n\toutput:\n\t\t                                   \n\t\tpath \"*bam\",        emit: bam\n\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n\t\n    script:\n\t\tsamtools_sort_options = samtools_sort_args\n\t\t\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] SAMTOOLS SORT ARGS: \" + samtools_sort_args)\n\t\t}\n\t\t\n\t\t                                                                     \n\n\t\t\"\"\"\n\t\tmodule load samtools\n\t\tsamtools sort $samtools_sort_options $bam -o ${bam}_sorted.bam \n\t\trename .bam_sorted _sorted *\n    \t\"\"\"\n\t\t\n\t\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/Vcftools", "s-andrews/nextflow_pipelines/SAMTOOLS_SORT"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "s-andrews/nextflow_pipelines"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_caw"], "list_contrib": ["javaidm"], "nb_contrib": 1, "codes": ["\nprocess CollectReadCounts {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(preprocessed_intervals)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.counts.hdf5\"), emit: 'sample_read_counts'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk CollectReadCounts \\\n        -I ${bam} \\\n        -L ${preprocessed_intervals} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O ${idSample}.counts.hdf5\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/CollectReadCounts"], "list_wf_names": ["ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["Sambamba", "GATK"], "nb_own": 2, "list_own": ["sagc-bioinformatics", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "a-lud"], "nb_contrib": 5, "codes": ["process markDupSambamba {\n    tag { \"Sambamba MarkDups - ${sample_id}\" }\n    publishDir \"${params.outdir}/markDuplicates\", mode: 'copy'\n    label 'process_medium'\n\n    input:\n    tuple val(sample_id),\n        file(bam),\n        file(bai)\n\n    output:\n    path \"${sample_id}.markdup.bam\", emit: bam\n    path \"${sample_id}.markdup.bam.bai\", emit: bai\n\n    script:\n    \"\"\"\n    sambamba markdup \\\n        --tmpdir=\\${PWD} \\\n        -t ${task.cpus} \\\n        ${bam} ${sample_id}.markdup.bam\n    \"\"\"\n}", "\nprocess CreateReadCountPon {\n    label 'container_llab'\n                \n    tag \"ReadCountPon\"\n    \n    publishDir \"${params.outdir}/Preprocessing/ReadCountPon/\", \n    mode: params.publish_dir_mode\n\n    \n    input:\n    file(read_count_hdf5s)\n    \n                      \n\n    output:\n    file(out_file)\n\n    script:\n    when:'gen_read_count_pon' in tools\n                                    \n    out_file = \"read_count_pon.hdf5\"\n    params_str = ''\n                                  \n    read_count_hdf5s.each{\n        params_str = \"${params_str} -I ${it}\"\n    }\n\n    \n    \"\"\"\n    init.sh\n    gatk CreateReadCountPanelOfNormals \\\n        $params_str \\\n        -O $out_file\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/markDupSambamba", "ryanlayerlab/layer_lab_caw/CreateReadCountPon"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "sagc-bioinformatics/modules"]}, {"nb_reuse": 2, "tools": ["FusionCatcher", "GATK"], "nb_own": 2, "list_own": ["sagc-bioinformatics", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "a-lud"], "nb_contrib": 5, "codes": ["\nprocess fusioncatcher_v099 {\n\n    tag { \"fusioncatcher - ${sample_id}\" } \n    publishDir \"${outdir}/${sampleProject}/fusioncatcher_v099\", mode: 'copy'\n    label 'process_fusioncatcher'\n\n    input:\n    tuple val(sample_id), file(reads)\n\tpath data_dir\n     \n    output:\n    tuple val(sample_id), val(outdir)\n    file(\"${sample_id}_fusioncatcher.txt\")\n\n    script:\n    \"\"\"\n\tfusioncatcher \\\\\n        -d ${data_dir} \\\\\n        --threads ${task.cpus} \\\\\n        --i \"${reads[0]},${reads[1]}\" \\\\\n        -o ${outdir} \\\\\n        --skip-blat \n\t\t\t\n\tmv final-list_candidate-fusion-genes.txt ${sample_id}_fusioncatcherv099.txt\n    \"\"\"\n}", "\nprocess DenoiseReadCounts {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DenoisedReadCounts/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file( \"${idSample}.counts.hdf5\")\n        file(read_count_somatic_pon)\n\n    output:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio), emit: 'denoised_cr'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    std_copy_ratio = \"${idSample}.standardizedCR.tsv\"\n    denoised_copy_ratio = \"${idSample}.denoisedCR.tsv\"\n    pon_option = params.read_count_pon ? \"--count-panel-of-normals ${read_count_somatic_pon}\" : \"\"\n    \"\"\"\n    init.sh\n    gatk DenoiseReadCounts \\\n        -I ${idSample}.counts.hdf5 \\\n        ${pon_option} \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio}\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/fusioncatcher_v099", "ryanlayerlab/layer_lab_caw/DenoiseReadCounts"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "sagc-bioinformatics/modules"]}, {"nb_reuse": 2, "tools": ["fastPHASE", "GATK"], "nb_own": 2, "list_own": ["sagc-bioinformatics", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "a-lud"], "nb_contrib": 5, "codes": ["process umiadd {\n\n    tag { \"Fastp UMI-ADD - ${sample_id}\" }\n    publishDir \"${outdir}/${sampleProject}/umi_add\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    tuple val(sample_id), file(reads), file(UMI)\n    val outdir\n    val sampleProject\n\n    output:\n    tuple val(sample_id), file(\"${sample_id}_U{1,2}.fastq.gz\"), emit: reads\n    file(\"${sample_id}.{html,json}\")\n\n    script:\n    \"\"\"\n    fastp \\\n        ${usr_args} \\\n        -i ${reads[0]}  \\\n        -I ${UMI}  \\\n        -o ${sample_id}_U1.fastq.gz \\\n        -O ${sample_id}_umi1.fastq.gz \\\n        --json ${sample_id}_U1.json \\\n        --html ${sample_id}_U1.html \\\n        --umi --umi_loc=read2 --umi_len=8 \\\n        -G -Q -A -L -w 1 -u 100 -n 8 -Y 100\n\n    fastp \\\n        ${usr_args} \\\n        -i ${reads[1]}  \\\n        -I ${UMI}  \\\n        -o ${sample_id}_U2.fastq.gz \\\n        -O ${sample_id}_umi2.fastq.gz \\\n        --json ${sample_id}_U2.json \\\n        --html ${sample_id}_U2.html \\\n        --umi --umi_loc=read2 --umi_len=8 \\\n        -G -Q -A -L -w 1 -u 100 -n 8 -Y 100\n    \"\"\"\n}", "\nprocess PlotDenoisedCopyRatios {\n    label 'container_llab'\n    label 'cpus_16'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio)\n        file(dict)\n    \n    output:\n        file(out_dir)  \n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    out_dir = \"PlotDenoisedReadCounts\" \n\n    \"\"\"\n    init.sh\n    mkdir ${out_dir}\n    gatk PlotDenoisedCopyRatios \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio} \\\n        --sequence-dictionary ${dict} \\\n        --output-prefix ${idSample} \\\n        -O ${out_dir}\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/umiadd", "ryanlayerlab/layer_lab_caw/PlotDenoisedCopyRatios"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "sagc-bioinformatics/modules"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BWA", "GATK"], "nb_own": 2, "list_own": ["salvadorlab", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["mbovpan", "layer_lab_caw"], "list_contrib": ["noahaus", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess PlotModeledSegments {\n    label 'container_llab'\n    label 'cpus_8'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(\"${idSample}.modelFinal.seg\"), file(\"${idSample}.denoisedCR.tsv\")\n        file(dict)\n    output:\n    file(out_dir)\n    \n    when: ('gatk_cnv_somatic' in tools)\n    script:\n    out_dir = \"PlotsModeledSegments\"\n    \n    \"\"\"\n    init.sh\n    mkdir $out_dir\n    gatk PlotModeledSegments \\\n        --denoised-copy-ratios ${idSample}.denoisedCR.tsv \\\n        --segments ${idSample}.modelFinal.seg \\\n        --sequence-dictionary ${dict} \\\n        --output-prefix ${idSample} \\\n        -O $out_dir\n    \"\"\"\n}", " process read_map {\n    publishDir = output \n\n    cpus threads\n    \n    conda \"$workflow.projectDir/envs/samtools.yaml\"\n   \n    input:\n    tuple file(trim1), file (trim2) from fastp_reads2 \n\n    output:\n    file(\"${trim1.baseName - ~/_trimmed_R*/}.bam\")  into map_ch \n\n    script:\n    \"\"\"\n    bwa mem -t ${task.cpus}-M -R \"@RG\\\\tID:${trim1.baseName - ~/_trimmed_R*/}\\\\tSM:${trim1.baseName - ~/_trimmed_R*/}\\\\tPL:ILLUMINA\\\\tPI:250\" ${ref} ${trim1} ${trim2} | samtools view -Sb | samtools sort -o ${trim1.baseName - ~/_trimmed_R*/}.bam\n    \"\"\"\n    }"], "list_proc": ["ryanlayerlab/layer_lab_caw/PlotModeledSegments", "salvadorlab/mbovpan/read_map"], "list_wf_names": ["salvadorlab/mbovpan", "ryanlayerlab/layer_lab_caw"]}, {"nb_reuse": 2, "tools": ["GATK", "mosdepth"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Mutect2TN{\n    label 'container_llab'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_2'\n\n    input:\n        tuple idPatient, \n            idSampleNormal, file(bamNormal), file(baiNormal),\n            idSampleTumor, file(bamTumor), file(baiTumor), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n        file(ponSomatic)\n        file(ponSomaticIndex)\n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\"), emit: vcf\n        \n        tuple idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), emit: stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.somatic_pon ? \"--panel-of-normals ${ponSomatic}\" : \"\"\n                \n    \"\"\"\n    init.sh\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess mosdepth {\n                                                       \n                            \n    label 'cpus_1'\n\n                     \n    publishDir \"${params.outdir}/CNV_Plotting/${idSample}/Mosdepth\", mode: params.publish_dir_mode\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.per-base.bed.gz\"), file(\"${idSample}.per-base.bed.gz.tbi\")\n\n\n                                       \n\n    script:\n    \"\"\"\n    mosdepth $idSample $bam\n    tabix -p bed ${idSample}.per-base.bed.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/Mutect2TN", "ryanlayerlab/layer_lab_chco/mosdepth"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["BEDTools", "GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeMutect2TNStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                                                                                                     \n        tuple idPatient, idSampleTumor, idSampleNormal, file(statsFiles)                         \n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\")\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}", "\nprocess gzip_probes{\n    label 'container_llab'\n    label 'cpus_1'\n\n    input:\n    file(probes)\n    \n    output:\n    tuple file(\"${probes}.sorted.gz\"), file(\"${probes}.sorted.gz.tbi\")\n    \n    \"\"\"\n    bedtools sort -i $probes > ${probes}.sorted\n    bgzip ${probes}.sorted\n    tabix ${probes}.sorted.gz -p bed\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/MergeMutect2TNStats", "ryanlayerlab/layer_lab_chco/gzip_probes"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["TIDDIT", "GATK"], "nb_own": 2, "list_own": ["sagc-bioinformatics", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "modules"], "list_contrib": ["nathanhaigh", "ashleethomson", "jimmybgammyknee", "javaidm", "a-lud"], "nb_contrib": 5, "codes": ["\nprocess RunGatkBaseRecalibration {\n    tag { sample_id + \" - BaseRecalibration\" }\n\n    publishDir \"${params.outdir}/BaseRecal\", mode: 'copy'\n    stageInMode 'copy'                                             \n\n    input:\n    file ref\n    file idx\n    file dict\n    file dbSNP\n    file dbIdx\n    tuple sample_id,\n        file(bam),\n        file(bai)\n\n    output:\n    tuple sample_id,\n        file(\"${sample_id}.recal.bam\"),\n        file(\"${sample_id}.recal.bam.bai\")\n\n    script:\n    \"\"\"\n    gatk BaseRecalibrator \\\n        -R ${ref} \\\n        -I ${bam} \\\n        -known-sites ${dbSNP} \\\n        -O ${sample_id}_recal.table\n\n    gatk ApplyBQSR \\\n        -R ${ref} \\\n        -I ${bam} \\\n        -bqsr-recal-file ${sample_id}_recal.table \\\n        -O ${sample_id}.recal.bam\n    \"\"\"\n}", "\nprocess TIDDIT {\n    label 'container_sarek'\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publish_dir_mode\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta) \n        file(fastaFai)\n\n    output:\n        tuple val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\"), emit: vcfTIDDIT\n        tuple file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\"), emit: tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/RunGatkBaseRecalibration", "ryanlayerlab/layer_lab_caw/TIDDIT"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "sagc-bioinformatics/modules"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess GatherBQSRReports {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n    echo true\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publish_dir_mode, overwrite: false\n\n    input:\n        tuple idPatient, idSample, file(recal)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.table\"), emit: recal_table\n                                                     \n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}", "\nprocess PlotModeledSegments {\n    label 'container_llab'\n    label 'cpus_8'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(\"${idSample}.modelFinal.seg\"), file(\"${idSample}.denoisedCR.tsv\")\n        file(dict)\n    output:\n    file(out_dir)\n    \n    when: ('gatk_cnv_somatic' in tools)\n    script:\n    out_dir = \"PlotsModeledSegments\"\n    \n    \"\"\"\n    init.sh\n    mkdir $out_dir\n    gatk PlotModeledSegments \\\n        --denoised-copy-ratios ${idSample}.denoisedCR.tsv \\\n        --segments ${idSample}.modelFinal.seg \\\n        --sequence-dictionary ${dict} \\\n        --output-prefix ${idSample} \\\n        -O $out_dir\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_caw/GatherBQSRReports", "ryanlayerlab/layer_lab_chco/PlotModeledSegments"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC"], "nb_own": 2, "list_own": ["s-andrews", "ryanlayerlab"], "nb_wf": 2, "list_wf": ["nextflow_pipelines", "layer_lab_caw"], "list_contrib": ["FelixKrueger", "laurabiggins", "s-andrews", "javaidm"], "nb_contrib": 4, "codes": ["\nprocess FASTQC {\n\n\ttag \"$name\"                                                        \n\n\tinput:\n\t    tuple val(name), path(reads)\n\t\tval (outputdir)\n\t\tval (fastqc_args)\n\t\tval (verbose)\n\n\toutput:\n\t    tuple val(name), path (\"*fastqc*\"), emit: all\n\t\tpath \"*.zip\",  emit: report\n\t\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n\tscript:\n\n\t\tif (params.nogroup){\n\t\t\t                                               \n\t\t\tfastqc_args += \" --nogroup \"\n\t\t}\n\t\t\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] FASTQC ARGS: \"+ fastqc_args)\n\t\t}\n\n\t\t\"\"\"\n\t\tmodule load fastqc\n\t\tfastqc $fastqc_args -q -t 2 ${reads}\n\t\t\"\"\"\n}", "\nprocess MergeBamRecal {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bai\"), emit: bam_recal\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), emit: bam_recal_qc\n                                                   \n\n                                   \n\n    script:\n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    mv ${idSample}.recal.bam.bai ${idSample}.recal.bai\n    \"\"\"\n}"], "list_proc": ["s-andrews/nextflow_pipelines/FASTQC", "ryanlayerlab/layer_lab_caw/MergeBamRecal"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "s-andrews/nextflow_pipelines"]}, {"nb_reuse": 2, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 2, "list_wf": ["layer_lab_caw", "layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CollectInsertSizeMetrics{\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/insert_size_metrics/\", mode: params.publish_dir_mode\n    \n    input:\n    tuple idPatient, idSample, file(bam)\n\n    output:\n    file(\"${bam.baseName}_insert_size_metrics.txt\")\n    file(\"${bam.baseName}_insert_size_histogram.pdf\")\n    \n    \n                                                 \n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx32G CollectInsertSizeMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}_insert_size_metrics.txt \\\n    -H ${bam.baseName}_insert_size_histogram.pdf \n    \"\"\"\n}", "\nprocess MergeMutect2SingleStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSample}\n\n                                                                                                                              \n\n    input:\n        tuple idPatient, idSample, file(statsFiles)                         \n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.vcf.gz.stats\")\n\n    when: 'mutect2_single' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSample}.vcf.gz.stats\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CollectInsertSizeMetrics", "ryanlayerlab/layer_lab_caw/MergeMutect2SingleStats"], "list_wf_names": ["ryanlayerlab/layer_lab_caw", "ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Mpileup {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    tag {idSample + \"-\" + intervalBed.baseName}\n    \n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup.gz\" ? \"VariantCalling/${idSample}/mpileup/${it}\" : '' }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.pileup.gz\")\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n    \"\"\"\n    init.sh\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} \\\n    | bgzip --threads ${task.cpus} -c > ${prefix}${idSample}.pileup.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/Mpileup"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess cnvkit_to_bed{\n    tag {idPatient + \"-\" + idSample}\n    label 'container_llab'\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/ReCalToMarkedRaw/\", mode: params.publish_dir_mode\n\n    input:\n    tuple idPatient, idSample, file(cns)\n    file(exon_file)\n\n    output:\n    tuple val('cnvkit'), idPatient, idSample, file(\"${idSample}.bed\")\n\n    script:\n    \"\"\"\n        # strip the first line # the next line is hard coded and needs to be fix before production as does the input parameter\n        sed '1 d' $cns | bedtools intersect -wb -b stdin -a $exon_file > cnvtemp.tsv\n        # sed '1 d' $cns | bedtools intersect -wb -b stdin -a $exon_file > cnvtemp.tsv\n        cnv-kit_to_bed.py $idSample ${idSample}.bed cnvtemp.tsv\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/cnvkit_to_bed"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess BuildDict {\n    label 'container_llab'\n    tag {fasta}\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_genome_index ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta)\n\n    output:\n        file(\"${fasta.baseName}.dict\")\n\n    when: !(params.dict) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        CreateSequenceDictionary \\\n        --REFERENCE ${fasta} \\\n        --OUTPUT ${fasta.baseName}.dict\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/BuildDict"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n    label 'container_llab'\n\n    tag {idPatient + \"-\" + idRun}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", \n    mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), \n        file(\"${idSample}_${idRun}_R2.fastq.gz\")\n\n    output:\n        file(\"*.{html,zip}\")\n\n                                                         \n    \n    script:\n    \"\"\"\n    init.sh\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/FastQCFQ"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["somalier"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SomalierExtraction {\n    label 'container_llab'\n    label 'cpus_8'\n    tag {idSample}\n    \n    publishDir \"${params.outdir}/Somalier/extracted/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta)\n        file(fasta_fai)\n        file(somalier_sites)\n\n    output:\n                                                                  \n        file(\"${idSample}.somalier\")\n                                                      \n\n    when: params.somalier_sites\n\n    script:\n    \"\"\"\n    init.sh\n    somalier extract  --sites ${somalier_sites} -f ${fasta}  ${bam}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/SomalierExtraction"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["somalier"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess SomalierAncestry {\n    label 'container_llab'\n    label 'cpus_32'\n                                       \n\n    publishDir \"${params.outdir}/Somalier/ancestry/\", mode: params.publish_dir_mode\n\n    input:\n        file(ancestry_labels_1kg_tsv)\n        file(somalier_extracted_1kg)\n        file(\"query_samples_somalier/*\")\n\n    output:\n        file(\"somalier-ancestry.somalier-ancestry.html\")\n        file(\"somalier-ancestry.somalier-ancestry.tsv\")\n    \n    when: 'somalier' in  tools\n\n    script:\n    \"\"\"\n    init.sh\n    somalier ancestry --labels ${ancestry_labels_1kg_tsv} ${somalier_extracted_1kg}/*.somalier ++ query_samples_somalier/*.somalier\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/SomalierAncestry"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["BEDTools", "GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess onTarget{\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/onTarget/\", mode: params.publish_dir_mode\n    publishDir \"${params.outdir}/QC/${idSample}/onTarget/\", mode: params.publish_dir_mode\n\n    input:\n    tuple idPatient, idSample, file(bam), file(bai)\n                                           \n    file(fasta)\n    file(fastaFai)\n    file(dict)\n    file(probes)\n    file(probes250)\n\n\n    output:\n    file(\"${bam.baseName}_on_target.txt\")\n\n    when: ! ('chco_qc' in _skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    gatk BedToIntervalList -I ${probes} -O probes.interval_list -SD ${dict}\n    gatk BedToIntervalList -I ${probes250} -O probes250.interval_list -SD ${dict}\n        bedtools intersect -a $bam -b ${probes250} | gatk --java-options -Xmx32G CollectHsMetrics \\\n    --VALIDATION_STRINGENCY SILENT \\\n        -I /dev/stdin \\\n        -O ${bam.baseName}_on_target.txt \\\n        -TI probes250.interval_list \\\n        -BI probes.interval_list \\\n        -R $fasta\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/onTarget"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Sambamba"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FilterBamRead2 {\n    label 'cpus_32'\n    label 'container_llab'\n    tag {idPatient + \"-\" + idSample + \"_\" + idRun}\n\n    input:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\")\n        \n    output:\n        tuple idPatient, idSample, idRun, file(\"${idSample}_${idRun}_filtered_r2.bam\"), emit: filtered_reads\n\n    when: params.filter_bams\n    script:\n    if( params.remove_supplementary_reads)\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(second_of_pair and mapping_quality >=${params.bam_mapping_q} \\\n                and not ([XA] != null or [SA] != null)) \\\n                or first_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r2.bam\"\n        \"\"\"\n    else\n        \"\"\"\n        init.sh\n        sambamba view -t ${task.cpus} -h \\\n            -F \"(second_of_pair and mapping_quality >=${params.bam_mapping_q}) \\\n                or first_of_pair\" \\\n                \"${idSample}_${idRun}.bam\" \\\n            | samtools sort -n --threads ${task.cpus} \\\n            | samtools fixmate - - \\\n            | samtools view -h -f0x02 > \"${idSample}_${idRun}_filtered_r2.bam\"\n        \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/FilterBamRead2"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Mutect2TN{\n    label 'container_llab'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal + \"-\" + intervalBed.baseName}\n    label 'cpus_2'\n\n    input:\n        tuple idPatient, \n            idSampleNormal, file(bamNormal), file(baiNormal),\n            idSampleTumor, file(bamTumor), file(baiTumor), \n            file(intervalBed)\n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n        file(ponSomatic)\n        file(ponSomaticIndex)\n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\"), emit: vcf\n        \n        tuple idPatient,\n            idSampleTumor,\n            idSampleNormal,\n            file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf.stats\"), emit: stats\n\n    when: 'mutect2' in tools\n\n    script:\n                                                                \n                                                                                                                    \n    PON = params.somatic_pon ? \"--panel-of-normals ${ponSomatic}\" : \"\"\n                \n    \"\"\"\n    init.sh\n    # Get raw calls\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n      Mutect2 \\\n      -R ${fasta}\\\n      -I ${bamTumor}  -tumor ${idSampleTumor} \\\n      -I ${bamNormal} -normal ${idSampleNormal} \\\n      -L ${intervalBed} \\\n      --germline-resource ${germlineResource} \\\n      ${PON} \\\n      -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/Mutect2TN"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeMutect2TNStats {\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idSampleTumor + \"_vs_\" + idSampleNormal}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                                                                                                     \n        tuple idPatient, idSampleTumor, idSampleNormal, file(statsFiles)                         \n\n    output:\n        tuple idPatient,\n            val(\"${idSampleTumor}_vs_${idSampleNormal}\"),\n            file(\"${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\")\n\n    when: 'mutect2' in tools\n\n    script:     \n      stats = statsFiles.collect{ \"-stats ${it} \" }.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        MergeMutectStats \\\n        ${stats} \\\n        -O ${idSampleTumor}_vs_${idSampleNormal}.vcf.gz.stats\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MergeMutect2TNStats"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess FilterMutect2TNCalls {\n    label 'container_llab'\n    label 'cpus_1'\n\n    tag {idSampleTN}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTN}/Mutect2\", mode: params.publish_dir_mode\n\n    input:\n                                \n                                                                              \n                                                  \n                                                        \n        tuple idPatient, \n            idSampleTN, \n            file(unfiltered), file(unfilteredIndex),\n            file(\"${idSampleTN}.vcf.gz.stats\")\n                                                        \n        \n        file(fasta)\n        file(fastaFai)\n        file(dict)\n        file(germlineResource)\n        file(germlineResourceIndex)\n                                            \n        \n    output:\n        tuple val(\"Mutect2\"), idPatient, idSampleTN,\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz\"),\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz.tbi\"),\n            file(\"filtered_mutect2_${idSampleTN}.vcf.gz.filteringStats.tsv\")\n\n                                             \n    when: 'mutect2' in tools\n\n    script:\n    \"\"\"\n    init.sh\n    # do the actual filtering\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g\" \\\n        FilterMutectCalls \\\n        -V ${unfiltered} \\\n        --stats ${idSampleTN}.vcf.gz.stats \\\n        -R ${fasta} \\\n        -O filtered_mutect2_${idSampleTN}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/FilterMutect2TNCalls"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess BaseRecalibrator {\n    label 'container_llab'\n                     \n    label 'cpus_8'\n                  \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                       \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(intervalBed)\n        file(fasta) \n        file(fastaFai)\n        file(dict)\n        file(dbsnp)\n        file(dbsnpIndex) \n        file(knownIndels)\n        file(knownIndelsIndex)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.table\")\n                                                          \n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                            \n                                         \n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/BaseRecalibrator"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess GatherBQSRReports {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n    echo true\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/DuplicateMarked\", mode: params.publish_dir_mode, overwrite: false\n\n    input:\n        tuple idPatient, idSample, file(recal)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.table\"), emit: recal_table\n                                                     \n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/GatherBQSRReports"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess ApplyBQSR {\n    label 'container_llab'\n    label 'memory_singleCPU_2_task'\n    label 'cpus_8'\n                      \n                         \n    tag {idPatient + \"-\" + idSample + \"-\" + intervalBed.baseName}\n                                        \n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed)\n        file(fasta)\n        file(fastaFai) \n        file(dict)\n\n    output:\n        tuple idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\")\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    init.sh\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/ApplyBQSR"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MergeBamRecal {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(bam)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bai\"), emit: bam_recal\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), emit: bam_recal_qc\n                                                   \n\n                                   \n\n    script:\n    \"\"\"\n    init.sh\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    mv ${idSample}.recal.bam.bai ${idSample}.recal.bai\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MergeBamRecal"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess IndexBamRecal {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\")\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\"), emit: bam_recal\n        tuple idPatient, idSample, file(\"${idSample}.recal.bam\"), emit: bam_recal_qc\n        \n    when: params.no_intervals\n\n    script:\n    \"\"\"\n    init.sh\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/IndexBamRecal"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["SAMtools", "SAMBLASTER"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MarkDuplicates {\n    label 'container_llab'\n    label 'cpus_max'\n    tag {idPatient + \"-\" + idSample}\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/${output_dir}/\", mode: params.publish_dir_mode\n\n    input:\n        tuple idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bai\")\n        val(output_dir)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bai\"), emit: marked_bams\n    when: step  ==  'mapping'\n\n    script:\n    \"\"\"\n    init.sh\n    samtools sort -n --threads ${task.cpus}  -O SAM  ${idSample}.bam | \\\n        samblaster -M --ignoreUnmated| \\\n        samtools sort --threads ${task.cpus}  -O BAM > ${idSample}.md.bam\n\n    samtools index ${idSample}.md.bam && \\\n        mv ${idSample}.md.bam.bai ${idSample}.md.bai\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MarkDuplicates"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CollectReadCounts {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n    \n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(preprocessed_intervals)\n\n    output:\n        tuple idPatient, idSample, file(\"${idSample}.counts.hdf5\"), emit: 'sample_read_counts'\n\n    when: ('gatk_cnv_somatic' in tools ||\n           'gatk_cnv_germline_cohort_mode' in tools ||\n           'gatk_cnv_germline' in tools\n           )\n\n    script:\n    \"\"\"\n    init.sh\n    gatk CollectReadCounts \\\n        -I ${bam} \\\n        -L ${preprocessed_intervals} \\\n        --interval-merging-rule OVERLAPPING_ONLY \\\n        -O ${idSample}.counts.hdf5\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CollectReadCounts"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess AnnotateIntervals {\n    label 'container_llab'\n    label 'cpus_16'\n    publishDir \"${params.outdir}/Preprocessing/gatk_gcnv/\", mode: params.publish_dir_mode\n    input:\n        file(fasta)\n        file(fasta_fai)\n        file(dict)\n        file(preprocessed_intervals)\n\n    output:\n        file(\"preprocessed_intervals.annotated.tsv\")\n\n    when: ('gatk_gcnv_cohort_mode' in tools)\n\n    script:\n    \"\"\"\n    init.sh\n     gatk AnnotateIntervals \\\n        -L ${preprocessed_intervals} \\\n        -R ${fasta} \\\n        -imr OVERLAPPING_ONLY \\\n        -O preprocessed_intervals.annotated.tsv\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/AnnotateIntervals"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess DetermineGermlineContigPloidyCohortMode {\n                             \n    label 'container_gatk'\n    label 'cpus_32'\n                \n    publishDir \"${params.outdir}/Preprocessing/gatk_gcnv/\", mode: params.publish_dir_mode\n    input:\n        file(filterd_intervals)\n        file(all_contig_ploidy_priors)\n        file(\"cvg/\")\n        \n\n    output:\n        file(\"ploidy-calls\")\n        file(\"ploidy-model\")\n\n    when:  'gatk_gcnv_cohort_mode' in tools \n\n    script:\n    \"\"\"\n    init.sh\n    for x in `ls cvg/*.tsv`\n    do\n        echo -n \"-I \\$x \" >> args.list\n    done\n\n    gatk DetermineGermlineContigPloidy \\\n    -L ${filterd_intervals} \\\n    --interval-merging-rule OVERLAPPING_ONLY \\\n    --arguments_file args.list \\\n    --contig-ploidy-priors ${all_contig_ploidy_priors} \\\n    --output . \\\n    --output-prefix ploidy \\\n    --verbosity DEBUG\n\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/DetermineGermlineContigPloidyCohortMode"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CollectHsMetrics{\n    label 'container_llab'\n    label 'cpus_16'\n    tag {idPatient + \"-\" + idSample}\n    \n    publishDir \"${params.outdir}/Reports/${idSample}/hs_metrics/\", mode: params.publish_dir_mode\n    \n    input:\n                                                      \n    tuple idPatient, idSample, file(bam)\n    file(fasta) \n    file(fastaFai)\n    file(dict)\n    file(targetBED)\n    file(baitBED)\n                          \n\n    output:\n    file(\"${bam.baseName}.txt\")\n                                                   \n    \n    \n                                                           \n    script:\n    \"\"\"\n    init.sh\n    gatk BedToIntervalList -I ${targetBED} -O target.interval_list -SD ${dict}\n    gatk BedToIntervalList -I ${baitBED} -O bait.interval_list -SD ${dict}\n\n    gatk --java-options -Xmx32G CollectHsMetrics --VALIDATION_STRINGENCY LENIENT \\\n    -I ${bam} \\\n    -O ${bam.baseName}.txt \\\n    -TI target.interval_list \\\n    -BI bait.interval_list \\\n    -R ${fasta}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CollectHsMetrics"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["TIDDIT"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess TIDDIT {\n    label 'container_sarek'\n    tag {idSample}\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}/TIDDIT\", mode: params.publish_dir_mode\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"TIDDIT_${idSample}.vcf\") \"VariantCalling/${idSample}/TIDDIT/${it}\"\n            else \"Reports/${idSample}/TIDDIT/${it}\"\n        }\n\n    input:\n        tuple idPatient, idSample, file(bam), file(bai)\n        file(fasta) \n        file(fastaFai)\n\n    output:\n        tuple val(\"TIDDIT\"), idPatient, idSample, file(\"*.vcf.gz\"), file(\"*.tbi\"), emit: vcfTIDDIT\n        tuple file(\"TIDDIT_${idSample}.old.vcf\"), file(\"TIDDIT_${idSample}.ploidy.tab\"), file(\"TIDDIT_${idSample}.signals.tab\"), file(\"TIDDIT_${idSample}.wig\"), file(\"TIDDIT_${idSample}.gc.wig\"), emit: tidditOut\n\n    when: 'tiddit' in tools\n\n    script:\n    \"\"\"\n    tiddit --sv -o TIDDIT_${idSample} --bam ${bam} --ref ${fasta}\n\n    mv TIDDIT_${idSample}.vcf TIDDIT_${idSample}.old.vcf\n\n    grep -E \"#|PASS\" TIDDIT_${idSample}.old.vcf > TIDDIT_${idSample}.vcf\n\n    bgzip --threads ${task.cpus} -c TIDDIT_${idSample}.vcf > TIDDIT_${idSample}.vcf.gz\n\n    tabix TIDDIT_${idSample}.vcf.gz\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/TIDDIT"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess BcftoolsStats {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/BCFToolsStats/${variantCaller}\", mode: params.publish_dir_mode\n    publishDir \"${params.outdir}/QC/${idSample}/BCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n                                                   \n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n                                                                 \n        file (\"*.bcf.tools.stats.out\")\n\n    when: !('bcftools' in skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    bcftools stats ${vcf} > ${reduceVCF(vcf.fileName)}.bcf.tools.stats.out\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/BcftoolsStats"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess Vcftools {\n    label 'container_llab'\n    label 'cpus_8'\n\n    tag {\"${variantCaller} - ${vcf}\"}\n\n    publishDir \"${params.outdir}/Reports/${idSample}/VCFTools/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple variantCaller, idPatient, idSample, file(vcf) , file(vcf_tbi)\n\n    output:\n        file (\"${reduceVCF(vcf.fileName)}.*\")\n\n    when: !('vcftools' in skip_qc)\n\n    script:\n    \"\"\"\n    init.sh\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-count \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --TsTv-by-qual \\\n    --out ${reduceVCF(vcf.fileName)}\n\n    vcftools \\\n    --gzvcf ${vcf} \\\n    --FILTER-summary \\\n    --out ${reduceVCF(vcf.fileName)}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/Vcftools"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CreateSomaticPON{\n    label 'container_llab'\n    label 'cpus_max'\n                         \n     publishDir \"${params.outdir}/Preprocessing/Somatic_pon\", mode: params.publish_dir_mode\n\n    input:\n    file(pon) \n    file(fasta)\n    file(fastaFai)\n    file(dict)\n    file(germlineResource)\n    file(germlineResourceIndex)\n    \n    output:\n    tuple file(out_file), file (\"${out_file}.tbi\")\n\n    when: 'gen_somatic_pon' in tools\n\n    script:\n    args_file = \"normals_for_pon_vcf.args\"\n    out_file = \"somatic_pon.vcf.gz\" \n    pon_db = \"gendb://${pon}\"\n    \n    \"\"\"\n    init.sh\n     gatk --java-options -Xmx${task.memory.toGiga()}g \\\n     CreateSomaticPanelOfNormals -R ${fasta} \\\n     --germline-resource ${germlineResource} \\\n    -V ${pon_db} \\\n    -O ${out_file}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CreateSomaticPON"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess MultiQC {\n    label 'container_sarek'\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n    input:\n        file (multiqcConfig) \n        file (versions) \n        file ('bamQC/*') \n        file ('FastQC/*') \n        file ('BCFToolsStats/*') \n        file ('VCFTools/*')\n                 'MarkDuplicates/*'  \n        file ('SamToolsStats/*') \n        file ('CollectAlignmentSummary/*')\n        file ('CollectInsertSizeMetrics/*')\n        file ('CollectHsMetrics/*')\n                 'snpEff/*'  \n\n    output:\n        tuple file(\"*multiqc_report.html\"), file(\"*multiqc_data\") \n\n    script:\n    \"\"\"\n    multiqc -f -v .\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/MultiQC"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["BCFtools", "BWA", "SAMtools", "FreeBayes", "MultiQC", "TIDDIT", "FastQC", "QualiMap", "GATK", "VCFtools"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess GetSoftwareVersions {\n    label 'container_llab'\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode\n\n    output:\n                                                                       \n        file 'software_versions_mqc.yaml'\n\n                                    \n\n    script:\n    \"\"\"\n    init.sh\n    bcftools version > v_bcftools.txt 2>&1 || true\n    bwa &> v_bwa.txt 2>&1 || true\n    configManta.py --version > v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version > v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    echo \"SNPEFF version\"\\$(snpEff -h 2>&1) > v_snpeff.txt\n    fastqc --version > v_fastqc.txt 2>&1 || true\n    freebayes --version > v_freebayes.txt 2>&1 || true\n    gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt  || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/GetSoftwareVersions"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess collect_allele_counts {\n    label 'container_llab'\n    label 'cpus_1'\n\n    publishDir \"${params.outdir}/CNV_Plotting/${idSample}/collect_allele_counts\", mode: params.publish_dir_mode\n\n    input:\n    tuple idPatient, idSample, file(bam), file(bai)\n    file(probes)\n    file(ref)    \n    file(fai)\n    file(dict)\n\n    output:\n    tuple idPatient, idSample, file(\"${idSample}.allele_count.tsv\")\n\n    script:\n    \"\"\"\n    gatk CollectAllelicCounts \\\n     -I $bam \\\n     -R $ref \\\n     -L $probes \\\n     -O ${idSample}.allele_count.tsv\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/collect_allele_counts"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CreateReadCountPon {\n    label 'container_llab'\n                \n    tag \"ReadCountPon\"\n    \n    publishDir \"${params.outdir}/Preprocessing/ReadCountPon/\", \n    mode: params.publish_dir_mode\n\n    \n    input:\n    file(read_count_hdf5s)\n    \n                      \n\n    output:\n    file(out_file)\n\n    script:\n    when:'gen_read_count_pon' in tools\n                                    \n    out_file = \"read_count_pon.hdf5\"\n    params_str = ''\n                                  \n    read_count_hdf5s.each{\n        params_str = \"${params_str} -I ${it}\"\n    }\n\n    \n    \"\"\"\n    init.sh\n    gatk CreateReadCountPanelOfNormals \\\n        $params_str \\\n        -O $out_file\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CreateReadCountPon"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess PlotDenoisedCopyRatios {\n    label 'container_llab'\n    label 'cpus_16'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/Preprocessing/${idSample}/\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(std_copy_ratio), file(denoised_copy_ratio)\n        file(dict)\n    \n    output:\n        file(out_dir)  \n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    out_dir = \"PlotDenoisedReadCounts\" \n\n    \"\"\"\n    init.sh\n    mkdir ${out_dir}\n    gatk PlotDenoisedCopyRatios \\\n        --standardized-copy-ratios ${std_copy_ratio} \\\n        --denoised-copy-ratios ${denoised_copy_ratio} \\\n        --sequence-dictionary ${dict} \\\n        --output-prefix ${idSample} \\\n        -O ${out_dir}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/PlotDenoisedCopyRatios"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess ModelSegments {\n    label 'container_llab'\n    label 'cpus_32'\n    tag \"${idSample}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSample}\", mode: params.publish_dir_mode\n    \n    input:\n         tuple idPatient, idSample, file(denoised_copy_ratio)\n\n    output:\n        tuple idPatient, idSample, file(\"${out_dir}/${idSample}.cr.seg\"), file(\"${out_dir}/${idSample}.modelFinal.seg\"), emit: 'modeled_seg'\n\n    when: ('gatk_cnv_somatic' in tools)\n\n    script:\n    out_dir = \"ModeledSegments\"\n\n    \"\"\"\n    init.sh\n    mkdir $out_dir\n    gatk ModelSegments \\\n        --denoised-copy-ratios ${denoised_copy_ratio} \\\n        --output-prefix ${idSample} \\\n        -O ${out_dir}\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/ModelSegments"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["ryanlayerlab"], "nb_wf": 1, "list_wf": ["layer_lab_chco"], "list_contrib": ["MSBradshaw", "javaidm"], "nb_contrib": 2, "codes": ["\nprocess CallCopyRatioSegments {\n    label 'container_llab'\n   label 'cpus_8'\n    tag \"${idSample}\"\n    \n    publishDir \"${params.outdir}/VariantCalling/${idSample}/CalledCopyRatioSegments\", mode: params.publish_dir_mode\n    \n    input:\n        tuple idPatient, idSample, file(\"${idSample}.cr.seg\")\n    \n    output:\n        file(\"${idSample}.called.seg\")\n\n    when: ('gatk_cnv_somatic' in tools)\n    script:\n    \n    \"\"\"\n    init.sh\n    gatk CallCopyRatioSegments \\\n        -I ${idSample}.cr.seg \\\n        -O ${idSample}.called.seg\n    \"\"\"\n}"], "list_proc": ["ryanlayerlab/layer_lab_chco/CallCopyRatioSegments"], "list_wf_names": ["ryanlayerlab/layer_lab_chco"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["rynge"], "nb_wf": 1, "list_wf": ["searchsra-nextflow"], "list_contrib": ["rynge"], "nb_contrib": 1, "codes": ["\nprocess buildIndex {\n    input:\n    file reference from reference_file\n      \n    output:\n    file '*.index*' into reference_index\n        \n    \"\"\"\n    bowtie2-build ${reference} ${reference}.index\n    \"\"\"\n}"], "list_proc": ["rynge/searchsra-nextflow/buildIndex"], "list_wf_names": ["rynge/searchsra-nextflow"]}, {"nb_reuse": 1, "tools": ["Bismark"], "nb_own": 1, "list_own": ["s-andrews"], "nb_wf": 1, "list_wf": ["nextflow_pipelines"], "list_contrib": ["FelixKrueger", "laurabiggins", "s-andrews"], "nb_contrib": 3, "codes": ["\nprocess BISMARK {\n\t\n\n\ttag \"$name\"                                                        \n\n\t                                                                 \n\t                 \n\n\tcpus { 5 }\n  \tmemory { 20.GB * task.attempt }  \n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 30 as long); return 'retry' }\n  \tmaxRetries 3\n\t\n\t                 \n\t                    \n\t                   \n\t\t\n    input:\n\t    tuple val(name), path(reads)\n\t\tval (outputdir)\n\t\tval (bismark_args)\n\t\tval (verbose)\n\n\toutput:\n\t    tuple val(name), path (\"*bam\"),        emit: bam\n\t\tpath \"*report.txt\", emit: report\n\t\t                                                                                                  \n\t\ttuple val(name), path (\"*unmapped_reads_1.fq.gz\"), optional: true, emit: unmapped1\n\t\ttuple val(name), path (\"*unmapped_reads_2.fq.gz\"), optional: true, emit: unmapped2\n\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n    script:\n\t\tcores = 1\n\t\treadString = \"\"\n\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] BISMARK ARGS: \" + bismark_args)\n\t\t}\n\n\t\t                     \n\t\tbismark_options = bismark_args\n\t\tif (params.singlecell){\n\t\t\tbismark_options += \" --non_directional \"\n\t\t}\n\t\telse{\n\t\t\n\t\t}\n\t\t\n\t\tunmapped_1_name = ''\n\t\tunmapped_2_name = ''\n\t\t\n\t\tif (params.unmapped){\n\t\t\tbismark_options += \" --unmapped \"\n\t\t\tunmapped_1_name = name + \"_unmapped_R1\"\n\t\t\tunmapped_2_name = name + \"_unmapped_R2\"\n\t\t}\n\n\t\tif (params.pbat){\n\t\t\tbismark_options += \" --pbat \"\n\t\t}\n\n\t\tif (reads instanceof List) {\n\t\t\treadString = \"-1 \"+reads[0]+\" -2 \"+reads[1]\n\t\t}\n\t\telse {\n\t\t\treadString = reads\n\t\t}\n\n\t\tindex = \"--genome \" + params.genome[\"bismark\"]\n\n\t\tunmapped_name = ''\t\n\t\t\t                                              \n\t\tif (params.read_identity == \"1\" || params.read_identity == \"2\"){\n\t\t\t                                  \n\t\t\tif (params.read_identity == \"1\"){\n\t\t\t\tunmapped_name = name + \"_unmapped_R1\"\n\t\t\t}\n\t\t\telse{\n\t\t\t\tunmapped_name = name + \"_unmapped_R2\"\n\t\t\t}\n\n\t\t\tif (bismark_args =~ /-hisat/){                                           \n\t\t\t\tbismark_name = unmapped_name + \"_\" + params.genome[\"name\"] + \"_bismark_hisat2\"\n\t\t\t}\n\t\t\telse{                       \n\t\t\t\tbismark_name = unmapped_name + \"_\" + params.genome[\"name\"] + \"_bismark_bt2\"\n\t\t\t}\n\t\t}\n\t\telse{\n\t\t\tif (bismark_args =~ /-hisat/){                                           \n\t\t\t\tbismark_name = name + \"_\" + params.genome[\"name\"] + \"_bismark_hisat2\"\n\t\t\t}\n\t\t\telse{                       \n\t\t\t\tbismark_name = name + \"_\" + params.genome[\"name\"] + \"_bismark_bt2\"\n\t\t\t}\n\t\t}\t\n\t\t                                             \n\t\t\n\t\t\"\"\"\n\t\tmodule load bismark\n\t\tbismark --parallel $cores --basename $bismark_name $index $bismark_options $readString\n\t\t\"\"\"\n\n}"], "list_proc": ["s-andrews/nextflow_pipelines/BISMARK"], "list_wf_names": ["s-andrews/nextflow_pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Bowtie"], "nb_own": 1, "list_own": ["s-andrews"], "nb_wf": 1, "list_wf": ["nextflow_pipelines"], "list_contrib": ["FelixKrueger", "laurabiggins", "s-andrews"], "nb_contrib": 3, "codes": ["\nprocess BOWTIE2 {\n\t\n\ttag \"$name\"                                                        \n\n\tlabel 'bigMem'\n\tlabel 'multiCore'\n\t\t\n    input:\n\t    tuple val(name), path(reads)\n\t\tval (outputdir)\n\t\tval (bowtie2_args)\n\t\tval (verbose)\n\n\toutput:\n\t    tuple val(name), path (\"*bam\"),        emit: bam\n\t\tpath \"*stats.txt\", emit: stats \n\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n\tscript:\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] BOWTIE2 ARGS: \" + bowtie2_args)\n\t\t}\n\n\t\tcores = 8\n\t\treadString = \"\"\n\n\t\t                     \n\t\tbowtie_options = bowtie2_args\n\t\tbowtie_options +=  \" --no-unal \"                                                 \n\t\t\n\t\tif (params.local == '--local'){\n\t\t\t                                              \n\t\t\tbowtie_options += \" ${params.local} \" \n\t\t}\n\n\t\tif (reads instanceof List) {\n\t\t\treadString = \"-1 \" + reads[0] + \" -2 \" + reads[1]\n\t\t\tbowtie_options += \" --no-discordant --no-mixed \"                                     \n\t\t}\n\t\telse {\n\t\t\treadString = \"-U \" + reads\n\t\t}\n\n\n\t\tindex = params.genome[\"bowtie2\"]\n\t\tbowtie_name = name + \"_\" + params.genome[\"name\"]\n\n\t\t\"\"\"\n\t\tmodule load bowtie2\n\t\tmodule load samtools\n\t\tbowtie2 -x ${index} -p ${cores} ${bowtie_options} ${readString}  2>${bowtie_name}_bowtie2_stats.txt | samtools view -bS -F 4 -F 8 -F 256 -> ${bowtie_name}_bowtie2.bam\n\t\t\"\"\"\n\n}"], "list_proc": ["s-andrews/nextflow_pipelines/BOWTIE2"], "list_wf_names": ["s-andrews/nextflow_pipelines"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["s-andrews"], "nb_wf": 1, "list_wf": ["nextflow_pipelines"], "list_contrib": ["FelixKrueger", "laurabiggins", "s-andrews"], "nb_contrib": 3, "codes": ["\nprocess MULTIQC {\n\t\n\tlabel 'quadCore'\n\n\t                    \n\tmemory { 20.GB * task.attempt }  \n\terrorStrategy { sleep(Math.pow(2, task.attempt) * 30 as long); return 'retry' }\n\tmaxRetries 3\n\n    input:\n\t    path (file)\n\t\tval (outputdir)\n\t\tval (multiqc_args)\n\t\tval (verbose)\n\n\toutput:\n\t    path \"*html\",       emit: html\n\t\t\n\tpublishDir \"$outputdir\",\n\t\tmode: \"link\", overwrite: true\n\n\tscript:\n\n\t\t\n\t\tif (verbose){\n\t\t\tprintln (\"[MODULE] MULTIQC ARGS: \" + multiqc_args)\n\t\t}\n\t\n\t\t\"\"\"\n\t\tmodule load multiqc\n\t\tmultiqc $multiqc_args -x work --filename ${params.prefix}multiqc_report.html .\n\t\t\"\"\"\n}"], "list_proc": ["s-andrews/nextflow_pipelines/MULTIQC"], "list_wf_names": ["s-andrews/nextflow_pipelines"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["sagc-bioinformatics"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["ashleethomson", "nathanhaigh", "a-lud", "jimmybgammyknee"], "nb_contrib": 4, "codes": ["\nprocess SalmonIndex {\n    tag \"$transcriptome.simpleName\"\n\n    input:\n    path transcriptome \n\n    output:\n    path 'index' \n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/SalmonIndex"], "list_wf_names": ["sagc-bioinformatics/modules"]}, {"nb_reuse": 1, "tools": ["FusionCatcher"], "nb_own": 1, "list_own": ["sagc-bioinformatics"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["ashleethomson", "nathanhaigh", "a-lud", "jimmybgammyknee"], "nb_contrib": 4, "codes": ["process fusioncatcher_v133 {\n\n    tag { \"fusioncatcher_v133 - ${filename}\" } \n    publishDir \"${outdir}/${group}/${filename}/fusioncatcher_v133\", mode: 'copy'\n    label 'process_fusioncatcher_v133'\n\n    input:\n    tuple val(filename), val(group), val(sample), val(path), file(reads)\n    val fusioncatcher_db\n\tval outdir\n     \n    output:\n    file \"${filename}_fusioncatcher_v133.txt\"\n\n    script:\n    \"\"\"\n\tfusioncatcher \\\n        -p ${task.cpus} \\\n        -d ${fusioncatcher_db} \\\n        -i ${reads[0]},${reads[1]} \\\n        -o \\${PWD} \\\n        --skip-blat\n\t\t\t\n\tmv final-list_candidate-fusion-genes.txt ${filename}_fusioncatcher_v133.txt\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/fusioncatcher_v133"], "list_wf_names": ["sagc-bioinformatics/modules"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["sagc-bioinformatics"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["ashleethomson", "nathanhaigh", "a-lud", "jimmybgammyknee"], "nb_contrib": 4, "codes": ["process starAlign {\n    tag { \"STAR align - ${sample_id}\"  }\n    publishDir \"${outdir}/STAR\", mode: 'copy'\n                                                                      \n    label 'process_high'\n\n    input:\n    tuple val(sample_id), file(reads)\n    path star_idx\n    val outdir\n    val opt_args\n\n    output:\n    tuple val(sample_id),\n        file(\"${sample_id}.Aligned.sortedByCoord.out.bam\"),\n        file(\"${sample_id}.Aligned.sortedByCoord.out.bam.bai\")\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    STAR \\\n    ${usr_args} \\\n    --genomeDir ${star_idx} \\\n    --readFilesIn ${reads} \\\n    --readFilesCommand zcat \\\n    --runThreadN ${task.cpus} \\\n    --outSAMtype BAM SortedByCoordinate \\\n    --outSAMattributes NH HI NM MD AS \\\n    --outFileNamePrefix ${sample_id}\".\" \\\n    --outSAMattrRGline ID:${sample_id} LB:Lib1 PL:illumina PU:machine SM:${sample_id}_SM \\\n    --chimOutType WithinBAM \\\n    --chimSegmentMin 20 \\\n    --twopassMode Basic \\\n    --outReadsUnmapped Fastx\n\n    samtools index -@ ${task.cpus} ${sample_id}.Aligned.sortedByCoord.out.bam\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/starAlign"], "list_wf_names": ["sagc-bioinformatics/modules"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["sagc-bioinformatics"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["ashleethomson", "nathanhaigh", "a-lud", "jimmybgammyknee"], "nb_contrib": 4, "codes": ["process kraken2 {\n\n    tag { \"Kraken2 - ${sample_id}\" } \n    publishDir \"${outdir}/${sampleProject}/QC-results/kraken2\", mode: 'copy'\n    label 'process_kraken'\n\n    input:\n    tuple val(sample_id), file(reads)\n    val outdir\n    val sampleProject\n\n    output:\n    path \"${sample_id}.kraken2\", emit: stats\n\n    script:\n    \"\"\"\n    kraken2 \\\n        --db /data/bioinformatics/bcbio_genomes/others/kraken2_standard_20200919\\\n        --quick \\\n        --threads ${task.cpus} \\\n        --gzip-compressed \\\n        --memory-mapping \\\n        --report ${sample_id}.kraken2 \\\n        ${reads}\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/kraken2"], "list_wf_names": ["sagc-bioinformatics/modules"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["sagc-bioinformatics"], "nb_wf": 1, "list_wf": ["modules"], "list_contrib": ["ashleethomson", "nathanhaigh", "a-lud", "jimmybgammyknee"], "nb_contrib": 4, "codes": ["process fastqc {\n\n    tag { \"FastQC - ${sample_id}\" } \n    publishDir \"${outdir}/${sampleProject}/QC-results/fastqc\", mode: 'copy'\n    label 'process_low'\n\n    input:\n    tuple val(sample_id), file(reads)\n    val outdir\n    val opt_args\n    val sampleProject\n\n    output:\n    tuple val(sample_id), path(\"*.{zip,html}\"), emit: fastqc_output\n\n    script:\n    def usr_args = opt_args ?: ''\n\n    \"\"\"\n    fastqc ${usr_args} -t ${task.cpus} -q ${reads[0]}\n    \"\"\"\n}"], "list_proc": ["sagc-bioinformatics/modules/fastqc"], "list_wf_names": ["sagc-bioinformatics/modules"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process pre_fastqc {\n\n    publishDir = output\n\n    input:\n    tuple sample_id, file(reads_file) from reads_process\n\n    output:\n    file(\"pre_fastqc_${sample_id}_logs\") into fastqc_ch1\n\n    script:\n    \"\"\"\n    mkdir  pre_fastqc_${sample_id}_logs\n    fastqc -o  pre_fastqc_${sample_id}_logs -f fastq -q ${reads_file}\n    \"\"\"\n    }"], "list_proc": ["salvadorlab/mbovpan/pre_fastqc"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process fastp {\n\n    publishDir = output\n\n    cpus threads\n\n    input:\n    tuple sample_id, file(reads_file) from reads_trim\n\n    output:\n    file(\"${sample_id}_trimmed_R*.fastq\") into fastp_ch\n \n    script:\n    \"\"\"\n    fastp -w ${task.cpus} -q 30 --detect_adapter_for_pe -i  ${reads_file[0]} -I  ${reads_file[1]} -o  ${sample_id}_trimmed_R1.fastq -O  ${sample_id}_trimmed_R2.fastq\n    \"\"\"\n\n    }"], "list_proc": ["salvadorlab/mbovpan/fastp"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process post_fastqc {\n\n    publishDir = output\n\n    input:\n    tuple file(trim1), file(trim2) from fastp_reads1\n\n    output:\n    file(\"post_fastqc_${trim1.baseName - ~/_trimmed_R*/}_logs\") into fastqc_ch2\n\n    script:\n    \"\"\"\n    mkdir  post_fastqc_${trim1.baseName - ~/_trimmed_R*/}_logs\n    fastqc -o  post_fastqc_${trim1.baseName - ~/_trimmed_R*/}_logs -f fastq -q ${trim1} ${trim2}\n    \"\"\"\n    }"], "list_proc": ["salvadorlab/mbovpan/post_fastqc"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process mark_dups {\n    publishDir = output \n\n    conda \"$workflow.projectDir/envs/picard.yaml\"    \n\n    input:\n    file(bam) from bam\n\n    output:\n    file(\"${bam.baseName}.nodup.bam\") into nodup_ch\n\n    script:\n    \"\"\"\n    picard MarkDuplicates INPUT=${bam} OUTPUT=${bam.baseName}.nodup.bam ASSUME_SORTED=true REMOVE_DUPLICATES=true METRICS_FILE=dup_metrics.csv USE_JDK_DEFLATER=true USE_JDK_INFLATER=true\n    samtools index ${bam.baseName}.nodup.bam\n    cp ${bam.baseName}.nodup.bam.bai $workflow.launchDir\n    \"\"\"\n    }"], "list_proc": ["salvadorlab/mbovpan/mark_dups"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["VCFFilterJS", "BEDTools"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process vcf_filter {\n    publishDir = output \n\n    conda \"$workflow.projectDir/envs/vcflib.yaml\"\n\n    input:\n    file(vcf) from freebayes_ch\n\n    output:\n    file(\"${vcf.baseName}.filtered.vcf\") into filter_ch\n\n    script:\n    \"\"\"\n    vcffilter -f \"QUAL > ${qual}\" ${vcf} | vcffilter -f \"DP > ${depth}\" | vcffilter -f \"MQM > ${mapq}\" |  bedtools intersect -header -a - -b $workflow.projectDir/ref/pe_ppe_regions.gff3 -v > ${vcf.baseName}.filtered.vcf\n    \"\"\"\n\n    }"], "list_proc": ["salvadorlab/mbovpan/vcf_filter"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process psuedo_assembly {\n        publishDir = output\n\n        conda \"$workflow.projectDir/envs/consensus.yaml\"\n\n        input:\n        file(vcf) from filter1_ch \n\n        output:\n        file(\"${vcf.baseName}.consensus.fasta\") into fasta_ch\n\n        script:\n        \"\"\"\n        bgzip ${vcf} \n        bcftools index ${vcf}.gz\n        cat ${ref} | vcf-consensus ${vcf.baseName}.vcf.gz > ${vcf.baseName}.dummy.fasta\n        sed 's|LT708304.1 Mycobacterium bovis AF2122/97 genome assembly, chromosome: Mycobacterium_bovis_AF212297|${vcf.baseName}|g' ${vcf.baseName}.dummy.fasta > ${vcf.baseName}.consensus.fasta\n        \"\"\"\n    }"], "list_proc": ["salvadorlab/mbovpan/psuedo_assembly"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["MEGAHIT"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": [" process assembly {\n    publishDir = output \n    \n    conda \"$workflow.projectDir/envs/megahit.yaml\"\n    \n    errorStrategy \"ignore\"\n\n    cpus threads\n\n    input:\n    tuple file(trim1), file(trim2) from fastp_reads3\n\n    output:\n    file(\"${trim1.baseName}.scaffold.fasta\") into shortassembly_ch\n\n    script:\n    \"\"\"\n    megahit -t ${task.cpus} -1 ${trim1} -2 ${trim2} -o ${trim1.baseName}\n    cd ${trim1.baseName}\n    mv final.contigs.fa  ../${trim1.baseName}.scaffold.fasta\n    \"\"\"\n}"], "list_proc": ["salvadorlab/mbovpan/assembly"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": ["\nprocess annotate {\n    publishDir = output\n    \n    cpus threads\n\n    conda \"$workflow.projectDir/envs/prokka.yaml\"\n                                           \n                                                 \n\n    input:\n    file(assembly) from assembly_ch2\n\n    output:\n    file(\"${assembly.baseName}.annot.gff\") into annotate_ch\n\n    script:\n    \"\"\"\n    prokka  --outdir ./${assembly.baseName} --cpus ${task.cpus} --prefix ${assembly.baseName}.annot ${assembly} \n    cp ./${assembly.baseName}/${assembly.baseName}.annot.gff ./\n    \"\"\"\n}"], "list_proc": ["salvadorlab/mbovpan/annotate"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["Roary"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": ["\nprocess roary {\n    publishDir = output\n\n    conda \"$workflow.projectDir/envs/roary.yaml\"\n\n    cpus threads\n\n    input:\n    file(gff) from annotate_ch.collect()\n\n    output:\n    file(\"*\") into roary_ch\n\n    script:\n    \"\"\"\n    roary -e --mafft -p ${task.cpus} $gff\n    \"\"\"\n}"], "list_proc": ["salvadorlab/mbovpan/roary"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["salvadorlab"], "nb_wf": 1, "list_wf": ["mbovpan"], "list_contrib": ["noahaus"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir = output\n    \n    conda \"$workflow.projectDir/envs/multiqc.yaml\"\n\n    input:\n    file(pre) from fastqc_ch1.collect().ifEmpty([])\n    file(post) from fastqc_ch2.collect().ifEmpty([])\n                                                                               \n\n    output:\n    file(\"mbovpan_report*\")\n\n    script:\n    \"\"\"\n    multiqc -n mbovpan_report .\n    \"\"\"\n}"], "list_proc": ["salvadorlab/mbovpan/multiqc"], "list_wf_names": ["salvadorlab/mbovpan"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["salzmanlab"], "nb_wf": 1, "list_wf": ["ReadZS"], "list_contrib": ["eameyer", "kaitlinchaung", "salzmanlab"], "nb_contrib": 3, "codes": ["process ANNOTATE_WINDOWS {\n  label 'process_medium'\n  publishDir \"${params.outdir}/annotated_files\",\n    mode: 'copy'\n\n  conda 'bioconda::bedtools=2.30.0'\n  input:\n  val isCellranger\n  path chr_lengths\n  path annotation_bed\n  val binSize\n\n  output:\n  path \"annotated_windows.file\",   emit: annotated_windows\n\n  script:\n  if (isCellranger)\n    \"\"\"\n    bedtools makewindows -g ${chr_lengths} -w ${binSize} -i srcwinnum |\n      awk -v OFS='\\t' '{print \"chr\"\\$1,\\$2,\\$3,\"chr\"\\$4, \".\", \"+\"}' |\n      sort -k1,1 -k2,2n > windows.file\n    bedtools intersect -a windows.file -b ${annotation_bed} -loj -wa |\n      awk -v OFS='\\t' '{print \\$1,\\$2,\\$3,\\$4,\\$10}' |\n      bedtools groupby -g 1,2,3,4 -c 5 -o collapse |\n      sed '1i chr\\tstart\\tend\\tchr_window\\tgene' > annotated_windows.file\n    \"\"\"\n  else\n    \"\"\"\n    bedtools makewindows -g ${chr_lengths} -w ${binSize} -i srcwinnum |\n      awk -v OFS='\\t' '{print \\$0, \".\", \"+\"}' |\n      sort -k1,1 -k2,2n > windows.file\n    bedtools intersect -a windows.file -b ${annotation_bed} -loj -wa |\n      awk -v OFS='\\t' '{print \\$1,\\$2,\\$3,\\$4,\\$10}' |\n      bedtools groupby -g 1,2,3,4 -c 5 -o collapse |\n      sed '1i chr\\tstart\\tend\\tchr_window\\tgene' > annotated_windows.file\n    \"\"\"\n}"], "list_proc": ["salzmanlab/ReadZS/ANNOTATE_WINDOWS"], "list_wf_names": ["salzmanlab/ReadZS"]}, {"nb_reuse": 6, "tools": ["SAMtools", "kraken2", "MultiQC", "bedGraphToBigWig", "VCFtools"], "nb_own": 5, "list_own": ["sebastianlzy", "tamara-hodgetts", "wtsi-hgi", "salzmanlab", "shaunchuah"], "nb_wf": 5, "list_wf": ["cfdna_nextflow", "nextflow-demo", "nextflow-pipelines", "ReadZS", "nf-atac-seq"], "list_contrib": ["shaunchuah", "eameyer", "gn5", "wtsi-mercury", "kaitlinchaung", "salzmanlab", "tamara-hodgetts"], "nb_contrib": 7, "codes": ["\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    } else {\n        container \"quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1\"\n    }\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"*.version.txt\"             , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig $bedgraph $sizes ${prefix}.bigWig\n    echo $VERSION > ${software}.version.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess merge_bam {\n\n    tag \"$sample_id\"\n    echo true\n\n    publishDir \"${params.outdir}/wgbs/$sample_id/unsortedButMerged_ForBismark_file\", mode:\"copy\", overwrite: true\n\n    input:\n    tuple val(sample_id), file(bam_file1), file(bam_file2) from ch_bismark_bam2\n\n    output:\n    tuple val(sample_id), file(\"*_unsorted_merged.bam\") into ch_bismark_merged_bam\n\n\n    script:\n    \"\"\"\n    ## module load samtools/1.3\n\n    samtools merge -nf ${sample_id}_unsorted_merged.bam $bam_file1 $bam_file2\n\n    ## remove individual bamfiles\n   #  rm $bam_file1 $bam_file2\n\n    \"\"\"\n}", "\nprocess crams_to_fastq_gz {\n    tag \"crams to fastq_gz ${samplename} ${batch}\"\n\n                                \n                                                   \n    \n    container \"samtools-1.6\"                                      \n    containerOptions = \"--bind /lustre/scratch117/core/sciops_repository/cram_cache --bind /lustre/scratch118/core/sciops_repository/cram_cache\"\n                                \n    errorStrategy 'retry'\n    maxRetries 3\n    time '300m'\n    cpus 1\n    memory '2G'\n\n                                                                                                            \n                                                                                                                            \n                                                                                \n    publishDir \"${params.outdir}/fastq12/\", mode: 'symlink'\n    \n    when:\n    params.run\n    \n    input: \n    set val(samplename), val(batch), file(crams) \n    output: \n    set val(samplename), val(batch), file(\"*.fastq.gz\")\n    file('*.lostcause.txt') optional true \n    file('numreads.txt') optional true \n    script:\n\n                                                                                \n                                                   \n                                                                                                                                    \n    def cramfile = \"${batch}.${samplename}_merged.cram\"\n    \"\"\"\n    export REF_PATH=/lustre/scratch117/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch118/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s\n\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    \n\n    f1=${batch}.${samplename}_1.fastq.gz\n    f2=${batch}.${samplename}_2.fastq.gz\n    f0=${batch}.${samplename}.fastq.gz\n\n    numreads=\\$(samtools view -c -F 0x900 $crams)\n    if (( numreads >= ${params.min_reads} )); then\n                              # -O {stdout} -u {no compression}\n                              # -N {always append /1 and /2 to the read name}\n                              # -F 0x900 (bit 1, 8, filter secondary and supplementary reads)\n      echo -n \\$numreads > numreads.txt\n      samtools collate    \\\\\n          -O -u           \\\\\n          -@ ${task.cpus} \\\\\n          ${crams} | \\\\\n      samtools fastq      \\\\\n          -N              \\\\\n          -F 0x900        \\\\\n          -@ ${task.cpus} \\\\\n          -1 \\$f1 -2 \\$f2  -0 \\$f0 \\\\\n          -\n      sleep 2\n      find . -name \\\"*.fastq.gz\\\" -type 'f' -size -160k -delete\n    else\n      echo -e \"${samplename}\\\\tcram\\\\tlowreads\" > ${batch}.${samplename}.lostcause.txt\n    fi\n    \"\"\"\n}", "\nprocess run_kraken2_direct {\n    publishDir \"$params.outdir/kraken2/report/\", mode: 'copy', pattern: \"*_kraken2report.txt\"\n    publishDir \"$params.outdir/kraken2/output/\", pattern: \"*_kraken2output.txt\"\n    container 'staphb/kraken2:2.1.2-no-db'\n    cpus \"$params.cpus\".toInteger()\n    tag \"$sample_id - kraken2_direct\"\n\n    input:\n    tuple val(sample_id), file(reads_file) from reads_for_direct_kraken2\n    file kraken2_db from kraken2_db_ch\n\n    output:\n    file '*_kraken2report.txt' into kraken_biom_ch\n    file '*_kraken2output.txt'\n\n    script:\n    \"\"\"\n    tar -xvf $kraken2_db\n    kraken2 \\\n        --db . \\\n        --report ${sample_id}_kraken2report.txt \\\n        --output ${sample_id}_kraken2output.txt \\\n        --use-names \\\n        --threads ${task.cpus} \\\n        ${reads_file[0]}\n    \"\"\"\n}", "\nprocess vcf_remove_chrXY {\n    memory '4G'\n    tag \"$samplename\"\n    cpus 2\n    disk '60 GB'\n    scratch '/tmp'\n    stageInMode 'copy'\n    stageOutMode 'rsync'\n    time '1000m'\n    container \"copy_number_v2\"\n    maxForks 40\n                                               \n                                \n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    publishDir \"${params.outdir}/copy_number/\", mode: 'symlink', overwrite: true \n    maxRetries 3\n\n    when:\n    params.run\n     \n    input: \n    tuple val(samplename), file(samplename_gt_vcf)\n    \n    output: \n    tuple val(samplename), file(\"${samplename}.noXY.recode.vcf\"), emit: samplename_cn_vcf\n\n    script:\n    \"\"\" \nexport ROOTSYS=/root\nexport MANPATH=/root/man:/usr/local/man:/usr/local/share/man:/usr/share/man\nexport USER_PATH=/home/ubuntu/error/speedseq/bin/:/home/ubuntu/anaconda3/envs/py2/bin:/home/ubuntu/anaconda3/condabin:/usr/local/go/bin:/home/ubuntu/error/root/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/go/bin:/home/ubuntu/go/bin:/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin\nexport LD_LIBRARY_PATH=/root/lib:/.singularity.d/libs\nexport LIBPATH=/root/lib\nexport JUPYTER_PATH=/root/etc/notebook\nexport DYLD_LIBRARY_PATH=/root/lib\nexport PYTHONPATH=/root/lib\nexport SHLIB_PATH=/root/lib\nexport CMAKE_PREFIX_PATH=/root\nexport CLING_STANDARD_PCH=none\n\nexport PATH=/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/speedseq/bin:/miniconda/bin:\\$PATH\n\n    eval \\\"\\$(conda shell.bash hook)\\\"\n    conda activate py2\n   vcftools --vcf ${samplename_gt_vcf} --not-chr chrX --not-chr chrY --recode --recode-INFO-all --out ${samplename}.noXY\n    \"\"\"\n}"], "list_proc": ["tamara-hodgetts/nf-atac-seq/UCSC_BEDGRAPHTOBIGWIG", "salzmanlab/ReadZS/MULTIQC", "sebastianlzy/nextflow-demo/merge_bam", "wtsi-hgi/nextflow-pipelines/crams_to_fastq_gz", "shaunchuah/cfdna_nextflow/run_kraken2_direct", "wtsi-hgi/nextflow-pipelines/vcf_remove_chrXY"], "list_wf_names": ["sebastianlzy/nextflow-demo", "wtsi-hgi/nextflow-pipelines", "shaunchuah/cfdna_nextflow", "salzmanlab/ReadZS", "tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 3, "tools": ["FastQC", "Bracken", "Minimap2"], "nb_own": 2, "list_own": ["xiaoli-dong", "salzmanlab"], "nb_wf": 3, "list_wf": ["ReadZS", "pathogen", "magph"], "list_contrib": ["eameyer", "kaitlinchaung", "xiaoli-dong", "salzmanlab"], "nb_contrib": 4, "codes": ["\nprocess BRACKEN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bracken=2.6.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bracken%3A2.6.1--py39h7cff6ad_2\"\n    } else {\n        container \"quay.io/biocontainers/bracken:2.6.1--py39h7cff6ad_2\"\n    }\n\n    input:\n    tuple val(meta), path(kraken2_report)\n    path kraken2_db\n\n    output:\n    tuple val(meta), path('*_bracken.output.txt'), emit: output\n    tuple val(meta), path('*_bracken.outreport.txt'), emit: outreport\n    path (\"versions.yml\"), emit: versions\n\n    script:\n    def prefix  = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n   \n    \"\"\"\n    bracken \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        -d $kraken2_db \\\\\n        -i ${kraken2_report} \\\\\n        -o ${prefix}_bracken.output.txt \\\\\n        -w ${prefix}_bracken.outreport.txt \n   \n    printf \"BRACKEN:\\n  bracken: 2.6.1\\n\" > versions.yml\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_SHORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: version\n\n    script:\n                                                                          \n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["xiaoli-dong/pathogen/BRACKEN", "xiaoli-dong/magph/MINIMAP2_ALIGN_SHORT", "salzmanlab/ReadZS/FASTQC"], "list_wf_names": ["xiaoli-dong/pathogen", "xiaoli-dong/magph", "salzmanlab/ReadZS"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["salzmanlab"], "nb_wf": 1, "list_wf": ["ReadZS"], "list_contrib": ["eameyer", "kaitlinchaung", "salzmanlab"], "nb_contrib": 3, "codes": ["process PICARD {\n    tag \"${bamName}\"\n    label 'process_medium'\n\n    conda 'bioconda::picard=2.26.2'\n\n    input:\n    tuple val(inputChannel), val(bamFileID), path(bam)\n\n    output:\n    tuple val(inputChannel), val(bamFileID), path(\"*dedup*\"), emit: bam_tuple\n\n    script:\n    outputFile = \"${bamFileID}.dedup\"\n    metrics = \"${bamFileID}.metrics\"\n    \"\"\"\n    picard MarkDuplicates -I ${bam} -O ${outputFile} -M ${metrics} --QUIET true\n    \"\"\"\n}"], "list_proc": ["salzmanlab/ReadZS/PICARD"], "list_wf_names": ["salzmanlab/ReadZS"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 1, "list_wf": ["hgtsimulation"], "list_contrib": ["samlhao"], "nb_contrib": 1, "codes": ["\nprocess fastp {\n    publishDir \"${params.outdir}/fastp/${plasmid}/${recipient}/\", mode: 'copy'\n\n    input:\n    tuple(plasmid, recipient, path(reads)) from fastp_ch\n\n    output:\n    tuple(plasmid, recipient, path(\"trim*.fastq.gz\")) into assembly_ch\n    path(\"*fastp.{json,html}\")\n\n    script:\n    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} -o trim_${reads[0]} -O trim_${reads[1]} -w ${task.cpus} --json ${plasmid}_${recipient}_fastp.json --html ${plasmid}_${recipient}_fastp.html\n    \"\"\"\n}"], "list_proc": ["samlhao/hgtsimulation/fastp"], "list_wf_names": ["samlhao/hgtsimulation"]}, {"nb_reuse": 1, "tools": ["Unicycler"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 1, "list_wf": ["hgtsimulation"], "list_contrib": ["samlhao"], "nb_contrib": 1, "codes": ["\nprocess unicycler {\n    label \"process_high\"\n    tag \"${plasmid}_${recipient}\"\n    publishDir \"${params.outdir}/unicycler/${plasmid}/${recipient}\", mode: 'copy'\n\n    input:\n    tuple(plasmid, recipient, path(reads)) from assembly_ch\n\n    output:\n    tuple (plasmid, recipient, path(\"${plasmid}_${recipient}_assembly.fasta\"), path(\"${plasmid}_${recipient}_assembly.gfa\")) into (quast_ch, abricate_ch)\n    path(\"${plasmid}_${recipient}_assembly.gfa\")\n    path(\"${plasmid}_${recipient}_unicycler.log\")\n\n    script:\n    \"\"\"\n    unicycler --threads ${task.cpus} ${params.unicycler_args} --keep 0 -o . -1 ${reads[0]} -2 ${reads[1]}\n    cp unicycler.log ${plasmid}_${recipient}_unicycler.log\n    cp assembly.gfa ${plasmid}_${recipient}_assembly.gfa\n    cp assembly.fasta ${plasmid}_${recipient}_assembly.fasta\n    \"\"\"\n}"], "list_proc": ["samlhao/hgtsimulation/unicycler"], "list_wf_names": ["samlhao/hgtsimulation"]}, {"nb_reuse": 1, "tools": ["QUAST"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 1, "list_wf": ["hgtsimulation"], "list_contrib": ["samlhao"], "nb_contrib": 1, "codes": ["\nprocess quast {\n    tag \"${plasmid}_${recipient}\"\n    publishDir \"${params.outdir}/quast/${plasmid}/${recipient}\", mode: 'copy'\n\n    input:\n    tuple (plasmid, recipient, path(assembly), path(gfa)) from quast_ch\n    \n    output:\n    path(\"${plasmid}_${recipient}_assembly_QC/\")\n    path(\"${plasmid}_${recipient}_assembly_QC/report.tsv\") into quast_log_ch\n\n    script:\n    \"\"\"\n    quast -t ${task.cpus} -o ${plasmid}_${recipient}_assembly_QC ${assembly}\n    \"\"\"\n}"], "list_proc": ["samlhao/hgtsimulation/quast"], "list_wf_names": ["samlhao/hgtsimulation"]}, {"nb_reuse": 1, "tools": ["ABRicate"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 1, "list_wf": ["hgtsimulation"], "list_contrib": ["samlhao"], "nb_contrib": 1, "codes": ["\nprocess abricate {\n    tag \"${plasmid}_${recipient}\"\n    publishDir \"${params.outdir}/abricate/${plasmid}/${recipient}\", mode: 'copy'\n\n    input:\n    tuple(plasmid, recipient, path(assembly), path(gfa)) from abricate_ch\n    \n    output:\n    tuple(plasmid, recipient, path(assembly), path(gfa), path(\"${plasmid}_${recipient}_assembly.genes\")) into amr_ch\n\n    shell:\n    '''\n    abricate --threads !{task.cpus} --db ncbi !{assembly} > !{plasmid}_!{recipient}_assembly.amr\n    cat !{plasmid}_!{recipient}_assembly.amr | awk 'FNR > 1 { print $6}' > !{plasmid}_!{recipient}_assembly.genes\n    '''\n    \n}"], "list_proc": ["samlhao/hgtsimulation/abricate"], "list_wf_names": ["samlhao/hgtsimulation"]}, {"nb_reuse": 4, "tools": ["SAMtools", "Minimap2", "MultiQC", "fastPHASE", "FastQC"], "nb_own": 4, "list_own": ["samlhao", "sanger-tol", "stevekm", "zamanianlab"], "nb_wf": 4, "list_wf": ["readmapping", "fastq-bed-subset", "nextflow-spid", "Core_RNAseq-nf"], "list_contrib": ["samlhao", "mzamanian", "chenthorn", "stevekm", "priyanka-surana"], "nb_contrib": 5, "codes": ["\nprocess fastq_merge_bam_index {\n                                                                                         \n    input:\n    set val(sampleID), file(fastq_r1: \"*\"), file(fastq_r2: \"*\"), file(bam) from samples_R1_R2\n\n    output:\n    set val(sampleID), file(\"${merged_fastq_R1}\"), file(\"${merged_fastq_R2}\"), file(bam), file(\"${bai}\") into samples_fastq_merged\n\n    script:\n    prefix = \"${sampleID}\"\n    merged_fastq_R1 = \"${prefix}_R1.fastq.gz\"\n    merged_fastq_R2 = \"${prefix}_R2.fastq.gz\"\n    bai = \"${bam}.bai\"\n    \"\"\"\n    cat ${fastq_r1} > \"${merged_fastq_R1}\"\n    cat ${fastq_r2} > \"${merged_fastq_R2}\"\n    samtools index \"${bam}\"\n    \"\"\"\n}", "process MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0' :\n        'quay.io/biocontainers/minimap2:2.21--h5bf99c6_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path fasta\n    path index\n\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    \"\"\"\n    minimap2 \\\\\n        --cs=short \\\\\n        $args \\\\\n        -t $task.cpus \\\\\n        $read_group \\\\\n        $fasta \\\\\n        $reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess trim_reads {\n\n   cpus large_core\n   tag { id }\n   publishDir \"${output}/trim_stats/\", mode: 'copy', pattern: '*.html'\n   publishDir \"${output}/trim_stats/\", mode: 'copy', pattern: '*.json'\n\n   input:\n       tuple val(id), file(reads) from fqs\n\n   output:\n       tuple id_out, file(\"${id_out}.fq.gz\") into trimmed_fqs\n       tuple file(\"*.html\"), file(\"*.json\")  into trim_log\n\n  script:\n      id_out = id.replace('.fastq.gz', '')\n\n   \"\"\"\n       fastp -i $reads -o ${id_out}.fq.gz -y -l 15 -h ${id_out}.html -j ${id_out}.json\n   \"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    fastp --version &> v_fastp.txt\n    raxmlHPC-PTHREADS -v > v_raxml.txt\n    samtools --version > v_samtools.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["stevekm/fastq-bed-subset/fastq_merge_bam_index", "sanger-tol/readmapping/MINIMAP2_ALIGN", "zamanianlab/Core_RNAseq-nf/trim_reads", "samlhao/nextflow-spid/get_software_versions"], "list_wf_names": ["zamanianlab/Core_RNAseq-nf", "sanger-tol/readmapping", "samlhao/nextflow-spid", "stevekm/fastq-bed-subset"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 1, "list_wf": ["nextflow-spid"], "list_contrib": ["samlhao"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/samples/$name\", mode: 'copy'\n\n    input:\n    set val(name), file(reads) from se_read_files_fastqc.mix(pe_read_files_fastqc)\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --threads $task.cpus $reads\n    \"\"\"\n}"], "list_proc": ["samlhao/nextflow-spid/fastqc"], "list_wf_names": ["samlhao/nextflow-spid"]}, {"nb_reuse": 6, "tools": ["RTREE", "SAMtools", "MMseqs", "Minimap2", "MultiQC", "STAR"], "nb_own": 5, "list_own": ["samlhao", "sarseq", "vpeddu", "zamanianlab", "stevekm"], "nb_wf": 6, "list_wf": ["nextflow-pipeline-demo", "nextflow-samplesheet-demo", "ev-meta", "nextflow-spid", "sarseq2", "Core_RNAseq-nf"], "list_contrib": ["samlhao", "mzamanian", "sarseq", "vpeddu", "chenthorn", "stevekm"], "nb_contrib": 6, "codes": ["\nprocess use_dir {\n    echo true\n\n    input:\n    file(dir) from samples_dir\n\n    script:\n    \"\"\"\n    echo \"[use_dir]\"\n    pwd\n    tree \"${dir}/\"\n    \"\"\"\n}", "\nprocess star_index {\n\n    cpus big\n\n    when:\n      params.star\n\n    input:\n        file(\"geneset.gtf.gz\") from geneset_star\n        file(\"reference.fa.gz\") from reference_star\n\n    output:\n        file(\"STAR_index/*\") into star_indices\n\n    script:\n        overhang = params.rlen - 1\n\n    \"\"\"\n        zcat reference.fa.gz > reference.fa\n        zcat geneset.gtf.gz > geneset.gtf\n        mkdir STAR_index\n\n        STAR --runThreadN ${task.cpus} --runMode genomeGenerate  --genomeDir STAR_index \\\n          --genomeFastaFiles reference.fa \\\n          --sjdbGTFfile geneset.gtf \\\n          --sjdbOverhang ${overhang}\n    \"\"\"\n\n}", " process aln2spike {\n\n      tag \"$bcset\"\n\n      input:\n      set val(bcset), file(reads) from ch_aln2spike_set\n      file index from ch_genome_indices\n\n      output:\n      set file(\"*bam\"), file(\"*flagstat\") optional true into ch_aln2spike_bam_log\n      file(\"*flagstat\") optional true into ch_aln2spike_multiqc\n\n      script:\n      \"\"\"\n      for infile in $reads; do\n        if [ \\$(wc -l \\$infile | cut -f 1 -d \" \") -ge \"${params.sample_minreads_fastq_paired}\" ]; then\n          name=\\$(basename \\$infile | sed 's/.R12.fastq//')\n          minimap2 -ax sr -t $task.cpus indices/sequence.mmi \\$infile | \\\\\n            samtools view -@ $task.cpus -b -h -F 0x0100 - | samtools sort -o \\$name.bam -\n          samtools index \\$name.bam\n          samtools flagstat -@ $task.cpus \\$name.bam > \\$name.flagstat\n        fi\n      done\n      \"\"\"\n  }", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n    path (fastp_results) from se_fastp_results.mix(pe_fastp_results).collect().ifEmpty([])\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . ${fastp_results}\n    \"\"\"\n}", "\nprocess Cluster_unclassified_reads { \npublishDir \"${params.OUTPUT}/Mmsesq2_unclassified_translated/${base}\", mode: 'symlink', overwrite: true\ncontainer \"quay.io/biocontainers/mmseqs2:13.45111--h95f258a_1\"\nbeforeScript 'chmod o+rw .'\ncpus 16\ninput: \n    tuple val(base), file(unassigned_fastq)\n                                                                       \noutput: \n    tuple val(\"${base}\"), file(\"${base}.mmseq.clustered.fasta\")\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \n\n#create mmseq db\nmmseqs createdb ${unassigned_fastq} ${base}.mmseq.DB\n\n# cluster with mmseq cluster2\nmmseqs cluster --threads ${task.cpus} ${base}.mmseq.DB ${base}.mmseq.DB_clu tmp\n#extract representative sequences and convert back to fasta\nmmseqs createsubdb ${base}.mmseq.DB_clu ${base}.mmseq.DB ${base}.mmseq.clu_rep\nmmseqs convert2fasta ${base}.mmseq.clu_rep ${base}.mmseq.clustered.fasta\n\n\"\"\"\n}", "\nprocess sambamba_dedup {\n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam-bwa-dd\", mode: 'copy', overwrite: true\n    clusterOptions '-pe threaded 1-8 -l mem_free=40G -l mem_token=4G'\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'samtools/1.3'\n\n    input:\n    set val(sample_ID), file(sample_bam) from samples_bam2\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.bam\") into samples_dd_bam, samples_dd_bam2, samples_dd_bam3, samples_dd_bam4, samples_dd_bam5, samples_dd_bam6, samples_dd_bam7\n    file(\"${sample_ID}.dd.bam.bai\")\n\n    script:\n    \"\"\"\n    \"${params.sambamba_bin}\" markdup --remove-duplicates --nthreads \\${NSLOTS:-1} --hash-table-size 525000 --overflow-list-size 525000 \"${sample_bam}\" \"${sample_ID}.dd.bam\"\n    samtools view \"${sample_ID}.dd.bam\"\n    \"\"\"\n}"], "list_proc": ["stevekm/nextflow-samplesheet-demo/use_dir", "zamanianlab/Core_RNAseq-nf/star_index", "sarseq/sarseq2/aln2spike", "samlhao/nextflow-spid/multiqc", "vpeddu/ev-meta/Cluster_unclassified_reads", "stevekm/nextflow-pipeline-demo/sambamba_dedup"], "list_wf_names": ["sarseq/sarseq2", "stevekm/nextflow-samplesheet-demo", "vpeddu/ev-meta", "stevekm/nextflow-pipeline-demo", "zamanianlab/Core_RNAseq-nf", "samlhao/nextflow-spid"]}, {"nb_reuse": 8, "tools": ["segmentSeq", "Annot", "RTREE", "tmax", "BWA", "SAMtools", "sampletrees", "MMseqs", "metabolic", "MultiQC", "FastQC", "STAR", "PowerShell"], "nb_own": 7, "list_own": ["taltechnlp", "samlhao", "vibbits", "vpeddu", "zamanianlab", "stevekm", "tamara-hodgetts"], "nb_wf": 7, "list_wf": ["nextflow-pipeline-demo", "NextFlow_pipelines", "est-asr-pipeline", "nf-core-abricate", "nextflow-samplesheet-demo", "ev-meta", "nf-atac-seq", "Core_RNAseq-nf"], "list_contrib": ["mzamanian", "ortzf", "vpeddu", "chenthorn", "aivo0", "stevekm", "tamara-hodgetts"], "nb_contrib": 7, "codes": ["\nprocess use_dir {\n    echo true\n\n    input:\n    file(dir) from samples_dir\n\n    script:\n    \"\"\"\n    echo \"[use_dir]\"\n    pwd\n    tree \"${dir}/\"\n    \"\"\"\n}", "\nprocess star_index {\n\n    cpus big\n\n    when:\n      params.star\n\n    input:\n        file(\"geneset.gtf.gz\") from geneset_star\n        file(\"reference.fa.gz\") from reference_star\n\n    output:\n        file(\"STAR_index/*\") into star_indices\n\n    script:\n        overhang = params.rlen - 1\n\n    \"\"\"\n        zcat reference.fa.gz > reference.fa\n        zcat geneset.gtf.gz > geneset.gtf\n        mkdir STAR_index\n\n        STAR --runThreadN ${task.cpus} --runMode genomeGenerate  --genomeDir STAR_index \\\n          --genomeFastaFiles reference.fa \\\n          --sjdbGTFfile geneset.gtf \\\n          --sjdbOverhang ${overhang}\n    \"\"\"\n\n}", "\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"*.version.txt\"         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}", "\nprocess Cluster_unclassified_reads { \npublishDir \"${params.OUTPUT}/Mmsesq2_unclassified_translated/${base}\", mode: 'symlink', overwrite: true\ncontainer \"quay.io/biocontainers/mmseqs2:13.45111--h95f258a_1\"\nbeforeScript 'chmod o+rw .'\ncpus 16\ninput: \n    tuple val(base), file(unassigned_fastq)\n                                                                       \noutput: \n    tuple val(\"${base}\"), file(\"${base}.mmseq.clustered.fasta\")\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \n\n#create mmseq db\nmmseqs createdb ${unassigned_fastq} ${base}.mmseq.DB\n\n# cluster with mmseq cluster2\nmmseqs cluster --threads ${task.cpus} ${base}.mmseq.DB ${base}.mmseq.DB_clu tmp\n#extract representative sequences and convert back to fasta\nmmseqs createsubdb ${base}.mmseq.DB_clu ${base}.mmseq.DB ${base}.mmseq.clu_rep\nmmseqs convert2fasta ${base}.mmseq.clu_rep ${base}.mmseq.clustered.fasta\n\n\"\"\"\n}", "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess wgcna {\n\tinput:\n\teach x from files.splitText()\n\n\toutput:\n\tstdout test\n\n\t\"\"\"\n\t#!/usr/bin/env Rscript\n\tdir.create(paste0(\"$myDir\",\"/WGCNA\"))\n\tsetwd(paste0(\"$myDir\",\"/WGCNA\"))\n\n\t#####################################\n\t#get filenames\n\t#####################################\n\tlibrary(tools)\n\t\n\tomicsset <- strsplit(trimws(\"$x\"), \" \")\n\n\tomics1 <- omicsset[[1]][1]\n\tomics2 <- omicsset[[1]][2]\n\n\tprint(omics1)\n\tprint(omics2)\n\n\theadDir = paste0(file_path_sans_ext(omics1, compression = FALSE),\"_-_\",file_path_sans_ext(omics2, compression = FALSE))\n\n\n\t############################################################################################################################################################################################\n\t# First part to read and make the data format in the correct format\n\t############################################################################################################################################################################################\n\t# WGCNA tutorial website https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/index.html\n\t\n\t#install.packages(\"Hmisc\")\n\t#install.packages(\"BiocManager\")\n\t#BiocManager::install(\"WGCNA\")\n\n\tlibrary(Hmisc)\n\tlibrary(WGCNA)\n\tlibrary(pheatmap)\n\t#enableWGCNAThreads()\n\t#allowWGCNAThreads() \n\toptions(stringsAsFactors = FALSE)\n\tdir.create(headDir)\n\tsetwd(headDir)\n\tdir.create(\"Figures\")\n\tdir.create(\"Info\")\n\tdir.create(\"ModuleMembership_vs_GeneSignificance\")\n\tdir.create(\"GeneInfo\")\n\tdata = read.csv((paste0(\"$myData\",\"/\",omics1)), header = T);  ## GE matrix ####\n\tdim(data);\n\tnames(data);\n\n\t# The first row of your transposed matrix was rownames of the original matrix, you should first remove them from data,\n\tdataEhsan = data[,-1]\n\t# And then add them as rownames to the new data frame\n\trownames(dataEhsan) = as.character(data[,1])\n\t# And finally transpose it\n\tdatExpr0 = as.data.frame(t(dataEhsan))\n\n\tgsg = goodSamplesGenes(datExpr0, verbose = 3);\n\tgsg\\$allOK # returns FALSE so we clean all bad rows\n\n\tif (!gsg\\$allOK) {\n\t  # Optionally, print the gene and sample names that were removed:\n\t  if (sum(!gsg\\$goodGenes) > 0)\n\t    printFlush(paste(\"Removing genes:\", paste(names(datExpr0)[!gsg\\$goodGenes], collapse = \", \")));\n\t  if (sum(!gsg\\$goodSamples) > 0)\n\t    printFlush(paste(\"Removing samples:\", paste(rownames(datExpr0)[!gsg\\$goodSamples], collapse = \", \")));\n\t  # Remove the offending genes and samples from the data:\n\t  datExpr0 = datExpr0[gsg\\$goodSamples, gsg\\$goodGenes]\n\t}\n\t# Next we cluster the samples (in contrast to clustering genes that will come later) to see for obvious outliers.  \n\n\tsampleTree = hclust(dist(datExpr0), method = \"average\");\n\n\tsizeGrWindow(12,9)\n\tpdf(file = \"Figures/sampleClustering.pdf\", width = 12, height = 9);\n\tpar(cex = 0.6);\n\tpar(mar = c(0,4,2,0))\n\tplot(sampleTree, main = \"Sample clustering to detect outliers\", sub = \"\", xlab = \"\", cex.lab = 1.5,\n\t     cex.axis = 1.5, cex.main = 2)\n\t# Plot a line to show the cut\n\tabline(h = 45000, col = \"red\");\n\tdev.off()\n\n\t###if removing an outlier sample, do it with a height cut-off\n\t#clust = cutreeStatic(sampleTree, cutHeight = 45000, minSize = 10)\n\t#table(clust)\n\t### clust 1 contains the samples we want to keep.\n\t#keepSamples = (clust==1)\n\t#datExpr = datExpr0[keepSamples, ]\n\t# IF NOT REMOVING ANY SAMPLE: \n\tdatExpr = datExpr0\n\tnGenes = ncol(datExpr)\n\tnSamples = nrow(datExpr)\n\t# datExpr; # The variable datExpr now contains the expression data ready for network analysis.\n\n\t# Save and return final data frame ready for analysis\n\tsave(datExpr, file = \"saveToFile.RData\")\n\n\t######################################################\n\t# Use this to parse phenotype data that you want to correlate with the expression data\n\t# Can be metabolite data, individual traits, etc.\n\ttraitData = read.csv((paste0(\"$myData\",\"/\",omics2)), header = TRUE);\n\tdim(traitData)\n\tnames(traitData)\n\trownames(traitData) = traitData[,1];\n\ttraitData = traitData [,-1]\n\tallTraits = t(traitData);\n\n\tMLsamples = rownames(datExpr);\n\n\ttraitRows = match(MLsamples, rownames(allTraits));\n\n\tdatTraits = allTraits[traitRows,];\n\trownames(datTraits) = rownames(allTraits[traitRows,]);\n\tcollectGarbage();\n\t  \n\tsampleTree2 = hclust(dist(datExpr), method = \"average\")\n\t# Convert traits to a color representation: white means low, red means high, grey means missing entry\n\ttraitColors = numbers2colors(datTraits, signed = FALSE);\n\tplotDendroAndColors(sampleTree2, traitColors,\n\t                    groupLabels = names(datTraits),\n\t                    main = \"Sample dendrogram and trait heatmap\")\n\n\tsave(datExpr, datTraits, file = \"01-dataInput.RData\")\n\n\n\n\n\t############################################################################################################################################################################################\n\t# Automatic, one-step network construction and module detection\n\t############################################################################################################################################################################################\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 1\n\t#\n\t#=====================================================================================\n\n\t# Display the current working directory\n\t#getwd();\n\t# If necessary, change the path below to the directory where the data files are stored. \n\t# \".\" means current directory. On Windows use a forward slash / instead of the usual .\n\t#workingDir = \".\";\n\t#setwd(headDir); \n\t# Load the WGCNA package\n\tlibrary(WGCNA)\n\t# The following setting is important, do not omit.\n\toptions(stringsAsFactors = FALSE);\n\t# Allow multi-threading within WGCNA. \n\t# Caution: skip this line if you run RStudio or other third-party R environments.\n\t# See note above.\n\tenableWGCNAThreads()\n\t# Load the data saved in the first part\n\tlnames = load(file = \"01-dataInput.RData\");\n\t# The variable lnames contains the names of loaded variables.\n\tlnames\n\t# Get the number of sets in the multiExpr structure.\n\t# nSets = checkSets(multiExpr)\\$nSets\n\tnSets = 1\n\tmultiExpr = datExpr\n\n\tprint(\"1 done\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 2\n\t#\n\t#=====================================================================================\n\n\t# Choose a set of soft-thresholding powers\n\tpowers = c(seq(4,10,by=1), seq(12,20, by=2));\n\t# Initialize a list to hold the results of scale-free analysis\n\tpowerTables = vector(mode = \"list\", length = nSets);\n\t# Call the network topology analysis function for each set in turn\n\tfor (set in 1:nSets)\n\t  powerTables[[set]] = list(data = pickSoftThreshold(multiExpr, powerVector=powers,\n\t                                                     verbose = 2)[[2]]);\n\tcollectGarbage();\n\t# Plot the results:\n\tcolors = c(\"black\", \"red\")\n\t# Will plot these columns of the returned scale free analysis tables\n\tplotCols = c(2,5,6,7)\n\tcolNames = c(\"Scale Free Topology Model Fit\", \"Mean connectivity\", \"Median connectivity\",\n\t\"Max connectivity\");\n\t# Get the minima and maxima of the plotted points\n\tylim = matrix(NA, nrow = 2, ncol = 4);\n\tfor (set in 1:nSets){\n\t  for (col in 1:length(plotCols))\n\t  {\n\t    ylim[1, col] = min(ylim[1, col], powerTables[[set]]\\$data[, plotCols[col]], na.rm = TRUE);\n\t    ylim[2, col] = max(ylim[2, col], powerTables[[set]]\\$data[, plotCols[col]], na.rm = TRUE);\n\t  }\n\t}\n\t# Plot the quantities in the chosen columns vs. the soft thresholding power\n\tsizeGrWindow(8, 6)\n\tpdf(file = \"Figures/scaleFreeAnalysis.pdf\", wi = 8, he = 6)\n\tpar(mfcol = c(2,2));\n\tpar(mar = c(4.2, 4.2 , 2.2, 0.5))\n\tcex1 = 0.7;\n\tfor (col in 1:length(plotCols)) for (set in 1:nSets)\n\t{\n\t  if (set==1)\n\t  {\n\t    plot(powerTables[[set]]\\$data[,1], -sign(powerTables[[set]]\\$data[,3])*powerTables[[set]]\\$data[,2],\n\t         xlab = \"Soft Threshold (power)\", ylab = colNames[col],type = \"n\", ylim = ylim[, col],\n\t         main = colNames[col]);\n\t    addGrid();\n\t  }\n\t  if (col==1)\n\t  {\n\t    text(powerTables[[set]]\\$data[,1], -sign(powerTables[[set]]\\$data[,3])*powerTables[[set]]\\$data[,2],\n\t         labels = powers, cex = cex1, col = colors[set]);\n\t  } else\n\t    text(powerTables[[set]]\\$data[,1], powerTables[[set]]\\$data[,plotCols[col]],\n\t         labels = powers, cex = cex1, col = colors[set]);\n\t  # if (col==1)\n\t  # {\n\t  #   legend(\"bottomright\", legend = setLabels, col = colors, pch = 20) ;\n\t  # } else\n\t  #   legend(\"topright\", legend = setLabels, col = colors, pch = 20) ;\n\t}\n\tdev.off();\n\n\tprint(\"2 done\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 3\n\t#\n\t#=====================================================================================\n\n\t# Form multi-set expression data: columns starting from 9 contain actual expression data.\n\tmultiExpr = vector(mode = \"list\", length = nSets)\n\tmultiExpr[[1]] = list(data = datExpr);\n\tnames(multiExpr[[1]]\\$data) = colnames(datExpr);\n\trownames(multiExpr[[1]]\\$data) = rownames(datExpr);\n\n\n\t# Check that the data has the correct format for many functions operating on multiple sets:\n\texprSize = checkSets(multiExpr)\n\n\tfor (i in 1:ncol(multiExpr[[1]]\\$data)) {\n\t  multiExpr[[1]]\\$data[,i] <- as.numeric(as.character(multiExpr[[1]]\\$data[,i]))\n\t  }\n\n\tsoftTreshold <- 8\n\n\n\tnet <- blockwiseConsensusModules(\n\t        multiExpr, power = softTreshold , minModuleSize = 20, deepSplit = 2,\n\t        pamRespectsDendro = FALSE, \n\t        mergeCutHeight = 0.25, numericLabels = TRUE,\n\t        minKMEtoStay = 0,\n\t        saveTOMs = TRUE, verbose = 5)\n\n\tprint(\"3 done\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 4\n\t#\n\t#=====================================================================================\n\n\tconsMEs = net\\$multiMEs;\n\tmoduleLabels = net\\$colors;\n\t# Convert the numeric labels to color labels\n\tmoduleColors = labels2colors(moduleLabels)\n\tconsTree = net\\$dendrograms[[1]]; \n\t# saving modules and genes in a table\n\tanno = read.csv(file = \"../../GO/Mouse2GO.csv\", sep = \",\");\n\tanno = anno[,c(1:2)]\n\t# Save annotation and data expression files to .csv file\n\twrite.csv(anno, file = \"Info/anno.csv\", row.names = FALSE)\n\twrite.csv(data\\$X, file = \"Info/datExpr.csv\", row.names = FALSE)\n\n\tt <- colnames(multiExpr[[1]]\\$data)\n\tt <- as.matrix(t)\n\thead(t)\n\tcolnames(t) <- \"GENE\"\n\n\t# Right merge of annotation and data datasets\n\t#genesAnno <- merge(x = anno, y = t, by= \"GENE\" ,all.y = TRUE). # THIS ONE TRIGGERS AN ERROR (OT)\n\tgenesAnno <- merge(x = anno, y = t, by.x= 'gene_id', by.y = 'GENE'  ,all.y = TRUE)\n\n\tgenesAnno <- genesAnno[,c(1:2)]\n\tcolnames(genesAnno) <- c(\"GENES\", \"Annotation\")\n\n\tlibrary(dplyr)\n\n\ttest <- genesAnno %>% group_by(GENES) %>% summarise(Annotations = paste(Annotation, collapse = \", \"))\n\tcolnames(test)\n\n\t# Write text file with genes and corresponding modules and annotations\n\tmod <- data.frame(\"Gene\" = t, \"Modules\" = labels2colors(moduleLabels), \"Annotation\" = test\\$Annotations)\n\tmod <- mod[order(mod\\$Modules),]\n\twrite.table(mod, file = \"Info/ModuleMembers.txt\", row.names = F, sep = \"\\t\", quote = F)\n\n\tsave(consMEs, moduleLabels, moduleColors, consTree, file = \"02-networkConstruction-auto.RData\")\n\n\tprint(\"4 done\")\n\n\t############################################################################################################################################################################################\n\t# 3 - Relating modules to external clinical traits and identifying important genes\n\t############################################################################################################################################################################################\n\n\tprint(\"start part 3\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 1\n\t#\n\t#=====================================================================================\n\n\t# Display the current working directory\n\t#getwd();\n\t# If necessary, change the path below to the directory where the data files are stored. \n\t# \".\" means current directory. On Windows use a forward slash / instead of the usual .\n\tworkingDir = \".\";\n\tsetwd(workingDir); \n\t# Load the WGCNA package\n\tlibrary(WGCNA)\n\t# The following setting is important, do not omit.\n\toptions(stringsAsFactors = FALSE);\n\t# Load the expression and trait data saved in the first part\n\tlnames = load(file = \"01-dataInput.RData\");\n\t#The variable lnames contains the names of loaded variables.\n\tlnames\n\t# Load network data saved in the second part.\n\tlnames = load(file = \"02-networkConstruction-auto.RData\");\n\tlnames\n\n\tprint(\"3.1 done\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 2\n\t#\n\t#=====================================================================================\n\n\t# Define numbers of genes and samples\n\tnGenes = ncol(datExpr);\n\tnSamples = nrow(datExpr);\n\t# Recalculate MEs with color labels\n\tMEs0 = moduleEigengenes(datExpr, moduleColors)\\$eigengenes\n\tMEs = orderMEs(MEs0)\n\tmoduleTraitCor = cor(MEs, datTraits, use = \"p\");\n\tmoduleTraitPvalue = corPvalueStudent(moduleTraitCor, nSamples);\n\n\twrite.table(moduleTraitCor, file = \"Info/metabolites_transcriptomics_corr_matrix.txt\", row.names = T, sep = \"\\t\", quote = F)\n\n\tprint(\"3.2 done\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 3\n\t#\n\t#=====================================================================================\n\n\tsizeGrWindow(10,6)\n\t# Will display correlations and their p-values\n\ttextMatrix =  paste(signif(moduleTraitCor, 2), \"\\n(\",\n\t                           signif(moduleTraitPvalue, 1), \")\", sep = \"\");\n\tdim(textMatrix) = dim(moduleTraitCor)\n\t# Display the correlation values within a heatmap plot\n\tpdf(file = \"Figures/Heatmap_metabolites_transcriptomics_corr_matrix.pdf\", wi = 10, he = 6)\n\tpar(mar = c(3, 6, 7, 3));\n\tlabeledHeatmap(Matrix = moduleTraitCor,\n\t               xLabels = colnames(moduleTraitCor),\n\t               xLabelsPosition = \"top\",\n\t               xLabelsAngle = -90,\n\t               yLabels = names(MEs),\n\t               ySymbols = names(MEs),\n\t               colorLabels = FALSE,\n\t               colors = greenWhiteRed(50),\n\t               setStdMargins = FALSE,\n\t               cex.text = 0.5,\n\t               cex.lab.x = 0.45,\n\t               cex.lab.y = 0.3,\n\t               zlim = c(-1,1))\n\ttitle(\"Module-trait relationships\", line = 5)\n\tdev.off()\n\n\tprint(\"3.3 done\")\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 4\n\t#\n\t#=====================================================================================\n\n\t# \n\tmetabo = list()\n\tfor (i in colnames(moduleTraitPvalue)) {\n\t  # Define variable meta containing the metabolite columns of datTrait\n\t  meta <- data.frame(datTraits[,i]);\n\t  meta\\$i <- i\n\t  metabo[[i]] <- meta\n\t}\n\n\tmetabol = do.call(cbind, metabo)\n\tmetabol <- metabol[c(T,F)]\n\tcolnames(metabol) = gsub(\".datTraits...i.\", \"\", colnames(metabol))\n\t  \n\t# Names (colors) of the modules\n\tmodNames = substring(names(MEs), 3)\n\tgeneModuleMembership = as.data.frame(cor(datExpr, MEs, use = \"p\"));\n\tMMPvalue = as.data.frame(corPvalueStudent(as.matrix(geneModuleMembership), nSamples), append = T);\n\n\tnames(geneModuleMembership) = paste(\"MM\", modNames, sep=\"\");\n\tnames(MMPvalue) = paste(\"p.MM\", modNames, sep=\"\");\n\t  \n\tgeneTraitSignificance = as.data.frame(cor(datExpr, metabol, use = \"p\"));\n\tGSPvalue = as.data.frame(corPvalueStudent(as.matrix(geneTraitSignificance), nSamples));\n\t  \n\tnames(geneTraitSignificance) = paste(\"GS.\", names(metabol), sep=\"\");\n\tnames(GSPvalue) = paste(\"p.GS.\", names(metabol), sep=\"\");\n\n\tprint(\"3.4 done\")\n\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 5 \n\t#\n\t#=====================================================================================\n\n\t# Calculate the module membership values (aka. module eigengene based connectivity kME):\n\tdatKME <- signedKME(datExpr, MEs)  # equals geneModuleMembership\n\tcolnames(datKME) <- sub(\"kME\", \"MM.\", colnames(datKME))\n\n\tcolorOfColumn <- substring(names(datKME),4)\n\n\tmetabolites <- colnames(moduleTraitPvalue)\n\tfor (metabolite in metabolites) {\n\t  cat(paste(\"\\nmetabolite:\", metabolite, \"\\n\"))\n\t  pdf(paste0(\"ModuleMembership_vs_GeneSignificance/\", metabolite, \"_ModuleMembershipVsgeneSig.pdf\"))\n\t  par(mar=c(6, 8, 4, 4) + 0.1)\n\t  modNames = substring(names(MEs), 3)\n\t  for (module in modNames) {\n\t    column <- match(module,colorOfColumn)\n\t    moduleGenes <- moduleColors == module;\n\t    GS = paste(\"GS.\", metabolite, sep = \"\")\n\t    verboseScatterplot(abs(geneModuleMembership[moduleGenes, column]),\n\t                       abs(geneTraitSignificance[moduleGenes, GS]),\n\t                       xlab = paste(\"Module membership in\", module, \"module\"),\n\t                       ylab = paste(\"Gene significance for\", metabolite),\n\t                       xlim = c(0,1),\n\t                       ylim = c(0,1),\n\t                       main = paste(\"Module membership vs. gene significance\\n\"),\n\t                       cex.main = 1.3,\n\t                       cex.lab = 1.1,\n\t                       cex.axis = 1,\n\t                       pch = 21, \n\t                       col = \"dark gray\", \n\t                       bg = module)\n\t  }\n\t  \n\t  dev.off()\n\t} # end of loop\n\n\tprint(\"3.5 done\")\n\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 6\n\t#\n\t#=====================================================================================\n\n\n\t# Define top genes for every metabolite in every module \n\ttopGene <- list()\n\ttopGene_ <- list()\n\tfor(metabolite in metabolites) {\n\t  for(module in modNames) {\n\t    GS = paste(\"GS.\", metabolite, sep = \"\")\n\t    MM = paste(\"MM\", module, sep = \"\")\n\t    topGenes = abs(geneModuleMembership[,MM]) > 0.8 & abs(geneTraitSignificance[,GS]) > 0.8\n\t    topGene[[module]] <- topGenes\n\t  }\n\t  topGene_[[metabolite]] <- topGene\n\t} # end of loop\n\n\t# Link with gene names\n\ttopGene2name <- list()\n\ttopGene2name_ <- list()\n\tfor(metabolite in metabolites) {\n\t  for(module in modNames) {\n\t    topGenes2names <- dimnames(datExpr)[[2]][topGene_[[metabolite]][[module]]]\n\t    topGene2name[[module]] <- topGenes2names\n\t  }\n\t  topGene2name_[[metabolite]] <- topGene2name\n\t} # end of loop\n\n\t# Filter empty list \n\tfilterGene1_ <- list()\n\tfor(metabolite in names(topGene2name_)) {\n\t  filter = topGene2name_[[metabolite]][lapply(topGene2name_[[metabolite]],length)>0]\n\t  filterGene1_[[metabolite]] <- filter\n\t} # end of loop\n\n\n\t# Filter empty list \n\tfilterGene_ <- list()\n\tfor (items in filterGene1_){\n\t  filter2 = filterGene1_[lapply(filterGene1_,length)>0]\n\t  filterGene_ <- filter2\n\t}\n\n\tprint(\"3.6 done\")\n\n\n\n\t#=====================================================================================\n\t#\n\t#  Code chunk 7\n\t#\n\t#=====================================================================================\n\n\t# Write output to files\n\tmetabolites2 <-names(filterGene_)\n\tfor(metabolite in metabolites2) {\n\t  modules <-names(filterGene_[[metabolite]])\n\t  GTS <- paste(\"GS.\", metabolite, sep = \"\")\n\t  \n\t  for(module in modules) {\n\t    cat(paste(\"\\nCreating output for top genes in\", module, \"for\", metabolite, \"\\n\"))\n\t    genes <- filterGene_[[metabolite]][[module]]\n\t    GMM = paste(\"MM\", module, sep = \"\")\n\t    # Create the starting data frame\n\t    infoGenes = list(genes = genes,\n\t                           moduleColor = moduleColors[topGene_[[metabolite]][[module]]],\n\t                           annotation = test\\$Annotations[topGene_[[metabolite]][[module]]],\n\t                           geneTraitSignificance = geneTraitSignificance[filterGene_[[metabolite]][[module]], GTS],\n\t                           geneModuleMembership = geneModuleMembership[filterGene_ [[metabolite]][[module]], GMM])\n\t    \n\n\t    # Order the genes in the infoTopGenes variable first by module color, then by geneTraitSignificance\n\t    #geneOrder <- order(infoGenes\\$moduleColor, -abs(infoGenes[,4]))\n\t    #infoGenes = infoGenes[geneOrder, ]\n\t    \n\t    write.csv(infoGenes, file = paste0(\"GeneInfo/\", module, \"_\", metabolite, \"_GeneInfo.csv\"), row.names = FALSE)\n\t  }\n\t} # end of loop\n\n\tprint(\"3.7 done\")\n\n\n\n\t\"\"\"\n}", "\nprocess bwa_mem {\n                                    \n    tag { \"${sample_ID}\" }\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'bwa/0.7.17'\n\n    input:\n    set val(sample_ID), file(fastq_R1_trim), file(fastq_R2_trim), file(ref_fa_bwa_dir) from samples_fastq_trimmed.combine(ref_fa_bwa_dir)\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.sam\") into samples_bwa_sam\n\n    script:\n    \"\"\"\n    bwa mem -M -v 1 -t \\${NSLOTS:-1} -R '@RG\\\\tID:${sample_ID}\\\\tSM:${sample_ID}\\\\tLB:${sample_ID}\\\\tPL:ILLUMINA' \"${ref_fa_bwa_dir}/genome.fa\" \"${fastq_R1_trim}\" \"${fastq_R2_trim}\" -o \"${sample_ID}.sam\"\n    \"\"\"\n}", "\nprocess sambamba_dedup {\n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/bam-bwa-dd\", mode: 'copy', overwrite: true\n    clusterOptions '-pe threaded 1-8 -l mem_free=40G -l mem_token=4G'\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    module 'samtools/1.3'\n\n    input:\n    set val(sample_ID), file(sample_bam) from samples_bam2\n\n    output:\n    set val(sample_ID), file(\"${sample_ID}.dd.bam\") into samples_dd_bam, samples_dd_bam2, samples_dd_bam3, samples_dd_bam4, samples_dd_bam5, samples_dd_bam6, samples_dd_bam7\n    file(\"${sample_ID}.dd.bam.bai\")\n\n    script:\n    \"\"\"\n    \"${params.sambamba_bin}\" markdup --remove-duplicates --nthreads \\${NSLOTS:-1} --hash-table-size 525000 --overflow-list-size 525000 \"${sample_bam}\" \"${sample_ID}.dd.bam\"\n    samtools view \"${sample_ID}.dd.bam\"\n    \"\"\"\n}", "\nprocess language_id {\n    input:\n      path init_datadir\n      file audio    \n    \n    output:\n      path 'datadir' into datadir\n    \n    \n    shell:\n    if ( params.do_language_id == \"yes\" )\n      '''\n      . !{projectDir}/bin/prepare_process.sh\n      \n      extract_lid_features_kaldi.py !{init_datadir} .\n      cat !{init_datadir}/segments | awk '{print($1, \"0\")}' > trials\n      threshold=`cat !{params.rootdir}/models/lid_et/threshold`\n      ivector-subtract-global-mean !{params.rootdir}/models/lid_et/xvector.global.vec scp:xvector.scp ark:- | \\\n      ivector-normalize-length --scaleup=false ark:- ark:- | \\\n      logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 \\\n        !{params.rootdir}/models/lid_et/lr.scale.model \\\n        ark:trials ark:- - | \\\n        awk '{print($1, $3 > '$threshold' ? \"et\" : \"other\")}' > utt2lang\n\n      mkdir datadir\n      \n      grep \"et$\" utt2lang | sort | \\\n        join <(sort !{init_datadir}/segments) - | \\\n        awk '{print($1, $2, $3, $4)}' | LC_ALL=C sort > datadir/segments\n      \n      cp -r !{init_datadir}/{wav.scp,utt2spk,spk2utt} datadir\n      utils/fix_data_dir.sh datadir\n      \n      '''      \n    else\n      '''\n      mkdir datadir\n      cp -r !{init_datadir}/{wav.scp,segments,utt2spk,spk2utt} datadir\n      '''\n}"], "list_proc": ["stevekm/nextflow-samplesheet-demo/use_dir", "zamanianlab/Core_RNAseq-nf/star_index", "tamara-hodgetts/nf-atac-seq/SAMTOOLS_VIEW", "vpeddu/ev-meta/Cluster_unclassified_reads", "vibbits/NextFlow_pipelines/wgcna", "stevekm/nextflow-pipeline-demo/bwa_mem", "stevekm/nextflow-pipeline-demo/sambamba_dedup", "taltechnlp/est-asr-pipeline/language_id"], "list_wf_names": ["stevekm/nextflow-samplesheet-demo", "vpeddu/ev-meta", "stevekm/nextflow-pipeline-demo", "taltechnlp/est-asr-pipeline", "zamanianlab/Core_RNAseq-nf", "tamara-hodgetts/nf-atac-seq", "vibbits/NextFlow_pipelines"]}, {"nb_reuse": 0, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 0, "list_wf": ["nf-core-abricate"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess fastp {\n    publishDir \"${params.outdir}/fastp/${sample_id}\", mode: 'copy'\n\n    input:\n    tuple(sample_id, path(reads)) from read_files_trimming\n\n    output:\n    tuple(sample_id, path(\"trim*\")) into unicycler_ch\n    path(\"*.html\")\n    path(\"*.json\") into fastp_results\n\n    when:\n    params.reads\n\n    script:\n    if (params.singleEnd)\n        \"\"\"\n        fastp -i ${reads[0]} -o trim_${reads[0]} -w ${task.cpus} --json ${sample_id}_fastp.json --html ${sample_id}_fastp.html\n        \"\"\"\n    else\n        \"\"\"\n        fastp -i ${reads[0]} -I ${reads[1]} -o trim_${reads[0]} -O trim_${reads[1]} -w ${task.cpus} --json ${sample_id}_fastp.json --html ${sample_id}_fastp.html\n        \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["QUAST"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 0, "list_wf": ["nf-core-abricate"], "list_contrib": [], "nb_contrib": 0, "codes": [" process quast {\n        publishDir \"${params.outdir}/QUAST/${sample_id}\", mode: 'copy'\n\n        input:\n        tuple (sample_id, path(assembly)) from quast_ch\n\n        output:\n        path(\"quast/*\") into quast_output\n\n        script:\n        \"\"\"\n        quast -o quast $assembly\n        \"\"\"\n    }"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["samlhao"], "nb_wf": 0, "list_wf": ["nf-core-abricate"], "list_contrib": [], "nb_contrib": 0, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n    path(\"fastp/*/*.json\") from fastp_results.collect().ifEmpty([])\n    path(\"quast/*\") from quast_output.collect().ifEmpty([])\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    when:\n    params.reads\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config . fastp/*/* quast/*\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["bam2fastq"], "nb_own": 1, "list_own": ["sanger-tol"], "nb_wf": 1, "list_wf": ["readmapping"], "list_contrib": ["priyanka-surana"], "nb_contrib": 1, "codes": ["process BAM2FASTQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bam2fastx=1.3.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bam2fastx:1.3.1--hb7da652_2' :\n        'quay.io/biocontainers/bam2fastx:1.3.1--hb7da652_2' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path(pbi)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bam2fastq \\\\\n        $args \\\\\n        -o ${prefix} \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bam2fastx: 1.3.1\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["sanger-tol/readmapping/BAM2FASTQ"], "list_wf_names": ["sanger-tol/readmapping"]}, {"nb_reuse": 4, "tools": ["Bracken", "MetaPhlAn", "SAMtools", "Minimap2", "Bowtie", "FastQC"], "nb_own": 3, "list_own": ["seandavi", "sanger-tol", "xiaoli-dong"], "nb_wf": 3, "list_wf": ["readmapping", "pathogen", "cmgd_coordinator", "nf-core-cmgd"], "list_contrib": ["seandavi", "xiaoli-dong", "priyanka-surana"], "nb_contrib": 3, "codes": ["\nprocess BRACKEN {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bracken=2.6.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bracken%3A2.6.1--py39h7cff6ad_2\"\n    } else {\n        container \"quay.io/biocontainers/bracken:2.6.1--py39h7cff6ad_2\"\n    }\n\n    input:\n    tuple val(meta), path(kraken2_report)\n    path kraken2_db\n\n    output:\n    tuple val(meta), path('*_bracken.output.txt'), emit: output\n    tuple val(meta), path('*_bracken.outreport.txt'), emit: outreport\n    path (\"versions.yml\"), emit: versions\n\n    script:\n    def prefix  = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n   \n    \"\"\"\n    bracken \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        -d $kraken2_db \\\\\n        -i ${kraken2_report} \\\\\n        -o ${prefix}_bracken.output.txt \\\\\n        -w ${prefix}_bracken.outreport.txt \n   \n    printf \"BRACKEN:\\n  bracken: 2.6.1\\n\" > versions.yml\n    \"\"\"\n}", "\nprocess metaphlan_markers {\n    publishDir \"${params.publish_dir}/${workflow.sessionId}/metaphlan_markers\"\n    \n    cpus 4\n    memory \"16g\"\n\n    input:\n    path metaphlan_bt2\n    path metaphlan_db\n\n    output:\n    path \"marker_abundance.tsv.gz\", emit: marker_abundance\n    path \"marker_presence.tsv.gz\", emit: marker_presence\n    path \".command*\"\n\n    script:\n    \"\"\"\n    metaphlan --input_type bowtie2out \\\n        --index ${params.metaphlan_index} \\\n        --bowtie2db metaphlan \\\n        -t marker_pres_table \\\n        -o marker_presence.tsv \\\n        <( gunzip -c ${metaphlan_bt2} )    \n    metaphlan --input_type bowtie2out \\\n        --index ${params.metaphlan_index} \\\n        --bowtie2db metaphlan \\\n        -t marker_ab_table \\\n        -o marker_abundance.tsv \\\n        <( gunzip -c ${metaphlan_bt2} )\n    gzip *.tsv\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir params.outdir\n\n    input:\n    tuple val(sample_id), path(reads) from read_pairs2_ch\n\n    output:\n    path \"fastqc_${sample_id}_logs\" into fastqc_ch\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}", "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN_LONG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    \n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/pathogen/BRACKEN", "seandavi/cmgd_coordinator/fastqc", "sanger-tol/readmapping/SAMTOOLS_IDXSTATS", "xiaoli-dong/pathogen/MINIMAP2_ALIGN_LONG"], "list_wf_names": ["xiaoli-dong/pathogen", "sanger-tol/readmapping", "seandavi/cmgd_coordinator"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FastQC", "MultiQC"], "nb_own": 2, "list_own": ["seandavi", "sanger-tol"], "nb_wf": 1, "list_wf": ["readmapping", "nf-core-cmgd"], "list_contrib": ["seandavi", "priyanka-surana"], "nb_contrib": 2, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(cram)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    samtools \\\\\n        fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        -o ${prefix}.fastq.gz \\\\\n        $cram\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["sanger-tol/readmapping/SAMTOOLS_FASTQ"], "list_wf_names": ["sanger-tol/readmapping"]}, {"nb_reuse": 2, "tools": ["totalVI", "MultiQC"], "nb_own": 2, "list_own": ["virus-evolution", "sanger-tol"], "nb_wf": 2, "list_wf": ["readmapping", "phylopipe"], "list_contrib": ["rmcolq", "priyanka-surana"], "nb_contrib": 2, "codes": ["\nprocess announce_unreliable_pruned_tree {\n       \n                                      \n                                    \n      \n\n    input:\n    path tree\n    path tips\n    path pruned_tree\n\n    output:\n    path \"unreliable_pruned_tree.json\"\n\n    script:\n        if (params.webhook)\n            \"\"\"\n            echo '{\"text\":\"' > unreliable_pruned_tree.json\n            echo \"*${params.whoami}: Pruned tree with unreliable tips removed for ${params.date} complete*\\\\n\" >> unreliable_pruned_tree.json\n            echo \"> Total number of sequences in original tree: \\$(gotree stats tips -i ${tree} | tail -n+2 | wc -l)\\\\n\" >> unreliable_pruned_tree.json\n            echo \"> Total number of sequences in pruned tree: \\$(gotree stats tips -i ${pruned_tree} | tail -n+2 | wc -l)\\\\n\" >> unreliable_pruned_tree.json\n            echo \"> Total number of sequences in unreliable list: \\$(tail -n+1 ${tips} | wc -l)\\\\n\" >> unreliable_pruned_tree.json\n            echo '\"}' >> metadata_pruned_tree.json\n\n            echo 'webhook ${params.webhook}'\n\n            curl -X POST -H \"Content-type: application/json\" -d @unreliable_pruned_tree.json ${params.webhook}\n            \"\"\"\n        else\n           \"\"\"\n            echo '{\"text\":\"' > unreliable_pruned_tree.json\n            echo \"*${params.whoami}: Pruned tree with unreliable tips removed for ${params.date} complete*\\\\n\" >> unreliable_pruned_tree.json\n            echo \"> Total number of sequences in original tree: \\$(gotree stats tips -i ${tree} | tail -n+2 | wc -l)\\\\n\" >> unreliable_pruned_tree.json\n            echo \"> Total number of sequences in pruned tree: \\$(gotree stats tips -i ${pruned_tree} | tail -n+2 | wc -l)\\\\n\" >> unreliable_pruned_tree.json\n            echo \"> Total number of sequences in unreliable list: \\$(tail -n+1 ${tips} | wc -l)\\\\n\" >> unreliable_pruned_tree.json\n            echo '\"}' >> metadata_pruned_tree.json\n\n           \"\"\"\n}", "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["virus-evolution/phylopipe/announce_unreliable_pruned_tree", "sanger-tol/readmapping/MULTIQC"], "list_wf_names": ["virus-evolution/phylopipe", "sanger-tol/readmapping"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["sanger-tol"], "nb_wf": 1, "list_wf": ["readmapping"], "list_contrib": ["priyanka-surana"], "nb_contrib": 1, "codes": ["process MINIMAP2_INDEX {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0' :\n        'quay.io/biocontainers/minimap2:2.21--h5bf99c6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"*.mmi\"        , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    minimap2 \\\\\n        -t $task.cpus \\\\\n        -d ${fasta.baseName}.mmi \\\\\n        $args \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        minimap2: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["sanger-tol/readmapping/MINIMAP2_INDEX"], "list_wf_names": ["sanger-tol/readmapping"]}, {"nb_reuse": 2, "tools": ["SAMtools", "rgpicfixmate"], "nb_own": 2, "list_own": ["sebastianlzy", "sanger-tol"], "nb_wf": 2, "list_wf": ["readmapping", "nextflow-demo"], "list_contrib": ["priyanka-surana"], "nb_contrib": 1, "codes": ["process SAMTOOLS_FIXMATE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        fixmate  \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        ${prefix}.bam \\\\\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess merge_bam {\n\n    tag \"$sample_id\"\n    echo true\n\n    publishDir \"${params.outdir}/wgbs/$sample_id/unsortedButMerged_ForBismark_file\", mode:\"copy\", overwrite: true\n\n    input:\n    tuple val(sample_id), file(bam_file1), file(bam_file2) from ch_bismark_bam2\n\n    output:\n    tuple val(sample_id), file(\"*_unsorted_merged.bam\") into ch_bismark_merged_bam\n\n\n    script:\n    \"\"\"\n    ## module load samtools/1.3\n\n    samtools merge -nf ${sample_id}_unsorted_merged.bam $bam_file1 $bam_file2\n\n    ## remove individual bamfiles\n   #  rm $bam_file1 $bam_file2\n\n    \"\"\"\n}"], "list_proc": ["sanger-tol/readmapping/SAMTOOLS_FIXMATE", "sebastianlzy/nextflow-demo/merge_bam"], "list_wf_names": ["sebastianlzy/nextflow-demo", "sanger-tol/readmapping"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sarseq"], "nb_wf": 1, "list_wf": ["sarseq2"], "list_contrib": ["sarseq"], "nb_contrib": 1, "codes": ["\nprocess aln2tile_mpile {\n\n    tag \"$setname\"\n\n    publishDir path: \"${params.outdirExt}/aln2tile\", mode: 'copy',\n    saveAs: {filename->\n      if (filename.lastIndexOf(\".mpileup\") > 0) \"mpileup/$filename\"\n      else if (filename.lastIndexOf(\".csv\") > 0) \"mpileup2csv/$filename\"\n      else null\n    }\n\n    input:\n    set val(setname), file (bams) from ch_aln2tile_bam_set\n    file index from ch_tile_indices\n\n    output:\n    file(\"*fractions.csv\") optional true into ch_aln2tile_fractions_consensus\n\n    script:\n    bqfilter = params.bqfilter > 0 ? \"--min-BQ ${params.bqfilter}\" : '--no-BAQ'\n\n    \"\"\"\n    for infile in $bams; do\n      name=\\$(basename \\$infile | sed 's/.bam//')\n      tile=\\$(echo \\$infile | sed 's/__.*\\$//')\n\n      # filter bam\n      samtools view -@ $task.cpus -b -f 2 -F4 -F 0x0100  \\$infile | samtools sort -o \\$name.filt.tmp.bam -\n      samtools mpileup -aa --max-depth 0 $bqfilter --min-MQ 1  -f indices/\\$tile/sequence.fasta   \\$name.filt.tmp.bam > \\$name.mpileup.txt\n      readstomper.pl \\$name.mpileup.txt > \\$name.mpileup.fractions.csv\n\n    done\n\n    \"\"\"\n}"], "list_proc": ["sarseq/sarseq2/aln2tile_mpile"], "list_wf_names": ["sarseq/sarseq2"]}, {"nb_reuse": 1, "tools": ["DBETH"], "nb_own": 1, "list_own": ["scilus"], "nb_wf": 1, "list_wf": ["tractoflow-pve"], "list_contrib": ["StongeEtienne", "jchoude"], "nb_contrib": 2, "codes": ["\nprocess Bet_DWI {\n    cpus 4\n\n    input:\n    set sid, file(dwi), file(b0) from dwi_b0_for_bet\n\n    output:\n    set sid, \"${sid}__b0_bet.nii.gz\", \"${sid}__b0_bet_mask_dilated.nii.gz\" into\\\n        b0_and_mask_for_crop\n    set sid, \"${sid}__dwi_bet.nii.gz\", \"${sid}__b0_bet.nii.gz\",\n        \"${sid}__b0_bet_mask_dilated.nii.gz\" into dwi_b0_b0_mask_for_n4\n\n    script:\n    if (params.dilate_b0_mask > 0)\n        dilate = \"maskfilter ${sid}__b0_bet_mask.nii.gz dilate ${sid}__b0_bet_mask_dilated.nii.gz --npass $params.dilate_b0_mask  \\n\"\n    else\n        dilate = \"mv ${sid}__b0_bet_mask.nii.gz ${sid}__b0_bet_mask_dilated.nii.gz \\n\"\n    \"\"\"\n    bet $b0 ${sid}__b0_bet.nii.gz -m -R -f $params.bet_dwi_final_f\n\n    $dilate\n\n    mrcalc ${sid}__b0.nii.gz ${sid}__b0_bet_mask_dilated.nii.gz\\\n        -mult ${sid}__b0_bet.nii.gz -quiet -force\n\n    mrcalc $dwi ${sid}__b0_bet_mask_dilated.nii.gz -mult ${sid}__dwi_bet.nii.gz -quiet\n    \"\"\"\n}"], "list_proc": ["scilus/tractoflow-pve/Bet_DWI"], "list_wf_names": ["scilus/tractoflow-pve"]}, {"nb_reuse": 1, "tools": ["DBETH"], "nb_own": 1, "list_own": ["scilus"], "nb_wf": 1, "list_wf": ["tractoflow"], "list_contrib": ["frheault", "jhlegarreta", "arnaudbore", "GuillaumeTh", "ppoulin91", "jchoude", "mdesco"], "nb_contrib": 7, "codes": ["\nprocess Eddy_Topup {\n    cpus params.processes_eddy\n\n    input:\n    set sid, file(dwi), file(bval), file(bvec), file(b0s_corrected),\n        file(field), file(movpar), readout, encoding\\\n        from dwi_gradients_mask_topup_files_for_eddy_topup\n    val(rev_b0_count) from rev_b0_counter\n\n    output:\n    set sid, \"${sid}__dwi_corrected.nii.gz\" into\\\n        dwi_from_eddy_topup\n    set sid, \"${sid}__bval_eddy\", \"${sid}__dwi_eddy_corrected.bvec\" into\\\n        gradients_from_eddy_topup\n    file \"${sid}__b0_bet_mask.nii.gz\"\n\n    when:\n    rev_b0_count > 0 && params.run_topup && params.run_eddy\n\n                                                                      \n                          \n    script:\n        slice_drop_flag=\"\"\n        if (params.use_slice_drop_correction)\n            slice_drop_flag=\"--slice_drop_correction\"\n        \"\"\"\n        export OMP_NUM_THREADS=$task.cpus\n        export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=$task.cpus\n        export OPENBLAS_NUM_THREADS=1\n        mrconvert $b0s_corrected b0_corrected.nii.gz -coord 3 0 -axes 0,1,2 -nthreads 1\n        bet b0_corrected.nii.gz ${sid}__b0_bet.nii.gz -m -R\\\n            -f $params.bet_topup_before_eddy_f\n        scil_prepare_eddy_command.py $dwi $bval $bvec ${sid}__b0_bet_mask.nii.gz\\\n            --topup $params.prefix_topup --eddy_cmd $params.eddy_cmd\\\n            --b0_thr $params.b0_thr_extract_b0\\\n            --encoding_direction $encoding\\\n            --readout $readout --out_script --fix_seed\\\n            $slice_drop_flag\n        sh eddy.sh\n        fslmaths dwi_eddy_corrected.nii.gz -thr 0 ${sid}__dwi_corrected.nii.gz\n        mv dwi_eddy_corrected.eddy_rotated_bvecs ${sid}__dwi_eddy_corrected.bvec\n        mv $bval ${sid}__bval_eddy\n        \"\"\"\n}"], "list_proc": ["scilus/tractoflow/Eddy_Topup"], "list_wf_names": ["scilus/tractoflow"]}, {"nb_reuse": 3, "tools": ["BLASTP-ACC", "Salmon", "MAFFT"], "nb_own": 3, "list_own": ["seandavi", "supark87", "tlawrence3"], "nb_wf": 3, "list_wf": ["covid_nextflow", "prac_nextflow", "cmgd_coordinator"], "list_contrib": ["seandavi", "supark87", "tlawrence3"], "nb_contrib": 3, "codes": ["\nprocess index {\n    tag \"$transcriptome.simpleName\"\n\n    input:\n    path transcriptome from params.transcriptome\n\n    output:\n    path 'index' into index_ch\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}", "\nprocess find {\n  input:\n  file fasta from proteins\n  val type from params.dbtype\n\n  when:\n  fasta.name =~ /^BB11.*/ && type == 'nr'\n\n  script:\n  \"\"\"\n  blastp -query $fasta -db nr\n  \"\"\"\n}", "\nprocess align_protein {\n    \n    publishDir \"$launchDir/results/AA_sequences/alignment\", mode: 'copy'\n\n    input:\n    file protein from filtered_proteins\n\n    output:\n    file \"${protein.baseName}.final.aligned.faa\" into aligned_proteins    \n\n    script:\n    \"\"\"\n    mafft --auto --thread 4 ${protein} > ${protein.baseName}.final.aligned.faa  \n    \"\"\"    \n}"], "list_proc": ["seandavi/cmgd_coordinator/index", "supark87/prac_nextflow/find", "tlawrence3/covid_nextflow/align_protein"], "list_wf_names": ["tlawrence3/covid_nextflow", "supark87/prac_nextflow", "seandavi/cmgd_coordinator"]}, {"nb_reuse": 2, "tools": ["MarkDuplicates (IP)", "MetaPhlAn"], "nb_own": 2, "list_own": ["seandavi", "seqeralabs"], "nb_wf": 2, "list_wf": ["curatedMetagenomicsNextflow", "gatk4-germline-snps-indels"], "list_contrib": ["seandavi", "shbrief", "abhi18av", "jwokaty", "evanfloden", "fbeghini"], "nb_contrib": 6, "codes": ["\nprocess GATK_MARK_DUPLICATES {\n    tag \"${sampleId}\"\n    label 'gatk4_container'\n\n    input:\n    val(sampleId)\n    path(input_mapped_merged_bam)\n\n    output:\n    val(sampleId)\n    path(\"${sampleId}_merged.deduped.bam\")\n    path(\"${sampleId}_merged.deduped.metrics.txt\")\n\n    script:\n\n    \"\"\"\n    ${params.gatk_path} --java-options \"-Dsamjdk.compression_level=${params.compression_level} ${params.java_opts}\" \\\n                        MarkDuplicates \\\n                        --INPUT ${input_mapped_merged_bam} \\\n                        --OUTPUT ${sampleId}_merged.deduped.bam \\\n                        --METRICS_FILE ${sampleId}_merged.deduped.metrics.txt \\\n                        --VALIDATION_STRINGENCY SILENT \\\n                        --OPTICAL_DUPLICATE_PIXEL_DISTANCE 2500 \\\n                        --ASSUME_SORT_ORDER \"queryname\" \\\n                        --CREATE_MD5_FILE true\n    \"\"\"\n\n    stub:\n\n    \"\"\"\n    touch ${sampleId}_merged.deduped.bam\n    touch ${sampleId}_merged.deduped.metrics.txt\n    \"\"\"\n\n}", "\nprocess install_metaphlan_db {\n    cpus 8\n    memory '32g'\n    disk '200 GB'\n\n    storeDir \"${params.store_dir}\"\n\n    output:\n    path 'metaphlan', emit: metaphlan_db, type: 'dir'\n    path \".command*\"\n\n    script:\n      \"\"\"\n      metaphlan --install --index latest --bowtie2db metaphlan\n      \"\"\"\n}"], "list_proc": ["seqeralabs/gatk4-germline-snps-indels/GATK_MARK_DUPLICATES", "seandavi/curatedMetagenomicsNextflow/install_metaphlan_db"], "list_wf_names": ["seqeralabs/gatk4-germline-snps-indels", "seandavi/curatedMetagenomicsNextflow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "Bowtie", "MetaPhlAn"], "nb_own": 2, "list_own": ["seandavi", "sguizard"], "nb_wf": 2, "list_wf": ["curatedMetagenomicsNextflow", "isoseq"], "list_contrib": ["seandavi", "shbrief", "fbeghini", "jwokaty", "sguizard", "peterwharrison", "Alexey-ebi"], "nb_contrib": 7, "codes": ["\nprocess metaphlan_markers {\n    publishDir \"${params.publish_dir}/${meta.sample}/metaphlan_markers/\"\n    \n    tag \"${meta.sample}\"\n\n    cpus 4\n    memory \"16g\"\n    disk \"200 GB\"\n\n    input:\n    val meta\n    path metaphlan_bt2\n    path metaphlan_db\n\n    output:\n    val meta, emit: meta\n    path \"marker_abundance.tsv.gz\", emit: marker_abundance\n    path \"marker_presence.tsv.gz\", emit: marker_presence\n    path \".command*\"\n\n    script:\n    \"\"\"\n    metaphlan --input_type bowtie2out \\\n        --index ${params.metaphlan_index} \\\n        --bowtie2db metaphlan \\\n        -t marker_pres_table \\\n        -o marker_presence.tsv \\\n        <( gunzip -c ${metaphlan_bt2} )    \n    metaphlan --input_type bowtie2out \\\n        --index ${params.metaphlan_index} \\\n        --bowtie2db metaphlan \\\n        -t marker_ab_table \\\n        -o marker_abundance.tsv \\\n        <( gunzip -c ${metaphlan_bt2} )\n    gzip *.tsv\n    \"\"\"\n}", "\nprocess SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.14--hb421002_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $options.args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["seandavi/curatedMetagenomicsNextflow/metaphlan_markers", "sguizard/isoseq/SAMTOOLS_FASTQ"], "list_wf_names": ["seandavi/curatedMetagenomicsNextflow", "sguizard/isoseq"]}, {"nb_reuse": 0, "tools": ["MetaPhlAn"], "nb_own": 1, "list_own": ["seandavi"], "nb_wf": 0, "list_wf": ["nf-core-cmgd"], "list_contrib": ["seandavi"], "nb_contrib": 1, "codes": ["\nprocess install_metaphlan_db {\n    cpus 8\n    memory '32g'\n\n    storeDir \"${params.store_dir}\"\n\n    output:\n    path 'metaphlan', emit: metaphlan_db, type: 'dir'\n    path \".command*\"\n\n    script:\n      \"\"\"\n      metaphlan --install --index latest --bowtie2db metaphlan\n      \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 7, "tools": ["Ensembler", "SAMtools", "kallisto", "Minimap2", "STEPS", "SCALCE", "Rgin", "STAR"], "nb_own": 5, "list_own": ["xiaoli-dong", "xmuyulab", "yuifu", "sguizard", "yqshao"], "nb_wf": 5, "list_wf": ["tips", "nextflow_cwl_example", "pathogen", "scRNAseq_pipelines", "isoseq"], "list_contrib": ["xiaoli-dong", "lmyiing", "yuifu", "huhehaotecrystal", "sguizard", "yqshao", "peterwharrison", "Alexey-ebi"], "nb_contrib": 8, "codes": ["\nprocess STAR {\n  cache \"deep\";tag \"${name}.step2\"\n  publishDir path: \"${params.outdir}/${name}\", mode: 'copy'\n\n  input:\n  set name, x from fqTrimmed1\n  \noutput:\n  file \"${name}.Aligned.sortedByCoord.out.bam\" into bam\n\n  \"\"\"\n  STAR \\\\\n  --runMode alignReads \\\\\n  --genomeDir $params.star_index \\\\\n  --outFileNamePrefix ${name}. \\\\\n  --readFilesIn $x \\\\\n  --outSAMtype BAM SortedByCoordinate \\\\\n  --outFilterMultimapNmax 1 \\\\\n  --runThreadN $params.threads \\\\\n  --limitIObufferSize 1500000000 --limitOutSJcollapsed 10000000\n  \"\"\"\n}", "\nprocess RGI_HEATMAP {\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:'') }\n\n    conda (params.enable_conda ? \"python=3.6 bioconda::rgi=5.1.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/rgi%3A5.1.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/rgi:5.1.1--py_0\"\n    }\n\n    input:\n               \n    path('?.json')\n\n    output:\n    path('*.png'), emit: heatmap\n    path('rgi_heatmap.eps'), emit: eps\n    path('rgi_heatmap.csv'),   emit: csv\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    \"\"\"\n    mkdir dir\n    cp ${json.join(' ')} dir\n    rgi heatmap -i dir -o rgi_heatmap\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(rgi main --version | sed 's/rgi //g')\n    END_VERSIONS \n    \"\"\"\n}", "\nprocess SamtoolSort {\n\n  cache \"deep\";tag \"${name}.step5\"\n  publishDir path: \"${params.outdir}/${name}\", mode: 'copy'\n\n  input:\n  set name, x from samtoolsort\n\n  output:\n  set val(\"${name}\"), file (\"${name}.sorted.bam\") into countone\n\n  \"\"\"\n  samtools sort $x -o ${name}.sorted.bam\n  \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess pinnSample {\n    label 'pinn'\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz')\n\n    script:\n    setup = getParams(sampleDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env python3\n    import pinn, os\n    import numpy as np\n    import tensorflow as tf\n    from ase import units\n    from ase.io import read, write\n    from ase.io.trajectory import Trajectory\n    from ase.md import MDLogger\n    from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n    from ase.md.nptberendsen import NPTBerendsen\n    from ase.md.nvtberendsen import NVTBerendsen\n\n    os.symlink(\"${file(setup.inp)}\", \"model\")\n    calc = pinn.get_calc(\"model\")\n    ensemble = \"$setup.pinnEnsemble\"\n    for seed in range($setup.pinnCopies):\n        rng = np.random.default_rng(seed)\n        atoms = read(\"${file(setup.init)}\")\n        atoms.set_calculator(calc)\n        MaxwellBoltzmannDistribution(atoms, $setup.pinnTemp*units.kB, rng=rng)\n        dt = 0.5 * units.fs\n        steps = int($setup.pinnTime*1e3*units.fs/dt)\n        if ensemble=='NPT':\n            dyn = NPTBerendsen(atoms, timestep=dt, temperature=$setup.pinnTemp, pressure=$setup.pinnPress,\n                               taut=dt*$setup.pinnTaut, taup=dt*$setup.pinnTaup, compressibility=$setup.pinnCompress)\n        elif ensemble=='NVT':\n            dyn = NVTBerendsen(atoms, timestep=dt, temperature=$setup.pinnTemp, taut=dt * 100)\n        else:\n            raise NotImplementedError(f\"Unkown ensemble {ensemble}\")\n        interval = int($setup.pinnEvery*1e3*units.fs/dt)\n        dyn.attach(MDLogger(dyn, atoms, 'output.log', mode=\"a\"), interval=interval)\n        dyn.attach(Trajectory('output.traj', 'a', atoms).write, interval=interval)\n        try:\n            dyn.run(steps)\n        except:\n            pass\n    traj = read('output.traj', index=':')\n    [atoms.wrap() for atoms in traj]\n    write('output.xyz', traj)\n    \"\"\"\n\n    stub:\n    setup = getParams(sampleDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}", "\nprocess MINIMAP2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.21' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.paf\"), emit: paf\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.paf\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess kallisto_se {\n    publishDir \"kallisto/$sample_id\", mode: 'move'\n\n    container \"quay.io/biocontainers/kallisto:0.44.0--h7d86c95_2\"\n\n    input:\n    set sample_id, file(fastq) from fastq_files\n    path kallisto_index from params.path_kallisto_index\n\n    output:\n    file \"$sample_id/*\" into kallisto_results\n\n    script:\n    \"\"\"\n    kallisto quant -i ${kallisto_index} -o ${sample_id} --single -l 200 -s 20 $fastq\n    \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/STAR", "xiaoli-dong/pathogen/RGI_HEATMAP", "xmuyulab/scRNAseq_pipelines/SamtoolSort", "sguizard/isoseq/MINIMAP2_ALIGN", "yqshao/tips/pinnSample", "xiaoli-dong/pathogen/MINIMAP2_ALIGN", "yuifu/nextflow_cwl_example/kallisto_se"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines", "sguizard/isoseq", "yqshao/tips", "yuifu/nextflow_cwl_example", "xiaoli-dong/pathogen"]}, {"nb_reuse": 1, "tools": ["BEDTools", "MultiQC"], "nb_own": 2, "list_own": ["sguizard", "thanhleviet"], "nb_wf": 1, "list_wf": ["nf-core-tamanmd", "nf-nanopore-assembly"], "list_contrib": ["thanhleviet", "sguizard"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"*.html\",mode: \"copy\"\n    \n    tag \"Multiqc\"\n    \n    errorStrategy 'ignore'\n\n    input:\n    path(input)\n  \n    output:\n    path \"multiqc_report.html\" optional true\n  \n    script:\n    \"\"\"\n    multiqc --interactive .  \n    \"\"\"\n}", "\nprocess BEDTOOLS_GETFASTA {\n    tag \"$bed\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    path bed\n    path fasta\n\n    output:\n    path \"*.fa\"         , emit: fasta\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${bed.baseName}${options.suffix}\" : \"${bed.baseName}\"\n    \"\"\"\n    bedtools \\\\\n        getfasta \\\\\n        $options.args \\\\\n        -fi $fasta \\\\\n        -bed $bed \\\\\n        -fo ${prefix}.fa\n\n    bedtools --version | sed -e \"s/bedtools v//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/multiqc"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly"]}, {"nb_reuse": 3, "tools": ["MetaPhlAn", "SAMtools", "Bowtie", "CANU", "FastQC", "BiocStyle"], "nb_own": 4, "list_own": ["xiaoli-dong", "sguizard", "thanhleviet", "vincenthhu"], "nb_wf": 2, "list_wf": ["nf-core-westest", "nf-core-tamanmd", "magph", "nf-nanopore-assembly"], "list_contrib": ["thanhleviet", "sguizard", "xiaoli-dong", "vincenthhu"], "nb_contrib": 4, "codes": ["\nprocess canu {\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", mode: \"copy\" \n    \n    tag {sample_id}\n    \n    errorStrategy 'ignore'\n\n    cpus 12\n\n    input:\n    tuple val(sample_id), path(reads), val(genome_size)\n    output:\n    tuple val(sample_id), path(\"${sample_id}\"), emit: canu_outdir\n    tuple val(sample_id), path(\"${sample_id}/${sample_id}.contigs.fasta\"), emit: contigs\n    \n    script:\n    \"\"\"\n    hostname > hostname\n    canu -p ${sample_id} -d ${sample_id} genomeSize=$genome_size maxThreads=${task.cpus} ${params.canu_options} -nanopore ${reads}\n    \"\"\"\n}", "process BWAMEM2_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa-mem2=2.2.1 bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' :\n        'quay.io/biocontainers/mulled-v2-e5d375990341c5aef3c9aff74f96f66f65375ef6:cf603b12db30ec91daa04ba45a8ee0f35bbcd1e2-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa-mem2 \\\\\n        mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 -@ $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwamem2: \\$(echo \\$(bwa-mem2 version 2>&1) | sed 's/.* //')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}", "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    \n    script:\n    println(versions)\n\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        print(\"*********************\")\n        print(f.name)\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}", "process METAPHLAN3 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::metaphlan=3.0.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/metaphlan:3.0.12--pyhb7b1952_0' :\n        'quay.io/biocontainers/metaphlan:3.0.12--pyhb7b1952_0' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path metaphlan_db\n\n    output:\n    tuple val(meta), path(\"*_profile.txt\")   ,                emit: profile\n    tuple val(meta), path(\"*.biom\")          ,                emit: biom\n    tuple val(meta), path('*.bowtie2out.txt'), optional:true, emit: bt2out\n    path \"versions.yml\"                      ,                emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def input_type  = (\"$input\".endsWith(\".fastq.gz\")) ? \"--input_type fastq\" :  (\"$input\".contains(\".fasta\")) ? \"--input_type fasta\" : (\"$input\".endsWith(\".bowtie2out.txt\")) ? \"--input_type bowtie2out\" : \"--input_type sam\"\n    def input_data  = (\"$input_type\".contains(\"fastq\")) && !meta.single_end ? \"${input[0]},${input[1]}\" : \"$input\"\n    def bowtie2_out = \"$input_type\" == \"--input_type bowtie2out\" || \"$input_type\" == \"--input_type sam\" ? '' : \"--bowtie2out ${prefix}.bowtie2out.txt\"\n\n    \"\"\"\n    metaphlan \\\\\n        --nproc $task.cpus \\\\\n        $input_type \\\\\n        $input_data \\\\\n        $args \\\\\n        $bowtie2_out \\\\\n        --bowtie2db ${metaphlan_db} \\\\\n        --biom ${prefix}.biom \\\\\n        --output_file ${prefix}_profile.txt\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        metaphlan3: \\$(metaphlan --version 2>&1 | awk '{print \\$3}')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/canu", "xiaoli-dong/magph/CUSTOM_DUMPSOFTWAREVERSIONS", "xiaoli-dong/magph/METAPHLAN3"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly", "xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["Picard", "MarkDuplicates (IP)"], "nb_own": 1, "list_own": ["sheenamt"], "nb_wf": 1, "list_wf": ["hrd_pipeline"], "list_contrib": ["sheenamt"], "nb_contrib": 1, "codes": ["\nprocess picard_remove_duplicates {\n    label 'picard'\n\n    tag \"${sample_id}-${sample_type}\"\n\n    input:\n        tuple val(sample_id), val(sample_type), file(bam_file) from raw_bams\n\n    output:\n        tuple val(sample_id), val(sample_type), file(\"${sample_id}.${sample_type}.rmdup.bam\"), file(\"${sample_id}.${sample_type}.rmdup.bai\") into rmdup_bams\n\n                                                             \n\n    script:\n    \"\"\"\n    picard -Xmx${task.memory.toGiga()}g -Djava.io.tmpdir=./ -Dpicard.useLegacyParser=false \\\n    MarkDuplicates \\\n    -INPUT ${bam_file} \\\n    -OUTPUT ${sample_id}.${sample_type}.rmdup.bam \\\n    -METRICS_FILE ${sample_id}.${sample_type}.quality_metrics \\\n    -REMOVE_DUPLICATES true \\\n    -ASSUME_SORTED true \\\n    -VALIDATION_STRINGENCY SILENT \\\n    -CREATE_INDEX true 2> picard_rmdupes.log\n    \"\"\"\n}"], "list_proc": ["sheenamt/hrd_pipeline/picard_remove_duplicates"], "list_wf_names": ["sheenamt/hrd_pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sheynkman-lab"], "nb_wf": 1, "list_wf": ["IsoSeq-Nextflow"], "list_contrib": ["bj8th"], "nb_contrib": 1, "codes": ["\nprocess merge {\n    tag \"$barcode\"\n    publishDir \"${params.outdir}/isoseq3/merge/${barcode}\", mode: \"copy\"\n    label \"isoseq3\"\n\n    input:\n        set barcode, file(bam_files) from ch_lima_grouped_by_barcode\n        val barcode_pairs from ch_barcode_pairs_use.collect()\n    when:\n        barcode_pairs.contains(barcode)\n    output:\n        tuple val(barcode), file(\"*.bam\") into ch_merged_reads\n    script:\n        \"\"\"\n        samtools merge ${barcode}.bam $bam_files\n        \"\"\"\n}"], "list_proc": ["sheynkman-lab/IsoSeq-Nextflow/merge"], "list_wf_names": ["sheynkman-lab/IsoSeq-Nextflow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "SAMBLASTER"], "nb_own": 2, "list_own": ["sickle-in-africa", "sripaladugu"], "nb_wf": 2, "list_wf": ["germline_somatic", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess GroupReadsByUmi {\n    publishDir \"${params.outdir}/Reports/${idSample}/UMI/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(alignedBam) from umi_aligned_bams_ch\n\n    output:\n        file(\"${idSample}_umi_histogram.txt\") into umi_histogram_ch\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi-grouped.bam\") into umi_grouped_bams_ch\n\n    when: params.umi\n\n    script:\n    \"\"\"\n    mkdir tmp\n\n    samtools view -h ${alignedBam} | \\\n    samblaster -M --addMateTags | \\\n    samtools view -Sb - >${idSample}_unsorted_tagged.bam\n\n    fgbio --tmp-dir=${PWD}/tmp \\\n    GroupReadsByUmi \\\n    -s Adjacency \\\n    -i ${idSample}_unsorted_tagged.bam \\\n    -o ${idSample}_umi-grouped.bam \\\n    -f ${idSample}_umi_histogram.txt\n    \"\"\"\n}", "\nprocess IndexBamFile {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (save_bam_mapped) \"Preprocessing/${idSample}/Mapped/${it}\"\n            else null\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged_to_index\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_mapped_merged_indexed\n        set idPatient, idSample into tsv_bam_indexed\n\n    when: save_bam_mapped || !(params.known_indels)\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/GroupReadsByUmi", "sripaladugu/germline_somatic/IndexBamFile"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess FilterVariantsFromFreebayes {\n\n    publishDir \"${params.outdir}/Filtered/${idSample}/Freebayes\", mode: 'copy'\n\n    input:\n        tuple val(variantCaller), val(idSample), path(vcf)\n\n    output:\n        path (outputVcf + '.gz')\n\n    script:\n        outputVcf = reduceVCF(vcf) + '.filtered.vcf'\n        \"\"\"\n        bcftools view -i'QUAL>20 && DP>10' ${vcf} > ${outputVcf}\n        gzip ${outputVcf}\n        \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/FilterVariantsFromFreebayes"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess FilterVariantsFromStrelka {\n\n    publishDir \"${params.outdir}/Filtered/${idSample}/Strelka\", mode: 'copy'\n\n    input:\n    tuple val(variantCaller), val(idSample), path(vcf)\n\n    script:\n    outputVcf = reduceVCF(vcf) + '.filtered.vcf'\n        \"\"\"\n        bcftools view -i'FILTER=\"PASS\"' ${vcf} > ${outputVcf}\n        gzip ${outputVcf}\n        \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/FilterVariantsFromStrelka"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess MergeBaseRecalibrationReportsForSample {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n    label 'withGatkContainer'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        tuple val(idPatient), val(idSample), file(recal)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.recal.table\")\n        tuple val(idPatient), val(idSample)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/MergeBaseRecalibrationReportsForSample"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess AnnotateVariantsWithSnpeff {\n    tag \"${idSample} - ${variantCaller} - ${vcf}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: {\n        if (it == \"${reducedVCF}_snpEff.ann.vcf\") null\n        else \"Reports/${idSample}/snpEff/${it}\"\n    }\n\n    input:\n        tuple val(variantCaller), val(idSample), file(vcf)\n        file(snpEff_config)\n        file(dataDir)\n        val snpeffDb\n\n    output:\n        tuple file(\"${reducedVCF}_snpEff.genes.txt\"), file(\"${reducedVCF}_snpEff.html\"), file(\"${reducedVCF}_snpEff.csv\")\n        tuple val(variantCaller), val(idSample), file(\"${reducedVCF}_snpEff.ann.vcf\")\n\n    script:\n    reducedVCF = reduceVCF(vcf.fileName)\n    \"\"\"\n    snpEff -Xmx${task.memory.toGiga()}g \\\n        ${snpeffDb} \\\n        -csvStats ${reducedVCF}_snpEff.csv \\\n        -nodownload \\\n        -dataDir \\${PWD}/${dataDir} \\\n        -canon \\\n        -v ${vcf} \\\n        > ${reducedVCF}_snpEff.ann.vcf\n\n    mv snpEff_summary.html ${reducedVCF}_snpEff.html\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/AnnotateVariantsWithSnpeff"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess MergeReadGroupsForSample {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), file(bam)\n\n    output:\n        tuple val(idPatient), val(idSample), file(\"${idSample}.bam\")\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/MergeReadGroupsForSample"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["process GetFastqcQualityReport {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: 'copy'\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), path(\"${idSample}_${idRun}_R1.fastq.gz\"), path(\"${idSample}_${idRun}_R2.fastq.gz\")\n\n    output:\n        path(\"*.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/GetFastqcQualityReport"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess GetUnmappedBamQualityReport {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: 'copy'\n\n    input:\n        tuple val(idPatient), val(idSample), val(idRun), path(\"${idSample}_${idRun}.bam\")\n\n    output:\n        path(\"*.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/GetUnmappedBamQualityReport"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 2, "tools": ["SAMtools", "MSIsensor"], "nb_own": 2, "list_own": ["sickle-in-africa", "sripaladugu"], "nb_wf": 2, "list_wf": ["germline_somatic", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}", "\nprocess MSIsensor_scan {\n    label 'cpus_1'\n    label 'memory_max'\n\n    tag \"${fasta}\"\n\n    input:\n    file(fasta) from ch_fasta\n    file(fastaFai) from ch_fai\n\n    output:\n    file \"microsatellites.list\" into msi_scan_ch\n\n    when: 'msisensor' in tools\n\n    script:\n    \"\"\"\n    msisensor scan -d ${fasta} -o microsatellites.list\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/BuildFastaFai", "sripaladugu/germline_somatic/MSIsensor_scan"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BWA"], "nb_own": 2, "list_own": ["sickle-in-africa", "sripaladugu"], "nb_wf": 2, "list_wf": ["saw.sarek", "nextflow_align"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "sripaladugu", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess mapping {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(read1), file(read2) from read_pairs\n    file genome_ref_dir\n    \n    output:\n    set sample_id, file(\"${sample_id}_aln.sam\") into sam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    bwa mem -M -t 24 ${genome_ref_dir}/${genome_ref_fname} ${read1} ${read2} > ${sample_id}_aln.sam \n    \"\"\"\n}", "\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}"], "list_proc": ["sripaladugu/nextflow_align/mapping", "sickle-in-africa/saw.sarek/MergeBamMapped"], "list_wf_names": ["sripaladugu/nextflow_align", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 2, "tools": ["FastQC", "GATK"], "nb_own": 2, "list_own": ["sickle-in-africa", "ssun1116"], "nb_wf": 2, "list_wf": ["meripseqpipe", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "kingzhuky", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "ssun1116", "lescai", "szilvajuhos", "FriederikeHanssen", "juneb4869", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess Fastqc{\n    tag \"$sample_name\"\n    publishDir path: { params.skip_fastqc ? params.outdir : \"${params.outdir}/QC\" },\n             saveAs: { params.skip_fastqc ? null : it }, mode: 'link'\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from fastqc_reads\n\n    output:\n    file \"fastqc/*\" into fastqc_results\n\n    when:\n    aligner != \"none\" && !params.skip_fastqc\n\n    shell:\n    skip_fastqc = params.skip_fastqc\n    if ( reads_single_end){\n        \"\"\"\n        mkdir fastqc\n        fastqc -o fastqc --noextract ${reads}\n        \"\"\"       \n    } else {\n        \"\"\"\n        mkdir fastqc   \n        fastqc -o fastqc --noextract ${reads[0]}\n        fastqc -o fastqc --noextract ${reads[1]}\n        \"\"\"      \n    }\n}", "\nprocess BaseRecalibrator {\n    label 'cpus_1'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamBaseRecalibrator\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(dict) from ch_dict\n        file(fastaFai) from ch_fai\n        file(knownIndels) from ch_known_indels\n        file(knownIndelsIndex) from ch_known_indels_tbi\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.table\") into tableGatherBQSRReports\n        set idPatient, idSample into recalTableTSVnoInt\n\n    when: params.known_indels\n\n    script:\n    dbsnpOptions = params.dbsnp ? \"--known-sites ${dbsnp}\" : \"\"\n    knownOptions = params.known_indels ? knownIndels.collect{\"--known-sites ${it}\"}.join(' ') : \"\"\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n                                         \n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        BaseRecalibrator \\\n        -I ${bam} \\\n        -O ${prefix}${idSample}.recal.table \\\n        --tmp-dir . \\\n        -R ${fasta} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        ${knownOptions} \\\n        --verbosity INFO\n    \"\"\"\n}"], "list_proc": ["ssun1116/meripseqpipe/Fastqc", "sickle-in-africa/saw.sarek/BaseRecalibrator"], "list_wf_names": ["sickle-in-africa/saw.sarek", "ssun1116/meripseqpipe"]}, {"nb_reuse": 2, "tools": ["TopHat", "GATK"], "nb_own": 2, "list_own": ["sickle-in-africa", "ssun1116"], "nb_wf": 2, "list_wf": ["meripseqpipe", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "kingzhuky", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "ssun1116", "lescai", "szilvajuhos", "FriederikeHanssen", "juneb4869", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess Tophat2Align {\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/tophat2\", mode: 'link', overwrite: true\n\n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from tophat2_reads\n    file index from tophat2_index.collect()\n    file gtf\n\n    output:\n    set val(sample_id), file(\"*_tophat2.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into tophat2_bam\n    file \"*_log.txt\" into tophat2_log\n    \n    when:\n    aligner == \"tophat2\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.rev)?(\\.\\d)?(\\.fa)?(\\.bt2)?$/\n    strand_info = params.stranded == \"no\" ? \"fr-unstranded\" : params.stranded == \"reverse\" ? \"fr-secondstrand\" : \"fr-firststrand\"\n    if (reads_single_end) {\n        \"\"\"\n        tophat  -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                $reads > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        tophat -p ${task.cpus} \\\n                -G $gtf \\\n                -o $sample_name \\\n                --no-novel-juncs \\\n                --library-type $strand_info \\\n                $index_base \\\n                ${reads[0]} ${reads[1]} > ${sample_name}_log.txt\n        mv $sample_name/accepted_hits.bam ${sample_name}_tophat2.bam\n        \"\"\"\n    }\n}", "\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["ssun1116/meripseqpipe/Tophat2Align", "sickle-in-africa/saw.sarek/GatherBQSRReports"], "list_wf_names": ["sickle-in-africa/saw.sarek", "ssun1116/meripseqpipe"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BWA", "GATK"], "nb_own": 2, "list_own": ["sickle-in-africa", "ssun1116"], "nb_wf": 2, "list_wf": ["meripseqpipe", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "kingzhuky", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "ssun1116", "lescai", "szilvajuhos", "FriederikeHanssen", "juneb4869", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess ApplyBQSR {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(recalibrationReport), file(intervalBed) from bamApplyBQSR\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.recal.bam\") into bam_recalibrated_to_merge\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        ApplyBQSR \\\n        -R ${fasta} \\\n        --input ${bam} \\\n        --output ${prefix}${idSample}.recal.bam \\\n        ${intervalsOptions} \\\n        --bqsr-recal-file ${recalibrationReport}\n    \"\"\"\n}", "\nprocess BWAAlign{\n    label 'aligners'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/bwa\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from bwa_reads\n    file index from bwa_index.collect()\n\n    output:\n    set val(sample_id), file(\"*_bwa.bam\"), val(reads_single_end), val(gzip), val(input), val(group) into bwa_bam\n    file \"*\" into bwa_result\n\n    when:\n    aligner == \"bwa\"\n\n    script:\n    index_base = index[0].toString() - ~/(\\.pac)?(\\.bwt)?(\\.ann)?(\\.amb)?(\\.sa)?(\\.fa)?$/\n    if (reads_single_end) {\n        \"\"\"\n        bwa aln -t ${task.cpus} \\\n                -f ${reads.baseName}.sai \\\n                $index_base \\\n                $reads\n        bwa samse \\\n                $index_base \\\n                ${reads.baseName}.sai \\\n                $reads  | \\\n            samtools view -@ ${task.cpus} -h -bS - > ${sample_name}_bwa.bam\n        \"\"\"\n    } else {\n        \"\"\"\n        bwa aln -t ${task.cpus} \\\n                -f ${reads[0].baseName}.sai \\\n                $index_base \\\n                ${reads[0]}\n        bwa aln -t ${task.cpus} \\\n                -f ${reads[1].baseName}.sai \\\n                $index_base \\\n                ${reads[1]}\n        bwa sampe \\\n                $index_base \\\n                ${reads[0].baseName}.sai ${reads[1].baseName}.sai \\\n                ${reads[0]} ${reads[1]} | \\\n            samtools view -@ ${task.cpus} -h -bS - > ${sample_name}_bwa.bam\n        \"\"\"\n    }\n}"], "list_proc": ["sickle-in-africa/saw.sarek/ApplyBQSR", "ssun1116/meripseqpipe/BWAAlign"], "list_wf_names": ["sickle-in-africa/saw.sarek", "ssun1116/meripseqpipe"]}, {"nb_reuse": 2, "tools": ["SAMtools"], "nb_own": 2, "list_own": ["sickle-in-africa", "ssun1116"], "nb_wf": 2, "list_wf": ["meripseqpipe", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "kingzhuky", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "ssun1116", "lescai", "szilvajuhos", "FriederikeHanssen", "juneb4869", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 22, "codes": ["\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}", "\nprocess SortRename {\n    label 'sort'\n    tag \"$sample_name\"\n    publishDir \"${params.outdir}/alignment/samtoolsSort/\", mode: 'link', overwrite: true\n    \n    input:\n    set val(sample_id), file(bam_file), val(reads_single_end), val(gzip), val(input), val(group) from merge_bam_file\n\n    output:\n    set val(group), val(sample_id), file(\"*.bam\"), file(\"*.bai\") into sorted_bam\n    file \"*.{bam,bai}\" into rseqc_bam, bedgraph_bam, feacount_bam, cuffbam, peakquan_bam, diffpeak_bam, sng_bam\n    file \"*.bam\" into bam_results\n\n    script:\n    sample_name = sample_id + (input ? \".input_\" : \".ip_\") + group\n    output = sample_name + \".bam\"\n    mapq_cutoff = (params.mapq_cutoff).toInteger() \n    if (!params.skip_sort){\n        \"\"\"\n        if [ \"$mapq_cutoff\" -gt \"0\" ]; then\n            samtools view -hbq $mapq_cutoff $bam_file | samtools sort -@ ${task.cpus} -O BAM -o $output -\n        else\n            samtools sort -@ ${task.cpus} -O BAM -o $output $bam_file\n        fi\n        samtools index -@ ${task.cpus} $output\n        \"\"\"\n    } else {\n        \"\"\"\n        if [ \"$mapq_cutoff\" -gt \"0\" ]; then\n            samtools view -hbq $mapq_cutoff $bam_file > $output\n        else\n            mv $bam_file $output\n        fi\n        samtools index -@ ${task.cpus} $output\n        \"\"\"\n    }\n}"], "list_proc": ["sickle-in-africa/saw.sarek/MergeBamRecal", "ssun1116/meripseqpipe/SortRename"], "list_wf_names": ["sickle-in-africa/saw.sarek", "ssun1116/meripseqpipe"]}, {"nb_reuse": 1, "tools": ["MultiQC", "GATK"], "nb_own": 2, "list_own": ["sickle-in-africa", "steepale"], "nb_wf": 1, "list_wf": ["saw.sarek", "nf-core-mutenrich"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "steepale", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/HaplotypeCaller"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 2, "tools": ["FastQC", "FreeBayes"], "nb_own": 2, "list_own": ["sickle-in-africa", "steepale"], "nb_wf": 2, "list_wf": ["saw.sarek", "wgsfastqtobam"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "steepale", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n    input:\n    set val(name), file(reads) from read_files_fastqc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/FreebayesSingle", "steepale/wgsfastqtobam/fastqc"], "list_wf_names": ["sickle-in-africa/saw.sarek", "steepale/wgsfastqtobam"]}, {"nb_reuse": 2, "tools": ["BCFtools", "FreeBayes"], "nb_own": 2, "list_own": ["sickle-in-africa", "stevekm"], "nb_wf": 2, "list_wf": ["MuTect2_target_chunking", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "stevekm", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess FreeBayes {\n    tag \"${idSampleTumor}_vs_${idSampleNormal}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n\n    input:\n        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from pairBamFreeBayes\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"FreeBayes\"), idPatient, val(\"${idSampleTumor}_vs_${idSampleNormal}\"), file(\"${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\") into vcfFreeBayes\n\n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --pooled-continuous \\\n        --pooled-discrete \\\n        --genotype-qualities \\\n        --report-genotype-likelihood-max \\\n        --allele-balance-priors-off \\\n        --min-alternate-fraction 0.03 \\\n        --min-repeat-entropy 1 \\\n        --min-alternate-count 2 \\\n        ${intervalsOptions} \\\n        ${bamTumor} \\\n        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf\n    \"\"\"\n}", "\nprocess mutect2_noChunk {\n    tag \"${prefix}\"\n    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n\n    input:\n    set val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(targets_bed), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_noChunk_ch\n\n    output:\n    set val(\"${label}\"), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_noChunk\n    file(\"${output_vcf}\")\n    file(\"${multiallelics_stats}\")\n    file(\"${realign_stats}\")\n\n    when: params.disable != \"true\"\n\n    script:\n    label = \"noChunk\"\n    prefix = \"${comparisonID}.${label}\"\n    output_vcf = \"${prefix}.vcf\"\n    output_norm_vcf = \"${prefix}.norm.vcf\"\n    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n    \"\"\"\n    gatk.sh -T MuTect2 \\\n    -dt NONE \\\n    --logging_level WARN \\\n    --standard_min_confidence_threshold_for_calling 30 \\\n    --max_alt_alleles_in_normal_count 10 \\\n    --max_alt_allele_in_normal_fraction 0.05 \\\n    --max_alt_alleles_in_normal_qscore_sum 40 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --cosmic \"${cosmic_ref_vcf}\" \\\n    --intervals \"${targets_bed}\" \\\n    --interval_padding 10 \\\n    --input_file:tumor \"${tumorBam}\" \\\n    --input_file:normal \"${normalBam}\" \\\n    --out \"${output_vcf}\"\n\n    # normalize and split vcf entries\n    cat ${output_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n    \"${output_norm_vcf}\"\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/FreeBayes", "stevekm/MuTect2_target_chunking/mutect2_noChunk"], "list_wf_names": ["stevekm/MuTect2_target_chunking", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 2, "tools": ["SAMtools", "MSIsensor"], "nb_own": 2, "list_own": ["sickle-in-africa", "sripaladugu"], "nb_wf": 2, "list_wf": ["germline_somatic", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess MSIsensor_scan {\n    label 'cpus_1'\n    label 'memory_max'\n\n    tag \"${fasta}\"\n\n    input:\n    file(fasta) from ch_fasta\n    file(fastaFai) from ch_fai\n\n    output:\n    file \"microsatellites.list\" into msi_scan_ch\n\n    when: 'msisensor' in tools\n\n    script:\n    \"\"\"\n    msisensor scan -d ${fasta} -o microsatellites.list\n    \"\"\"\n}", "\nprocess Mpileup {\n    label 'cpus_1'\n    label 'memory_singleCPU_2_task'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode, saveAs: { it == \"${idSample}.pileup\" ? \"VariantCalling/${idSample}/Control-FREEC/${it}\" : null }\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamMpileup\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, file(\"${prefix}${idSample}.pileup\") into mpileupMerge\n        set idPatient, idSample into tsv_mpileup\n\n    when: 'controlfreec' in tools || 'mpileup' in tools\n\n    script:\n    prefix = params.no_intervals ? \"\" : \"${intervalBed.baseName}_\"\n    intervalsOptions = params.no_intervals ? \"\" : \"-l ${intervalBed}\"\n\n    \"\"\"\n    # Control-FREEC reads uncompresses the zipped file TWICE in single-threaded mode.\n    # we are therefore not using compressed pileups here\n    samtools mpileup \\\n        -f ${fasta} ${bam} \\\n        ${intervalsOptions} > ${prefix}${idSample}.pileup\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/MSIsensor_scan", "sripaladugu/germline_somatic/Mpileup"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic"]}, {"nb_reuse": 4, "tools": ["FREEC", "Picard", "BWA", "GATK"], "nb_own": 2, "list_own": ["sickle-in-africa", "sripaladugu"], "nb_wf": 2, "list_wf": ["saw.sarek", "nextflow_align"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "sripaladugu", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 20, "codes": ["\nprocess mapping {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(read1), file(read2) from read_pairs\n    file genome_ref_dir\n    \n    output:\n    set sample_id, file(\"${sample_id}_aln.sam\") into sam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    bwa mem -M -t 24 ${genome_ref_dir}/${genome_ref_fname} ${read1} ${read2} > ${sample_id}_aln.sam \n    \"\"\"\n}", "\nprocess RecalibrateIndelQualityScores {\n    label 'process_low'\n    tag \"${variantCaller}-${idSample}\"\n\n    publishDir \"${params.outdir}/recalibratedVariants/${idSample}/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(variantCaller), val(idSample), path(vcf), path(vcfIndex)\n        tuple path(recalTable), path(tranches)\n\n    script:\n        \"\"\"\n        gatk --java-options \"-Xmx5g -Xms5g\" \\\n            ApplyVQSR \\\n            -V ${vcf} \\\n            --recal-file ${recalTable} \\\n            --tranches-file ${tranches} \\\n            --truth-sensitivity-filter-level 99.7 \\\n            --create-output-variant-index true \\\n            -mode INDEL \\\n            -O ${idSample}_indel.recalibrated.vcf.gz\n        \"\"\"\n\n}", "\nprocess ControlFREEC {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}_vs_${idSampleNormal}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOut\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleNormal, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecViz\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}_vs_${idSampleNormal}.config.txt\") into controlFreecOut\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}_vs_${idSampleNormal}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n    \n    echo \"[control]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupNormal}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}", "\nprocess bamsorter {\n    publishDir \"${params.results_dir}/${sample_id}/\", mode: 'copy', overwrite: true\n    \n    input:\n    set val(sample_id), file(samfile) from sam_files\n    \n    output:\n    set sample_id, file(\"${sample_id}_sorted_aln.bam\") into sorted_bam_files\n\n    script:\n    \"\"\"\n    echo \"${sample_id}\"\n    picard -Xmx16g AddOrReplaceReadGroups \\\n           INPUT=${samfile} OUTPUT=${sample_id}_sorted_aln.bam SORT_ORDER=coordinate \\\n           RGID=${sample_id}-id RGLB=${sample_id}-lib RGPL=ILLUMINA RGPU=${sample_id}-01 RGSM=${sample_id}\n    \"\"\"\n}"], "list_proc": ["sripaladugu/nextflow_align/mapping", "sickle-in-africa/saw.sarek/RecalibrateIndelQualityScores", "sickle-in-africa/saw.sarek/ControlFREEC", "sripaladugu/nextflow_align/bamsorter"], "list_wf_names": ["sripaladugu/nextflow_align", "sickle-in-africa/saw.sarek"]}, {"nb_reuse": 2, "tools": ["BCFtools", "SAMtools", "QualiMap", "FreeBayes", "MultiQC", "TIDDIT", "FREEC", "FastQC", "MSIsensor", "snpEff", "GATK", "VCFtools"], "nb_own": 2, "list_own": ["sickle-in-africa", "sripaladugu"], "nb_wf": 2, "list_wf": ["germline_somatic", "saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess IndexBamMergedForSentieon {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_sentieon_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\"), file(\"${idSample}.bam.bai\") into bam_sentieon_mapped_merged_indexed\n\n    script:\n    \"\"\"\n    samtools index ${idSample}.bam\n    \"\"\"\n}", "\nprocess GetSoftwareVersions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml'\n                                      \n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    ${params.sarekDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/IndexBamMergedForSentieon", "sickle-in-africa/saw.sarek/GetSoftwareVersions"], "list_wf_names": ["sickle-in-africa/saw.sarek", "sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.sarek"], "list_contrib": ["chelauk", "jfnavarro", "arontommi", "alneberg", "lconde-ucl", "davidmasp", "jackmo375", "pcantalupo", "malinlarsson", "olgabot", "skrakau", "lescai", "szilvajuhos", "FriederikeHanssen", "nf-core-bot", "maxulysse", "ggabernet", "apeltzer", "adrlar"], "nb_contrib": 19, "codes": ["\nprocess RecalibrateVariantQualityScores {\n    label 'cpus_1'\n    label 'withGatkContainer'\n\n    tag \"${variantCaller}-${idSample}\"\n\n    publishDir \"${params.outdir}/recalibratedVariants/${idSample}/${variantCaller}\", mode: params.publish_dir_mode\n\n    input:\n        tuple val(variantCaller), val(idSample), file(vcf)\n        path recalibrationReport\n\n    script:\n        \"\"\"\n        gatk ApplyVQSR \\\n            -V ${vcf} \\\n            --recal-file ${recalibrationReport} \\\n            -O ${variantCaller}_${idSample}.vcf.gz\n        \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.sarek/RecalibrateVariantQualityScores"], "list_wf_names": ["sickle-in-africa/saw.sarek"]}, {"nb_reuse": 1, "tools": ["MarkDuplicates (IP)", "GATK"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.snv-indel"], "list_contrib": ["jackmo375"], "nb_contrib": 1, "codes": ["\nprocess markDuplicateReads {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.gatk4Image\n\n    input:\n    tuple val(name), path(bamFile)\n\n    output:\n    tuple val(name), path(\"${name}.marked.bam\")\n\n    script:\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        MarkDuplicates \\\n        -I ${bamFile} \\\n        -O \"${name}.marked.bam\" \\\n        -M \"${name}.marked_dup_metrics.txt\"\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/markDuplicateReads"], "list_wf_names": ["sickle-in-africa/saw.snv-indel"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.snv-indel"], "list_contrib": ["jackmo375"], "nb_contrib": 1, "codes": ["\nprocess recalibrateBaseQualityScores {\n    label 'withMaxMemory'\n    label 'withMaxCpus'\n    label 'withMaxTime'\n    container params.gatk4Image\n\n    input:\n    tuple val(name), path(bamFile)\n\n    output:\n    tuple val(name), path(\"${name}.recal.bam\")\n\n    script:\n    if ( file(\"${params.baseQualityRecalibrationTable}\").exists() )\n        \"\"\"\n        gatk ApplyBQSR \\\n            -R reference.fasta \\\n            -I ${bamFile} \\\n            --bqsr-recal-file ${params.baseQualityRecalibrationTable} \\\n            -O ${name}.recal.bam\n        \"\"\"\n    else\n        \"\"\"\n        ln -s ${bamFile} ${name}.recal.bam\n        \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/recalibrateBaseQualityScores"], "list_wf_names": ["sickle-in-africa/saw.snv-indel"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.snv-indel"], "list_contrib": ["jackmo375"], "nb_contrib": 1, "codes": ["\nprocess buildBwaIndex {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n    container params.bwaImage\n\n\twhen:\n\t!file(params.referenceSequence['dir'] + 'bwa.' + params.referenceSequence['label'] + '.amb').exists()\n\n\tscript:\n\t\"\"\"\n\tbwa index \\\n\t\t\t-p ${params.referenceSequence['dir']}bwa.${params.referenceSequence['label']} \\\n\t\t\t${params.referenceSequence['path']}\n\t\"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/buildBwaIndex"], "list_wf_names": ["sickle-in-africa/saw.snv-indel"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.snv-indel"], "list_contrib": ["jackmo375"], "nb_contrib": 1, "codes": ["\nprocess buildSamtoolsIndex {\n\tlabel 'withMaxMemory'\n\tlabel 'withMaxCpus'\n\tlabel 'withMaxTime'\n    container params.samtoolsImage\n\n\twhen:\n\t!file(params.referenceSequence['path'] + '.fai').exists()\n\n\tscript:\n\t\"\"\"\n\tsamtools faidx ${params.referenceSequence['path']}\n\t\"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.snv-indel/buildSamtoolsIndex"], "list_wf_names": ["sickle-in-africa/saw.snv-indel"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.structural-variants"], "list_contrib": ["jackmo375", "waffle-iron", "marcelm", "pallolason", "kusalananda", "viklund", "glormph", "jhagberg", "samuell"], "nb_contrib": 9, "codes": ["\nprocess artifact_mask_vcfs {\n    input:\n        set file(svfile), val(uuid), val(dir) from ch_vcfs\n    output:\n        set file(svfile), val(uuid), val(dir) into ch_artifact_masked_vcfs\n\n    tag \"$uuid $svfile\"\n\n    executor choose_executor()\n\n    \"\"\"\n    BNAME=\\$( echo $svfile | cut -d. -f1 )\n    MASK_DIR=$params.mask_artifacts_dir\n\n    # We don't want to change the filename in this process so we copy the\n    # infile and remove the symbolic link. And then recreate the file at the\n    # end.\n    cp \"$svfile\" workfile\n    rm \"$svfile\" # It's a link, should be ok :)\n    for mask in \\$MASK_DIR/*; do\n        if [[ ! -f \"\\$mask\" ]]; then\n            continue\n        fi\n        cat workfile \\\n            | bedtools intersect -header -v -a stdin -b \"\\$mask\" -f 0.25 \\\n            > tempfile\n        mv tempfile workfile\n    done\n    mv workfile \"$svfile\"\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.structural-variants/artifact_mask_vcfs"], "list_wf_names": ["sickle-in-africa/saw.structural-variants"]}, {"nb_reuse": 1, "tools": ["vt"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.structural-variants"], "list_contrib": ["jackmo375", "waffle-iron", "marcelm", "pallolason", "kusalananda", "viklund", "glormph", "jhagberg", "samuell"], "nb_contrib": 9, "codes": ["\nprocess normalize_vcf {\n    input:\n        set file(infile), val(uuid), val(dir) from ch_normalize_vcf\n    output:\n        set file(\"*.vt.vcf\"), val(uuid), val(dir) into ch_normalized_vcf\n\n    tag \"$uuid - $infile\"\n\n    executor choose_executor()\n\n    \"\"\"\n    INFILE=\"$infile\"\n    OUTFILE=\"\\${INFILE%.vcf}.vt.vcf\"\n\n    ## If the input file is empty, just copy it\n    if [[ -f \"\\$INFILE\" && -s \"\\$INFILE\" ]]; then\n        cp \"\\$INFILE\" \"\\$OUTFILE\"\n        exit\n    fi\n\n\n    ## Normalization\n    sed 's/ID=AD,Number=./ID=AD,Number=R/' \"\\$INFILE\" \\\n        | vt decompose -s - > \"\\$INFILE.vt_temp\"\n\n    if ! vt normalize -r \"$params.ref_fasta\" \"\\$INFILE.vt_temp\" > \"\\$OUTFILE\"\n    then\n        printf \"VT normalisation Failed\\n\" >&2\n        cp \"\\$INFILE.vt_temp\" \"\\$OUTFILE\"\n    fi\n    \"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.structural-variants/normalize_vcf"], "list_wf_names": ["sickle-in-africa/saw.structural-variants"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sickle-in-africa"], "nb_wf": 1, "list_wf": ["saw.structural-variants"], "list_contrib": ["jackmo375", "waffle-iron", "marcelm", "pallolason", "kusalananda", "viklund", "glormph", "jhagberg", "samuell"], "nb_contrib": 9, "codes": ["\nprocess indexBamFile {\n\tcontainer params.samtoolsImage\n\n\tinput:\n\tpath bamFile\n\n\toutput:\n\ttuple path(\"${bamFile}\"), path(\"${bamFile}.bai\")\n\n\tscript:\n\t\"\"\"\n\tsamtools index ${bamFile}\n\t\"\"\"\n}"], "list_proc": ["sickle-in-africa/saw.structural-variants/indexBamFile"], "list_wf_names": ["sickle-in-africa/saw.structural-variants"]}, {"nb_reuse": 1, "tools": ["VCFtools"], "nb_own": 1, "list_own": ["silastittes"], "nb_wf": 1, "list_wf": ["panand_structure"], "list_contrib": ["silastittes"], "nb_contrib": 1, "codes": ["\nprocess drop_ind{\n\n    publishDir \"$baseDir/data/vcf/filtered/\"\n\n    input:\n    tuple val(prefix), file(vcf) from filter_ch\n\n    output:\n    tuple val(prefix), file(\"${prefix}_filtered_dropIND${params.indfilter}.vcf.gz\") into vcfind_ch\n\n\n    \"\"\"\n    vcftools --gzvcf ${vcf} --missing-indv --stdout | awk '\\$5 <= ${params.indfilter} {print \\$1}' > ${prefix}_${params.indfilter}_ind\n    vcftools --gzvcf ${vcf} --keep ${prefix}_${params.indfilter}_ind --recode --stdout | gzip -c > ${prefix}_filtered_dropIND${params.indfilter}.vcf.gz\n    \"\"\"\n\n}"], "list_proc": ["silastittes/panand_structure/drop_ind"], "list_wf_names": ["silastittes/panand_structure"]}, {"nb_reuse": 1, "tools": ["SAMtools", "FastQC", "Bowtie", "MultiQC"], "nb_own": 1, "list_own": ["simozhou"], "nb_wf": 1, "list_wf": ["epi-awesome"], "list_contrib": ["simozhou"], "nb_contrib": 1, "codes": ["\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    bowtie2 --version > v_bowtie2.txt\n    samtools --version > v_samtools.txt\n    macs2 --version > v_macs2.txt\n    scrape_software_versions.py > software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["simozhou/epi-awesome/get_software_versions"], "list_wf_names": ["simozhou/epi-awesome"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["simozhou"], "nb_wf": 1, "list_wf": ["epi-awesome"], "list_contrib": ["simozhou"], "nb_contrib": 1, "codes": ["\nprocess buildIndex {\n    input:\n    file fasta from fasta\n\n    output:\n    file 'genome.index*' into genome_index\n\n    \"\"\"\n    bowtie2-build -f ${fasta} genome.index --threads ${params.max_cpus}\n    \"\"\"\n}"], "list_proc": ["simozhou/epi-awesome/buildIndex"], "list_wf_names": ["simozhou/epi-awesome"]}, {"nb_reuse": 5, "tools": ["segmentSeq", "kraken2", "MultiQC", "FastQC", "GATK"], "nb_own": 5, "list_own": ["taltechnlp", "simozhou", "zamanianlab", "wslh-bio", "suzannejin"], "nb_wf": 5, "list_wf": ["RNAseq-VC-nf", "est-asr-pipeline", "epi-awesome", "nf-proportionality", "spriggan"], "list_contrib": ["k-florek", "simozhou", "wheelern", "AbigailShockey", "aivo0", "suzannejin"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml\n             'alignment/*'                              \n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess kraken {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/kraken\", mode: 'copy', pattern: \"*.kraken2.txt*\"\n\n  input:\n  set val(name), file(reads) from read_files_kraken\n\n  output:\n  tuple name, file(\"${name}.kraken2.txt\") into kraken_files, kraken_multiqc\n  file(\"Kraken2_DB.txt\") into kraken_version\n\n  script:\n  \"\"\"\n  kraken2 --db /kraken2-db/minikraken2_v1_8GB --threads ${task.cpus} --report ${name}.kraken2.txt --paired ${reads[0]} ${reads[1]}\n\n  ls /kraken2-db/ > Kraken2_DB.txt\n  \"\"\"\n}", "\nprocess fastqc {\n    tag \"$name\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      filename.indexOf('.zip') > 0 ? \"zips/$filename\" : \"$filename\"\n        }\n\n    input:\n    set val(name), file(reads) from ch_read_files_fastqc\n\n    output:\n    file '*_fastqc.{zip,html}' into ch_fastqc_results\n\n    script:\n    \"\"\"\n    fastqc --quiet --threads $task.cpus $reads\n    \"\"\"\n}", "\nprocess prepare_initial_data_dir {\n    input:\n    file show_seg\n    file audio\n\n    output:\n    path 'init_datadir' into init_datadir optional true\n\n    shell:\n    '''\n    . !{projectDir}/bin/prepare_process.sh\n    \n    if [ -s !{show_seg} ]; then\n      mkdir init_datadir\n      echo audio audio A > init_datadir/reco2file_and_channel\n      cat !{show_seg} | cut -f \"3,4,8\" -d \" \" | \\\n      while read LINE ; do \\\n        start=`echo $LINE | cut -f 1,2 -d \" \" | perl -ne '@t=split(); $start=$t[0]/100.0; printf(\"%08.3f\", $start);'`; \\\n        end=`echo $LINE   | cut -f 1,2 -d \" \" | perl -ne '@t=split(); $start=$t[0]/100.0; $len=$t[1]/100.0; $end=$start+$len; printf(\"%08.3f\", $end);'`; \\\n        sp_id=`echo $LINE | cut -f 3 -d \" \"`; \\\n        echo audio-${sp_id}---${start}-${end} audio $start $end; \\\n      done > init_datadir/segments\n      cat init_datadir/segments | perl -npe 's/\\\\s+.*//; s/((.*)---.*)/\\\\1 \\\\2/' > init_datadir/utt2spk\n      utils/utt2spk_to_spk2utt.pl init_datadir/utt2spk > init_datadir/spk2utt\n          echo \"audio !{audio}\" > init_datadir/wav.scp\n      \n    fi\n    '''\n}", "\nprocess plot_recalibration {\n\n      publishDir \"${output}/base_recalibration\", mode: 'copy', pattern: '*_AnalyzeCovariates.pdf'\n\n      input:\n          tuple val(id), file(recal_table) from brdt2\n\n      output:\n          file(\"${id}_AnalyzeCovariates.pdf\") into recal_plots\n\n      when:\n          params.bam\n\n      \"\"\"\n          gatk AnalyzeCovariates \\\n            -bqsr ${recal_table} \\\n            -plots \"${id}_AnalyzeCovariates.pdf\"\n \"\"\"\n}"], "list_proc": ["simozhou/epi-awesome/multiqc", "wslh-bio/spriggan/kraken", "suzannejin/nf-proportionality/fastqc", "taltechnlp/est-asr-pipeline/prepare_initial_data_dir", "zamanianlab/RNAseq-VC-nf/plot_recalibration"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf", "simozhou/epi-awesome", "taltechnlp/est-asr-pipeline", "wslh-bio/spriggan", "suzannejin/nf-proportionality"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["simozhou"], "nb_wf": 1, "list_wf": ["epi-awesome"], "list_contrib": ["simozhou"], "nb_contrib": 1, "codes": ["\nprocess samtools {\n    publishDir \"${params.outdir}/alignment\", mode: 'copy'\n\n    input:\n    file unsorted from bowtie_output\n\n    output:\n    file \"*.sorted.bam\" into sorted_bams\n\n    script:\n    prefix=unsorted[0].toString() - ~/(.bam)?$/\n    \"\"\"\n    samtools sort -@ ${params.max_cpus} -o ${prefix}.sorted.bam $unsorted\n    \"\"\"\n}"], "list_proc": ["simozhou/epi-awesome/samtools"], "list_wf_names": ["simozhou/epi-awesome"]}, {"nb_reuse": 1, "tools": ["PLINK"], "nb_own": 1, "list_own": ["sinonkt"], "nb_wf": 1, "list_wf": ["gemini-load-nf"], "list_contrib": ["sinonkt"], "nb_contrib": 1, "codes": ["\nprocess VCFgz {\n\n  tag { dataset.project }\n\n  input:\n  set dataset, \"${dataset.fileSetId}.bed\", \"${dataset.fileSetId}.bim\", \"${dataset.fileSetId}.fam\" from BedBimFam.map(attachBedBimFamFile)\n\n  output:\n  set dataset, \"${dataset.fileSetId}.vcf.gz\" into convertedVcfgzFromBedBimFam\n\n  shell:\n  '''\n  plink --bfile !{dataset.fileSetId} --recode vcf --out !{dataset.fileSetId}\n  bgzip --threads 8 -c !{dataset.fileSetId}.vcf > !{dataset.fileSetId}.vcf.gz\n  '''\n}"], "list_proc": ["sinonkt/gemini-load-nf/VCFgz"], "list_wf_names": ["sinonkt/gemini-load-nf"]}, {"nb_reuse": 1, "tools": ["vt"], "nb_own": 1, "list_own": ["sinonkt"], "nb_wf": 1, "list_wf": ["gemini-load-nf"], "list_contrib": ["sinonkt"], "nb_contrib": 1, "codes": ["\nprocess decomposeNormalizeAnnotate {\n\n    tag { \"${chunk.project}_${chunk.chrIdx}\" }\n\n    input:\n    set chunk, \"file.vcf.gz\", \"ref.fasta\" from DatasetChunks\n\n    output:\n    set chunk, \"annotated.vcf.gz\", \"annotated.vcf.gz.tbi\" into AnnotatedVCFChunks\n\n    shell:\n    '''\n    bgzip --decompress --threads 8 -c file.vcf.gz |\n        sed 's/ID=AD,Number=./ID=AD,Number=R/' | \n        vt decompose -s - |\n        vt normalize -r ref.fasta - |\n        java -Xmx8G -jar $SNPEFF_JAR -t !{chunk.refBuild} |\n        bgzip --threads 8 -c > annotated.vcf.gz\n    tabix -p vcf annotated.vcf.gz\n    '''\n}"], "list_proc": ["sinonkt/gemini-load-nf/decomposeNormalizeAnnotate"], "list_wf_names": ["sinonkt/gemini-load-nf"]}, {"nb_reuse": 1, "tools": ["GEMINI"], "nb_own": 1, "list_own": ["sinonkt"], "nb_wf": 1, "list_wf": ["gemini-load-nf"], "list_contrib": ["sinonkt"], "nb_contrib": 1, "codes": ["\nprocess geminiLoad {\n\n    tag { \"${chunk.project}_${chunk.chrIdx}\" }\n\n    input: \n    set chunk, \"annotated.vcf.gz\", \"annotated.vcf.gz.tbi\" from AnnotatedVCFChunks\n\n    output: \n    set chunk, \"${chunk.vcfgzId}.db\" into DBChunks\n\n    shell:\n    if(chunk.ped == null)\n        '''\n        gemini load --cores 32 --tempdir ./tmp -t snpEff -v annotated.vcf.gz !{chunk.vcfgzId}.db\n        '''\n    else\n        '''\n        gemini load --cores 32 --tempdir ./temp -t snpEff -v annotated.vcf.gz -p !{chunk.ped} !{chunk.vcfgzId}.db\n        '''\n}"], "list_proc": ["sinonkt/gemini-load-nf/geminiLoad"], "list_wf_names": ["sinonkt/gemini-load-nf"]}, {"nb_reuse": 1, "tools": ["snpEff"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess BuildCache_snpEff {\n  tag {snpeff_db}\n\n  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n\n  input:\n    val snpeff_db from ch_snpeff_db\n\n  output:\n    file(\"*\") into snpeff_cache_out\n\n  when: params.snpeff_cache\n\n  script:\n  \"\"\"\n  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n  \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/BuildCache_snpEff"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["BCFtools", "SAMtools", "QualiMap", "FreeBayes", "MultiQC", "TIDDIT", "FREEC", "FastQC", "MSIsensor", "snpEff", "GATK", "VCFtools"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: {it.indexOf(\".csv\") > 0 ? it : null}\n\n    output:\n        file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n        file \"software_versions.csv\"\n\n    when: !('versions' in skipQC)\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    alleleCounter --version &> v_allelecount.txt 2>&1 || true\n    bcftools --version &> v_bcftools.txt 2>&1 || true\n    ${aligner} version &> v_bwa.txt 2>&1 || true\n    cnvkit.py version &> v_cnvkit.txt 2>&1 || true\n    configManta.py --version &> v_manta.txt 2>&1 || true\n    configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true\n    echo \"${workflow.manifest.version}\" &> v_pipeline.txt 2>&1 || true\n    echo \"${workflow.nextflow.version}\" &> v_nextflow.txt 2>&1 || true\n    snpEff -version &> v_snpeff.txt 2>&1 || true\n    fastqc --version &> v_fastqc.txt 2>&1 || true\n    freebayes --version &> v_freebayes.txt 2>&1 || true\n    freec &> v_controlfreec.txt 2>&1 || true\n    gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true\n    msisensor &> v_msisensor.txt 2>&1 || true\n    multiqc --version &> v_multiqc.txt 2>&1 || true\n    qualimap --version &> v_qualimap.txt 2>&1 || true\n    R --version &> v_r.txt 2>&1 || true\n    R -e \"library(ASCAT); help(package='ASCAT')\" &> v_ascat.txt 2>&1 || true\n    samtools --version &> v_samtools.txt 2>&1 || true\n    tiddit &> v_tiddit.txt 2>&1 || true\n    trim_galore -v &> v_trim_galore.txt 2>&1 || true\n    vcftools --version &> v_vcftools.txt 2>&1 || true\n    vep --help &> v_vep.txt 2>&1 || true\n\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/get_software_versions"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess BuildFastaFai {\n    tag \"${fasta}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {params.save_reference ? \"reference_genome/${it}\" : null }\n\n    input:\n        file(fasta) from ch_fasta\n\n    output:\n        file(\"${fasta}.fai\") into fai_built\n\n    when: !(params.fasta_fai) && params.fasta && !('annotate' in step)\n\n    script:\n    \"\"\"\n    samtools faidx ${fasta}\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/BuildFastaFai"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess FastQCFQ {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}_R1.fastq.gz\"), file(\"${idSample}_${idRun}_R2.fastq.gz\") from inputPairReadsFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCFQReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}_R1.fastq.gz ${idSample}_${idRun}_R2.fastq.gz\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/FastQCFQ"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess FastQCBAM {\n    label 'FastQC'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idRun}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/FastQC/${idSample}_${idRun}\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") from inputBamFastQC\n\n    output:\n        file(\"*.{html,zip}\") into fastQCBAMReport\n\n    when: !('fastqc' in skipQC)\n\n    script:\n    \"\"\"\n    fastqc -t 2 -q ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/FastQCBAM"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess UMIMapBamFile {\n    input:\n        set idPatient, idSample, idRun, file(convertedBam) from umi_converted_bams_ch\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        tuple val(idPatient), val(idSample), val(idRun), file(\"${idSample}_umi_unsorted.bam\") into umi_aligned_bams_ch\n\n    when: params.umi\n\n    script:\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    samtools bam2fq -T RX ${convertedBam} | \\\n    ${aligner} mem -p -t ${task.cpus} -C -M -R \\\"@RG\\\\tID:${idSample}\\\\tSM:${idSample}\\\\tPL:Illumina\\\" \\\n    ${fasta} - | \\\n    samtools view -bS - > ${idSample}_umi_unsorted.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/UMIMapBamFile"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess MapReads {\n    label 'cpus_max'\n\n    tag \"${idPatient}-${idRun}\"\n\n    input:\n        set idPatient, idSample, idRun, file(inputFile1), file(inputFile2) from inputPairReads\n        file(bwaIndex) from ch_bwa\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set idPatient, idSample, idRun, file(\"${idSample}_${idRun}.bam\") into bamMapped\n        set idPatient, val(\"${idSample}_${idRun}\"), file(\"${idSample}_${idRun}.bam\") into bamMappedBamQC\n\n    when: !(params.sentieon)\n\n    script:\n                                                                                   \n                                                           \n                                                                                \n                                                                                          \n                                                                                                                                                                               \n    CN = params.sequencing_center ? \"CN:${params.sequencing_center}\\\\t\" : \"\"\n    readGroup = \"@RG\\\\tID:${idRun}\\\\t${CN}PU:${idRun}\\\\tSM:${idSample}\\\\tLB:${idSample}\\\\tPL:illumina\"\n                                                \n    status = statusMap[idPatient, idSample]\n    extra = status == 1 ? \"-B 3\" : \"\"\n    convertToFastq = hasExtension(inputFile1, \"bam\") ? \"gatk --java-options -Xmx${task.memory.toGiga()}g SamToFastq --INPUT=${inputFile1} --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \\\\\" : \"\"\n    input = hasExtension(inputFile1, \"bam\") ? \"-p /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2)\" : \"${inputFile1} ${inputFile2}\"\n    aligner = params.aligner == \"bwa-mem2\" ? \"bwa-mem2\" : \"bwa\"\n    \"\"\"\n    ${convertToFastq}\n    ${aligner} mem -K 100000000 -R \\\"${readGroup}\\\" ${extra} -t ${task.cpus} -M ${fasta} \\\n    ${input} | \\\n    samtools sort --threads ${task.cpus} -m 2G - > ${idSample}_${idRun}.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MapReads"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess MergeBamMapped {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    input:\n        set idPatient, idSample, idRun, file(bam) from multipleBam\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.bam\") into bam_mapped_merged\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MergeBamMapped"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess MarkDuplicates {\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.bam.metrics\") \"Reports/${idSample}/MarkDuplicates/${it}\"\n            else \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(\"${idSample}.bam\") from bam_mapped_merged\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.md.bam\"), file(\"${idSample}.md.bam.bai\") into bam_duplicates_marked\n        set idPatient, idSample into tsv_bam_duplicates_marked\n        file (\"${idSample}.bam.metrics\") optional true into duplicates_marked_report\n\n    when: !(params.skip_markduplicates)\n\n    script:\n    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : \"\\\"-Xms\" +  (task.memory.toGiga() / 2).trunc() + \"g -Xmx\" + (task.memory.toGiga() - 1) + \"g\\\"\"\n    metrics = 'markduplicates' in skipQC ? '' : \"-M ${idSample}.bam.metrics\"\n    if (params.use_gatk_spark)\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicatesSpark \\\n        -I ${idSample}.bam \\\n        -O ${idSample}.md.bam \\\n        ${metrics} \\\n        --tmp-dir . \\\n        --create-output-bam-index true \\\n        --spark-master local[${task.cpus}]\n    \"\"\"\n    else\n    \"\"\"\n    gatk --java-options ${markdup_java_options} \\\n        MarkDuplicates \\\n        --INPUT ${idSample}.bam \\\n        --METRICS_FILE ${idSample}.bam.metrics \\\n        --TMP_DIR . \\\n        --ASSUME_SORT_ORDER coordinate \\\n        --CREATE_INDEX true \\\n        --OUTPUT ${idSample}.md.bam\n    \n    mv ${idSample}.md.bai ${idSample}.md.bam.bai\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MarkDuplicates"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess GatherBQSRReports {\n    label 'memory_singleCPU_2_task'\n    label 'cpus_2'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir params.outdir, mode: params.publish_dir_mode,\n        saveAs: {\n            if (it == \"${idSample}.recal.table\" && !params.skip_markduplicates) \"Preprocessing/${idSample}/DuplicatesMarked/${it}\"\n            else \"Preprocessing/${idSample}/Mapped/${it}\"\n        }\n\n    input:\n        set idPatient, idSample, file(recal) from tableGatherBQSRReports\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.table\") into recalTable\n        file(\"${idSample}.recal.table\") into baseRecalibratorReport\n        set idPatient, idSample into recalTableTSV\n\n    when: !(params.no_intervals)\n\n    script:\n    input = recal.collect{\"-I ${it}\"}.join(' ')\n    \"\"\"\n    gatk --java-options -Xmx${task.memory.toGiga()}g \\\n        GatherBQSRReports \\\n        ${input} \\\n        -O ${idSample}.recal.table \\\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/GatherBQSRReports"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess MergeBamRecal {\n    label 'cpus_8'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Preprocessing/${idSample}/Recalibrated\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bam_recalibrated_to_merge\n\n    output:\n        set idPatient, idSample, file(\"${idSample}.recal.bam\"), file(\"${idSample}.recal.bam.bai\") into bam_recalibrated\n        set idPatient, idSample, file(\"${idSample}.recal.bam\") into bam_recalibrated_qc\n        set idPatient, idSample into tsv_bam_recalibrated\n\n    when: !(params.no_intervals)\n\n    script:\n    \"\"\"\n    samtools merge --threads ${task.cpus} ${idSample}.recal.bam ${bam}\n    samtools index ${idSample}.recal.bam\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MergeBamRecal"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess BamQC {\n    label 'memory_max'\n    label 'cpus_16'\n\n    tag \"${idPatient}-${idSample}\"\n\n    publishDir \"${params.outdir}/Reports/${idSample}/bamQC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSample, file(bam) from bamBamQC\n        file(targetBED) from ch_target_bed\n\n    output:\n        file(\"${bam.baseName}\") into bamQCReport\n\n    when: !('bamqc' in skipQC)\n\n    script:\n    use_bed = params.target_bed ? \"-gff ${targetBED}\" : ''\n    \"\"\"\n    qualimap --java-mem-size=${task.memory.toGiga()}G \\\n        bamqc \\\n        -bam ${bam} \\\n        --paint-chromosome-limits \\\n        --genome-gc-distr HUMAN \\\n        $use_bed \\\n        -nt ${task.cpus} \\\n        -skip-duplicated \\\n        --skip-dup-mode 0 \\\n        -outdir ${bam.baseName} \\\n        -outformat HTML\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/BamQC"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess HaplotypeCaller {\n    label 'memory_singleCPU_task_sq'\n    label 'cpus_2'\n\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamHaplotypeCaller\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(dict) from ch_dict\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n\n    output:\n        set val(\"HaplotypeCallerGVCF\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfHaplotypeCaller\n        set idPatient, idSample, file(intervalBed), file(\"${intervalBed.baseName}_${idSample}.g.vcf\") into gvcfGenotypeGVCFs\n\n    when: 'haplotypecaller' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-L ${intervalBed}\"\n    dbsnpOptions = params.dbsnp ? \"--D ${dbsnp}\" : \"\"\n    \"\"\"\n    gatk --java-options \"-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10\" \\\n        HaplotypeCaller \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        ${intervalsOptions} \\\n        ${dbsnpOptions} \\\n        -O ${intervalBed.baseName}_${idSample}.g.vcf \\\n        -ERC GVCF\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/HaplotypeCaller"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["FreeBayes"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess FreebayesSingle {\n    tag \"${idSample}-${intervalBed.baseName}\"\n\n    label 'cpus_1'\n    \n    input:\n        set idPatient, idSample, file(bam), file(bai), file(intervalBed) from bamFreebayesSingle\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_software_versions_yaml\n    \n    output:\n        set val(\"FreeBayes\"), idPatient, idSample, file(\"${intervalBed.baseName}_${idSample}.vcf\") into vcfFreebayesSingle\n    \n    when: 'freebayes' in tools\n\n    script:\n    intervalsOptions = params.no_intervals ? \"\" : \"-t ${intervalBed}\"\n    \"\"\"\n    freebayes \\\n        -f ${fasta} \\\n        --min-alternate-fraction 0.1 \\\n        --min-mapping-quality 1 \\\n        ${intervalsOptions} \\\n        ${bam} > ${intervalBed.baseName}_${idSample}.vcf\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/FreebayesSingle"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["FREEC"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess ControlFREECSingle {\n    label 'cpus_8'\n\n    tag \"${idSampleTumor}\"\n\n    publishDir \"${params.outdir}/VariantCalling/${idSampleTumor}/Control-FREEC\", mode: params.publish_dir_mode\n\n    input:\n        set idPatient, idSampleTumor, file(mpileupTumor) from mpileupOutSingle\n        file(chrDir) from ch_chr_dir\n        file(mappability) from ch_mappability\n        file(chrLength) from ch_chr_length\n        file(dbsnp) from ch_dbsnp\n        file(dbsnpIndex) from ch_dbsnp_tbi\n        file(fasta) from ch_fasta\n        file(fastaFai) from ch_fai\n        file(targetBED) from ch_target_bed\n\n    output:\n        set idPatient, idSampleTumor, file(\"${idSampleTumor}.pileup_CNVs\"), file(\"${idSampleTumor}.pileup_ratio.txt\"), file(\"${idSampleTumor}.pileup_BAF.txt\") into controlFreecVizSingle\n        set file(\"*.pileup*\"), file(\"${idSampleTumor}.config.txt\") into controlFreecOutSingle\n\n    when: 'controlfreec' in tools\n\n    script:\n    config = \"${idSampleTumor}.config.txt\"\n    gender = genderMap[idPatient]\n                                                                           \n    window = params.cf_window ? \"window = ${params.cf_window}\" : \"\"\n    coeffvar = params.cf_coeff ? \"coefficientOfVariation = ${params.cf_coeff}\" : \"\"\n    use_bed = params.target_bed ? \"captureRegions = ${targetBED}\" : \"\"\n                                                                                              \n                                                                                      \n                                                    \n    min_subclone = 100\n    readCountThreshold = params.target_bed ? \"50\" : \"10\"\n    breakPointThreshold = params.target_bed ? \"1.2\" : \"0.8\"\n    breakPointType = params.target_bed ? \"4\" : \"2\"\n    mappabilitystr = params.mappability ? \"gemMappabilityFile = \\${PWD}/${mappability}\" : \"\"\n\n    \"\"\"\n    touch ${config}\n    echo \"[general]\" >> ${config}\n    echo \"BedGraphOutput = TRUE\" >> ${config}\n    echo \"chrFiles = \\${PWD}/${chrDir.fileName}\" >> ${config}\n    echo \"chrLenFile = \\${PWD}/${chrLength.fileName}\" >> ${config}\n    echo \"forceGCcontentNormalization = 1\" >> ${config}\n    echo \"maxThreads = ${task.cpus}\" >> ${config}\n    echo \"minimalSubclonePresence = ${min_subclone}\" >> ${config}\n    echo \"ploidy = ${params.cf_ploidy}\" >> ${config}\n    echo \"sex = ${gender}\" >> ${config}\n    echo \"readCountThreshold = ${readCountThreshold}\" >> ${config}\n    echo \"breakPointThreshold = ${breakPointThreshold}\" >> ${config}\n    echo \"breakPointType = ${breakPointType}\" >> ${config}\n    echo \"${window}\" >> ${config}\n    echo \"${coeffvar}\" >> ${config}\n    echo \"${mappabilitystr}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[sample]\" >> ${config}\n    echo \"inputFormat = pileup\" >> ${config}\n    echo \"mateFile = \\${PWD}/${mpileupTumor}\" >> ${config}\n    echo \"mateOrientation = FR\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[BAF]\" >> ${config}\n    echo \"SNPfile = ${dbsnp.fileName}\" >> ${config}\n    echo \"\" >> ${config}\n\n    echo \"[target]\" >> ${config}\n    echo \"${use_bed}\" >> ${config}\n\n    freec -conf ${config}\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/ControlFREECSingle"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["sripaladugu"], "nb_wf": 1, "list_wf": ["germline_somatic"], "list_contrib": ["nf-core-bot", "chelauk", "jfnavarro", "arontommi", "maxulysse", "ggabernet", "lescai", "apeltzer", "lconde-ucl", "malinlarsson", "pcantalupo", "alneberg", "szilvajuhos", "olgabot", "FriederikeHanssen", "skrakau", "davidmasp", "adrlar"], "nb_contrib": 18, "codes": ["\nprocess MultiQC {\n    publishDir \"${params.outdir}/Reports/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n        file (multiqcConfig) from ch_multiqc_config\n        file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n        file (versions) from ch_software_versions_yaml.collect()\n        file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n        file ('bamQC/*') from bamQCReport.collect().ifEmpty([])\n        file ('BCFToolsStats/*') from bcftoolsReport.collect().ifEmpty([])\n        file ('FastQC/*') from fastQCReport.collect().ifEmpty([])\n        file ('TrimmedFastQC/*') from trimGaloreReport.collect().ifEmpty([])\n        file ('MarkDuplicates/*') from duplicates_marked_report.collect().ifEmpty([])\n        file ('DuplicatesMarked/*.recal.table') from baseRecalibratorReport.collect().ifEmpty([])\n        file ('SamToolsStats/*') from samtoolsStatsReport.collect().ifEmpty([])\n        file ('snpEff/*') from snpeffReport.collect().ifEmpty([])\n        file ('VCFTools/*') from vcftoolsReport.collect().ifEmpty([])\n\n    output:\n        file \"*multiqc_report.html\" into ch_multiqc_report\n        file \"*_data\"\n        file \"multiqc_plots\"\n\n    when: !('multiqc' in skipQC)\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f ${rtitle} ${rfilename} ${custom_config_file} .\n    \"\"\"\n}"], "list_proc": ["sripaladugu/germline_somatic/MultiQC"], "list_wf_names": ["sripaladugu/germline_somatic"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["ssun1116"], "nb_wf": 1, "list_wf": ["meripseqpipe"], "list_contrib": ["kingzhuky", "ssun1116", "juneb4869"], "nb_contrib": 3, "codes": ["\nprocess makechromesize {\n    label 'build_index'\n    tag \"gtf2bed12\"\n    publishDir path: { params.saveReference ? \"${params.outdir}/Genome/reference_genome\" : params.outdir },\n                saveAs: { params.saveReference ? it : null }, mode: 'copy'\n\n    when:\n    true\n\n    input:\n    file fasta\n    \n    output:\n    file \"chromsizes.file\" into chromsizesfile\n\n    shell:      \n    \"\"\"\n    samtools faidx ${fasta}\n    cut -f1,2 ${fasta}.fai > chromsizes.file\n    \"\"\"\n}"], "list_proc": ["ssun1116/meripseqpipe/makechromesize"], "list_wf_names": ["ssun1116/meripseqpipe"]}, {"nb_reuse": 8, "tools": ["segmentSeq", "pysradb", "MUSCLE", "MultiQC", "SCALCE", "FastQC", "SyConn", "STAR"], "nb_own": 8, "list_own": ["taltechnlp", "ssun1116", "supark87", "vpeddu", "steepale", "yqshao", "vib-singlecell-nf", "stevekm"], "nb_wf": 8, "list_wf": ["meripseqpipe", "prac_nextflow", "tips", "nextflow-demos", "wgsfastqtobam", "vsn-pipelines", "est-asr-pipeline", "ev-meta"], "list_contrib": ["dependabot[bot]", "ssun1116", "KrisDavie", "kingzhuky", "supark87", "pditommaso", "ghuls", "dweemx", "vpeddu", "steepale", "yqshao", "aivo0", "cflerin", "stevekm", "juneb4869"], "nb_contrib": 15, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n            if (filename.indexOf(\".csv\") > 0) filename\n            else null\n        }\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    python ${baseDir}/bin/scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess Host_depletion { \npublishDir \"${params.OUTPUT}/Star_PE/${base}\", mode: 'symlink', overwrite: true\ncontainer \"quay.io/biocontainers/star:2.7.9a--h9ee0642_0\"\nbeforeScript 'chmod o+rw .'\ncpus 8\ninput: \n    tuple val(base), file(r1), file(r2)\n    file starindex\noutput: \n    file \"${base}.star*\"\n    file \"${base}.starAligned.out.bam\"\n    tuple val(\"${base}\"), file(\"${base}.starUnmapped.out.mate1.fastq.gz\"), file(\"${base}.starUnmapped.out.mate2.fastq.gz\")\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \nSTAR   \\\n    --runThreadN ${task.cpus}  \\\n    --genomeDir ${starindex}   \\\n    --readFilesIn ${r1} ${r2} \\\n    --readFilesCommand zcat      \\\n    --outSAMtype BAM Unsorted \\\n    --outReadsUnmapped Fastx \\\n    --outFileNamePrefix ${base}.star  \n\nmv ${base}.starUnmapped.out.mate1 ${base}.starUnmapped.out.mate1.fastq\nmv ${base}.starUnmapped.out.mate2 ${base}.starUnmapped.out.mate2.fastq\n\ngzip ${base}.starUnmapped.out.mate1.fastq\ngzip ${base}.starUnmapped.out.mate2.fastq\n\"\"\"\n}", "\nprocess GET_SRA_DB {\n\n    container params.utils.container\n    publishDir \"${processParams.sraDbOutDir}\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    output:\n        file(\"SRAmetadb.sqlite\")\n\n    script:\n        \"\"\"\n        pysradb metadb \\\n            --out-dir \".\"\n        \"\"\"\n\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess alginment{\n    container 'my-image-spades'\n\n    tag \"$sample_id\"\n    publishDir \"${params.outdir}/muscle_output\",mode:'copy'\n\n    input:\n    file('*') from sequences.collect()\n    val sample_id_db from seq_name\n\n    output:\n    file(\"cpmp.align.fasta\") into (alignments_ch, call_genotype_ch)\n\n    script:\n    \"\"\"\n    cat * > cpmp_all.fasta\n    muscle -in cpmp_all.fasta -out cpmp.align.fasta\n    \"\"\"\n\n }", "\nprocess concat_dbs {\n    publishDir \"${params.output_dir}\", overwrite: true\n\n    input:\n    file(all_dbs: \"db?\") from sample_dbs.collect()\n\n    output:\n    file(\"${output_sqlite}\") into plots_db\n\n    script:\n    output_sqlite = \"plots.sqlite\"\n    \"\"\"\n    python - ${all_dbs} <<E0F\nimport sys\nimport sqlite3\n\ndbs = sys.argv[1:]\n\n# setup new output db\noutput_db = \"${output_sqlite}\"\nconn = sqlite3.connect(output_db)\nconn.execute(\"CREATE TABLE TBL1(sampleID text, x text, y text, a text, baseplot blob, ggplot blob)\")\n\n# add each input db to the output db\nfor db in dbs:\n    conn.execute('''ATTACH '{0}' as dba'''.format(db))\n    conn.execute(\"BEGIN\")\n    for row in conn.execute('''SELECT * FROM dba.sqlite_master WHERE type=\"table\"'''):\n        combine_sql = \"INSERT INTO \"+ row[1] + \" SELECT * FROM dba.\" + row[1]\n        conn.execute(combine_sql)\n    conn.commit()\n    conn.execute(\"detach database dba\")\nE0F\n    \"\"\"\n}", "\nprocess pinnLabel {\n    label 'pinn'\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz')\n\n    script:\n    setup = getParams(labelDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env python3\n    import pinn, yaml, os\n    import tensorflow as tf\n    from ase import units\n    from ase.io import read, write\n    os.symlink(\"${file(setup.inp)}\", \"model\")\n    calc = pinn.get_calc(\"model\")\n    traj = read(\"${file(setup.ds)}\", index=':')\n    with open('output.xyz', 'w') as f:\n        for atoms in traj:\n            atoms.wrap()\n            atoms.set_calculator(calc)\n            atoms.get_potential_energy()\n            write(f, atoms, format='extxyz', append='True')\n    \"\"\"\n\n    stub:\n    setup = getParams(labelDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}", "\nprocess language_id {\n    input:\n      path init_datadir\n      file audio    \n    \n    output:\n      path 'datadir' into datadir\n    \n    \n    shell:\n    if ( params.do_language_id == \"yes\" )\n      '''\n      . !{projectDir}/bin/prepare_process.sh\n      \n      extract_lid_features_kaldi.py !{init_datadir} .\n      cat !{init_datadir}/segments | awk '{print($1, \"0\")}' > trials\n      threshold=`cat !{params.rootdir}/models/lid_et/threshold`\n      ivector-subtract-global-mean !{params.rootdir}/models/lid_et/xvector.global.vec scp:xvector.scp ark:- | \\\n      ivector-normalize-length --scaleup=false ark:- ark:- | \\\n      logistic-regression-eval --apply-log=true --max-steps=20 --mix-up=0 \\\n        !{params.rootdir}/models/lid_et/lr.scale.model \\\n        ark:trials ark:- - | \\\n        awk '{print($1, $3 > '$threshold' ? \"et\" : \"other\")}' > utt2lang\n\n      mkdir datadir\n      \n      grep \"et$\" utt2lang | sort | \\\n        join <(sort !{init_datadir}/segments) - | \\\n        awk '{print($1, $2, $3, $4)}' | LC_ALL=C sort > datadir/segments\n      \n      cp -r !{init_datadir}/{wav.scp,utt2spk,spk2utt} datadir\n      utils/fix_data_dir.sh datadir\n      \n      '''      \n    else\n      '''\n      mkdir datadir\n      cp -r !{init_datadir}/{wav.scp,segments,utt2spk,spk2utt} datadir\n      '''\n}"], "list_proc": ["ssun1116/meripseqpipe/get_software_versions", "vpeddu/ev-meta/Host_depletion", "vib-singlecell-nf/vsn-pipelines/GET_SRA_DB", "steepale/wgsfastqtobam/multiqc", "supark87/prac_nextflow/alginment", "stevekm/nextflow-demos/concat_dbs", "yqshao/tips/pinnLabel", "taltechnlp/est-asr-pipeline/language_id"], "list_wf_names": ["ssun1116/meripseqpipe", "vpeddu/ev-meta", "steepale/wgsfastqtobam", "taltechnlp/est-asr-pipeline", "yqshao/tips", "vib-singlecell-nf/vsn-pipelines", "supark87/prac_nextflow", "stevekm/nextflow-demos"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["stevekm"], "nb_wf": 1, "list_wf": ["MuTect2_target_chunking"], "list_contrib": ["stevekm"], "nb_contrib": 1, "codes": ["\nprocess mutect2_lineChunk {\n    tag \"${prefix}\"\n    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n    echo true\n\n    input:\n    set val(chunkLabel), val(targetChunkNum), val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(\"targets.bed\"), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_lineChunk_ch\n\n    output:\n    set val(chunkLabel), val(targetChunkNum), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_lineChunk\n    file(\"${output_vcf}\")\n    file(\"${multiallelics_stats}\")\n    file(\"${realign_stats}\")\n\n    when: params.disable != \"true\"\n\n    script:\n                                           \n    prefix = \"${comparisonID}.${chunkLabel}.${targetChunkNum}\"\n    output_vcf = \"${prefix}.vcf\"\n    output_norm_vcf = \"${prefix}.norm.vcf\"\n    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n    \"\"\"\n    gatk.sh -T MuTect2 \\\n    -dt NONE \\\n    --logging_level WARN \\\n    --standard_min_confidence_threshold_for_calling 30 \\\n    --max_alt_alleles_in_normal_count 10 \\\n    --max_alt_allele_in_normal_fraction 0.05 \\\n    --max_alt_alleles_in_normal_qscore_sum 40 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --cosmic \"${cosmic_ref_vcf}\" \\\n    --intervals \"targets.bed\" \\\n    --interval_padding 10 \\\n    --input_file:tumor \"${tumorBam}\" \\\n    --input_file:normal \"${normalBam}\" \\\n    --out \"${output_vcf}\"\n\n    # normalize and split vcf entries\n    cat ${output_vcf} | \\\n    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n    \"${output_norm_vcf}\"\n    \"\"\"\n}"], "list_proc": ["stevekm/MuTect2_target_chunking/mutect2_lineChunk"], "list_wf_names": ["stevekm/MuTect2_target_chunking"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["stevekm"], "nb_wf": 1, "list_wf": ["nextflow-demos"], "list_contrib": ["stevekm", "pditommaso"], "nb_contrib": 2, "codes": ["\nprocess fastqc {\n    tag \"${fastq}\"\n    publishDir \"${params.output_dir}/fastqc\", mode: 'copy', overwrite: true\n    echo true\n\n    input:\n    file(fastq) from input_fastqs\n\n    output:\n    file(output_html)\n    file(output_zip)\n\n    script:\n    output_html = \"${fastq}\".replaceFirst(/.fastq.gz$/, \"_fastqc.html\")\n    output_zip = \"${fastq}\".replaceFirst(/.fastq.gz$/, \"_fastqc.zip\")\n    \"\"\"\n    which fastqc\n    fastqc -o . \"${fastq}\"\n    \"\"\"\n}"], "list_proc": ["stevekm/nextflow-demos/fastqc"], "list_wf_names": ["stevekm/nextflow-demos"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["stevekm"], "nb_wf": 1, "list_wf": ["nextflow-pipeline-demo"], "list_contrib": ["stevekm"], "nb_contrib": 1, "codes": ["\nprocess fastqc_raw {\n    tag { \"${fastq}\" }\n    module \"fastqc/0.11.7\"\n    publishDir \"${params.output_dir}/fastqc-raw\", mode: 'copy', overwrite: true\n\n    input:\n    file(fastq) from samples_each_fastq\n\n    output:\n    file(output_html)\n    file(output_zip)\n\n    script:\n    output_html = \"${fastq}\".replaceFirst(/.fastq.gz$/, \"_fastqc.html\")\n    output_zip = \"${fastq}\".replaceFirst(/.fastq.gz$/, \"_fastqc.zip\")\n    \"\"\"\n    echo \"output_zip: ${output_zip}, output_html: ${output_html}\"\n    fastqc -o . \"${fastq}\"\n    \"\"\"\n\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/fastqc_raw"], "list_wf_names": ["stevekm/nextflow-pipeline-demo"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["stevekm"], "nb_wf": 1, "list_wf": ["nextflow-pipeline-demo"], "list_contrib": ["stevekm"], "nb_contrib": 1, "codes": ["\nprocess fastqc_trim {\n    tag { \"${sample_ID}\" }\n    module \"fastqc/0.11.7\"\n    publishDir \"${params.output_dir}/fastqc-trim\", mode: 'copy', overwrite: true\n\n    input:\n    set val(sample_ID),  file(fastq_R1_trim), file(fastq_R2_trim) from samples_fastq_trimmed2\n\n    output:\n    file(output_R1_html)\n    file(output_R1_zip)\n    file(output_R2_html)\n    file(output_R2_zip)\n\n    script:\n    output_R1_html = \"${fastq_R1_trim}\".replaceFirst(/.fastq.gz$/, \"_fastqc.html\")\n    output_R1_zip = \"${fastq_R1_trim}\".replaceFirst(/.fastq.gz$/, \"_fastqc.zip\")\n    output_R2_html = \"${fastq_R2_trim}\".replaceFirst(/.fastq.gz$/, \"_fastqc.html\")\n    output_R2_zip = \"${fastq_R2_trim}\".replaceFirst(/.fastq.gz$/, \"_fastqc.zip\")\n    \"\"\"\n    fastqc -o . \"${fastq_R1_trim}\"\n    fastqc -o . \"${fastq_R2_trim}\"\n    \"\"\"\n\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/fastqc_trim"], "list_wf_names": ["stevekm/nextflow-pipeline-demo"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["stevekm"], "nb_wf": 1, "list_wf": ["nextflow-pipeline-demo"], "list_contrib": ["stevekm"], "nb_contrib": 1, "codes": ["\nprocess gatk_hc {\n    tag { \"${sample_ID}\" }\n    publishDir \"${params.output_dir}/vcf_hc\", mode: 'copy', overwrite: true\n    beforeScript \"${params.beforeScript_str}\"\n    afterScript \"${params.afterScript_str}\"\n    clusterOptions '-pe threaded 4-16 -l mem_free=40G -l mem_token=4G'\n    module 'samtools/1.3'\n\n    input:\n    set val(sample_ID), file(sample_bam), file(sample_bai), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed_file), file(dbsnp_ref_vcf) from samples_dd_ra_rc_bam_ref_dbsnp2\n\n    output:\n    file(\"${sample_ID}.vcf\")\n    set val(sample_ID), file(\"${sample_ID}.norm.vcf\") into sample_vcf_hc\n    file(\"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\") into gatk_hc_annotations\n    set val(sample_ID), file(\"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\") into gatk_hc_annotations2\n    file(\"${sample_ID}.eval.grp\")\n    val(sample_ID) into sample_gatk_hc_done\n\n    script:\n    \"\"\"\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T HaplotypeCaller \\\n    -dt NONE \\\n    --logging_level ERROR \\\n    -nct \\${NSLOTS:-1} \\\n    --max_alternate_alleles 3 \\\n    --standard_min_confidence_threshold_for_calling 50 \\\n    --reference_sequence \"${ref_fasta}\" \\\n    --intervals \"${targets_bed_file}\" \\\n    --interval_padding 10 \\\n    --input_file \"${sample_bam}\" \\\n    --out \"${sample_ID}.vcf\"\n\n    cat \"${sample_ID}.vcf\" | \\\n    bcftools norm \\\n    --multiallelics \\\n    -both \\\n    --output-type v - | \\\n    bcftools norm \\\n    --fasta-ref \"${ref_fasta}\" \\\n    --output-type v - | \\\n    bcftools view \\\n    --exclude 'DP<5' \\\n    --output-type v > \"${sample_ID}.norm.vcf\"\n\n    # annotate the vcf\n    annotate_vcf.sh \"${sample_ID}.norm.vcf\" \"${sample_ID}.norm\"\n\n    # add a column with the sample ID\n    paste_col.py -i \"${sample_ID}.norm.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\" -o \"${sample_ID}.norm.sample.${params.ANNOVAR_BUILD_VERSION}_multianno.txt\" --header \"Sample\" -v \"${sample_ID}\" -d \"\\t\"\n\n    java -Xms16G -Xmx16G -jar \"${params.gatk_bin}\" -T VariantEval \\\n    -R \"${ref_fasta}\" \\\n    -o \"${sample_ID}.eval.grp\" \\\n    --dbsnp \"${dbsnp_ref_vcf}\" \\\n    --eval \"${sample_ID}.norm.vcf\"\n    \"\"\"\n}"], "list_proc": ["stevekm/nextflow-pipeline-demo/gatk_hc"], "list_wf_names": ["stevekm/nextflow-pipeline-demo"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["supark87"], "nb_wf": 1, "list_wf": ["prac_nextflow"], "list_contrib": ["supark87"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n       \n    input:\n    path '*' from quant_ch.mix(fastqc_ch).collect() \n\n    output:\n    'multiqc_report.html'\n     \n    script:\n    \"\"\"\n\n    multiqc . \n    \"\"\"\n}"], "list_proc": ["supark87/prac_nextflow/multiqc"], "list_wf_names": ["supark87/prac_nextflow"]}, {"nb_reuse": 2, "tools": ["STAR", "MultiQC"], "nb_own": 2, "list_own": ["suzannejin", "vib-singlecell-nf"], "nb_wf": 2, "list_wf": ["vsn-pipelines", "nf-proportionality"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin", "suzannejin"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = ''\n    rfilename = ''\n    if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {\n        rtitle = \"--title \\\"${workflow.runName}\\\"\"\n        rfilename = \"--filename \" + workflow.runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\"\n    }\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}", "\nprocess SC__STAR__BUILD_INDEX {\n\n    container params.tools.star.container\n    label 'compute_resources__star_build_genome'\n\n    input:\n        file(annotation)\n        file(genome)\n\n    output:\n        file(\"STAR_index\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.star.build_genome)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        mkdir STAR_index\n        STAR \\\n            --runThreadN ${task.cpus} \\\n            --runMode genomeGenerate \\\n            --genomeDir STAR_index \\\n            --genomeFastaFiles ${genome} \\\n            --sjdbGTFfile ${annotation} \\\n            --sjdbOverhang ${processParams.sjdbOverhang} \\\n            --genomeSAindexNbases ${processParams.genomeSAindexNbases} # Suggested by STAR (default: 14), otherwise keeps on hanging\n        \"\"\"\n\n}"], "list_proc": ["suzannejin/nf-proportionality/multiqc", "vib-singlecell-nf/vsn-pipelines/SC__STAR__BUILD_INDEX"], "list_wf_names": ["suzannejin/nf-proportionality", "vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["FastQC", "MultiQC"], "nb_own": 1, "list_own": ["t-neumann"], "nb_wf": 1, "list_wf": ["slamseq"], "list_contrib": ["drpatelh", "maxulysse", "t-neumann"], "nb_contrib": 3, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    trim_galore --version > v_trimgalore.txt\n    slamdunk --version > v_slamdunk.txt\n    echo \\$(R --version 2>&1) > v_R.txt\n    R -e 'packageVersion(\"DESeq2\")' | grep \"\\\\[1\\\\]\" > v_DESeq2.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["t-neumann/slamseq/get_software_versions"], "list_wf_names": ["t-neumann/slamseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["t-neumann"], "nb_wf": 1, "list_wf": ["slamseq"], "list_contrib": ["drpatelh", "maxulysse", "t-neumann"], "nb_contrib": 3, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n    file(\"rates/*\") from alleyoopRatesOut.collect().ifEmpty([])\n    file(\"utrrates/*\") from alleyoopUtrRatesOut.collect().ifEmpty([])\n    file(\"tcperreadpos/*\") from alleyoopTcPerReadPosOut.collect().ifEmpty([])\n    file(\"tcperutrpos/*\") from alleyoopTcPerUtrPosOut.collect().ifEmpty([])\n    file(summary) from summaryQC\n    file (\"TrimGalore/*\") from trimgaloreQC.collect().ifEmpty([])\n    file (\"TrimGalore/*\") from trimgaloreFastQC.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    \"\"\"\n    multiqc -m fastqc -m cutadapt -m slamdunk -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}"], "list_proc": ["t-neumann/slamseq/multiqc"], "list_wf_names": ["t-neumann/slamseq"]}, {"nb_reuse": 3, "tools": ["SAMtools", "Vireo", "MultiQC"], "nb_own": 3, "list_own": ["wtsi-hgi", "xiaoli-dong", "tamara-hodgetts"], "nb_wf": 3, "list_wf": ["nf_scrna_deconvolution", "nf-atac-seq", "magph"], "list_contrib": ["tamara-hodgetts", "xiaoli-dong", "wtsi-mercury", "gn5"], "nb_contrib": 4, "codes": ["\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}", "process vireo_with_genotype {\n    tag \"${samplename}.${donors_gt_vcf}\"\n\n    publishDir \"${params.outdir}/vireo_gt/${samplename}/\", mode: \"${params.vireo.copy_mode}\", overwrite: true,\n\t  saveAs: {filename -> filename.replaceFirst(\"vireo_${samplename}/\",\"\") }\n    \n    when: \n      params.run_with_genotype_input\n    \n    input:\n      tuple val(samplename), path(cell_data), path(donors_gt_vcf)\n    \n    output:\n      tuple val(samplename), path(\"vireo_${samplename}/*\"), emit: output_dir\n      tuple val(samplename), path(\"vireo_${samplename}/donor_ids.tsv\"), emit: sample_donor_ids \n      path(\"vireo_${samplename}/${samplename}.sample_summary.txt\"), emit: sample_summary_tsv\n      path(\"vireo_${samplename}/${samplename}__exp.sample_summary.txt\"), emit: sample__exp_summary_tsv\n\n    script:\n    \"\"\"\n\n      umask 2 # make files group_writable\n\n      vireo -c $cell_data -o vireo_${samplename} -d ${donors_gt_vcf} -t GT\n\n      # add samplename to summary.tsv,\n      # to then have Nextflow concat summary.tsv of all samples into a single file:\n\n      cat vireo_${samplename}/summary.tsv | \\\\\n        tail -n +2 | \\\\\n        sed s\\\"/^/${samplename}\\\\t/\\\"g > vireo_${samplename}/${samplename}.sample_summary.txt\n\n      cat vireo_${samplename}/summary.tsv | \\\\\n        tail -n +2 | \\\\\n        sed s\\\"/^/${samplename}__/\\\"g > vireo_${samplename}/${samplename}__exp.sample_summary.txt\n\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/SAMTOOLS_VIEW", "tamara-hodgetts/nf-atac-seq/MULTIQC", "wtsi-hgi/nf_scrna_deconvolution/vireo_with_genotype"], "list_wf_names": ["tamara-hodgetts/nf-atac-seq", "xiaoli-dong/magph", "wtsi-hgi/nf_scrna_deconvolution"]}, {"nb_reuse": 1, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["tamara-hodgetts"], "nb_wf": 1, "list_wf": ["nf-atac-seq"], "list_contrib": ["tamara-hodgetts"], "nb_contrib": 1, "codes": ["\nprocess BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path('*.bed'), emit: bed\n    path  '*.version.txt'         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $bed \\\\\n        $options.args \\\\\n        > ${prefix}.bed \\\\\n    | sort -k1,1 -k2,2n - > sorted.bedGraph\n\n    bedtools --version | sed -e \"s/bedtools v//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["tamara-hodgetts/nf-atac-seq/BEDTOOLS_SORT"], "list_wf_names": ["tamara-hodgetts/nf-atac-seq"]}, {"nb_reuse": 3, "tools": ["FastQC", "shovill", "Diamond"], "nb_own": 3, "list_own": ["telatin", "tiagofilipe12", "tiwarialok"], "nb_wf": 3, "list_wf": ["pATLAS-db-creation", "nextflow-example", "new-rnaseq"], "list_contrib": ["tiagofilipe12", "tiwarialok", "pditommaso", "vmikk", "evanfloden", "manoj039", "misssoft", "telatin", "dvolk"], "nb_contrib": 9, "codes": ["\nprocess assembly {\n       \n                                                                              \n      \n    tag { sample_id }\n    \n    publishDir \"$params.outdir/assemblies/\", \n        mode: 'copy'\n    \n    input:\n    tuple val(sample_id), path(reads)  \n    \n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}.fa\")\n\n    script:\n    \"\"\"\n    shovill --R1 ${reads[0]} --R2 ${reads[1]} --outdir shovill --cpus ${task.cpus}\n    mv shovill/contigs.fa ${sample_id}.fa\n    \"\"\"\n}", "\nprocess diamond {\n\n    tag {\"running diamond\"}\n\n    input:\n    file masterFastaFile from masterFasta_diamond\n    each db from params.diamondDatabases\n\n    output:\n    file \"*.txt\" into diamondOutputs\n\n    \"\"\"\n    diamond blastx -d /ngstools/bin/bacmet/bacmet -q ${masterFastaFile} \\\n    -o ${db}.txt -e 1E-20 -p ${task.cpus} \\\n    -f 6 qseqid sseqid pident length mismatch gapopen qstart qend slen sstart send evalue bitscore\n    \"\"\"\n\n}", "\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    publishDir \"${params.outdir}/\", mode:'copy'\n\n    input:\n    set sample_id, file(reads) from read_pairs2_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n    file(\"fastqc_${sample_id}_logs/${sample_id}_1_fastqc.zip\")\n    file(\"fastqc_${sample_id}_logs/${sample_id}_1_fastqc.html\")\n    file(\"fastqc_${sample_id}_logs/${sample_id}_2_fastqc.zip\")\n    file(\"fastqc_${sample_id}_logs/${sample_id}_2_fastqc.html\")\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}"], "list_proc": ["telatin/nextflow-example/assembly", "tiagofilipe12/pATLAS-db-creation/diamond", "tiwarialok/new-rnaseq/fastqc"], "list_wf_names": ["telatin/nextflow-example", "tiagofilipe12/pATLAS-db-creation", "tiwarialok/new-rnaseq"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["telatin"], "nb_wf": 1, "list_wf": ["nextflow-example"], "list_contrib": ["telatin", "vmikk"], "nb_contrib": 2, "codes": ["\nprocess prokka {\n    tag { sample_id }\n    \n    publishDir \"$params.outdir/annotation/\", \n        mode: 'copy'\n    \n    input:\n    tuple val(sample_id), path(assembly)  \n    \n    \n    output:\n    path(\"${sample_id}\")\n\n    script:\n    \"\"\"\n    prokka --cpus ${task.cpus} --fast --outdir ${sample_id} --prefix ${sample_id} ${assembly}\n    \"\"\"\n}"], "list_proc": ["telatin/nextflow-example/prokka"], "list_wf_names": ["telatin/nextflow-example"]}, {"nb_reuse": 1, "tools": ["Unicycler"], "nb_own": 1, "list_own": ["telatin"], "nb_wf": 1, "list_wf": ["nextflow-example"], "list_contrib": ["telatin", "vmikk"], "nb_contrib": 2, "codes": ["\nprocess UNICYCLER {\n \n    tag { sample_id }\n    \n    publishDir \"$params.outdir/assemblies/\", \n        mode: 'copy'\n    \n    input:\n    tuple val(sample_id), path(reads)  \n    \n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}.fa\")\n\n    script:\n    \"\"\"\n    unicycler -1 ${reads[0]} -2 ${reads[1]} -o unicycler -t ${task.cpus} --keep 0 --min_fasta_length 200\n    mv unicycler/assembly.fasta ${sample_id}.fa\n    \"\"\"\n}"], "list_proc": ["telatin/nextflow-example/UNICYCLER"], "list_wf_names": ["telatin/nextflow-example"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["thanhleviet"], "nb_wf": 1, "list_wf": ["nf-nanopore-assembly"], "list_contrib": ["thanhleviet"], "nb_contrib": 1, "codes": ["\nprocess kraken2 {\n    publishDir \"${params.outdir}\", mode: \"copy\",\n     saveAs: {filename -> if ((filename =~ /_[1,2]\\.fastq\\.gz/) || (filename =~ /host.fastq\\.gz/) || filename.endsWith(\".report\") || filename.endsWith(\".output\"))\n                            { if (params.remove_host) {\n                              \"removed_host/${filename}\"\n                              } else {\n                                if (filename.endsWith(\".report\")) {\n                                  \"${task.process.replaceAll(\":\",\"_\")}/${filename}\"\n                                } else if (filename.endsWith(\".kraken2\")) {\n                                  \"${task.process.replaceAll(\":\",\"_\")}/${sample_id}.kraken2\"\n                                } else if (filename.endsWith(\".output\")) {\n                                  \"${task.process.replaceAll(\":\",\"_\")}/${sample_id}.uncut.kraken2\"\n                                } \n                              }\n                            }\n                          else null}\n    \n    errorStrategy 'ignore'\n                 \n\n    label 'kraken2'\n\n    tag {\"${action}-->${sample_id}\"}\n    cpus 8\n    memory '200 GB'\n\n    input:\n    tuple val(sample_id), path(reads), path(db)\n    output:\n    tuple val(sample_id), path(reads), emit: reads\n    tuple val(sample_id), path(\"*.fq.gz\"), emit: host_fastq  optional true\n    tuple val(sample_id), path(\"*.fastq.gz\"), emit: non_host_fastq optional true\n    tuple val(sample_id), path(\"${sample_id}.kraken2.report\"), emit: kraken2_report\n    tuple val(sample_id), path(\"${sample_id}.kraken2\") , emit: kraken2_output optional true\n    tuple val(sample_id), path(\"kraken2.output\"), emit: kraken2_raw_output optional true\n\n    script:\n    action = params.remove_host ? \"removing host\" : \"classifying\"\n    pe = params.single_end ? \"\" : \"--paired\"\n    classified = params.single_end ? \"${sample_id}.host.fq\" : \"${sample_id}.host#.fq\"\n    unclassified = params.single_end ? \"${sample_id}.non_host.fastq\" : \"${sample_id}#.non_host.fastq\"\n    write_fastq = params.remove_host ? \"--unclassified-out $unclassified --classified-out $classified\" : \"\"\n    compress_output = params.remove_host ? \"pigz -p $task.cpus *.fastq; pigz -p $task.cpus *.fq\" : \"\"\n    kraken2_output = (params.remove_host || params.kraken_output) ? \"/dev/null\" : \"kraken2.output\"\n    krona_output = params.remove_host ? \"\" : \"cat kraken2.output | cut -f 2,3 > ${sample_id}.kraken2\"\n    use_name = params.kraken2_use_name ? \"--use-names\" : \"\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --confidence $params.confidence \\\\\n        --threads $task.cpus \\\\\n        --memory-mapping $write_fastq \\\\\n        --report ${sample_id}.kraken2.report \\\\\n        $use_name \\\\\n        $pe \\\\\n        $reads > ${kraken2_output}\n    ${compress_output}\n    ${krona_output}\n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/kraken2"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly"]}, {"nb_reuse": 1, "tools": ["Filtlong"], "nb_own": 1, "list_own": ["thanhleviet"], "nb_wf": 1, "list_wf": ["nf-nanopore-assembly"], "list_contrib": ["thanhleviet"], "nb_contrib": 1, "codes": ["\nprocess filtlong {\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", pattern: \"*.gz\",mode: \"copy\"\n    \n    tag {sample_id}\n    \n    input:\n    tuple val(sample_id), path(reads)\n    \n    output:\n    tuple val(sample_id), path(\"${sample_id}_filtlong.fastq.gz\")\n    \n    script:\n    \"\"\"\n    filtlong --min_length $params.min_length --keep_percent 95 ${reads} | gzip > ${sample_id}_filtlong.fastq.gz \n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/filtlong"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly"]}, {"nb_reuse": 1, "tools": ["BUSCO"], "nb_own": 1, "list_own": ["thanhleviet"], "nb_wf": 1, "list_wf": ["nf-nanopore-assembly"], "list_contrib": ["thanhleviet"], "nb_contrib": 1, "codes": ["\nprocess busco {\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", mode: \"copy\",\n    saveAs: {filename -> if (filename.endsWith(\".tsv\")) { \n        \"./reports/${filename}\"\n    } else { \n        \"./${filename}\"\n        }\n    }\n    \n    container 'quay.io/biocontainers/busco:5.2.2--pyhdfd78af_0'\n\n    errorStrategy 'ignore'\n\n    tag {sample_id}\n    \n    cpus 8\n    \n    memory '16.GB'\n    \n    input:\n    tuple val(sample_id), path(contigs)\n    \n    output:\n    path(\"${sample_id}\")\n    path(\"${sample_id}_busco.tsv\"), emit: report\n    script:\n    db = params.busco_db_location ? \"--download_path ${params.busco_db_location}\" : \"\"\n    \"\"\"\n    export TMPDIR=.;\n    busco \\\n    -i ${contigs} \\\n    -o ${sample_id} \\\n    --auto-lineage-prok \\\n    -m geno \\\n    -c ${task.cpus} \\\n    $db \\\n    grep -e \"C:\" ${sample_id}/short_summary.specific* > ${sample_id}_busco\n    awk '{print \\$0=\"${sample_id}\"\\$0}' ${sample_id}_busco > ${sample_id}_busco.tsv\n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/busco"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["thanhleviet"], "nb_wf": 1, "list_wf": ["nf-nanopore-assembly"], "list_contrib": ["thanhleviet"], "nb_contrib": 1, "codes": ["\nprocess miniasm {\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", mode: \"copy\" \n    \n    tag{ sample_id }\n    \n    cpus 8\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_assembly_miniasm.fasta\"), emit: contigs\n    path(\"${sample_id}_graph_miniasm.gfa\")\n    \n    script:\n    \"\"\"\n    minimap2 -x ava-ont -t ${task.cpus} ${reads} ${reads} > ovlp.paf\n    miniasm -f ${reads} ovlp.paf > ${sample_id}_graph_miniasm.gfa\n    awk '/^S/{print \">\"\\$2\"\\\\n\"\\$3}' ${sample_id}_graph_miniasm.gfa | fold > ${sample_id}_assembly_miniasm.fasta\n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/miniasm"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly"]}, {"nb_reuse": 1, "tools": ["Unicycler"], "nb_own": 1, "list_own": ["thanhleviet"], "nb_wf": 1, "list_wf": ["nf-nanopore-assembly"], "list_contrib": ["thanhleviet"], "nb_contrib": 1, "codes": ["\nprocess unicycler {\n    publishDir \"${params.outdir}/${task.process.replaceAll(\":\",\"_\")}\", mode: \"copy\"  \n    \n    tag {sample_id}\n    \n    errorStrategy 'ignore'\n\n    cpus 12\n\n    input:\n    tuple val(sample_id), path(short_reads), path(long_reads)\n    \n    output:\n    tuple path(\"${sample_id}_unicycler.gfa\"), path(\"${sample_id}_unicycler.log\")\n    tuple val(sample_id), path(\"${sample_id}_unicycler.fasta\"), emit: contigs\n\n    script:\n    \"\"\"\n    unicycler -l ${long_reads} -1 ${short_reads[0]} -2 ${short_reads[1]} -o output -t ${task.cpus} --mode ${params.unicycler_mode}\n    mv output/assembly.fasta \"${sample_id}_unicycler.fasta\"\n    mv output/assembly.gfa \"${sample_id}_unicycler.gfa\"\n    mv output/unicycler.log \"${sample_id}_unicycler.log\"\n    \"\"\"\n}"], "list_proc": ["thanhleviet/nf-nanopore-assembly/unicycler"], "list_wf_names": ["thanhleviet/nf-nanopore-assembly"]}, {"nb_reuse": 1, "tools": ["gmap_build"], "nb_own": 1, "list_own": ["thibaultdestanque"], "nb_wf": 1, "list_wf": ["RNA_seq_Nextflow"], "list_contrib": ["thibaultdestanque"], "nb_contrib": 1, "codes": ["\nprocess Index_Genome {\n\n    time'24h'\n    cpus 1\n    queue 'sequentiel'\n    memory '30 GB'\n    echo true\n    scratch '/home1/scratch/tdestanq/'\n    conda 'bioconda::gmap=2018.07.04'\n    \n    input:\n    file genome_fa from genome_fa_file\n    params.genome_path\n    params.genomeIndex\n\n    output:\n    file \"genome.fa\" into genome\n\n    shell:\n    \"\"\"\n    gmap_build --dir=${params.genome_path} ${genome_fa} -d ${params.genomeIndex} >& /home1/scratch/tdestanq/Index_Genome.log 2>&1\n    \"\"\"\n}"], "list_proc": ["thibaultdestanque/RNA_seq_Nextflow/Index_Genome"], "list_wf_names": ["thibaultdestanque/RNA_seq_Nextflow"]}, {"nb_reuse": 1, "tools": ["htseqcount"], "nb_own": 1, "list_own": ["thibaultdestanque"], "nb_wf": 1, "list_wf": ["RNA_seq_Nextflow"], "list_contrib": ["thibaultdestanque"], "nb_contrib": 1, "codes": ["\nprocess Htseq_count {\n    publishDir params.outdir, mode: 'copy'\n\n    time'4h'\n    cpus 1\n    queue 'sequentiel'\n    memory '50 GB'\n    echo true\n    scratch '/home1/scratch/tdestanq/'\n    conda 'bioconda::htseq=0.6.1'\n    \n    input:\n    file sorted_bam from read_mapped_sort_bam\n    file GFF3_annotation from annotation_file\n\n    output:\n    file \"*_htseq_count.txt\" into htseq_count\n\n    shell:\n    \"\"\"\n    #. /appli/bioinfo/htseq-count/latest/env.sh\n    htseq-count -f \"bam\" -s \"no\" -r \"pos\" -t \"gene\" -i \"Name\" --mode \"union\" ${sorted_bam} ${GFF3_annotation} >& ${sorted_bam}_htseq_count.txt 2> /home1/scratch/tdestanq/HtseqCount.log\n    \"\"\"\n}"], "list_proc": ["thibaultdestanque/RNA_seq_Nextflow/Htseq_count"], "list_wf_names": ["thibaultdestanque/RNA_seq_Nextflow"]}, {"nb_reuse": 1, "tools": ["Bioservices"], "nb_own": 1, "list_own": ["tiagofilipe12"], "nb_wf": 1, "list_wf": ["pATLAS-db-creation"], "list_contrib": ["tiagofilipe12"], "nb_contrib": 1, "codes": ["\nprocess runMASHix {\n\n    tag {\"Running MASHix\"}\n\n    publishDir \"results/MASHix/\"\n\n    input:\n    file fastas from downloadedFastas\n    val db_name_var from IN_db_name\n    val sequencesToRemove from IN_sequences_removal\n\n    output:\n    file \"${db_name_var}/*.fas\" into (masterFasta_abricate, masterFasta_abricatepf, masterFasta_samtools, masterFasta_bowtie2, masterFasta_diamond)\n    file \"${db_name_var}/results/*.json\" into patlasJson\n    file \"*.json\" into taxaTree\n    file \"*sql\" into sqlFileMashix\n    file \"${db_name_var}/*json\" into lenghtJson\n    file \"${db_name_var}/reference_sketch/${db_name_var}_reference.msh\" into mashIndex\n    file \"${db_name_var}/*.txt\" into actualRemovedSequences\n\n    \"\"\"\n    echo \"Configuring psql and creating $db_name_var\"\n    service postgresql start\n    service postgresql status\n    sudo -u postgres createuser -w -s root\n    createdb $db_name_var\n    db_create.py $db_name_var\n    echo \"Downloading ncbi taxonomy\"\n    wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz\n    tar -xvzf taxdump.tar.gz\n    echo \"Running MASHix.py\"\n    MASHix.py -i ${fastas} -o ${db_name_var} -t ${task.cpus} -non nodes.dmp \\\n    -nan names.dmp -rm ${sequencesToRemove} -db ${db_name_var}\n    echo \"Dumping to database file\"\n    pg_dump ${db_name_var} > ${db_name_var}.sql\n    rm *.dmp *.prt *.txt *.tar.gz\n    \"\"\"\n\n}"], "list_proc": ["tiagofilipe12/pATLAS-db-creation/runMASHix"], "list_wf_names": ["tiagofilipe12/pATLAS-db-creation"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["tiagofilipe12"], "nb_wf": 1, "list_wf": ["pATLAS-db-creation"], "list_contrib": ["tiagofilipe12"], "nb_contrib": 1, "codes": ["\nprocess samtoolsIndex{\n\n    tag {\"Creating samtools index\"}\n\n    publishDir \"results/samtools_indexes/\"\n\n    input:\n    file masterFastaFile from masterFasta_samtools\n\n    output:\n    file \"*.fai\" into samtoolsIndexChannel\n\n    \"\"\"\n    echo \"Creating samtools index\"\n    samtools faidx ${masterFastaFile}\n    \"\"\"\n\n}"], "list_proc": ["tiagofilipe12/pATLAS-db-creation/samtoolsIndex"], "list_wf_names": ["tiagofilipe12/pATLAS-db-creation"]}, {"nb_reuse": 1, "tools": ["ABRicate"], "nb_own": 1, "list_own": ["tiagofilipe12"], "nb_wf": 1, "list_wf": ["pATLAS-db-creation"], "list_contrib": ["tiagofilipe12"], "nb_contrib": 1, "codes": ["\nprocess abricate_plasmidfinder_db {\n\n    tag {\"updating plasmidfinder database and running abricate\"}\n\n    input:\n    file masterFastaFile from masterFasta_abricatepf\n\n    output:\n    file \"*.tsv\" into abricateOutputsPlasmidFinder\n\n    \"\"\"\n    git clone https://bitbucket.org/genomicepidemiology/plasmidfinder_db/\n    cd plasmidfinder_db/ && cat *.fsa >> sequences\n    makeblastdb -in sequences -title tinyamr -dbtype nucl -parse_seqids -hash_index\n    cd ..\n    abricate --db plasmidfinder_db --datadir ./ ${masterFastaFile} > abr_plasmidfinder_db.tsv\n    \"\"\"\n}"], "list_proc": ["tiagofilipe12/pATLAS-db-creation/abricate_plasmidfinder_db"], "list_wf_names": ["tiagofilipe12/pATLAS-db-creation"]}, {"nb_reuse": 1, "tools": ["Bioservices"], "nb_own": 1, "list_own": ["tiagofilipe12"], "nb_wf": 1, "list_wf": ["pATLAS-db-creation"], "list_contrib": ["tiagofilipe12"], "nb_contrib": 1, "codes": ["\nprocess abricate2db {\n\n    tag {\"sending abricate to database\"}\n\n    publishDir \"results/sql_file/\"\n\n    input:\n    file abricate from abricateOutputs.collect()\n    file diamond from diamondOutputs.collect()\n    file abricatePlasmidFinder from abricateOutputsPlasmidFinder\n    file sqlFile from sqlFileMashix\n    val db_name_var from IN_db_name\n\n    output:\n    file \"*final.sql\" into FinalDbSql\n    file \"*.json\" into dropdownJsons\n\n    \"\"\"\n    echo ${abricate}\n    echo \"Configuring psql and creating $db_name_var\"\n    service postgresql start\n    service postgresql status\n    sudo -u postgres createuser -w -s root\n    createdb $db_name_var\n    psql -d ${db_name_var} -f ${db_name_var}.sql\n    echo \"Dumping into database - resistance\"\n    abricate2db.py -i abr_card.tsv abr_resfinder.tsv -db resistance \\\n    -id ${params.abricateId} -cov ${params.abricateCov} -csv ${params.cardCsv} \\\n    -db_psql ${db_name_var}\n    echo \"Dumping into database - plasmidfinder_db latest\"\n    abricate2db.py -i abr_plasmidfinder_db.tsv -db plasmidfinder \\\n    -id ${params.abricateId} -cov ${params.abricateCov} -csv ${params.cardCsv} \\\n    -db_psql ${db_name_var}\n    echo \"Dumping into database - virulence\"\n    abricate2db.py -i abr_vfdb.tsv -db virulence \\\n    -id ${params.abricateId} -cov ${params.abricateCov} -csv ${params.cardCsv} \\\n    -db_psql ${db_name_var}\n    echo \"Dumping into database - bacmet\"\n    diamond2db.py -db metal -i bacmet.txt -db_psql ${db_name_var}\n    echo \"Writing to sql file\"\n    pg_dump ${db_name_var} > ${db_name_var}_final.sql\n    \"\"\"\n\n}"], "list_proc": ["tiagofilipe12/pATLAS-db-creation/abricate2db"], "list_wf_names": ["tiagofilipe12/pATLAS-db-creation"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["tiagofilipe12"], "nb_wf": 1, "list_wf": ["pATLAS-db-creation"], "list_contrib": ["tiagofilipe12"], "nb_contrib": 1, "codes": ["\nprocess bowtieIndex {\n\n    tag {\"creating bowtie2 index\"}\n\n    publishDir \"results/bowtie_indexes/\"\n\n    input:\n    file masterFastaFile from masterFasta_bowtie2\n\n    output:\n    file \"*bowtie2_index.*\" into bowtieIndexChannel\n                                            \n\n    \"\"\"\n    echo \"Creating bowtie2 index\"\n    bowtie2-build -q ${masterFastaFile} --threads ${task.cpus} \\\n    patlas_bowtie2_index\n    \"\"\"\n\n}"], "list_proc": ["tiagofilipe12/pATLAS-db-creation/bowtieIndex"], "list_wf_names": ["tiagofilipe12/pATLAS-db-creation"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["tiwarialok"], "nb_wf": 1, "list_wf": ["new-rnaseq"], "list_contrib": ["dvolk", "pditommaso", "evanfloden", "manoj039", "misssoft", "tiwarialok"], "nb_contrib": 6, "codes": ["\nprocess index {\n    tag \"$transcriptome_file.simpleName\"\n\n    input:\n    file transcriptome from transcriptome_file\n\n    output:\n    file 'index' into index_ch\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["tiwarialok/new-rnaseq/index"], "list_wf_names": ["tiwarialok/new-rnaseq"]}, {"nb_reuse": 2, "tools": ["seqtk", "Salmon"], "nb_own": 2, "list_own": ["trev-f", "tiwarialok"], "nb_wf": 2, "list_wf": ["SRAlign", "new-rnaseq"], "list_contrib": ["t-f-freeman", "dvolk", "trev-f", "pditommaso", "evanfloden", "manoj039", "misssoft", "tiwarialok"], "nb_contrib": 8, "codes": ["\nprocess quant {\n    tag \"$pair_id\"\n\n    input:\n    file index from index_ch\n    set pair_id, file(reads) from read_pairs_ch\n\n    output:\n    file(pair_id) into quant_ch\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}", "\nprocess SeqtkSample {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/seqtk:1.3--h7132678_4'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n\n    output:\n        tuple val(metadata), file('*fastq.gz'), val(toolIDs), emit: sampleReads\n\n    script:\n                                       \n        toolIDs += \"skS\"\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        if (metadata.readType == 'single') {\n            \"\"\"\n            seqtk sample -s${task.ext.seed} ${reads} ${task.ext.sampleSize} | \\\n                gzip -c > ${metadata.sampleName}${suffix}.fastq.gz \n            \"\"\"\n        } else {\n            \"\"\"\n            seqtk sample -s${task.ext.seed} ${reads[0]} ${task.ext.sampleSize} | \\\n                gzip -c > ${metadata.sampleName}${suffix}_R1.fastq.gz \n            seqtk sample -s${task.ext.seed} ${reads[1]} ${task.ext.sampleSize} | \\\n                gzip -c > ${metadata.sampleName}${suffix}_R2.fastq.gz \n            \"\"\"\n        }\n}"], "list_proc": ["tiwarialok/new-rnaseq/quant", "trev-f/SRAlign/SeqtkSample"], "list_wf_names": ["trev-f/SRAlign", "tiwarialok/new-rnaseq"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["tiwarialok"], "nb_wf": 1, "list_wf": ["new-rnaseq"], "list_contrib": ["dvolk", "pditommaso", "evanfloden", "manoj039", "misssoft", "tiwarialok"], "nb_contrib": 6, "codes": ["\nprocess multiqc {\n    publishDir params.outdir, mode:'copy'\n\n    input:\n    file('*') from quant_ch.mix(fastqc_ch).collect()\n    file(config) from multiqc_file\n\n    output:\n    file('multiqc_report.html')\n\n    script:\n    \"\"\"\n    cp $config/* .\n    echo \"custom_logo: \\$PWD/logo.png\" >> multiqc_config.yaml\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["tiwarialok/new-rnaseq/multiqc"], "list_wf_names": ["tiwarialok/new-rnaseq"]}, {"nb_reuse": 1, "tools": ["MAFFT"], "nb_own": 1, "list_own": ["tlawrence3"], "nb_wf": 1, "list_wf": ["covid_nextflow"], "list_contrib": ["tlawrence3"], "nb_contrib": 1, "codes": ["\nprocess alignReference {\n  publishDir \"$launchDir/results/genome_alignment\", mode: 'copy'\n  \n  output:\n  path \"${genomes.baseName}.aln\" into reference_aligned\n\n  shell:\n  \"\"\"\n     mafft --auto --thread -1 --keeplength --addfragments !{genomes} !{reference} > !{genomes.baseName}.aln\n  \"\"\"\n}"], "list_proc": ["tlawrence3/covid_nextflow/alignReference"], "list_wf_names": ["tlawrence3/covid_nextflow"]}, {"nb_reuse": 1, "tools": ["PMFastR"], "nb_own": 1, "list_own": ["tlawrence3"], "nb_wf": 1, "list_wf": ["covid_nextflow"], "list_contrib": ["tlawrence3"], "nb_contrib": 1, "codes": ["\nprocess extract_genes {\n    \n    publishDir \"$launchDir/results/AA_sequences/initial\", mode: 'copy'\n\n    input:\n    tuple gene_name, coords from mat_peptides_AA\n    each file(genomes) from filtered_align\n\n    output:\n    file \"${gene_name}.${genomes.baseName}.faa\" into proteins\n    \n    script:\n    \"\"\"\n    fascut ${coords} ${genomes} | fastr --degap | fasxl | fassub '\\\\-xl0' '' > ${gene_name}.${genomes.baseName}.faa\n    \"\"\"\n}"], "list_proc": ["tlawrence3/covid_nextflow/extract_genes"], "list_wf_names": ["tlawrence3/covid_nextflow"]}, {"nb_reuse": 1, "tools": ["PMFastR"], "nb_own": 1, "list_own": ["tlawrence3"], "nb_wf": 1, "list_wf": ["covid_nextflow"], "list_contrib": ["tlawrence3"], "nb_contrib": 1, "codes": ["\nprocess extract_cds {\n    publishDir \"$launchDir/results/cds_sequences/initial\", mode: 'copy'\n    \n    input:\n    tuple gene_name, coords from mat_peptides_cds\n    each file(genomes) from filtered_align\n    \n    output:\n    file \"${gene_name}.${genomes.baseName}.cds\" into cds\n\n    script:\n    \"\"\"\n    fascut ${coords} ${genomes} | fastr --degap > ${gene_name}.${genomes.baseName}.cds\n    \"\"\" \n}"], "list_proc": ["tlawrence3/covid_nextflow/extract_cds"], "list_wf_names": ["tlawrence3/covid_nextflow"]}, {"nb_reuse": 2, "tools": ["Trimmomatic", "MultiQC"], "nb_own": 2, "list_own": ["trev-f", "tmc-hematopath"], "nb_wf": 2, "list_wf": ["SRAlign", "NextSeq_MIPS_NextflowMutationDetector"], "list_contrib": ["t-f-freeman", "trev-f", "tmc-hematopath"], "nb_contrib": 3, "codes": ["\nprocess FullMultiQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}\",   mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        val  runName\n        path config\n        path multiqcFiles\n\n    output:\n        path \"*\"\n\n    script:\n        \"\"\"\n        multiqc \\\n            -n ${runName} -i ${runName} \\\n            -c ${config} \\\n            ${multiqcFiles}\n        \"\"\"\n}", "\nprocess trimming_trimmomatic {\n\tmaxForks 10\n\tpublishDir \"$PWD/${Sample}/trimmed\", mode: 'copy'\n\tinput:\n\t\t\tval Sample\n\toutput:\n\t\t\ttuple val (Sample), file(\"*.fq.gz\")\n\tscript:\n\t\"\"\"\n\ttrimmomatic PE \\\n\t${params.sequences}/${Sample}_*R1_*.fastq.gz ${params.sequences}/${Sample}_*R2_*.fastq.gz \\\n\t-baseout ${Sample}.fq.gz \\\n\tILLUMINACLIP:${params.adaptors}:2:30:10:2:keepBothReads \\\n\tLEADING:3 SLIDINGWINDOW:4:15 MINLEN:40\n\t\"\"\"\n}"], "list_proc": ["trev-f/SRAlign/FullMultiQC", "tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector/trimming_trimmomatic"], "list_wf_names": ["trev-f/SRAlign", "tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector"]}, {"nb_reuse": 2, "tools": ["Bowtie", "BWA"], "nb_own": 2, "list_own": ["trev-f", "tmc-hematopath"], "nb_wf": 2, "list_wf": ["SRAlign", "NextSeq_MIPS_NextflowMutationDetector"], "list_contrib": ["t-f-freeman", "trev-f", "tmc-hematopath"], "nb_contrib": 3, "codes": ["\nprocess Bowtie2Build {\n    tag \"${refName}\"\n\n    container 'quay.io/biocontainers/bowtie2:2.4.5--py38hfbc8389_2'\n\n    input:\n        path reference\n        val refName\n\n    output:\n        path '*', emit: bowtie2Index\n\n    script:\n                             \n        bt2Base = reference.toString() - ~/.fa?/\n        \"\"\"\n        bowtie2-build \\\n            --threads ${task.cpus} \\\n            ${reference} \\\n            ${bt2Base}\n        \"\"\"\n}", "\nprocess mapping_reads{\n\tmaxForks 15\n\tpublishDir \"${PWD}/${Sample}/mapped_reads/\", mode: 'copy'\n\tinput:\n\t\ttuple val (Sample), file (pairAssembled)\n\toutput:\n\t\ttuple val (Sample), file (\"*.sam\")\n\tscript:\n\t\"\"\"\n\tbwa mem -R \"@RG\\\\tID:AML\\\\tPL:ILLUMINA\\\\tLB:LIB-MIPS\\\\tSM:${Sample}\\\\tPI:200\" -M -t 20 ${params.genome} ${pairAssembled[0]} > ${Sample}.sam\n\t\"\"\"\n}"], "list_proc": ["trev-f/SRAlign/Bowtie2Build", "tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector/mapping_reads"], "list_wf_names": ["trev-f/SRAlign", "tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector"]}, {"nb_reuse": 1, "tools": ["Pindel", "pindel2vcf"], "nb_own": 1, "list_own": ["tmc-hematopath"], "nb_wf": 1, "list_wf": ["NextSeq_MIPS_NextflowMutationDetector"], "list_contrib": ["tmc-hematopath"], "nb_contrib": 1, "codes": ["\nprocess pindel {\n\tpublishDir \"$PWD/${Sample}/pindel/\", mode: 'copy', pattern: '*pindel_SI.vcf'\n\tpublishDir \"$PWD/${Sample}/pindel/\", mode: 'copy', pattern: '*.avinput'\n\tpublishDir \"$PWD/${Sample}/pindel/\", mode: 'copy', pattern: '*_pindel.hg19_multianno.csv'\n\n\tinput:\n\t\ttuple val (Sample), file(finalBam), file (finalBamBai)\n\toutput:\n\t\ttuple val (Sample), file (\"*\")\n\tscript:\n\t\"\"\"\n\texport BAM_2_PINDEL_ADAPT=${params.pindel}/Adaptor.pm\n\tsh ${params.pindel_config_script} -s ${Sample}\n\t${params.pindel}/pindel -f ${params.genome} -i $PWD/config.txt -c chr13 -o ${Sample}_pindel\n\t${params.pindel}/pindel2vcf -r ${params.genome} -p ${Sample}_pindel_SI -R hg19 -d 07102019 -v ${Sample}_pindel_SI.vcf\n\n\n\tperl ${params.annovarLatest_path}/convert2annovar.pl -format vcf4 ${Sample}_pindel_SI.vcf --outfile ${Sample}_pindel.avinput --withzyg --includeinfo\n\n\tperl ${params.annovarLatest_path}/table_annovar.pl ${Sample}_pindel.avinput ${params.annovarLatest_path}/humandb/ -buildver hg19 -out ${Sample}_pindel --remove -protocol refGene,cytoBand,cosmic84 --operation g,r,f -nastring '.' --otherinfo --csvout --thread 10 --xreffile ${params.annovarLatest_path}/example/gene_fullxref.txt\n \t\"\"\"\n}"], "list_proc": ["tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector/pindel"], "list_wf_names": ["tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector"]}, {"nb_reuse": 2, "tools": ["preseq", "XCAVATOR"], "nb_own": 2, "list_own": ["trev-f", "tmc-hematopath"], "nb_wf": 2, "list_wf": ["SRAlign", "NextSeq_MIPS_NextflowMutationDetector"], "list_contrib": ["t-f-freeman", "trev-f", "tmc-hematopath"], "nb_contrib": 3, "codes": ["\nprocess cava {\n\tpublishDir \"$PWD/${Sample}/CAVA/\", mode: 'copy'\n\t\n\tinput:\n\t\ttuple val(Sample), file (somaticVcf), file (somaticseqMultianno), file(combinedVcf)\n\t\n\toutput:\n\t\ttuple val(Sample), file (\"*.cava.csv\")\n\tscript:\n\t\"\"\"\n\t${params.cava_path}/cava -c ${params.cava_path}/config_v2.txt -t 10 -i $PWD/${Sample}/variants/${Sample}.somaticseq.vcf -o ${Sample}.somaticseq\n\t${params.cava_path}/cava -c ${params.cava_path}/config_v2.txt -t 10 -i $PWD/${Sample}/variants/${Sample}.combined.vcf -o ${Sample}.combined\n\tpython3.6 ${params.cava_script_path} ${Sample}.somaticseq.txt ${Sample}.combined.txt ${Sample}.cava.csv\n\t\"\"\"\n}", "\nprocess Preseq {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/preseq:3.1.2--h2c25361_3'\n\n    publishDir \"${params.baseDirData}/align/preseq\", mode: 'copy', pattern: '*.txt'\n\n    input:\n        tuple val(metadata), path(bam), val(toolIDs)\n\n    output:\n        path '*_psL.txt', emit: psL\n\n    script:\n                                        \n        toolIDspsL = toolIDs\n        toolIDspsL += 'psL'\n        suffixpsL = toolIDspsL ? \"__${toolIDspsL.join('_')}\" : ''\n\n        toolIDs += ['psL']\n\n                              \n        readTypeArg = metadata.readType == 'single' ? '' : '-pe'\n\n        \"\"\"\n        preseq lc_extrap \\\n            -o ${metadata.sampleName}${suffixpsL}.txt \\\n            ${readTypeArg} \\\n            -bam ${bam}\n        \"\"\"\n}"], "list_proc": ["tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector/cava", "trev-f/SRAlign/Preseq"], "list_wf_names": ["trev-f/SRAlign", "tmc-hematopath/NextSeq_MIPS_NextflowMutationDetector"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["tmc-hematopath"], "nb_wf": 1, "list_wf": ["smMIPS-Detection-Pipeline"], "list_contrib": ["nidhi12k", "tmc-hematopath"], "nb_contrib": 2, "codes": ["\nprocess mapping_reads{\n\tmaxForks 15\n\tpublishDir \"${PWD}/${Sample}/mapped_reads/\", mode: 'copy'\n\tinput:\n\t\ttuple val (Sample), file (pairAssembled)\n\toutput:\n\t\ttuple val (Sample), file (\"*.sam\")\n\tscript:\n\t\"\"\"\n\tbwa mem -R \"@RG\\\\tID:AML\\\\tPL:ILLUMINA\\\\tLB:LIB-MIPS\\\\tSM:${Sample}\\\\tPI:200\" -M -t 20 ${params.genome} ${pairAssembled[0]} > ${Sample}.sam\n\t\"\"\"\n}"], "list_proc": ["tmc-hematopath/smMIPS-Detection-Pipeline/mapping_reads"], "list_wf_names": ["tmc-hematopath/smMIPS-Detection-Pipeline"]}, {"nb_reuse": 1, "tools": ["XCAVATOR"], "nb_own": 1, "list_own": ["tmc-hematopath"], "nb_wf": 1, "list_wf": ["smMIPS-Detection-Pipeline"], "list_contrib": ["nidhi12k", "tmc-hematopath"], "nb_contrib": 2, "codes": ["\nprocess cava {\n\tpublishDir \"$PWD/${Sample}/CAVA/\", mode: 'copy'\n\t\n\tinput:\n\t\ttuple val(Sample), file (somaticVcf), file (somaticseqMultianno), file(combinedVcf)\n\t\n\toutput:\n\t\ttuple val(Sample), file (\"*.cava.csv\")\n\tscript:\n\t\"\"\"\n\t${params.cava_path}/cava -c ${params.cava_path}/config.txt -t 10 -i $PWD/${Sample}/variants/${Sample}.somaticseq.vcf -o ${Sample}.somaticseq\n\t${params.cava_path}/cava -c ${params.cava_path}/config.txt -t 10 -i $PWD/${Sample}/variants/${Sample}.combined.vcf -o ${Sample}.combined\n\tpython3.6 ${params.cava_script_path} ${Sample}.somaticseq.txt ${Sample}.combined.txt ${Sample}.cava.csv\n\t\"\"\"\n}"], "list_proc": ["tmc-hematopath/smMIPS-Detection-Pipeline/cava"], "list_wf_names": ["tmc-hematopath/smMIPS-Detection-Pipeline"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["tmc-hematopath"], "nb_wf": 1, "list_wf": ["smMIPS-Detection-Pipeline"], "list_contrib": ["nidhi12k", "tmc-hematopath"], "nb_contrib": 2, "codes": ["\nprocess trimming_trimmomatic {\n\tmaxForks 10\n\tpublishDir \"$PWD/${Sample}/trimmed\", mode: 'copy'\n\tinput:\n\t\tval Sample\n\toutput:\n\t\ttuple val (Sample), file(\"*.fq.gz\")\n\tscript:\n\t\"\"\"\n\ttrimmomatic PE \\\n\t${params.sequences}/${Sample}*_R1_*.fastq.gz ${params.sequences}/${Sample}*_R2_*.fastq.gz \\\n\t-baseout ${Sample}.fq.gz \\\n\tILLUMINACLIP:${params.adapters}:2:30:10:2:keepBothReads \\\n\tLEADING:3 SLIDINGWINDOW:4:15 MINLEN:40\n\t\"\"\"\n}"], "list_proc": ["tmc-hematopath/smMIPS-Detection-Pipeline/trimming_trimmomatic"], "list_wf_names": ["tmc-hematopath/smMIPS-Detection-Pipeline"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["tmc-hematopath"], "nb_wf": 1, "list_wf": ["smMIPS-Detection-Pipeline"], "list_contrib": ["nidhi12k", "tmc-hematopath"], "nb_contrib": 2, "codes": ["\nprocess lofreq_run_amplicon{\n\tmaxForks 15\n\tpublishDir \"$PWD/${Sample}/VariantCalling/Lofreq/\", mode: 'copy'\n\tinput:\n\t\ttuple val (Sample), file (sortedBam), file (sortedBamBai)\n\toutput:\n\t\ttuple val (Sample), file (\"*.hg19_multianno.csv\")\n\t\n\tscript:\n\t\"\"\"\n\t${params.lofreq_path} viterbi -f ${params.genome} -o ${Sample}.lofreq.pre.bam ${sortedBam}\n\tsamtools sort ${Sample}.lofreq.pre.bam > ${Sample}.lofreq.bam\n\t${params.lofreq_path} call -b dynamic -C 50 -a 0.00005 -q 30 -Q 30 -m 50 -f ${params.genome} -l ${params.bedfile}.bed -o ${Sample}.lofreq.vcf ${Sample}.lofreq.bam\n\t${params.lofreq_path} filter -a 0.01 -i ${Sample}.lofreq.vcf -o ${Sample}.lofreq.filtered.vcf\n\tperl ${params.annovar2_path}/convert2annovar.pl -format vcf4 ${Sample}.lofreq.filtered.vcf --outfile ${Sample}.avinput --withzyg --includeinfo\n\tperl ${params.annovar2_path}/table_annovar.pl ${Sample}.avinput --out ${Sample}_final --remove --protocol refGene,cosmic84,exac03 --operation g,f,f --buildver hg19 --nastring '-1' --otherinfo --csvout ${params.annovar2_path}/humandb/\n\t\"\"\"\n}"], "list_proc": ["tmc-hematopath/smMIPS-Detection-Pipeline/lofreq_run_amplicon"], "list_wf_names": ["tmc-hematopath/smMIPS-Detection-Pipeline"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["BADAS_Nextflow_Tutorial"], "list_contrib": ["pditommaso", "KevinSayers", "evanfloden", "tobsecret", "stevekm"], "nb_contrib": 5, "codes": ["\nprocess index {\n    conda \"bioconda::salmon\"\n    input:\n    file transcriptome from transcriptome_file\n     \n    output:\n    file 'index' into index_ch\n\n    script:       \n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n}"], "list_proc": ["tobsecret/BADAS_Nextflow_Tutorial/index"], "list_wf_names": ["tobsecret/BADAS_Nextflow_Tutorial"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["BADAS_Nextflow_Tutorial"], "list_contrib": ["pditommaso", "KevinSayers", "evanfloden", "tobsecret", "stevekm"], "nb_contrib": 5, "codes": ["\nprocess quantification {\n    tag \"$pair_id\"\n    conda \"bioconda::salmon\"\n         \n    input:\n    file index from index_ch\n    set pair_id, file(reads) from read_pairs_ch\n \n    output:\n    file(pair_id) into quant_ch\n \n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U -i index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id\n    \"\"\"\n}"], "list_proc": ["tobsecret/BADAS_Nextflow_Tutorial/quantification"], "list_wf_names": ["tobsecret/BADAS_Nextflow_Tutorial"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["BADAS_Nextflow_Tutorial"], "list_contrib": ["pditommaso", "KevinSayers", "evanfloden", "tobsecret", "stevekm"], "nb_contrib": 5, "codes": ["\nprocess fastqc {\n    tag \"FASTQC on $sample_id\"\n    conda \"bioconda::fastqc\"\n\n    input:\n    set sample_id, file(reads) from read_pairs2_ch\n\n    output:\n    file(\"fastqc_${sample_id}_logs\") into fastqc_ch\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"  \n}"], "list_proc": ["tobsecret/BADAS_Nextflow_Tutorial/fastqc"], "list_wf_names": ["tobsecret/BADAS_Nextflow_Tutorial"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["covid19"], "list_contrib": ["drpatelh", "apeltzer", "heuermh", "tobsecret"], "nb_contrib": 4, "codes": [" process BWA_INDEX {\n        tag \"$fasta\"\n        label 'process_high'\n        publishDir path: { params.save_reference ? \"${params.outdir}/genome\" : params.outdir },\n            saveAs: { params.save_reference ? it : null }, mode: params.publish_dir_mode\n\n        input:\n        file fasta from ch_fasta\n\n        output:\n        file \"BWAIndex\" into ch_bwa_index\n\n        script:\n        \"\"\"\n        bwa index -a bwtsw $fasta\n        mkdir BWAIndex && mv ${fasta}* BWAIndex\n        \"\"\"\n    }"], "list_proc": ["tobsecret/covid19/BWA_INDEX"], "list_wf_names": ["tobsecret/covid19"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["covid19"], "list_contrib": ["drpatelh", "apeltzer", "heuermh", "tobsecret"], "nb_contrib": 4, "codes": [" process MINIMAP2_INDEX {\n        tag \"$fasta\"\n        label 'process_medium'\n        publishDir path: { params.save_reference ? \"${params.outdir}/genome/Minimap2Index\" : params.outdir },\n            saveAs: { params.save_reference ? it : null }, mode: params.publish_dir_mode\n\n        input:\n        file fasta from ch_fasta\n\n        output:\n        file \"*.mmi\" into ch_minimap2_index\n\n        script:\n        \"\"\"\n        minimap2 -ax map-ont  -t $task.cpus -d ${fasta}.mmi $fasta\n        \"\"\"\n    }"], "list_proc": ["tobsecret/covid19/MINIMAP2_INDEX"], "list_wf_names": ["tobsecret/covid19"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["covid19"], "list_contrib": ["drpatelh", "apeltzer", "heuermh", "tobsecret"], "nb_contrib": 4, "codes": ["\nprocess FASTP {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir \"${params.outdir}/fastp\", mode: params.publish_dir_mode\n    \n    when:\n    !long_reads\n\n    input:\n    set val(sample), val(single_end), val(long_reads), file(reads) from ch_reads_bwa\n\n    output:\n    set val(sample), val(single_end), val(long_reads), file(\"*trimmed.fastq{,.gz}\") into ch_reads_trimmed\n\n    script:\n    fastp_params = \"-q 20 -u 20 -l 50\"\n    if (single_end) {\n        out_read = (reads.getExtension() == \"gz\") ? \"${sample}.trimmed.fastq.gz\" : \"${sample}.trimmed.fastq\"\n        \"\"\"\n        fastp -i ${reads} -o $out_read $fastp_params\n        \"\"\"\n    } else {\n        out_read1 = (reads[0].getExtension() == \"gz\") ? \"${sample}_1.trimmed.fastq.gz\" : \"${sample}_1.trimmed.fastq\"\n        out_read2 = (reads[1].getExtension() == \"gz\") ? \"${sample}_2.trimmed.fastq.gz\" : \"${sample}_2.trimmed.fastq\"\n\n        \"\"\"\n        fastp -i ${reads[0]} -I ${reads[1]} -o $out_read1 -O $out_read2 $fastp_params\n        \"\"\"\n    }\n}"], "list_proc": ["tobsecret/covid19/FASTP"], "list_wf_names": ["tobsecret/covid19"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["covid19"], "list_contrib": ["drpatelh", "apeltzer", "heuermh", "tobsecret"], "nb_contrib": 4, "codes": ["\nprocess BWA_MEM {\n    tag \"$sample\"\n    label 'process_high'\n    if (params.save_align_intermeds) {\n        publishDir path: \"${params.outdir}/bwa\", mode: params.publish_dir_mode\n    }\n\n    when:\n    !long_reads\n\n    input:\n    set val(sample), val(single_end), val(long_reads), file(reads) from ch_reads_trimmed\n    file index from ch_bwa_index.collect()\n\n    output:\n    set val(sample), val(single_end), val(long_reads), file(\"*.bam\") into ch_bwa_bam\n\n    script:\n    rg = \"\\'@RG\\\\tID:${sample}\\\\tSM:${sample.split('_')[0..-2].join('_')}\\\\tPL:ILLUMINA\\\\tLB:${sample}\\\\tPU:1\\'\"\n    \"\"\"\n    bwa mem \\\\\n        -t $task.cpus \\\\\n        -M \\\\\n        -R $rg \\\\\n        ${index}/${bwa_base} \\\\\n        $reads \\\\\n        | samtools view -@ $task.cpus -b -h -F 0x0100 -O BAM -o ${sample}.bam -\n    \"\"\"\n}"], "list_proc": ["tobsecret/covid19/BWA_MEM"], "list_wf_names": ["tobsecret/covid19"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["covid19"], "list_contrib": ["drpatelh", "apeltzer", "heuermh", "tobsecret"], "nb_contrib": 4, "codes": ["\nprocess SORT_BAM {\n    tag \"$sample\"\n    label 'process_medium'\n    publishDir path: \"${params.outdir}/${aligner}\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (params.save_align_intermeds) {\n                          if (filename.endsWith(\".flagstat\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".idxstats\")) \"samtools_stats/$filename\"\n                          else if (filename.endsWith(\".stats\")) \"samtools_stats/$filename\"\n                          else filename\n                      }\n                }\n\n    input:\n    set val(sample), val(single_end), val(long_reads), file(bam) from ch_bwa_bam.concat(ch_minimap2_bam)\n\n    output:\n    set val(sample), val(single_end), val(long_reads), file(\"*.sorted.{bam,bam.bai}\") into ch_sort_bam\n    file \"*.{flagstat,idxstats,stats}\" into ch_sort_bam_flagstat_mqc\n\n    script:\n    aligner = long_reads ? \"minimap2\" : \"bwa\"\n    \"\"\"\n    samtools sort -@ $task.cpus -o ${sample}.sorted.bam -T $sample $bam\n    samtools index ${sample}.sorted.bam\n    samtools flagstat ${sample}.sorted.bam > ${sample}.sorted.bam.flagstat\n    samtools idxstats ${sample}.sorted.bam > ${sample}.sorted.bam.idxstats\n    samtools stats ${sample}.sorted.bam > ${sample}.sorted.bam.stats\n    \"\"\"\n}"], "list_proc": ["tobsecret/covid19/SORT_BAM"], "list_wf_names": ["tobsecret/covid19"]}, {"nb_reuse": 1, "tools": ["Picard", "SAMtools", "Minimap2", "MultiQC", "FastQC"], "nb_own": 1, "list_own": ["tobsecret"], "nb_wf": 1, "list_wf": ["covid19"], "list_contrib": ["drpatelh", "apeltzer", "heuermh", "tobsecret"], "nb_contrib": 4, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    NanoPlot --version &> v_nanoplot.txt\n    echo \\$(bwa 2>&1) > v_bwa.txt\n    minimap2 --version &> v_minimap2.txt\n    samtools --version > v_samtools.txt\n    picard MarkDuplicates --version &> v_picard.txt  || true\n    echo \\$(R --version 2>&1) > v_R.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["tobsecret/covid19/get_software_versions"], "list_wf_names": ["tobsecret/covid19"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BEDTools"], "nb_own": 1, "list_own": ["tolkit"], "nb_wf": 1, "list_wf": ["nemADSQ"], "list_contrib": ["pgonzale60", "lstevens17"], "nb_contrib": 2, "codes": ["\nprocess gc_by_windows {\n    tag \"${assembler}\"\n    label 'nemaQC'\n    publishDir \"$params.outdir/gc\", mode: 'copy'\n\n    input:\n      tuple val(strain), val(assembler), path(assembly)\n\n    output:\n      tuple val(assembler), path(\"${assembler}.gc.bed.gz\")\n\n    script:\n      \"\"\"\n      samtools faidx ${assembly}\n      cut -f 1,2 ${assembly}.fai > ${assembler}.seqlen.tsv\n      bedtools makewindows -g ${assembler}.seqlen.tsv \\\n        -w ${params.teloRepeatWindowSize} > windows.bed\n\n      bedtools nuc -fi assembly.fasta -bed windows.bed | \\\n        cut -f 1-3,5 | tail -n+2 | \\\n        gzip -c > ${assembler}.gc.bed.gz\n      \n      rm assembly.fasta* ${assembler}.seqlen.tsv windows.bed\n      \"\"\"\n}"], "list_proc": ["tolkit/nemADSQ/gc_by_windows"], "list_wf_names": ["tolkit/nemADSQ"]}, {"nb_reuse": 1, "tools": ["Minimap2"], "nb_own": 1, "list_own": ["tolkit"], "nb_wf": 1, "list_wf": ["nemADSQ"], "list_contrib": ["pgonzale60", "lstevens17"], "nb_contrib": 2, "codes": ["\nprocess map_telomeric_reads {\n    tag \"${assemblies[1]}\"\n    publishDir \"$params.outdir/teloMaps\", mode: 'copy'\n    label 'nemaQC'\n\n    input:\n      tuple val(reads), val(assemblies)\n\n    output:\n      tuple val(\"${assemblies[1]}\"), path(\"${assemblies[1]}.teloMapped.paf.gz\")\n\n    script:\n      \"\"\"\n      minimap2 ${assemblies[2]} ${reads[1]} | \\\n        gzip -c > ${assemblies[1]}.teloMapped.paf.gz\n      \"\"\"\n}"], "list_proc": ["tolkit/nemADSQ/map_telomeric_reads"], "list_wf_names": ["tolkit/nemADSQ"]}, {"nb_reuse": 1, "tools": ["Assemble2"], "nb_own": 1, "list_own": ["tolkit"], "nb_wf": 1, "list_wf": ["nemADSQ"], "list_contrib": ["pgonzale60", "lstevens17"], "nb_contrib": 2, "codes": ["\nprocess scaffold {\n    tag \"${assemName}\"\n    publishDir \"$params.outdir/scaffold\", mode: 'copy'\n\n    input:\n      tuple val(strain), val(assemName), path(h5_matrix), path(assembly)\n\n    output:\n      tuple val(strain), val(assemName), path(\"${assemName}_scaf\")\n\n    script:\n      \"\"\"\n      if [ -f *.gz ]; then\n            gunzip -c $assembly > assembly.fasta\n        else\n            ln -s $assembly assembly.fasta\n      fi\n      assemble -m $h5_matrix -o ${assemName}_scaf \\\n        --min_scaffold_length 100000 --bin_size 10000 \\\n        --misassembly_zscore_threshold -5 \\\n        --num_iterations 3 -f assembly.fasta\n      rm ${assemName}_scaf/*h5 ${assemName}_scaf/*graphml\n      bgzip ${assemName}_scaf/super_scaffolds.fa\n      \"\"\"\n}"], "list_proc": ["tolkit/nemADSQ/scaffold"], "list_wf_names": ["tolkit/nemADSQ"]}, {"nb_reuse": 1, "tools": ["CANU"], "nb_own": 1, "list_own": ["tolkit"], "nb_wf": 1, "list_wf": ["nemADSQ"], "list_contrib": ["pgonzale60", "lstevens17"], "nb_contrib": 2, "codes": ["\nprocess canu {\n    tag \"${strain}\"\n    publishDir \"$params.outdir\", mode: 'symlink'\n\n    input:\n      tuple val(strain), path(reads)\n\n    output:\n      tuple val(strain), path(\"${strain}\")\n\n    script:\n      \"\"\"\n      /software/tola/bin/canu-2.1.1/bin/canu -d ${strain} \\\n        -p ${strain} gridEngineResourceOption='-M MEMORY -R \"select[mem>MEMORY] rusage[mem=MEMORY]\" -n THREADS -R \"span[hosts=1]\"' \\\n        genomeSize=60M -pacbio-hifi $reads\n      \"\"\"\n}"], "list_proc": ["tolkit/nemADSQ/canu"], "list_wf_names": ["tolkit/nemADSQ"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["torchij"], "nb_wf": 1, "list_wf": ["nextflow"], "list_contrib": ["torchij"], "nb_contrib": 1, "codes": ["\nprocess bamFilter {\n    label 'bamFilter'\n    tag \"_${id}\"\n    cpus 8\n    memory '64 GB'\n    container 'mblanche/bwa-samtools'\n    \n    input:\n    path(bam) from bam_ch\n\n    output:\n    tuple id, path(\"${id}_filtered.bam\") into bamFilt_ch\n                                                                                           \n    \n    script:\n    id = bam.name.toString().take(bam.name.toString().lastIndexOf('.'))\n    \"\"\"\n    samtools view -h -f1 -F 3328 ${bam} | samtools sort -n -O BAM -@ ${task.cpus} -o ${id}_filtered.bam -\n\t#samtools index ${id}_filtered.bam\n    \"\"\"\n\n}"], "list_proc": ["torchij/nextflow/bamFilter"], "list_wf_names": ["torchij/nextflow"]}, {"nb_reuse": 2, "tools": ["SAMtools", "AIVAR"], "nb_own": 2, "list_own": ["trev-f", "usafsam"], "nb_wf": 2, "list_wf": ["mad_river_wf", "SRAlign"], "list_contrib": ["fanninpm", "t-f-freeman", "friesac", "trev-f"], "nb_contrib": 4, "codes": ["\nprocess IVAR_VARIANTS {\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"logs/${task.process}/*.{log,err}\"\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"${task.process}/*.variants.tsv\"\n    tag \"${sample}\"\n    echo false\n    container params.container_ivar\n    memory {2.GB * task.attempt}\n    errorStrategy {'retry'}\n    maxRetries 2\n\n    input:\n        tuple val(sample), file(bamfile), file(ref_genome), file(ref_gff)\n\n    output:\n        path(\"${task.process}/${sample}.variants.tsv\"), emit: ivar_tsvs\n        path(\"logs/${task.process}/${sample}.${workflow.sessionId}.{log,err}\")\n\n    shell:\n    '''\n        mkdir -p !{task.process} logs/!{task.process}\n        log_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.log\n        err_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.err\n\n        # time stamp + capturing tool versions\n        date | tee -a $log_file $err_file > /dev/null\n        samtools --version >> $log_file\n\n        samtools mpileup -aa -A -B -Q 0 --reference !{ref_genome} !{bamfile} 2>> $err_file | \\\n            ivar variants -t 0.05 \\\n            -m !{params.mincov} \\\n            -r !{ref_genome} \\\n            -g !{ref_gff} \\\n            -p !{task.process}/!{sample}.variants \\\n            2>> $err_file >> $log_file\n    '''\n}", "\nprocess CompressSortSam {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samtools:1.15--h1170115_1'\n\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*.bam'\n\n    input:\n        tuple val(metadata), file(sam), val(toolIDs)\n\n    output:\n        tuple val(metadata), file('*.bam'), val(toolIDs), emit: bam\n\n    script:\n                                       \n        toolIDs += 'sSR'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        \"\"\"\n        samtools view -bh ${sam} | \\\n        samtools sort -o ${metadata.sampleName}${suffix}.bam -\n        \"\"\"\n}"], "list_proc": ["usafsam/mad_river_wf/IVAR_VARIANTS", "trev-f/SRAlign/CompressSortSam"], "list_wf_names": ["usafsam/mad_river_wf", "trev-f/SRAlign"]}, {"nb_reuse": 2, "tools": ["FastQC", "eFetch Pmc", "QResearch"], "nb_own": 2, "list_own": ["vib-singlecell-nf", "trev-f"], "nb_wf": 2, "list_wf": ["vsn-pipelines", "SRAlign"], "list_contrib": ["dependabot[bot]", "t-f-freeman", "KrisDavie", "trev-f", "ghuls", "dweemx", "cflerin"], "nb_contrib": 7, "codes": ["process FastQC {\n    tag \"${metadata.sampleName}\"\n\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n\n    publishDir \"${params.baseDirReport}/readsQC/trim/fastQC\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/readsQC/trim/fastQC\",   mode: 'copy', pattern: '*.zip'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n\n    output:\n        path '*.html', emit: html\n        path '*.zip', emit: zip \n        val toolIDs, emit: tools\n\n    script:\n                                       \n        toolIDs += 'fqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        if (metadata.readType == 'single') {\n            read1 = \"${metadata.sampleName}${suffix}_R1.fastq.gz\"\n\n            \"\"\"\n            [ ! -f ${read1} ] && ln -s ${reads} ${read1}\n\n            fastqc ${read1}\n            \"\"\"\n        } else {\n            read1 = \"${metadata.sampleName}${suffix}_R1.fastq.gz\"\n            read2 = \"${metadata.sampleName}${suffix}_R2.fastq.gz\"\n\n            \"\"\"\n            [ ! -f ${read1} ] && ln -s ${reads[0]} ${read1}\n            [ ! -f ${read2} ] && ln -s ${reads[1]} ${read2}\n\n            fastqc ${read1} ${read2}\n            \"\"\"\n        }\n}", "\nprocess EDIRECT__SRAID_TO_SAMPLENAME {\n    \n    container params.edirect.container\n    label 'compute_resources__default'\n    maxForks 1\n\n    input:\n        val(sraId)\n    output:\n        tuple val(sraId), stdout\n    shell:\n        \"\"\"\n        esearch -db sra -query ${sraId} \\\n           | efetch --format native \\\n           | sed -r 's/(.*)<TITLE>(.*)<\\\\/TITLE>(.*)/\\\\2/' \\\n           | grep \"^[^<;]\" \\\n           | tr -d '\\\\n' \n        \"\"\"\n}"], "list_proc": ["trev-f/SRAlign/FastQC", "vib-singlecell-nf/vsn-pipelines/EDIRECT__SRAID_TO_SAMPLENAME"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines", "trev-f/SRAlign"]}, {"nb_reuse": 3, "tools": ["STAR", "MultiQC", "GATK"], "nb_own": 3, "list_own": ["vib-singlecell-nf", "trev-f", "varshini712"], "nb_wf": 3, "list_wf": ["vsn-pipelines", "SRAlign", "mutect2-nf"], "list_contrib": ["dependabot[bot]", "t-f-freeman", "KrisDavie", "trev-f", "ghuls", "dweemx", "cflerin", "varshini712"], "nb_contrib": 8, "codes": ["\nprocess mutect2 {\n\n    publishDir \"${params.outputdir}/mutect2\", mode: 'copy', overwrite: true\n    echo true\n\n    input:\n    set val(sampleID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(ref_fasta), file(ref_fai), file(ref_dict), file(targets_bed), file(germline_resource_gz), file(germline_resource_gz_tbi) from sample_bam_pairs_ref_germlinevcf\n\n    output:\n    file(\"${vcf_file}\")\n\n    script:\n    prefix = \"${tumorID}.${normalID}\"\n    vcf_file = \"${prefix}.vcf\"\n\n    \"\"\"\n    gatk --java-options \\\"-Xms8G -Xmx8G\\\" Mutect2 \\\n    --seconds-between-progress-updates 600 \\\n    --native-pair-hmm-threads 4 \\\n    --reference \"${ref_fasta}\" \\\n    --germline-resource \"${germline_resource_gz}\" \\\n    --dont-use-soft-clipped-bases \\\n    --max-reads-per-alignment-start 100 \\\n    --intervals \"${targets_bed}\" \\\n    --interval-padding 10 \\\n    --input \"${tumorBam}\" \\\n    --input \"${normalBam}\" \\\n    --tumor-sample \"${tumorID}\" \\\n    --normal-sample \"${normalID}\" \\\n    --output \"${vcf_file}\"\n    \"\"\"\n}", "\nprocess SC__STAR__MAP_COUNT {\n\n\tcontainer params.tools.star.container\n    label 'compute_resources__star_map_count'\n\n\tinput:\n\t\tfile(starIndex)\n\t\tval starIndexLoaded\n\t\ttuple val(sample), path(fastqs)\n\n\toutput:\n\t\tval success, emit: isDone\n\t\ttuple val(sample), path(\"*ReadsPerGene.out.tab\"), emit: counts optional processParams.containsKey('quantMode') && processParams.quantMode == \"GeneCounts\" ? true: false\n\t\ttuple val(sample), path(\"*.STAR_Aligned.sortedByCoord.out.bam\"), emit: bam\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.tools.star.map_count)\n\t\tprocessParams = sampleParams.local\n\t\tsuccess = true\n\t\t\"\"\"\n\t\tSTAR \\\n\t\t\t--genomeLoad LoadAndKeep \\\n\t\t\t--genomeDir ${starIndex} \\\n            --runThreadN ${task.cpus} \\\n\t\t\t${(processParams.containsKey('limitBAMsortRAM')) ? '--limitBAMsortRAM ' + processParams.limitBAMsortRAM: ''} \\\n\t\t\t${(processParams.containsKey('outSAMtype')) ? '--outSAMtype ' + processParams.outSAMtype: ''} \\\n\t\t\t${(processParams.containsKey('quantMode')) ? '--quantMode ' + processParams.quantMode: ''} \\\n\t\t\t${(processParams.containsKey('outReadsUnmapped')) ? '--outReadsUnmapped ' + processParams.outReadsUnmapped: ''} \\\n\t\t\t--readFilesIn ${fastqs} \\\n\t\t\t${(fastqs.name.endsWith(\".gz\")) ? '--readFilesCommand zcat' : ''} \\\n\t\t\t--outFileNamePrefix ${sample}.STAR_\n\t\t\"\"\"\n\n}", "\nprocess SamStatsMultiQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}/align\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        file sST\n        file sIX\n        val runName\n        val toolIDs\n\n    output:\n        path '*'\n\n    script:\n                                       \n        toolIDs += 'mqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n\n        \"\"\"\n        multiqc \\\n            -n ${runName}${suffix} \\\n            --module samtools \\\n            ${sST} \\\n            ${sIX}\n        \"\"\"\n}"], "list_proc": ["varshini712/mutect2-nf/mutect2", "vib-singlecell-nf/vsn-pipelines/SC__STAR__MAP_COUNT", "trev-f/SRAlign/SamStatsMultiQC"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines", "trev-f/SRAlign", "varshini712/mutect2-nf"]}, {"nb_reuse": 4, "tools": ["STAR", "genefilter", "Bowtie", "SAMtools"], "nb_own": 3, "list_own": ["vib-singlecell-nf", "trev-f", "usafsam"], "nb_wf": 3, "list_wf": ["vsn-pipelines", "SRAlign", "mad_river_wf"], "list_contrib": ["dependabot[bot]", "t-f-freeman", "KrisDavie", "friesac", "trev-f", "ghuls", "dweemx", "fanninpm", "cflerin"], "nb_contrib": 9, "codes": ["\nprocess SC__SCANPY__GENE_FILTER {\n\n    container params.tools.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n    input:\n        tuple val(sampleId), path(f)\n\n\toutput:\n        tuple val(sampleId), path(\"${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off}\")\n\n\tscript:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            genefilter \\\n            $f \\\n            ${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off} \\\n            ${processParams?.geneFilterMinNCells ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"\n\n}", "\nprocess SC__STAR__BUILD_INDEX {\n\n    container params.tools.star.container\n    label 'compute_resources__star_build_genome'\n\n    input:\n        file(annotation)\n        file(genome)\n\n    output:\n        file(\"STAR_index\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.star.build_genome)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        mkdir STAR_index\n        STAR \\\n            --runThreadN ${task.cpus} \\\n            --runMode genomeGenerate \\\n            --genomeDir STAR_index \\\n            --genomeFastaFiles ${genome} \\\n            --sjdbGTFfile ${annotation} \\\n            --sjdbOverhang ${processParams.sjdbOverhang} \\\n            --genomeSAindexNbases ${processParams.genomeSAindexNbases} # Suggested by STAR (default: 14), otherwise keeps on hanging\n        \"\"\"\n\n}", "process Bowtie2Align {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/bowtie2:2.4.5--py38hfbc8389_2'\n\n    label 'cpu_mid'\n    label 'mem_mid'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n        path bt2Indexes\n        val refName\n\n    output:\n        tuple val(metadata), file('*.sam'), val(toolIDs), emit: sam\n\n    script:\n                              \n        if (metadata.readType == 'single') {\n            argReads = \"-1 ${reads}\"\n        } else {\n            argReads = \"-1 ${reads[0]} -2 ${reads[1]}\"\n        }\n\n\n                                       \n        toolIDs += \"bt2-${refName}\"\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n\n                              \n        bt2IndexBaseName = bt2Indexes[0].toString() - ~/.rev.\\d.bt2?/ - ~/.\\d.bt2?/ - ~/.fa?/\n\n                        \n        def options = task.ext.args ?: ''\n\n        \"\"\"\n        bowtie2 \\\n            --threads ${task.cpus} \\\n            ${options} \\\n            -x ${bt2IndexBaseName} \\\n            ${argReads} \\\n            -S ${metadata.sampleName}${suffix}.sam\n        \"\"\"\n}", "process SAMTOOLS_SORT_INDEX {\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"logs/${task.process}/*.{log,err}\"\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"${task.process}/*_trimclip.sorted.bam\"\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"${task.process}/*_trimclip.sorted.bam.bai\"\n    tag \"${sample}\"\n    echo false\n    cpus params.medcpus\n    container params.container_samtools\n\n    input:\n        tuple val(sample), file(samfile)\n\n    output:\n        tuple val(sample), path(\"${task.process}/${sample}_trimclip.sorted.bam\"), emit: bamfile\n        path(\"${task.process}/${sample}_trimclip.sorted.bam.bai\"), emit: bamfile_index\n        path(\"logs/${task.process}/${sample}.${workflow.sessionId}.{log,err}\")\n\n    shell:\n    '''\n        mkdir -p !{task.process} logs/!{task.process}\n        log_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.log\n        err_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.err\n\n        # time stamp + capturing tool versions\n        date | tee -a $log_file $err_file > /dev/null\n        samtools --version >> $log_file\n\n        samtools view -bShu !{samfile} | \\\n            samtools sort \\\n                -o !{task.process}/!{sample}_trimclip.sorted.bam \\\n                -@ !{task.cpus} \\\n                2>> $err_file >> $log_file\n        samtools index \\\n            !{task.process}/!{sample}_trimclip.sorted.bam \\\n            !{task.process}/!{sample}_trimclip.sorted.bam.bai \\\n            2>> $err_file >> $log_file\n    '''\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/SC__SCANPY__GENE_FILTER", "vib-singlecell-nf/vsn-pipelines/SC__STAR__BUILD_INDEX", "trev-f/SRAlign/Bowtie2Align", "usafsam/mad_river_wf/SAMTOOLS_SORT_INDEX"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines", "trev-f/SRAlign", "usafsam/mad_river_wf"]}, {"nb_reuse": 2, "tools": ["SAMBLASTER", "GATK"], "nb_own": 2, "list_own": ["vib-singlecell-nf", "trev-f"], "nb_wf": 2, "list_wf": ["vsn-pipelines", "SRAlign"], "list_contrib": ["dependabot[bot]", "t-f-freeman", "KrisDavie", "trev-f", "ghuls", "dweemx", "cflerin"], "nb_contrib": 7, "codes": ["\nprocess Samblaster {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samblaster:0.1.26--h9f5acd7_2'\n\n    input:\n        tuple val(metadata), file(sam), val(toolIDs)\n\n    output:\n        tuple val(metadata), file('*.sam'), val(toolIDs), emit: sam\n\n    script:\n                                       \n        toolIDs += 'sbl'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n                        \n        def options = task.ext.args ?: ''\n\n        \"\"\"\n        samblaster \\\n            ${options} \\\n            -i ${sam} \\\n            -o ${metadata.sampleName}${suffix}.sam\n        \"\"\"\n}", "\nprocess PICARD__MARK_DUPLICATES_AND_SORT {\n\n    container toolParams.container\n    label 'compute_resources__default','compute_resources__24hqueue'\n\n    input:\n        tuple val(sampleId),\n              path(bam)\n\n    output:\n        tuple val(sampleId),\n              path(\"${sampleId}.bwa.out.fixmate.picard_markdup.possorted.bam\"),\n              path(\"${sampleId}.bwa.out.fixmate.picard_markdup.possorted.bai\"),\n              path(\"${sampleId}.bwa.out.fixmate.picard_markdup.metrics.txt\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams)\n        processParams = sampleParams.local\n        \"\"\"\n        set -euo pipefail\n        gatk MarkDuplicates \\\n            -I ${bam} \\\n            -O /dev/stdout \\\n            --METRICS_FILE ${sampleId}.bwa.out.fixmate.picard_markdup.metrics.txt \\\n            --BARCODE_TAG CB \\\n            --COMPRESSION_LEVEL 0 \\\n            --QUIET true \\\n            --ASSUME_SORT_ORDER queryname \\\n        | gatk SortSam \\\n            -I /dev/stdin \\\n            -O ${sampleId}.bwa.out.fixmate.picard_markdup.possorted.bam \\\n            --SORT_ORDER coordinate \\\n            --CREATE_INDEX true\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAlign/Samblaster", "vib-singlecell-nf/vsn-pipelines/PICARD__MARK_DUPLICATES_AND_SORT"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines", "trev-f/SRAlign"]}, {"nb_reuse": 3, "tools": ["genefilter", "STAR", "HISAT2"], "nb_own": 2, "list_own": ["vib-singlecell-nf", "trev-f"], "nb_wf": 2, "list_wf": ["vsn-pipelines", "SRAlign"], "list_contrib": ["dependabot[bot]", "t-f-freeman", "KrisDavie", "trev-f", "ghuls", "dweemx", "cflerin"], "nb_contrib": 7, "codes": ["\nprocess Hisat2Align {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/hisat2:2.2.1--h87f3376_4'\n\n    label 'cpu_mid'\n    label 'mem_mid'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n        path ht2Indexes\n        val refName\n\n    output:\n        tuple val(metadata), file('*.sam'), val(toolIDs), emit: sam\n\n    script:\n                              \n        if (metadata.readType == 'single') {\n            argReads = \"-U ${reads}\"\n        } else {\n            argReads = \"-1 ${reads[0]} -2 ${reads[1]}\"\n        }\n\n                                       \n        toolIDs += \"ht2-${refName}\"\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n                              \n        def ht2IndexBaseName = ht2Indexes[0].toString() - ~/.rev.\\d.ht2?/ - ~/.\\d.ht2?/ - ~/.fa?/\n\n                        \n        def options = task.ext.args ?: ''\n\n        \"\"\"\n        hisat2 \\\n            --threads ${task.cpus} \\\n            ${options} \\\n            -x ${ht2IndexBaseName} \\\n            ${argReads} \\\n            -S ${metadata.sampleName}${suffix}.sam\n        \"\"\"\n}", "\nprocess SC__STAR__LOAD_GENOME {\n\n  \tcontainer params.tools.star.container\n    label 'compute_resources__default'\n\n\tinput:\n\t\tfile(starIndex)\n\n\toutput:\n\t\tval starIndexLoaded \n\n\tscript:\n\t\tstarIndexLoaded = true\n\t\t\"\"\"\n\t\tSTAR \\\n\t\t\t--genomeLoad LoadAndExit \\\n\t\t\t--genomeDir ${starIndex}\n\t\t\"\"\"\n\n}", "\nprocess SC__SCANPY__GENE_FILTER {\n\n    container params.tools.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n    input:\n        tuple val(sampleId), path(f)\n\n\toutput:\n        tuple val(sampleId), path(\"${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off}\")\n\n\tscript:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            genefilter \\\n            $f \\\n            ${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off} \\\n            ${processParams?.geneFilterMinNCells ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"\n\n}"], "list_proc": ["trev-f/SRAlign/Hisat2Align", "vib-singlecell-nf/vsn-pipelines/SC__STAR__LOAD_GENOME", "vib-singlecell-nf/vsn-pipelines/SC__SCANPY__GENE_FILTER"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines", "trev-f/SRAlign"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAlign"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess SamStats {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samtools:1.15--h1170115_1'\n\n    publishDir \"${params.baseDirData}/align/stats\", mode: 'copy', pattern: '*.txt'\n\n    input:\n        tuple val(metadata), path(bam), path(bai), val(toolIDs)\n\n    output:\n        path '*_sST.txt', emit: sST\n        path '*_sIX-idxstat.txt', emit: sIX\n        path '*_pctDup.txt', emit: pctDup\n        val toolIDs, emit: tools\n\n    script:\n                                       \n        toolIDsST = toolIDs\n        toolIDsST += 'sST'\n        suffixsST = toolIDsST ? \"__${toolIDsST.join('_')}\" : ''\n\n        toolIDsIX = toolIDs\n        toolIDsIX += 'sIX-idxstat'\n        suffixsIX = toolIDsIX ? \"__${toolIDsIX.join('_')}\" : ''\n\n        toolIDspd = toolIDs\n        toolIDspd += 'pctDup'\n        suffixspd = toolIDspd ? \"__${toolIDspd.join('_')}\" : ''\n\n        toolIDs += ['sST', 'sIX-idxstat']\n\n\n        \"\"\"\n        samtools stats ${bam} > ${metadata.sampleName}${suffixsST}.txt\n        samtools idxstats ${bam} > ${metadata.sampleName}${suffixsIX}.txt\n\n        # extract number of mapped reads\n        MAPPED=\\$( \\\n            grep ^SN ${metadata.sampleName}${suffixsST}.txt \\\n            | cut -f 2- \\\n            | grep 'reads mapped:' \\\n            | cut -f 2)\n        # extract number of duplicate reads\n        DUPPED=\\$( \\\n            grep ^SN ${metadata.sampleName}${suffixsST}.txt \\\n            | cut -f 2- \\\n            | grep 'reads duplicated:' \\\n            | cut -f 2)\n        # calculate percent of duplicated reads\n        PCTDUP=\\$(awk -v MAPPED=\\$MAPPED -v DUPPED=\\$DUPPED 'BEGIN{print DUPPED / MAPPED * 100}')\n        echo -e \"${metadata.sampleName}${suffixspd}\\t\\$PCTDUP\" > ${metadata.sampleName}${suffixspd}.txt\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAlign/SamStats"], "list_wf_names": ["trev-f/SRAlign"]}, {"nb_reuse": 2, "tools": ["MultiQC", "GATK"], "nb_own": 2, "list_own": ["vib-singlecell-nf", "trev-f"], "nb_wf": 2, "list_wf": ["vsn-pipelines", "SRAtac"], "list_contrib": ["dependabot[bot]", "t-f-freeman", "KrisDavie", "trev-f", "ghuls", "dweemx", "cflerin"], "nb_contrib": 7, "codes": ["\nprocess GATK__MARK_DUPLICATES_SPARK {\n\n    container toolParams.container\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sampleId),\n              path(bam)\n\n    output:\n        tuple val(sampleId),\n              path(\"${sampleId}.bwa.out.fixmate.picard_markdup.possorted.bam\"),\n              path(\"${sampleId}.bwa.out.fixmate.picard_markdup.possorted.bam.bai\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams)\n                                            \n        \"\"\"\n        gatk MarkDuplicatesSpark \\\n            -I ${bam} \\\n            -O ${sampleId}.bwa.out.fixmate.picard_markdup.possorted.bam \\\n            -- \\\n            --spark-runner LOCAL \\\n            --spark-master local[${task.cpus}]\n        \"\"\"\n}", "\nprocess ContaminantStatsQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}/align\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        file sFS\n        val runName\n        val toolIDs\n\n    output:\n        path '*'\n\n    script:\n                                       \n        toolIDs += 'mqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        \"\"\"\n        multiqc \\\n            -n ${runName}${suffix} \\\n            --module samtools \\\n            ${sFS}\n        \"\"\"\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/GATK__MARK_DUPLICATES_SPARK", "trev-f/SRAtac/ContaminantStatsQC"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines", "trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess SamtoolsFlagstat {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samtools:1.15--h1170115_1'\n\n    publishDir \"${params.baseDirData}/align/stats\", mode: 'copy', pattern: '*.txt'\n\n    input:\n        tuple val(metadata), path(bam), val(toolIDs)\n\n    output:\n        path '*_sFS.txt', emit: sFS\n        val toolIDs,      emit: tools\n\n    script:\n        toolIDs += 'sFS'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        \"\"\"\n        samtools flagstat ${bam} > ${metadata.sampleName}${suffix}.txt\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/SamtoolsFlagstat"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["seqtk"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess SeqtkSample {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/seqtk:1.3--h7132678_4'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n\n    output:\n        tuple val(metadata), file('*fastq.gz'), val(toolIDs), emit: sampleReads\n\n    script:\n                                       \n        toolIDs += \"skS\"\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        if (metadata.readType == 'single') {\n            \"\"\"\n            seqtk sample -s${task.ext.seed} ${reads} ${task.ext.sampleSize} | \\\n                gzip -c > ${metadata.sampleName}${suffix}.fastq.gz \n            \"\"\"\n        } else {\n            \"\"\"\n            seqtk sample -s${task.ext.seed} ${reads[0]} ${task.ext.sampleSize} | \\\n                gzip -c > ${metadata.sampleName}${suffix}_R1.fastq.gz \n            seqtk sample -s${task.ext.seed} ${reads[1]} ${task.ext.sampleSize} | \\\n                gzip -c > ${metadata.sampleName}${suffix}_R2.fastq.gz \n            \"\"\"\n        }\n}"], "list_proc": ["trev-f/SRAtac/SeqtkSample"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess CompressSortSam {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samtools:1.15--h1170115_1'\n\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*.bam'\n\n    input:\n        tuple val(metadata), file(sam), val(toolIDs)\n\n    output:\n        tuple val(metadata), file('*.bam'), val(toolIDs), emit: bam\n\n    script:\n                                       \n        toolIDs += 'sSR'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        \"\"\"\n        samtools view -bh ${sam} | \\\n        samtools sort -o ${metadata.sampleName}${suffix}.bam -\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/CompressSortSam"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["process Fastp {\n    tag \"${metadata.sampleName}\"\n\n    label 'mem_mid'\n\n    container 'quay.io/biocontainers/fastp:0.23.2--h79da9fb_0'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n\n    output:\n        tuple val(metadata), path('*.fastq.gz'), val(toolIDs), emit: trimReads\n\n    script:\n                                       \n        toolIDs += 'fsp'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        if (metadata.readType == 'single') {\n                                   \n            adapterTrim = params.adapterR1 ? \"--adapter_sequence ${params.adapterR1}\" : ''\n\n            \"\"\"\n            fastp \\\n                ${adapterTrim} \\\n                -i ${reads} \\\n                -o ${metadata.sampleName}${suffix}_R1.fastq.gz\n            \"\"\"\n        } else {\n                                      \n            adapterTrimR1 = params.adapterR1 ? \"--adapter_sequence ${params.adapterR1}\" : ''\n            adapterTrimR2 = params.adapterR2 ? \"--adapter_sequence_r2 ${params.adapterR2}\" : ''\n\n            \"\"\"\n            fastp \\\n                ${adapterTrimR1} \\\n                ${adapterTrimR2} \\\n                -i ${reads[0]} \\\n                -I ${reads[1]} \\\n                -o ${metadata.sampleName}${suffix}_R1.fastq.gz \\\n                -O ${metadata.sampleName}${suffix}_R2.fastq.gz\n            \"\"\"\n        }\n}"], "list_proc": ["trev-f/SRAtac/Fastp"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess Bowtie2Build {\n    tag \"${refName}\"\n\n    container 'quay.io/biocontainers/bowtie2:2.4.5--py38hfbc8389_2'\n\n    input:\n        path reference\n        val refName\n\n    output:\n        path '*', emit: bowtie2Index\n\n    script:\n                             \n        bt2Base = reference.toString() - ~/.fa?/\n        \"\"\"\n        bowtie2-build \\\n            --threads ${task.cpus} \\\n            ${reference} \\\n            ${bt2Base}\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/Bowtie2Build"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess FullMultiQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}\",   mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        val  runName\n        path config\n        path multiqcFiles\n\n    output:\n        path \"*\"\n\n    script:\n        \"\"\"\n        multiqc \\\n            -n ${runName} -i ${runName} \\\n            -c ${config} \\\n            ${multiqcFiles}\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/FullMultiQC"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["process FastQC {\n    tag \"${metadata.sampleName}\"\n\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n\n    publishDir \"${params.baseDirReport}/readsQC/trim/fastQC\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/readsQC/trim/fastQC\",   mode: 'copy', pattern: '*.zip'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n\n    output:\n        path '*.html', emit: html\n        path '*.zip', emit: zip \n        val toolIDs, emit: tools\n\n    script:\n                                       \n        toolIDs += 'fqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n        if (metadata.readType == 'single') {\n            read1 = \"${metadata.sampleName}${suffix}_R1.fastq.gz\"\n\n            \"\"\"\n            [ ! -f ${read1} ] && ln -s ${reads} ${read1}\n\n            fastqc ${read1}\n            \"\"\"\n        } else {\n            read1 = \"${metadata.sampleName}${suffix}_R1.fastq.gz\"\n            read2 = \"${metadata.sampleName}${suffix}_R2.fastq.gz\"\n\n            \"\"\"\n            [ ! -f ${read1} ] && ln -s ${reads[0]} ${read1}\n            [ ! -f ${read2} ] && ln -s ${reads[1]} ${read2}\n\n            fastqc ${read1} ${read2}\n            \"\"\"\n        }\n}"], "list_proc": ["trev-f/SRAtac/FastQC"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["preseq"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess Preseq {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/preseq:3.1.2--h2c25361_3'\n\n    label 'mem_mid'\n\n    publishDir \"${params.baseDirData}/align/preseq\", mode: 'copy', pattern: '*.txt'\n\n    input:\n        tuple val(metadata), path(bam), val(toolIDs)\n\n    output:\n        path '*_psL.txt', emit: psL\n\n    script:\n                                        \n        toolIDspsL = toolIDs\n        toolIDspsL += 'psL'\n        suffixpsL = toolIDspsL ? \"__${toolIDspsL.join('_')}\" : ''\n\n        toolIDs += ['psL']\n\n                              \n        readTypeArg = metadata.readType == 'single' ? '' : '-pe'\n\n        \"\"\"\n        preseq lc_extrap \\\n            -o ${metadata.sampleName}${suffixpsL}.txt \\\n            ${readTypeArg} \\\n            -bam ${bam}\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/Preseq"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess SamStatsMultiQC {\n    tag \"${runName}\"\n\n    container 'ewels/multiqc:v1.11'\n\n    publishDir \"${params.baseDirReport}/align\", mode: 'copy', pattern: '*.html'\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*multiqc_data*'\n\n    input:\n        file sST\n        file sIX\n        val runName\n        val toolIDs\n\n    output:\n        path '*'\n\n    script:\n                                       \n        toolIDs += 'mqc'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n\n        \"\"\"\n        multiqc \\\n            -n ${runName}${suffix} \\\n            --module samtools \\\n            ${sST} \\\n            ${sIX}\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/SamStatsMultiQC"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 2, "tools": ["STAR", "Bowtie"], "nb_own": 2, "list_own": ["trickytank", "trev-f"], "nb_wf": 2, "list_wf": ["rna_circular-nf", "SRAtac"], "list_contrib": ["t-f-freeman", "trickytank", "trev-f"], "nb_contrib": 3, "codes": ["\nprocess alignment_star {\n  label 'star'\n  tag \"$sample_id\"\n\n  input:\n    path star_index_dir\n    tuple sample_id, path(reads)\n    \n  output:\n    tuple sample_id,\n      path(\"out/${sample_id}.Aligned.out.sam\"),\n      path(\"out/${sample_id}.Chimeric.out.junction\")\n    path(\"out/${sample_id}.Log.final.out\")\n\n  script:\n  \"\"\"\n    mkdir out\n\n    STAR \\\n        --chimSegmentMin 10 \\\n        --runThreadN ${task.cpus} \\\n        --genomeDir star_index_dir \\\n        --outSAMstrandField intronMotif \\\n        --readFilesCommand gunzip -c \\\n        --readFilesIn $reads \\\n        --outFileNamePrefix out/${sample_id}.\n  \"\"\"\n                                                  \n                                                          \n                                               \n}", "process Bowtie2Align {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/bowtie2:2.4.5--py38hfbc8389_2'\n\n    label 'cpu_mid'\n    label 'mem_mid'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n        path bt2Indexes\n        val refName\n\n    output:\n        tuple val(metadata), file('*.sam'), val(toolIDs), emit: sam\n\n    script:\n                              \n        if (metadata.readType == 'single') {\n            argReads = \"-1 ${reads}\"\n        } else {\n            argReads = \"-1 ${reads[0]} -2 ${reads[1]}\"\n        }\n\n\n                                       \n        toolIDs += \"bt2-${refName}\"\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n\n                              \n        bt2IndexBaseName = bt2Indexes[0].toString() - ~/.rev.\\d.bt2?/ - ~/.\\d.bt2?/ - ~/.fa?/\n\n                        \n        def options = task.ext.args ?: ''\n\n        \"\"\"\n        bowtie2 \\\n            --threads ${task.cpus} \\\n            ${options} \\\n            -x ${bt2IndexBaseName} \\\n            ${argReads} \\\n            -S ${metadata.sampleName}${suffix}.sam\n        \"\"\"\n}"], "list_proc": ["trickytank/rna_circular-nf/alignment_star", "trev-f/SRAtac/Bowtie2Align"], "list_wf_names": ["trickytank/rna_circular-nf", "trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess IndexBam {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samtools:1.15--h1170115_1'\n\n    publishDir \"${params.baseDirData}/align\", mode: 'copy', pattern: '*.bai'\n\n    input:\n        tuple val(metadata), path(bam), val(toolIDs)\n\n    output:\n        tuple val(metadata), path(bam), path('*.bai'), val(toolIDs), emit: bamBai\n\n    script:\n        \"\"\"\n        samtools index -b ${bam}\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/IndexBam"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["SAMBLASTER"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess Samblaster {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samblaster:0.1.26--h9f5acd7_2'\n\n    input:\n        tuple val(metadata), file(sam), val(toolIDs)\n\n    output:\n        tuple val(metadata), file('*.sam'), val(toolIDs), emit: sam\n\n    script:\n                                       \n        toolIDs += 'sbl'\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n                        \n        def options = task.ext.args ?: ''\n\n        \"\"\"\n        samblaster \\\n            ${options} \\\n            -i ${sam} \\\n            -o ${metadata.sampleName}${suffix}.sam\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/Samblaster"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["HISAT2"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess Hisat2Align {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/hisat2:2.2.1--h87f3376_4'\n\n    label 'cpu_mid'\n    label 'mem_mid'\n\n    input:\n        tuple val(metadata), file(reads), val(toolIDs)\n        path ht2Indexes\n        val refName\n\n    output:\n        tuple val(metadata), file('*.sam'), val(toolIDs), emit: sam\n\n    script:\n                              \n        if (metadata.readType == 'single') {\n            argReads = \"-U ${reads}\"\n        } else {\n            argReads = \"-1 ${reads[0]} -2 ${reads[1]}\"\n        }\n\n                                       \n        toolIDs += \"ht2-${refName}\"\n        suffix = toolIDs ? \"__${toolIDs.join('_')}\" : ''\n\n                              \n        def ht2IndexBaseName = ht2Indexes[0].toString() - ~/.rev.\\d.ht2?/ - ~/.\\d.ht2?/ - ~/.fa?/\n\n                        \n        def options = task.ext.args ?: ''\n\n        \"\"\"\n        hisat2 \\\n            --threads ${task.cpus} \\\n            ${options} \\\n            -x ${ht2IndexBaseName} \\\n            ${argReads} \\\n            -S ${metadata.sampleName}${suffix}.sam\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/Hisat2Align"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["trev-f"], "nb_wf": 1, "list_wf": ["SRAtac"], "list_contrib": ["t-f-freeman", "trev-f"], "nb_contrib": 2, "codes": ["\nprocess SamStats {\n    tag \"${metadata.sampleName}\"\n\n    container 'quay.io/biocontainers/samtools:1.15--h1170115_1'\n\n    publishDir \"${params.baseDirData}/align/stats\", mode: 'copy', pattern: '*.txt'\n\n    input:\n        tuple val(metadata), path(bam), path(bai), val(toolIDs)\n\n    output:\n        path '*_sST.txt', emit: sST\n        path '*_sIX-idxstat.txt', emit: sIX\n        path '*sST-IS.txt', emit: sSTIS\n        path '*_pctDup.txt', emit: pctDup\n        val toolIDs, emit: tools\n\n    script:\n                                       \n        toolIDsST = toolIDs\n        toolIDsST += 'sST'\n        suffixsST = toolIDsST ? \"__${toolIDsST.join('_')}\" : ''\n\n        toolIDsIX = toolIDs\n        toolIDsIX += 'sIX-idxstat'\n        suffixsIX = toolIDsIX ? \"__${toolIDsIX.join('_')}\" : ''\n\n        toolIDsSTIS = toolIDs\n        toolIDsSTIS += 'sST-IS'\n        suffixsSTIS = toolIDsSTIS ? \"__${toolIDsSTIS.join('_')}\" : ''\n\n        toolIDspd = toolIDs\n        toolIDspd += 'pctDup'\n        suffixspd = toolIDspd ? \"__${toolIDspd.join('_')}\" : ''\n\n        toolIDs += ['sST', 'sIX-idxstat']\n\n        \"\"\"\n        samtools stats ${bam} > ${metadata.sampleName}${suffixsST}.txt\n        samtools idxstats ${bam} > ${metadata.sampleName}${suffixsIX}.txt\n\n        # extract insert sizes\n        grep ^IS ${metadata.sampleName}${suffixsST}.txt \\\n            | cut -f 2,3 \\\n            > ${metadata.sampleName}${suffixsSTIS}.txt\n\n        # extract number of mapped reads\n        MAPPED=\\$( \\\n            grep ^SN ${metadata.sampleName}${suffixsST}.txt \\\n            | cut -f 2- \\\n            | grep 'reads mapped:' \\\n            | cut -f 2)\n        # extract number of duplicate reads\n        DUPPED=\\$( \\\n            grep ^SN ${metadata.sampleName}${suffixsST}.txt \\\n            | cut -f 2- \\\n            | grep 'reads duplicated:' \\\n            | cut -f 2)\n        # calculate percent of duplicated reads\n        PCTDUP=\\$(awk -v MAPPED=\\$MAPPED -v DUPPED=\\$DUPPED 'BEGIN{print DUPPED / MAPPED * 100}')\n        echo -e \"${metadata.sampleName}${suffixspd}\\t\\$PCTDUP\" > ${metadata.sampleName}${suffixspd}.txt\n        \"\"\"\n}"], "list_proc": ["trev-f/SRAtac/SamStats"], "list_wf_names": ["trev-f/SRAtac"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["trickytank"], "nb_wf": 1, "list_wf": ["rna_circular-nf"], "list_contrib": ["trickytank"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n  label 'seq_qc'\n\n  input:\n    tuple sample_id, path(reads)\n  output:\n    path '*'\n\n  script:\n  \"\"\"\n    fastqc \\\n      --threads ${task.cpus} \\\n      $reads\n  \"\"\"\n}"], "list_proc": ["trickytank/rna_circular-nf/fastqc"], "list_wf_names": ["trickytank/rna_circular-nf"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["trickytank"], "nb_wf": 1, "list_wf": ["rna_circular-nf"], "list_contrib": ["trickytank"], "nb_contrib": 1, "codes": ["\nprocess create_star_index {\n  label 'star'\n\n  input:\n      path genome                   \n      path gtf         \n\n  output:\n      path \"star_index_dir\"\n\n  script:\n  \"\"\"\n  mkdir star_index_dir\n\n  STAR --runMode genomeGenerate \\\n       --genomeDir star_index_dir \\\n       --genomeFastaFiles ${genome} \\\n       --sjdbGTFfile ${gtf} \\\n       --runThreadN ${task.cpus} \\\n       --genomeSAindexNbases 12\n  ## TODO: allow adjustment of --genomeSAindexNbases outside\n  \"\"\"\n\n}"], "list_proc": ["trickytank/rna_circular-nf/create_star_index"], "list_wf_names": ["trickytank/rna_circular-nf"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["trickytank"], "nb_wf": 1, "list_wf": ["rna_circular-nf"], "list_contrib": ["trickytank"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n  label 'seq_qc'\n\n  input:\n    path files\n\n  output:\n    path '*'\n\n  script:\n  \"\"\"\n  multiqc --interactive .\n  \"\"\"\n}"], "list_proc": ["trickytank/rna_circular-nf/multiqc"], "list_wf_names": ["trickytank/rna_circular-nf"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess ReadTrimming {\n  tag \"Trimming ${pair_id}\"\n  memory threadmem_more\n  cpus 4\n\n  publishDir \"$params.results/${pair_id}\"\n\n  input:\n  tuple val(pair_id), file(mate1), file(mate2) from read_pairs_ch\n\n  output:\n  tuple val(pair_id), file(\"${pair_id}_trim_R1.fastq.gz\"), file(\"${pair_id}_trim_R2.fastq.gz\") into trimmed_reads, readsforqc\n\n  script:\n\n  \"\"\"\n  fastp -i $mate1 -I $mate2 -o ${pair_id}_trim_R1.fastq.gz -O ${pair_id}_trim_R2.fastq.gz\n  \"\"\"\n\n}"], "list_proc": ["tristanpwdennis/basicwgs/ReadTrimming"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess FastQC {\n  tag \"Performing fastqc on ${pair_id}\"\n  input:\n  tuple val(pair_id), file(read1), file(read2) from readsforqc\n  output:\n  file(\"fastqc_${pair_id}_logs\") into fastqc_ch\n\n  script:\n    \"\"\"\n    mkdir fastqc_${pair_id}_logs\n    fastqc -o fastqc_${pair_id}_logs -f fastq -q $read1 $read2\n    \"\"\"  \n}"], "list_proc": ["tristanpwdennis/basicwgs/FastQC"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess bwaAlign{\n  tag \"Aligning ${pair_id} to reference: ${fasta1}\"\n  memory threadmem_more\n  cpus 4\n\n  input:\n  tuple val(pair_id), path(read1), path(read2), file(bwt), file(ann), file(pac), file(sa), file(amb), file(fasta1) from ref1\n\n  output:\n  tuple val(pair_id), file(\"${pair_id}.sam\") into bwa_bam\n\n  script:\n\n  \"\"\"\n  bwa mem -t ${task.cpus} ${fasta1} $read1 $read2 > ${pair_id}.sam\n  \"\"\"\n\n}"], "list_proc": ["tristanpwdennis/basicwgs/bwaAlign"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess bwaSort{\n  tag \"Samtools sorting ${pair_id}\"\n  memory '4 GB'\n  input:\n  tuple val(pair_id), file(sam) from bwa_bam\n\n  output:\n  tuple val(pair_id), file(\"${pair_id}.srt.bam\") into bwa_sorted\n\n  script:\n\n  \"\"\"\n samtools view -bS $sam | samtools sort - -o ${pair_id}.srt.bam\n  \"\"\"\n\n}"], "list_proc": ["tristanpwdennis/basicwgs/bwaSort"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess MarkDuplicates {\n  tag \"Marking duplicate reads for: ${pair_id}\"\n\n  input:\n  tuple val(pair_id), path(bam_file) from bwa_sorted\n\n  output:\n  tuple val(pair_id), file(\"${pair_id}.dupmarked.bam\") into dupmarked_ch\n\n  script:\n  \"\"\"\n  gatk MarkDuplicates \\\n  -I $bam_file \\\n  -M ${pair_id}.metrics \\\n  -O ${pair_id}.dupmarked.bam \n  \"\"\"\n}"], "list_proc": ["tristanpwdennis/basicwgs/MarkDuplicates"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["SAMtools", "GATK"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess AddOrReplaceReadGroups {\n  tag \"Adding read groups to: ${pair_id}\"\n  publishDir \"$params.results/${pair_id}\", mode: 'copy'\n  input:\n  tuple val(pair_id), file(bam_file) from dupmarked_ch\n\n  output:\n  tuple val(pair_id), file(\"${pair_id}.rg.bam\"), file(\"${pair_id}.rg.bai\") into rg_bam, bamqc1, bamqc2, bamqc3\n\n  script:\n  \"\"\"\n  gatk AddOrReplaceReadGroups \\\n  -I ${bam_file} \\\n  -O ${pair_id}.rg.bam \\\n  -LB ${pair_id} \\\n  -PL ILLUMINA \\\n  -PU NA \\\n  -SM ${pair_id} \n\n  samtools index ${pair_id}.rg.bam ${pair_id}.rg.bai\n  \"\"\"\n}"], "list_proc": ["tristanpwdennis/basicwgs/AddOrReplaceReadGroups"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["QualiMap"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess Qualimap {\n  tag \"${pair_id}\"\n  publishDir \"$params.results/${pair_id}\"\n  memory threadmem_more\n\n  input:\n  tuple val(pair_id), file(bam), file(bai) from bamqc1\n\n  output:\n  file (\"${pair_id}\") into qualimap_results\n\n  script:\n  \"\"\"\n  qualimap bamqc -bam ${pair_id}.rg.bam -outdir ${pair_id}\n  \"\"\"\n}"], "list_proc": ["tristanpwdennis/basicwgs/Qualimap"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess FlagstatRun {\n  tag \"Collecting read metadata from ${pair_id}\"\n  publishDir \"$baseDir/results/individual_reports\"\n\n  input:\n  tuple val(pair_id), file(bam), file(bai) from bamqc2\n\n  output:\n  file (\"${pair_id}.stats.txt\") into flagstat_results\n  file (\"${pair_id}.stats.txt\") into flagstat_collect\n\n\n  script:\n  \"\"\"\n  samtools flagstat ${pair_id}.rg.bam > ${pair_id}.stats.txt\n  \"\"\"\n}"], "list_proc": ["tristanpwdennis/basicwgs/FlagstatRun"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["tristanpwdennis"], "nb_wf": 1, "list_wf": ["basicwgs"], "list_contrib": ["tristanpwdennis"], "nb_contrib": 1, "codes": ["\nprocess MultiQC {\n  tag \"Performing multiqc on fastqc output\"\n  publishDir \"$params.results\"\n  input:\n  file('*')  from fastqc_ch.collect()\n  file('*') from qualimap_results.collect()\n  file('*')from flagstat_results.collect()\n\n  output:\n  file('multiqc_report.html')\n\n  script:\n  \"\"\"\n  export LC_ALL=C.UTF-8\n  export LANG=C.UTF-8\n  multiqc .\n  \"\"\"  \n}"], "list_proc": ["tristanpwdennis/basicwgs/MultiQC"], "list_wf_names": ["tristanpwdennis/basicwgs"]}, {"nb_reuse": 1, "tools": ["BamTools"], "nb_own": 1, "list_own": ["twesigomwedavid"], "nb_wf": 1, "list_wf": ["open-science"], "list_contrib": ["twesigomwedavid"], "nb_contrib": 1, "codes": ["\nprocess split_by_chrom {\n\n   input:\n      file(bam) from input_ch\n\n   output:\n      set val(name), file(\"${name}*.bam\") into splitbams_ch\n\n   script:\n      name = bam.baseName.replaceFirst(\".bam\",\"\")\n     \"\"\"\n         bamtools split -in ${bam} -reference \n     \"\"\"\n\n}"], "list_proc": ["twesigomwedavid/open-science/split_by_chrom"], "list_wf_names": ["twesigomwedavid/open-science"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["uct-cbio"], "nb_wf": 1, "list_wf": ["bacterial_variant_calling"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process convertGFFtoGTF {\n      tag \"$gff\"\n\n      input:\n      file gff from gffFile\n\n      output:\n      file \"${gff.baseName}.gtf\" into yolo\n      file \"${gff.baseName}.gff3\" into snpeff_gff, gff_makeBED12, gtf_makeSTARindex, gtf_star, gtf_dupradar, gtf_featureCounts\n\n      script:\n      \"\"\"\n      gffread -E $gff -o ${gff.baseName}.gtf\n      \"\"\"\n  }"], "list_proc": ["uct-cbio/bacterial_variant_calling/convertGFFtoGTF"], "list_wf_names": ["uct-cbio/bacterial_variant_calling"]}, {"nb_reuse": 1, "tools": ["gffread"], "nb_own": 1, "list_own": ["uct-cbio"], "nb_wf": 1, "list_wf": ["bacterial_variant_calling"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": [" process convertGTFtoGFF {\n\n  input:\n  file gtf from gtfFile\n\n  output:\n\n  file \"${gtf.baseName}.gtf\" into gtf_makeSTARindex, gtf_star, gtf_dupradar, gtf_featureCounts\n  file \"${gtf.baseName}.gff3\" into snpeff_gff, gff_makeBED12\n\n  script:\n  \"\"\"\n  gffread $gtf -o ${gtf.baseName}.gff3\n  \"\"\"\n\n  }"], "list_proc": ["uct-cbio/bacterial_variant_calling/convertGTFtoGFF"], "list_wf_names": ["uct-cbio/bacterial_variant_calling"]}, {"nb_reuse": 1, "tools": ["Cutadapt", "Picard", "SAMtools", "MultiQC", "FastQC"], "nb_own": 1, "list_own": ["uct-cbio"], "nb_wf": 1, "list_wf": ["bacterial_variant_calling"], "list_contrib": ["jambler24"], "nb_contrib": 1, "codes": ["\nprocess get_software_versions {\n\n    output:\n    file 'software_versions_mqc.yaml' into software_versions_yaml\n\n    script:\n    \"\"\"\n    echo $workflow.manifest.version &> v_ngi_rnaseq.txt\n    echo $workflow.nextflow.version &> v_nextflow.txt\n    fastqc --version &> v_fastqc.txt                                    # Not working, works in Docker\n    cutadapt --version &> v_cutadapt.txt                                # Working\n    trim_galore --version &> v_trim_galore.txt                          # Working\n    #bwa &> v_bwa.txt                                                   # Working, not parsing\n    #preseq &> v_preseq.txt                                             # Not working libgsl.so.0: cannot open shared object file also in docker\n    read_duplication.py --version &> v_rseqc.txt                        # Working\n    echo \\$(bamCoverage --version 2>&1) > v_deeptools.txt               # unknown\n    picard MarkDuplicates --version &> v_markduplicates.txt  || true    # Not working, not in docker either\n    samtools --version &> v_samtools.txt                                # Working\n    multiqc --version &> v_multiqc.txt                                  # Working\n    #scrape_software_versions.py &> software_versions_mqc.yaml          # unknown\n    echo \"this\" &> software_versions_mqc.yaml\n    \"\"\"\n}"], "list_proc": ["uct-cbio/bacterial_variant_calling/get_software_versions"], "list_wf_names": ["uct-cbio/bacterial_variant_calling"]}, {"nb_reuse": 1, "tools": ["LineagePulse"], "nb_own": 1, "list_own": ["usafsam"], "nb_wf": 1, "list_wf": ["mad_river_wf"], "list_contrib": ["fanninpm", "friesac"], "nb_contrib": 2, "codes": ["\nprocess LINEAGE_EXCEL {\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"logs/${task.process}.{log,err}\"\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"*_lineage.xlsx\"\n    echo false\n    conda \"${workflow.projectDir}/env/performance_lineage_excel.yml\"\n\n    input:\n        file(pangolin_csv)\n        file(nextclade_csv)\n        file(vadr_annotations)\n\n    output:\n        path(\"${file(params.outdir).getSimpleName()}_lineage.xlsx\")\n        path(\"logs/${task.process}.{log,err}\")\n\n    shell:\n    '''\n        mkdir -p !{task.process} logs/\n        log_file=logs/!{task.process}.log\n        err_file=logs/!{task.process}.err\n\n        # time stamp + capturing tool versions\n        date | tee -a $log_file $err_file > /dev/null\n        python3 --version >> $log_file\n\n        performance_lineage_excel.py $(basename !{params.outdir}) \\\n            lineage \\\n            --pangolin_summary !{pangolin_csv} \\\n            --nextclade_summary !{nextclade_csv} \\\n            --vadr_annotations !{vadr_annotations} \\\n            2>> $err_file >> $log_file\n    '''\n}"], "list_proc": ["usafsam/mad_river_wf/LINEAGE_EXCEL"], "list_wf_names": ["usafsam/mad_river_wf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "AIVAR"], "nb_own": 1, "list_own": ["usafsam"], "nb_wf": 1, "list_wf": ["mad_river_wf"], "list_contrib": ["fanninpm", "friesac"], "nb_contrib": 2, "codes": ["process IVAR_CONSENSUS {\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"logs/${task.process}/*.{log,err}\"\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"${task.process}/*.consensus.fa\"\n    tag \"${sample}\"\n    echo false\n    container params.container_ivar\n    memory {2.GB * task.attempt}\n    errorStrategy {'retry'}\n    maxRetries 2\n\n    input:\n        tuple val(sample), file(bamfile)\n\n    output:\n        tuple val(sample), path(\"${task.process}/${sample}.consensus.fa\"), emit: consensus\n        path(\"${task.process}/${sample}.consensus.fa\"), emit: consensus_collect\n        path(\"logs/${task.process}/${sample}.${workflow.sessionId}.{log,err}\")\n\n    shell:\n    '''\n        mkdir -p !{task.process} logs/!{task.process}\n        log_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.log\n        err_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.err\n\n        # time stamp + capturing tool versions\n        date | tee -a $log_file $err_file > /dev/null\n        samtools --version >> $log_file\n        ivar --version >> $log_file\n\n        samtools mpileup -aa -A -Q 0 !{bamfile} 2>> $err_file | \\\n        ivar consensus -p !{task.process}/!{sample}.consensus \\\n            -t 0.6 \\\n            -m !{params.mincov} \\\n            -i !{sample} \\\n            2>> $err_file >> $log_file\n    '''\n}"], "list_proc": ["usafsam/mad_river_wf/IVAR_CONSENSUS"], "list_wf_names": ["usafsam/mad_river_wf"]}, {"nb_reuse": 1, "tools": ["restrict"], "nb_own": 1, "list_own": ["usafsam"], "nb_wf": 1, "list_wf": ["mad_river_wf"], "list_contrib": ["fanninpm", "friesac"], "nb_contrib": 2, "codes": ["\nprocess BBMERGE {\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"${task.process}/*_merge_ihist.txt\"\n    publishDir \"${params.outdir}\", mode: 'copy', pattern: \"logs/${task.process}/*.{log,err}\"\n    tag \"${sample}\"\n    echo false\n    cpus params.medcpus\n    container params.container_bbtools\n    memory 1.GB\n\n    input:\n        tuple val(sample), file(specimen)\n\n    output:\n        tuple val(sample), path(\"${task.process}/${sample}_merged.fq.gz\"), emit: bbmerged_specimens\n        path(\"${task.process}/${sample}_merge_ihist.txt\")\n        path(\"logs/${task.process}/${sample}.${workflow.sessionId}.{log,err}\")\n\n    shell:\n    '''\n        mkdir -p !{task.process} logs/!{task.process}\n        log_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.log\n        err_file=logs/!{task.process}/!{sample}.!{workflow.sessionId}.err\n\n        # time stamp + capturing tool versions\n        date | tee -a $log_file $err_file > /dev/null\n        echo \"BBTools bbmerge.sh: $(bbmerge.sh -h | grep 'Last modified')\" >> $log_file\n\n        bbmerge.sh \\\n            in=!{specimen} \\\n            out=!{task.process}/!{sample}_merged.fq.gz \\\n            adapter=default \\\n            strict \\\n            ihist=!{task.process}/!{sample}_merge_ihist.txt \\\n            2>> $err_file >> $log_file\n    '''\n}"], "list_proc": ["usafsam/mad_river_wf/BBMERGE"], "list_wf_names": ["usafsam/mad_river_wf"]}, {"nb_reuse": 2, "tools": ["Roary", "FastQC", "BCFtools", "MultiQC"], "nb_own": 3, "list_own": ["wslh-bio", "zamanianlab", "veitveit"], "nb_wf": 2, "list_wf": ["nf-core-wombatp", "RNAseq-VC-nf", "dryad"], "list_contrib": ["veitveit", "AbigailShockey", "wheelern", "k-florek"], "nb_contrib": 4, "codes": ["\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastqc --version > v_fastqc.txt\n    multiqc --version > v_multiqc.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}", "\nprocess minimum_depth {\n\n      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*_filter.bcf'\n\n      input:\n          tuple val(id), file(vcf) from quality_filter\n\n      output:\n          tuple val(id), file(\"${id}_3_filter.bcf\") into min_depth_filter\n\n      when:\n          params.vcf\n\n      \"\"\"\n          bcftools filter --threads 8 -e 'INFO/DP < 10' -Ob -o ${id}_3_filter.bcf ${vcf}\n          bcftools index ${id}_3_filter.bcf\n      \"\"\"\n}", "\nprocess roary {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  numGenomes = 0\n  input:\n  file(genomes) from annotated_genomes.collect()\n\n  output:\n  file(\"core_gene_alignment.aln\") into core_aligned_genomes\n  file(\"core_genome_statistics.txt\") into core_aligned_stats\n\n  script:\n  if(params.roary_mafft == true){\n    mafft=\"-n\"\n  }else{mafft=\"\"}\n  \"\"\"\n  cpus=`grep -c ^processor /proc/cpuinfo`\n  roary -e ${mafft} -p \\$cpus ${genomes}\n  mv summary_statistics.txt core_genome_statistics.txt\n  \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/minimum_depth", "wslh-bio/dryad/roary"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf", "wslh-bio/dryad"]}, {"nb_reuse": 0, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["veitveit"], "nb_wf": 0, "list_wf": ["nf-core-wombatp"], "list_contrib": ["veitveit"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n\n    input:\n    file (multiqc_config) from ch_multiqc_config\n    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n                                                                                  \n    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from ch_software_versions_yaml.collect()\n    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n\n    output:\n    file \"*multiqc_report.html\" into ch_multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename $custom_config_file .\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["vib-singlecell-nf"], "nb_wf": 1, "list_wf": ["vsn-pipelines"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 5, "codes": ["\nprocess SC__STAR__UNLOAD_GENOME {\n\n\tcontainer params.tools.star.container\n    label 'compute_resources__default'\n\n\tinput:\n\t\tfile(transcriptome)\n\t\tval allDone\n\n\tscript:\n\t\t\"\"\"\n\t\techo \"--genomeDir ${transcriptome}\"\n\t\tSTAR \\\n\t\t\t--genomeLoad Remove \\\n\t\t\t--genomeDir ${transcriptome}\n\t\t\"\"\"\n\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/SC__STAR__UNLOAD_GENOME"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["vib-singlecell-nf"], "nb_wf": 1, "list_wf": ["vsn-pipelines"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 5, "codes": ["\nprocess BWAMAPTOOLS__BWA_MEM_PE {\n\n    container toolParams.container\n    label 'compute_resources__bwa_mem'\n\n    input:\n        tuple path(bwa_fasta),\n              path(bwa_index),\n              val(unique_sampleId),\n              val(sampleId),\n              path(fastq_PE1),\n              path(fastq_PE2)\n\n    output:\n        tuple val(sampleId),\n              path(\"${sampleId}.bwa.out.fixmate.bam\")\n\n    script:\n        def sampleParams = params.parseConfig(unique_sampleId, params.global, toolParams)\n        processParams = sampleParams.local\n        \"\"\"\n        id=\\$(zcat ${fastq_PE1} | head -n 1 | cut -f 1-4 -d':' | sed 's/@//')\n        ${toolParams.bwa_version} mem \\\n            -t ${task.cpus} \\\n            -C \\\n            -R \"@RG\\\\tID:\\${id}\\\\tSM:${unique_sampleId}\\\\tLB:\\${id}\"__\"${unique_sampleId}\\\\tPL:ILLUMINA\" \\\n            ${bwa_fasta} \\\n            ${fastq_PE1} \\\n            ${fastq_PE2} \\\n        | samtools fixmate -m -O bam - ${sampleId}.bwa.out.fixmate.bam\n        \"\"\"\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/BWAMAPTOOLS__BWA_MEM_PE"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["fastPHASE"], "nb_own": 1, "list_own": ["vib-singlecell-nf"], "nb_wf": 1, "list_wf": ["vsn-pipelines"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 5, "codes": ["\nprocess FASTP__ADAPTER_TRIMMING {\n\n    container toolParams.container\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sampleId),\n              path(fastq_PE1),\n              path(fastq_PE2)\n\n    output:\n        tuple val(sampleId),\n              path(\"${sampleId}_dex_R1_val_1.fq.gz\"),\n              path(\"${sampleId}_dex_R2_val_2.fq.gz\"),\n              path(\"${sampleId}_fastp.html\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams)\n        processParams = sampleParams.local\n        def max_threads = (task.cpus > 6) ? 6 : task.cpus\n        \"\"\"\n        fastp \\\n            --in1 ${fastq_PE1} \\\n            --in2 ${fastq_PE2} \\\n            --out1 ${sampleId}_dex_R1_val_1.fq.gz \\\n            --out2 ${sampleId}_dex_R2_val_2.fq.gz \\\n            --detect_adapter_for_pe \\\n            --html ${sampleId}_fastp.html \\\n            --thread ${max_threads}\n        \"\"\"\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/FASTP__ADAPTER_TRIMMING"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["Computel"], "nb_own": 1, "list_own": ["vib-singlecell-nf"], "nb_wf": 1, "list_wf": ["vsn-pipelines"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 5, "codes": ["\nprocess SC__SCANPY__COMPUTE_QC_STATS {\n\n  \tcontainer params.tools.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n  \tinput:\n        tuple val(sampleId), path(f)\n\n\toutput:\n        tuple val(sampleId), path(\"${sampleId}.SC__SCANPY__COMPUTE_QC_STATS.${processParams.off}\")\n\n\tscript:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.tools.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            compute \\\n            $f \\\n            ${sampleId}.SC__SCANPY__COMPUTE_QC_STATS.${processParams.off} \\\n            ${processParams?.cellFilterStrategy ? '--cell-filter-strategy ' + processParams.cellFilterStrategy : ''} \\\n            ${processParams?.cellFilterMinNCounts ? '--min-n-counts ' + processParams.cellFilterMinNCounts : ''} \\\n            ${processParams?.cellFilterMaxNCounts ? '--max-n-counts ' + processParams.cellFilterMaxNCounts : ''} \\\n            ${processParams?.cellFilterMinNGenes ? '--min-n-genes ' + processParams.cellFilterMinNGenes : ''} \\\n            ${processParams?.cellFilterMaxNGenes ? '--max-n-genes ' + processParams.cellFilterMaxNGenes : ''} \\\n            ${processParams?.cellFilterMaxPercentMito ? '--max-percent-mito ' + processParams.cellFilterMaxPercentMito : ''} \\\n            ${processParams?.geneFilterMinNCells ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"\n\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/SC__SCANPY__COMPUTE_QC_STATS"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["vib-singlecell-nf"], "nb_wf": 1, "list_wf": ["vsn-pipelines"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 5, "codes": ["\nprocess PICARD__ESTIMATE_LIBRARY_COMPLEXITY {\n\n    container toolParams.container\n    label 'compute_resources__default','compute_resources__24hqueue'\n\n    input:\n        tuple val(sampleId),\n              path(bam)\n\n    output:\n        tuple val(sampleId),\n              path(\"${sampleId}.picard_library_complexity_metrics.txt\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams.estimate_library_complexity)\n        processParams = sampleParams.local\n        \"\"\"\n        gatk EstimateLibraryComplexity \\\n            -I ${bam} \\\n            -O ${sampleId}.picard_library_complexity_metrics.txt \\\n            --BARCODE_TAG ${processParams.barcode_tag} \\\n        \"\"\"\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/PICARD__ESTIMATE_LIBRARY_COMPLEXITY"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["vib-singlecell-nf"], "nb_wf": 1, "list_wf": ["vsn-pipelines"], "list_contrib": ["dependabot[bot]", "KrisDavie", "ghuls", "dweemx", "cflerin"], "nb_contrib": 5, "codes": ["\nprocess SC__STAR__SOLO_MAP_COUNT {\n\n  container params.tools.star.container\n  label 'compute_resources__star_map_count'\n\n  input:\n    file(transcriptome)\n    val genome_loaded\n    file(fastqs)\n\n  output:\n    val success\n                                   \n\n  script:\n    sample = fastqs.getName()\n    _sampleName = sample\n    success = true\n\n    \"\"\"\n    STAR \\\n      --genomeLoad LoadAndKeep \\\n      --soloType Droplet \\\n      --genomeDir ${transcriptome} \\\n      --runThreadN ${task.cpus} \\\n      ${(params.tools.star.map_count.containsKey('limitBAMsortRAM')) ? '--limitBAMsortRAM ' + params.tools.star.map_count.limitBAMsortRAM: ''} \\\n      ${(params.tools.star.map_count.containsKey('outSAMtype')) ? '--outSAMtype ' + params.tools.star.map_count.outSAMtype: ''} \\\n      ${(params.tools.star.map_count.containsKey('quantMode')) ? '--quantMode ' + params.tools.star.map_count.quantMode: ''} \\\n      ${(params.tools.star.map_count.containsKey('outReadsUnmapped')) ? '--outReadsUnmapped ' + params.tools.star.map_count.outReadsUnmapped: ''} \\\n      --readFilesIn ${fastqs} \\\n      ${(fastqs.name.endsWith(\".gz\")) ? '--readFilesCommand zcat' : ''} \\\n      --outFileNamePrefix ${_sampleName}\n    \"\"\"\n}"], "list_proc": ["vib-singlecell-nf/vsn-pipelines/SC__STAR__SOLO_MAP_COUNT"], "list_wf_names": ["vib-singlecell-nf/vsn-pipelines"]}, {"nb_reuse": 2, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 2, "list_wf": ["chipseq-nextflow", "rnaseq-editing"], "list_contrib": ["tmuylder", "alex-botzki", "abotzki"], "nb_contrib": 3, "codes": ["\nprocess star_idx {\n    label 'high'\n    container \"quay.io/biocontainers/star:2.6.1d--0\"\n\n    input:\n    path genome\n    path gtf\n    \n    output:\n    path \"index_dir/\", emit: index\n\n    script:\n    \"\"\"\n    mkdir index_dir\n    \n    STAR --runThreadN $task.cpus \\\\\n      --runMode genomeGenerate \\\\\n      --genomeDir index_dir/ \\\\\n      --genomeFastaFiles $genome \\\\\n      --genomeSAindexNbases $params.genomeSAindexNbases \\\\\n      --sjdbGTFfile $gtf\n    \"\"\"\n}", "\nprocess SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }    \n\nif (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.12--hd5e65b6_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"*.version.txt\"           , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    samtools stats $bam > ${bam}.stats\n    echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["vibbits/chipseq-nextflow/star_idx", "vibbits/rnaseq-editing/SAMTOOLS_STATS"], "list_wf_names": ["vibbits/rnaseq-editing", "vibbits/chipseq-nextflow"]}, {"nb_reuse": 2, "tools": ["STAR", "BEDTools"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 2, "list_wf": ["chipseq-nextflow", "rnaseq-editing"], "list_contrib": ["tmuylder", "alex-botzki", "abotzki"], "nb_contrib": 3, "codes": ["\nprocess BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0\"\n    } else {\n        container \"quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.forward.bedGraph\"), emit: bedgraph_forward\n    tuple val(meta), path(\"*.reverse.bedGraph\"), emit: bedgraph_reverse\n    path \"*.version.txt\"                       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def prefix_forward = \"${prefix}.forward\"\n    def prefix_reverse = \"${prefix}.reverse\"\n    if (meta.strandedness == 'reverse') {\n        prefix_forward = \"${prefix}.reverse\"\n        prefix_reverse = \"${prefix}.forward\"\n    }\n    \"\"\"\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand + \\\\\n        $options.args \\\\\n        | bedtools sort > ${prefix_forward}.bedGraph\n\n    bedtools \\\\\n        genomecov \\\\\n        -ibam $bam \\\\\n        -bg \\\\\n        -strand - \\\\\n        $options.args \\\\\n        | bedtools sort > ${prefix_reverse}.bedGraph\n\n    bedtools --version | sed -e \"s/bedtools v//g\" > ${software}.version.txt\n    \"\"\"\n}", "\nprocess star_alignment {\n    publishDir \"$params.outdir/mapped-reads/\", mode: 'copy', overwrite: true  //, pattern: \"*.bam\"  \n    label 'high'\n    container \"quay.io/biocontainers/star:2.6.1d--0\"\n\n    input:\n                              \n    tuple val(sample), path(reads) \n    path indexDir\n    path gtf\n\n    output:\n                                                         \n    path(\"*.bam\"), emit: align_bam\n\n    script:\n    \"\"\"\n    mkdir -p $params.outdir/mapped-reads/\n\n\n    STAR  \\\\\n        --readFilesIn ${reads} \\\\\n        --runThreadN $task.cpus \\\\\n        --outSAMtype BAM SortedByCoordinate \\\\\n        --sjdbGTFfile ${gtf} \\\\\n        --outFileNamePrefix $sample. \\\\\n        --genomeDir ${indexDir}\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/BEDTOOLS_GENOMECOV", "vibbits/chipseq-nextflow/star_alignment"], "list_wf_names": ["vibbits/rnaseq-editing", "vibbits/chipseq-nextflow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["chipseq-nextflow"], "list_contrib": ["tmuylder"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n  publishDir \"$params.outdir/quality-control-$sample/\", mode: 'copy', overwrite: true\n  label 'low'\n  container 'quay.io/biocontainers/fastqc:0.11.9--0'\n  \n  input:\n  tuple val(sample), path(reads)\n\n  output:\n  path(\"*_fastqc.{zip,html}\"), emit: fastqc_out\n\n  script:\n                                                      \n  \"\"\"\n  fastqc ${reads}\n  \"\"\"\n}"], "list_proc": ["vibbits/chipseq-nextflow/fastqc"], "list_wf_names": ["vibbits/chipseq-nextflow"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["chipseq-nextflow"], "list_contrib": ["tmuylder"], "nb_contrib": 1, "codes": ["\nprocess bowtie_idx {\n    publishDir \"$params.refdir/bt2idx/\", mode: 'copy', pattern: \"*.bt2\"  \n    label 'high'\n    container \"quay.io/biocontainers/bowtie2:2.2.5--py38h8c62d01_8\"\n\n    input:\n    path genome\n    \n    output:\n    tuple val(\"${params.idxname}\"), path(\"${params.idxname}*\"), emit: index\n\n                                                 \n\n    script:\n    \"\"\"\n    bowtie2-build ${genome} ${params.idxname}\n    \"\"\"\n}"], "list_proc": ["vibbits/chipseq-nextflow/bowtie_idx"], "list_wf_names": ["vibbits/chipseq-nextflow"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["chipseq-nextflow"], "list_contrib": ["tmuylder"], "nb_contrib": 1, "codes": ["\nprocess bowtie_alignment {\n    publishDir \"$params.outdir/mapped-reads/\", mode: 'copy', overwrite: true  //, pattern: \"*.bam\"  \n    label 'high'\n    container \"quay.io/biocontainers/bowtie2:2.2.5--py38h8c62d01_8\"\n\n    input:\n    tuple val(sample), path(reads) \n    tuple val(bt2idx), path(bt2idx_files)\n\n    output:\n    path(\"${sample}.sam\"), emit: aligend_reads_sam\n\n    script:\n    \"\"\"\n    bowtie2 \\\\\n        --dovetail \\\\\n        --phred33 \\\\\n        -x ${bt2idx} \\\\\n        -1 ${reads[0]} \\\\\n        -2 ${reads[1]} \\\\\n        -S ${sample}.sam\n    \"\"\"\n                                                                                 \n                                                                               \n}"], "list_proc": ["vibbits/chipseq-nextflow/bowtie_alignment"], "list_wf_names": ["vibbits/chipseq-nextflow"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["nextflow-jnj"], "list_contrib": ["tmuylder"], "nb_contrib": 1, "codes": ["\nprocess fastqc {\n  publishDir \"$params.outdir/quality-control-$sample/\", mode: 'copy', overwrite: true\n    \n  input:\n  tuple val(sample), path(reads)                    \n\n  script:\n  \"\"\"\n  mkdir -p $params.outdir/quality-control-$sample\n  fastqc --outdir $params.outdir/quality-control-$sample ${reads}\n  \"\"\"\n}"], "list_proc": ["vibbits/nextflow-jnj/fastqc"], "list_wf_names": ["vibbits/nextflow-jnj"]}, {"nb_reuse": 1, "tools": ["Trimmomatic"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["nextflow-jnj"], "list_contrib": ["tmuylder"], "nb_contrib": 1, "codes": ["\nprocess trimmomatic {\n    publishDir \"$params.outdir/trimmed-reads\", mode: 'copy', overwrite: true\n\n                                                                       \n    input:\n    tuple val(sample), path(reads) \n\n    output:\n    tuple val(\"${sample}\"), path(\"${sample}*_P.fq\"), emit: trim_fq\n    tuple val(\"${sample}\"), path(\"${sample}*_U.fq\"), emit: untrim_fq\n\n    script:\n    \"\"\"\n    mkdir -p $params.outdir/trimmed-reads/\n    trimmomatic PE -threads $params.threads ${reads[0]} ${reads[1]} ${sample}1_P.fq ${sample}1_U.fq ${sample}2_P.fq ${sample}2_U.fq $params.slidingwindow $params.avgqual \n    \"\"\"\n}"], "list_proc": ["vibbits/nextflow-jnj/trimmomatic"], "list_wf_names": ["vibbits/nextflow-jnj"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["nextflow-jnj"], "list_contrib": ["tmuylder"], "nb_contrib": 1, "codes": ["\nprocess multiqc {\n    publishDir(\"$params.outdir/multiqc/\", mode: 'copy', overwrite: true)\n    label 'low'\n    container 'quay.io/biocontainers/multiqc:1.9--py_1'\n\n    input:\n    path (inputfiles)\n\n    output:\n    path \"multiqc_report.html\"\t\t\t\t\t\n\n    script:\n    \"\"\"\n    mkdir -p $params.outdir/multiqc/\n    multiqc .\n    \"\"\"\n}"], "list_proc": ["vibbits/nextflow-jnj/multiqc"], "list_wf_names": ["vibbits/nextflow-jnj"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess HISAT2_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::hisat2=2.2.0 bioconda::samtools=1.10\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-a97e90b3b802d1da3d6958e0867610c718cb5eb1:2880dd9d8ad0a7b221d4eacda9a818e92983128d-0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  splicesites\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    tuple val(meta), path(\"*.log\"), emit: summary\n    path  \"*.version.txt\"         , emit: version\n\n    tuple val(meta), path(\"*fastq.gz\"), optional:true, emit: fastq\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def strandedness = ''\n    if (meta.strandedness == 'forward') {\n        strandedness = meta.single_end ? '--rna-strandness F' : '--rna-strandness FR'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = meta.single_end ? '--rna-strandness R' : '--rna-strandness RF'\n    }\n    def seq_center = params.seq_center ? \"--rg-id ${prefix} --rg SM:$prefix --rg CN:${params.seq_center.replaceAll('\\\\s','_')}\" : \"--rg-id ${prefix} --rg SM:$prefix\"\n    if (meta.single_end) {\n        def unaligned = params.save_unaligned ? \"--un-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -U $reads \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            $options.args \\\\\n            | samtools view -bS -F 4 -F 256 - > ${prefix}.bam\n\n        echo $VERSION > ${software}.version.txt\n        \"\"\"\n    } else {\n        def unaligned = params.save_unaligned ? \"--un-conc-gz ${prefix}.unmapped.fastq.gz\" : ''\n        \"\"\"\n        INDEX=`find -L ./ -name \"*.1.ht2\" | sed 's/.1.ht2//'`\n        hisat2 \\\\\n            -x \\$INDEX \\\\\n            -1 ${reads[0]} \\\\\n            -2 ${reads[1]} \\\\\n            $strandedness \\\\\n            --known-splicesite-infile $splicesites \\\\\n            --summary-file ${prefix}.hisat2.summary.log \\\\\n            --threads $task.cpus \\\\\n            $seq_center \\\\\n            $unaligned \\\\\n            --no-mixed \\\\\n            --no-discordant \\\\\n            $options.args \\\\\n            | samtools view -bS -F 4 -F 8 -F 256 - > ${prefix}.bam\n\n        if [ -f ${prefix}.unmapped.fastq.1.gz ]; then\n            mv ${prefix}.unmapped.fastq.1.gz ${prefix}.unmapped_1.fastq.gz\n        fi\n        if [ -f ${prefix}.unmapped.fastq.2.gz ]; then\n            mv ${prefix}.unmapped.fastq.2.gz ${prefix}.unmapped_2.fastq.gz\n        fi\n\n        echo $VERSION > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["vibbits/rnaseq-editing/HISAT2_ALIGN"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--pyhdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--pyhdfd78af_1\"\n    }\n\n    input:\n    path multiqc_config\n    path multiqc_custom_config\n    path software_versions\n    path workflow_summary\n    path fail_mapping_summary\n    path fail_strand_check\n    path ('fastqc_trim/*')\n    path ('fastqc/*')\n    path ('star/*')\n    path ('qualimap/*')\n    path ('dupradar/*')\n    path ('rseqc/bam_stat/*')\n    path ('rseqc/infer_experiment/*')\n    path ('rseqc/inner_distance/*')\n    path ('rseqc/junction_annotation/*')\n    path ('rseqc/junction_saturation/*')\n    path ('rseqc/read_distribution/*')\n    path ('rseqc/read_duplication/*')\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def custom_config = params.multiqc_config ? \"--config $multiqc_custom_config\" : ''\n    \"\"\"\n    multiqc -f $options.args $custom_config .\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/MULTIQC"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["STAR", "Mgenome"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess RSEM_PREPAREREFERENCE {\n    tag \"$fasta\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::rsem=1.3.3 bioconda::star=2.7.6a\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-cf0123ef83b3c38c13e3b0696a3f285d3f20f15b:606b713ec440e799d53a2b51a6e79dbfd28ecf3e-0\"\n    }\n\n    input:\n    path fasta, stageAs: \"rsem/*\"\n    path gtf\n\n    output:\n    path \"rsem\"                , emit: index\n    path \"rsem/*transcripts.fa\", emit: transcript_fasta\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def args     = options.args.tokenize()\n    if (args.contains('--star')) {\n        args.removeIf { it.contains('--star') }\n        def memory = task.memory ? \"--limitGenomeGenerateRAM ${task.memory.toBytes() - 100000000}\" : ''\n        \"\"\"\n        STAR \\\\\n            --runMode genomeGenerate \\\\\n            --genomeDir rsem/ \\\\\n            --genomeFastaFiles $fasta \\\\\n            --sjdbGTFfile $gtf \\\\\n            --runThreadN $task.cpus \\\\\n            $memory \\\\\n            $options.args2\n\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            ${args.join(' ')} \\\\\n            $fasta \\\\\n            rsem/genome\n\n        rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        rsem-prepare-reference \\\\\n            --gtf $gtf \\\\\n            --num-threads $task.cpus \\\\\n            $options.args \\\\\n            $fasta \\\\\n            rsem/genome\n\n        rsem-calculate-expression --version | sed -e \"s/Current version: RSEM v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["vibbits/rnaseq-editing/RSEM_PREPAREREFERENCE"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["Salmon"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess SALMON_INDEX {\n    tag \"$transcript_fasta\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'index', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::salmon=1.4.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/salmon:1.4.0--hf69c8f4_0\"\n    } else {\n        container \"quay.io/biocontainers/salmon:1.4.0--hf69c8f4_0\"\n    }\n\n    input:\n    path genome_fasta\n    path transcript_fasta\n\n    output:\n    path \"salmon\"       , emit: index\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software      = getSoftwareName(task.process)\n    def get_decoy_ids = \"grep '^>' $genome_fasta | cut -d ' ' -f 1 > decoys.txt\"\n    def gentrome      = \"gentrome.fa\"\n    if (genome_fasta.endsWith('.gz')) {\n        get_decoy_ids = \"grep '^>' <(gunzip -c $genome_fasta) | cut -d ' ' -f 1 > decoys.txt\"\n        gentrome      = \"gentrome.fa.gz\"\n    }\n    \"\"\"\n    $get_decoy_ids\n    sed -i.bak -e 's/>//g' decoys.txt\n    cat $transcript_fasta $genome_fasta > $gentrome\n\n    salmon \\\\\n        index \\\\\n        --threads $task.cpus \\\\\n        -t $gentrome \\\\\n        -d decoys.txt \\\\\n        $options.args \\\\\n        -i salmon\n    salmon --version | sed -e \"s/salmon //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/SALMON_INDEX"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["RNASEQR", "QualiMap"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess QUALIMAP_RNASEQ {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::qualimap=2.2.2d\" : null)\n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n    \n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/qualimap:2.2.2d--1\"\n    } else {\n        container \"quay.io/biocontainers/qualimap:2.2.2d--1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n    path  gtf\n\n    output:\n    tuple val(meta), path(\"${prefix}\"), emit: results\n    path  \"*.version.txt\"             , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    prefix         = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    def memory     = task.memory.toGiga() + \"G\"\n\n    def strandedness = 'non-strand-specific'\n    if (meta.strandedness == 'forward') {\n        strandedness = 'strand-specific-forward'\n    } else if (meta.strandedness == 'reverse') {\n        strandedness = 'strand-specific-reverse'\n    }\n    \"\"\"\n    unset DISPLAY\n    mkdir tmp\n    export _JAVA_OPTIONS=-Djava.io.tmpdir=./tmp\n    qualimap \\\\\n        --java-mem-size=$memory \\\\\n        rnaseq \\\\\n        $options.args \\\\\n        -bam $bam \\\\\n        -gtf $gtf \\\\\n        -p $strandedness \\\\\n        $paired_end \\\\\n        -outdir $prefix\n\n    echo \\$(qualimap 2>&1) | sed 's/^.*QualiMap v.//; s/Built.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/QUALIMAP_RNASEQ"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess STAR_ALIGN {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n                                                         \n    conda (params.enable_conda ? 'bioconda::star=2.7.3a' : null)\n    \n    if (params.enable_aks) {\n       pod nodeSelector: 'agentpool=cpumem'\n    }\n\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/star:2.7.3a--0'\n    } else {\n        container 'quay.io/biocontainers/star:2.7.3a--0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    path  gtf\n\n    output:\n    tuple val(meta), path('*d.out.bam')       , emit: bam\n    tuple val(meta), path('*Log.final.out')   , emit: log_final\n    tuple val(meta), path('*Log.out')         , emit: log_out\n    tuple val(meta), path('*Log.progress.out'), emit: log_progress\n    path  '*.version.txt'                     , emit: version\n\n    tuple val(meta), path('*sortedByCoord.out.bam')  , optional:true, emit: bam_sorted\n    tuple val(meta), path('*toTranscriptome.out.bam'), optional:true, emit: bam_transcript\n    tuple val(meta), path('*Aligned.unsort.out.bam') , optional:true, emit: bam_unsorted\n    tuple val(meta), path('*fastq.gz')               , optional:true, emit: fastq\n    tuple val(meta), path('*.tab')                   , optional:true, emit: tab\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def ignore_gtf = params.star_ignore_sjdbgtf ? '' : \"--sjdbGTFfile $gtf\"\n    def seq_center = params.seq_center ? \"--outSAMattrRGline ID:$prefix 'CN:$params.seq_center' 'SM:$prefix'\" : \"--outSAMattrRGline ID:$prefix 'SM:$prefix'\"\n    def out_sam_type = (options.args.contains('--outSAMtype')) ? '' : '--outSAMtype BAM Unsorted'\n    def mv_unsorted_bam = (options.args.contains('--outSAMtype BAM Unsorted SortedByCoordinate')) ? \"mv ${prefix}.Aligned.out.bam ${prefix}.Aligned.unsort.out.bam\" : ''\n    \"\"\"\n    STAR \\\\\n        --genomeDir $index \\\\\n        --readFilesIn $reads  \\\\\n        --runThreadN $task.cpus \\\\\n        --outFileNamePrefix $prefix. \\\\\n        $out_sam_type \\\\\n        $ignore_gtf \\\\\n        $seq_center \\\\\n        $options.args\n\n    $mv_unsorted_bam\n\n    if [ -f ${prefix}.Unmapped.out.mate1 ]; then\n        mv ${prefix}.Unmapped.out.mate1 ${prefix}.unmapped_1.fastq\n        gzip ${prefix}.unmapped_1.fastq\n    fi\n    if [ -f ${prefix}.Unmapped.out.mate2 ]; then\n        mv ${prefix}.Unmapped.out.mate2 ${prefix}.unmapped_2.fastq\n        gzip ${prefix}.unmapped_2.fastq\n    fi\n\n    STAR --version | sed -e \"s/STAR_//g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/STAR_ALIGN"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["SortMeRna"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess SORTMERNA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::sortmerna=4.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/sortmerna:4.2.0--0\"\n    } else {\n        container \"quay.io/biocontainers/sortmerna:4.2.0--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  fasta\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: reads\n    tuple val(meta), path(\"*.log\")     , emit: log\n    path  \"*.version.txt\"              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    def Refs = \"\"\n    for (i=0; i<fasta.size(); i++) { Refs+= \" --ref ${fasta[i]}\" }\n    if (meta.single_end) {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads $reads \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            $options.args\n\n        gzip -f < non_rRNA_reads.fq > ${prefix}.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        sortmerna \\\\\n            $Refs \\\\\n            --reads ${reads[0]} \\\\\n            --reads ${reads[1]} \\\\\n            --threads $task.cpus \\\\\n            --workdir . \\\\\n            --aligned rRNA_reads \\\\\n            --other non_rRNA_reads \\\\\n            --paired_in \\\\\n            --out2 \\\\\n            $options.args\n\n        gzip -f < non_rRNA_reads_fwd.fq > ${prefix}_1.fastq.gz\n        gzip -f < non_rRNA_reads_rev.fq > ${prefix}_2.fastq.gz\n        mv rRNA_reads.log ${prefix}.sortmerna.log\n\n        echo \\$(sortmerna --version 2>&1) | sed 's/^.*SortMeRNA version //; s/ Build Date.*\\$//' > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["vibbits/rnaseq-editing/SORTMERNA"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["preseq"], "nb_own": 1, "list_own": ["vibbits"], "nb_wf": 1, "list_wf": ["rnaseq-editing"], "list_contrib": ["alex-botzki", "abotzki"], "nb_contrib": 2, "codes": ["\nprocess PRESEQ_LCEXTRAP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::preseq=3.1.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/preseq:3.1.2--h06ef8b0_1\"\n    } else {\n        container \"quay.io/biocontainers/preseq:3.1.2--h06ef8b0_1\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.ccurve.txt\"), emit: ccurve\n    tuple val(meta), path(\"*.log\")       , emit: log\n    path  \"*.version.txt\"                , emit: version\n\n    script:\n    def software   = getSoftwareName(task.process)\n    def prefix     = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def paired_end = meta.single_end ? '' : '-pe'\n    \"\"\"\n    preseq \\\\\n        lc_extrap \\\\\n        $options.args \\\\\n        $paired_end \\\\\n        -output ${prefix}.ccurve.txt \\\\\n        $bam\n    cp .command.err ${prefix}.command.log\n\n    echo \\$(preseq 2>&1) | sed 's/^.*Version: //; s/Usage:.*\\$//' > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["vibbits/rnaseq-editing/PRESEQ_LCEXTRAP"], "list_wf_names": ["vibbits/rnaseq-editing"]}, {"nb_reuse": 1, "tools": ["Sambamba"], "nb_own": 1, "list_own": ["viktorlj"], "nb_wf": 1, "list_wf": ["SarGet"], "list_contrib": ["viktorlj"], "nb_contrib": 1, "codes": ["\nprocess coverage_analysis_UMI{\n    tag {idSample}\n\n    publishDir directoryMap.coverage, mode: 'link'\n\n    input:\n      set idPatient, idSample, file(bam), file(bai) from trimmed_umiBAM_forcoverage\n      file(regions) from Channel.value(regionsFile)\n\n    output:\n      file(\"${idSample}.UMI.coverage.txt\") into umi_coverage\n\n    script:\n    \"\"\"\n    sambamba depth region --filter 'mapping_quality > 20' -q 20 -T 10 -T 50 -T 100 -T 500 -T 1000 -L ${regions} -o ${idSample}.rawcoverage.txt ${bam}\n    ParseSamCoverage.py -i ${idSample}.rawcoverage.txt -o ${idSample}.UMI.coverage.txt\n    \"\"\"\n}"], "list_proc": ["viktorlj/SarGet/coverage_analysis_UMI"], "list_wf_names": ["viktorlj/SarGet"]}, {"nb_reuse": 1, "tools": ["Pindel", "pindel2vcf"], "nb_own": 1, "list_own": ["viktorlj"], "nb_wf": 1, "list_wf": ["SarGet"], "list_contrib": ["viktorlj"], "nb_contrib": 1, "codes": ["\nprocess Pindel {\n  tag {idSample}\n\n  publishDir \"${directoryMap.VariantCallingPindel}\", mode: 'link'\n\n  input:\n    set idPatient, idSample, file(bam), file(bai) from trimmed_BAM_forpindel\n    file(genomeFile) from Channel.fromPath(referenceMap.genomeFile)\n    file(genomeIndex) from Channel.fromPath(referenceMap.genomeIndex)\n    file(regions) from Channel.value(regionsFile)\n\n  output:\n    set idPatient, idSample, file(\"*.vcf\") into pindelvariants\n\n  \"\"\"\n  echo '${bam}\\t225\\t${idSample}' > ${idSample}.txt\n\n  /pindel/pindel -i ${idSample}.txt -f ${genomeFile} -o ${idSample} -c 13 -B 60 -x 2 -T 4\n\n  /pindel/pindel2vcf -p ${idSample}_SI -r ${genomeFile} -R human_g1k_v37_decoy -d 20101123 -e 35 -mc 10 -G -is 5 -c 13\n  /pindel/pindel2vcf -p ${idSample}_D -r ${genomeFile} -R human_g1k_v37_decoy -d 20101123 -e 35 -mc 10 -G -is 5 -c 13\n\n  \"\"\"\n}"], "list_proc": ["viktorlj/SarGet/Pindel"], "list_wf_names": ["viktorlj/SarGet"]}, {"nb_reuse": 1, "tools": ["seqTools"], "nb_own": 1, "list_own": ["viktorlj"], "nb_wf": 1, "list_wf": ["SarekCLLPostProcess"], "list_contrib": ["viktorlj"], "nb_contrib": 1, "codes": ["\nprocess finishVCF {\n    tag {vcf}\n\n    publishDir directoryMap.txtAnnotate, mode: 'link', pattern: '*.txt'\n\n    input:\n        file(vcf) from filteredcosmicvcf\n        val(sampleID) from Channel.value(sampleID)\n\n    output:\n        file(\"${vcf.baseName}.anno.done.txt\") into finishedFile\n        file(\"${vcf.baseName}.ADfiltered.vcf\") into finishedVCFFile\n\n    script:\n    \"\"\"\n    seqtool vcf strelka -f ${vcf} -o ${vcf.baseName}.strelkaadjusted.vcf\n\n    java -Xmx4g \\\n\t  -jar \\$SNPEFF_HOME/SnpSift.jar \\\n\t  filter \"( TUMVAF >= 0.1 ) & ( TUMALT > 4 )\" \\\n\t  -f ${vcf.baseName}.strelkaadjusted.vcf \\\n\t  > ${vcf.baseName}.ADfiltered.vcf\n\n    seqtool vcf melt -f ${vcf.baseName}.ADfiltered.vcf -o ${vcf.baseName}.melt.txt -s ${vcf.baseName} --includeHeader\n\n    pyenv global 3.6.3\n    eval \"\\$(pyenv init -)\"\n    strelka2pandas.py -i ${vcf.baseName}.melt.txt -o ${vcf.baseName}.anno.txt\n\n    grep -E -v 'LCRfiltered|IGRegion' ${vcf.baseName}.anno.txt > ${vcf.baseName}.anno.done.txt\n\n    \"\"\" \n\n}"], "list_proc": ["viktorlj/SarekCLLPostProcess/finishVCF"], "list_wf_names": ["viktorlj/SarekCLLPostProcess"]}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["\nprocess GATK4_MODELSEGMENTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(tumor_allelicCounts_tsv), path(normal_allelicCounts_tsv), path(tumor_denoisedCR_tsv)\n\n    output:\n    tuple val(meta), path(\"*.hets.csv\"), emit: hets_csv\n    tuple val(meta), path(\"*.hets.normal.csv\"), emit: hets_normal_csv\n    tuple val(meta), path(\"*.cr.seg\"), emit: cr_seg\n    tuple val(meta), path(\"*.cr.igv.seg\"), emit: cr_igv_seg\n    tuple val(meta), path(\"*.af.igv.seg\"), emit: af_igv_seg\n    tuple val(meta), path(\"*.modelBegin.seg\"), emit: modelBegin_seg\n    tuple val(meta), path(\"*.modelBegin.cr.param\"), emit: modelBegin_cr_param\n    tuple val(meta), path(\"*.modelBegin.af.param\"), emit: modelBegin_af_param\n    tuple val(meta), path(\"*.modelFinal.seg\"), emit: modelFinal_seg\n    tuple val(meta), path(\"*.modelFinal.cr.param\"), emit: modelFinal_cr_param\n    tuple val(meta), path(\"*.modelFinal.af.param\"), emit: modelFinal_af_param\n\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK ModelSegments] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" ModelSegments \\\\\n            --denoised-copy-ratios $tumor_denoisedCR_tsv \\\\\n            --allelic-counts $tumor_allelicCounts_tsv \\\\\n            --normal-allelic-counts $normal_allelicCounts_tsv \\\\\n            --maximum-number-of-segments-per-chromosome 0 \\\\\n            --output './' \\\\\n            --output-prefix ${prefix} \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process GATK4_COLLECTALLELICCOUNTS {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n\n    output:\n    tuple val(meta), path(\"*.allelicCounts.tsv\"), emit: allelicCounts_tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CollectAllelicCounts] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CollectAllelicCounts \\\\\n            -L $intervals \\\\\n            -I $input \\\\\n            -R $fasta \\\\\n            -O ${prefix}.allelicCounts.tsv\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_INDEXFEATUREFILE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(feature_file)\n\n    output:\n    tuple val(meta), path(\"*.{tbi,idx}\"), emit: index\n    path  \"versions.yml\"                , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IndexFeatureFile] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        IndexFeatureFile \\\\\n        $args \\\\\n        -I $feature_file\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "process GATK4_GENOMICSDBIMPORT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(vcf), path(tbi), path(intervalfile), val(intervalval), path(wspace)\n    val run_intlist\n    val run_updatewspace\n    val input_map\n\n    output:\n    tuple val(meta), path(\"${prefix}\")      , optional:true, emit: genomicsdb\n    tuple val(meta), path(\"$updated_db\")    , optional:true, emit: updatedb\n    tuple val(meta), path(\"*.interval_list\"), optional:true, emit: intervallist\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n\n                                                     \n    inputs_command = input_map ? \"--sample-name-map ${vcf[0]}\" : \"${'-V ' + vcf.join(' -V ')}\"\n    dir_command = \"--genomicsdb-workspace-path ${prefix}\"\n    intervals_command = intervalfile ? \" -L ${intervalfile} \" : \" -L ${intervalval} \"\n\n                                                                                  \n    if (run_intlist) {\n        inputs_command = ''\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = \"--output-interval-list-to-file ${prefix}.interval_list\"\n    }\n\n                                                                                                                                        \n    if (run_updatewspace) {\n        dir_command = \"--genomicsdb-update-workspace-path ${wspace}\"\n        intervals_command = ''\n        updated_db = wspace.toString()\n    }\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK GenomicsDBImport] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" GenomicsDBImport \\\\\n        $inputs_command \\\\\n        $dir_command \\\\\n        $intervals_command \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process GATK4_COLLECTREADCOUNTS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n\n    output:\n    tuple val(meta), path(\"*.counts.hdf5\"), emit: hdf5\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CollectReadCounts] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" CollectReadCounts \\\\\n            -I $input \\\\\n            -L $intervals \\\\\n            --reference $fasta \\\\\n            --format HDF5 \\\\\n            --interval-merging-rule OVERLAPPING_ONLY \\\\\n            -O ${prefix}.counts.hdf5 \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["Picard"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process PICARD_SORTSAM {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    val sort_order\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[Picard SortSam] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    picard \\\\\n        SortSam \\\\\n        -Xmx${avail_mem}g \\\\\n        --INPUT $bam \\\\\n        --OUTPUT ${prefix}.bam \\\\\n        --SORT_ORDER $sort_order\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        picard: \\$(picard SortSam --version 2>&1 | grep -o 'Version:.*' | cut -f2- -d:)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["FastQC", "BEDTools"], "nb_own": 2, "list_own": ["vladsaveliev", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "cawdor"], "list_contrib": ["vladsaveliev", "vincenthhu"], "nb_contrib": 2, "codes": ["\nprocess RunFastQC {\n  tag {idPatient + \"-\" + idLane}\n\n  publishDir \"${params.outDir}/Reports/FastQC/${idLane}\", mode: params.publishDirMode\n\n  input:\n  set idPatient, status, idSample, idLane, file(inputFile1), file(inputFile2) from inputFilesforFastQC\n\n  output:\n  file \"*_fastqc.{zip,html}\" into fastQCreport\n\n  script:\n  def inputs = Utils.isFq(inputFile1) ? \"${inputFile1} ${inputFile2}\" : \"${inputFile1}\"\n  \"\"\"\n  fastqc -t 2 -q ${inputs}\n  \"\"\"\n}", "process BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["vladsaveliev/cawdor/RunFastQC"], "list_wf_names": ["vladsaveliev/cawdor"]}, {"nb_reuse": 0, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process GATK4_MUTECT2 {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta) , path(input) , path(input_index) , val(which_norm)\n    val   run_single\n    val   run_pon\n    val   run_mito\n    val   interval_label\n    path  interval_list\n    path  fasta\n    path  fai\n    path  dict\n    path  germline_resource\n    path  germline_resource_tbi\n    path  panel_of_normals\n    path  panel_of_normals_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\")     , emit: vcf\n    tuple val(meta), path(\"*.tbi\")        , emit: tbi\n    tuple val(meta), path(\"*.stats\")      , emit: stats\n    tuple val(meta), path(\"*.f1r2.tar.gz\"), optional:true, emit: f1r2\n    tuple val(meta), path(\"*.bam\")        , emit: bam\n    path \"versions.yml\"                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def panels_command = ''\n    def normals_command = ''\n    def interval_command = ''\n\n    def inputs_command = '-I ' + input.join( ' -I ')\n\n    if(run_pon) {\n        panels_command = ''\n        normals_command = ''\n\n    } else if(run_single) {\n        panels_command = \" --germline-resource $germline_resource --panel-of-normals $panel_of_normals\"\n        normals_command = ''\n\n    } else if(run_mito){\n        panels_command = \"-L ${interval_label} --mitochondria-mode\"\n        normals_command = ''\n\n    } else {\n        panels_command = \" --germline-resource $germline_resource --panel-of-normals $panel_of_normals --f1r2-tar-gz ${prefix}.f1r2.tar.gz\"\n        normals_command = '-normal ' + which_norm.join( ' -normal ')\n    }\n\n    if(interval_list) {\n        interval_command = '-L ' + interval_list.join( ' -L ')\n    }\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK Mutect2] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" Mutect2 \\\\\n        -R ${fasta} \\\\\n        ${inputs_command} \\\\\n        ${normals_command} \\\\\n        ${panels_command} \\\\\n        ${interval_command} \\\\\n        -O ${prefix}.vcf.gz \\\\\n        -bamout ${prefix}.bam \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process GATK4_PLOTDENOISEDCOPYRATIOS {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(standardizedCR_tsv), path(denoisedCR_tsv)\n    path dict\n\n    output:\n    tuple val(meta), path(\"*.denoised.png\"          ), emit: denoised_png\n    tuple val(meta), path(\"*.standardizedMAD.txt\"   ), emit: standardizedMAD_txt\n    tuple val(meta), path(\"*.denoisedMAD.txt\"       ), emit: denoisedMAD_txt\n    tuple val(meta), path(\"*.deltaMAD.txt\"          ), emit: deltaMAD_txt\n    tuple val(meta), path(\"*.scaledDeltaMAD.txt\"    ), emit: scaledDeltaMAD_txt\n\n    path \"versions.yml\"                              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK ModelSegments] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" PlotDenoisedCopyRatios \\\\\n            --standardized-copy-ratios $standardizedCR_tsv \\\\\n            --denoised-copy-ratios  $denoisedCR_tsv \\\\\n            --sequence-dictionary ${dict} \\\\\n            --output './' \\\\\n            --output-prefix ${prefix} \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process GATK4_HAPLOTYPECALLER {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index), path(intervals)\n    path fasta\n    path fai\n    path dict\n    path dbsnp\n    path dbsnp_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def interval_option = intervals ? \"-L ${intervals}\" : \"\"\n    def dbsnp_option    = dbsnp ? \"-D ${dbsnp}\" : \"\"\n    def avail_mem       = 3\n    if (!task.memory) {\n        log.info '[GATK HaplotypeCaller] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk \\\\\n        --java-options \"-Xmx${avail_mem}g\" \\\\\n        HaplotypeCaller \\\\\n        -R $fasta \\\\\n        -I $input \\\\\n        ${dbsnp_option} \\\\\n        ${interval_option} \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --tmp-dir .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["GATK"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process GATK4_CREATESOMATICPANELOFNORMALS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(genomicsdb)\n    path fasta\n    path fai\n    path dict\n    path germline_resource\n    path germline_resource_tbi\n\n    output:\n    tuple val(meta), path(\"*.vcf.gz\"), emit: vcf\n    tuple val(meta), path(\"*.tbi\")   , emit: tbi\n\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK CreateSomaticPanelOfNormals] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" \\\\\n        CreateSomaticPanelOfNormals \\\\\n        -R $fasta \\\\\n        --germline-resource $germline_resource \\\\\n        -V gendb://$genomicsdb \\\\\n        -O ${prefix}.vcf.gz \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["randompat", "GATK"], "nb_own": 2, "list_own": ["vntasis", "vincenthhu"], "nb_wf": 1, "list_wf": ["nf-core-westest", "stan-nf"], "list_contrib": ["vntasis", "vincenthhu"], "nb_contrib": 2, "codes": ["process GATK4_ANNOTATEINTERVALS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(interval_list)\n    path ref_fasta\n    path ref_fai\n    path ref_dict\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK AnnotateIntervals] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" AnnotateIntervals \\\\\n        -L $interval_list \\\\\n        -R $ref_fasta\n        -O ${prefix}.tsv \\\\\n        --interval-merging-rule OVERLAPPING_ONLY \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess sampling {\n  tag \"$modelName-$sampleID-$chain\"\n  publishDir \"$params.outdir/$sampleID/samples\", mode: 'copy', pattern: \"*.csv\"\n\n  input:\n  tuple val(sampleID), path(data), val(modelName), path(model), val(chain) from model2sample_ch\n  val(sampleParams) from params.sampleParams\n  val(seed) from params.seed\n  val(numSamples) from params.numSamples\n  val(numWarmup) from params.numWarmup\n  val(threads) from threads\n\n  output:\n  tuple val(modelName), val(sampleID), path(\"${sampleID}_${modelName}_${chain}.csv\") into samples2summary_ch\n  tuple val(modelName), val(sampleID), path(model), path(data), path(\"${sampleID}_${modelName}_${chain}.csv\") into samples2gen_quan_ch\n\n  when:\n  runSample\n\n  script:\n  \"\"\"\n  ./$model sample \\\n    num_samples=$numSamples \\\n    num_warmup=$numWarmup \\\n    $sampleParams \\\n    random seed=$seed id=$chain \\\n    data file=$data \\\n    output file=\"${sampleID}_${modelName}_${chain}.csv\" \\\n    $threads\n  \"\"\"\n}"], "list_proc": ["vntasis/stan-nf/sampling"], "list_wf_names": ["vntasis/stan-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "GATK"], "nb_own": 2, "list_own": ["vpeddu", "vincenthhu"], "nb_wf": 1, "list_wf": ["ev-meta", "nf-core-westest"], "list_contrib": ["vpeddu", "vincenthhu"], "nb_contrib": 2, "codes": ["process GATK4_INTERVALLISTTOBED {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gatk4=4.2.4.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gatk4:4.2.4.1--hdfd78af_0' :\n        'quay.io/biocontainers/gatk4:4.2.4.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(interval_list)\n\n    output:\n    tuple val(meta), path(\"*.bed\"), emit: bed\n\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def avail_mem = 3\n    if (!task.memory) {\n        log.info '[GATK IntervalListToBed] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n    } else {\n        avail_mem = task.memory.giga\n    }\n    \"\"\"\n    gatk --java-options \"-Xmx${avail_mem}g\" IntervalListToBed \\\\\n        -I $interval_list \\\\\n        -O ${prefix}.bed \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gatk4: \\$(echo \\$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess Sam_conversion { \npublishDir \"${params.OUTPUT}/sam_conversion/${base}\", mode: 'symlink', overwrite: true\ncontainer \"staphb/samtools\"\nbeforeScript 'chmod o+rw .'\ncpus 8\ninput: \n    tuple val(base), file(sam)\noutput: \n    tuple val(\"${base}\"), file(\"${base}.sorted.bam\"), file(\"${base}.sorted.bam.bai\")\n    file \"${base}.unclassfied.bam\"\n\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \n\nsamtools view -Sb -@  ${task.cpus} -F 4 ${sam} > ${base}.bam\nsamtools sort -@ ${task.cpus} ${base}.bam > ${base}.sorted.bam\nsamtools index ${base}.sorted.bam\n\nsamtools view -Sb -@  ${task.cpus} -f 4 ${sam} > ${base}.unclassfied.bam\n\n\n\"\"\"\n}"], "list_proc": ["vpeddu/ev-meta/Sam_conversion"], "list_wf_names": ["vpeddu/ev-meta"]}, {"nb_reuse": 0, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process BCFTOOLS_NORM {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::bcftools=1.14' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bcftools:1.14--h88f3f91_0' :\n        'quay.io/biocontainers/bcftools:1.14--h88f3f91_0' }\"\n\n    input:\n    tuple val(meta), path(vcf)\n    path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.gz\") , emit: vcf\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bcftools norm \\\\\n        --fasta-ref ${fasta} \\\\\n        --output ${prefix}.vcf.gz \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${vcf}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcftools: \\$(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 0, "tools": ["BEDTools"], "nb_own": 1, "list_own": ["vincenthhu"], "nb_wf": 0, "list_wf": ["nf-core-westest"], "list_contrib": ["vincenthhu"], "nb_contrib": 1, "codes": ["process BEDTOOLS_INTERSECT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals1), path(intervals2)\n    val extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: intersect\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        intersect \\\\\n        -a $intervals1 \\\\\n        -b $intervals2 \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": [], "list_wf_names": []}, {"nb_reuse": 1, "tools": ["THREADER"], "nb_own": 1, "list_own": ["virus-evolution"], "nb_wf": 1, "list_wf": ["phylopipe"], "list_contrib": ["rmcolq"], "nb_contrib": 1, "codes": ["\nprocess get_unreliable_tips {\n       \n                                        \n                     \n      \n\n    input:\n    path metadata\n\n    output:\n    path \"${metadata.baseName}.tips.txt\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python3\n    import csv\n\n    filter_column = \"is_unreliable_in_tree\"\n    index_column = \"sequence_name\"\n    with open(\"${metadata}\", 'r', newline = '') as csv_in, \\\n         open(\"${metadata.baseName}.tips.txt\", 'w') as tips_out:\n        reader = csv.DictReader(csv_in, delimiter=\",\", quotechar='\\\"', dialect = \"unix\")\n        if index_column not in reader.fieldnames:\n            sys.exit(\"Index column %s not in CSV\" %index_column)\n        if filter_column in reader.fieldnames:\n            for row in reader:\n                if row[filter_column] in [\"Y\",\"Yes\",\"yes\",\"y\",True,\"True\"]:\n                    name = row[index_column].replace('\"','').replace(\"'\",\"\")\n                    tips_out.write(\"'%s'\\\\n\" %name)\n                    tips_out.write(\"%s\\\\n\" %name)\n    \"\"\"\n}"], "list_proc": ["virus-evolution/phylopipe/get_unreliable_tips"], "list_wf_names": ["virus-evolution/phylopipe"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["vladsaveliev"], "nb_wf": 1, "list_wf": ["cawdor"], "list_contrib": ["vladsaveliev"], "nb_contrib": 1, "codes": ["\nprocess RunVarDict {\n  tag {idPatient + \"-vardict-\" + intervalBed.baseName}\n\n  input:\n  set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumour, file(bamTumour),\n    file(baiTumour), file(intervalBed) from bamsForVardict\n  file(genomeFasta) from Channel.value(genomeFasta)\n  file(genomeIndex) from Channel.value(genomeIndex)\n  file(callable) from Channel.value(callable ? file(callable) : \"null\")\n\n  output:\n  set val(\"vardict\"), idPatient, idSampleNormal, idSampleTumour,\n      file(\"*.vcf.gz\"), file(\"*.vcf.gz.tbi\") into vardictOutput\n\n  when: !params.strelkaOnly && !params.onlyQC\n\n  script:\n  def makeTarget = \"\"\n  def targetBed = intervalBed\n  if (callable) {\n    targetBed = 'target.bed'\n    makeTarget = \"bedtools intersect -a ${intervalBed} -b ${callable} > ${targetBed}\"\n  }\n  tmpDir = file(\"tmp\").mkdir()\n  outFile = \"${idPatient}-interval_${intervalBed.baseName}.vcf.gz\"\n  \"\"\"\n  ${makeTarget}\n  unset JAVA_HOME && \\\n  export VAR_DICT_OPTS='-Xms750m -Xmx${task.memory.toGiga()}g -XX:+UseSerialGC -Djava.io.tmpdir=${tmpDir}' && \\\n  vardict-java -G ${genomeFasta} \\\n  -N ${idSampleNormal} \\\n  -b \"${bamTumour}|${bamNormal}\" \\\n  -c 1 -S 2 -E 3 -g 4 --nosv --deldupvar -Q 10 -F 0x700 -f 0.1 \\\n  ${targetBed} \\\n  | awk 'NF>=48' \\\n  | testsomatic.R \\\n  | var2vcf_paired.pl -P 0.9 -m 4.25 -f 0.01 -M  -N \"${idSampleTumour}|${idSampleNormal}\" \\\n  | bcftools filter -m '+' -s 'REJECT' -e 'STATUS !~ \".*Somatic\"' \\\n  2> /dev/null \\\n  | bcftools filter --soft-filter 'LowFreqBias' --mode '+' -e 'FORMAT/AF[0] < 0.02 && \\\n  FORMAT/VD[0] < 30 && FORMAT/SBF[0] < 0.1 && FORMAT/NM[0] >= 2.0' \\\n  | bcftools filter -i 'QUAL >= 0' \\\n  | sed 's/\\\\\\\\.*Somatic\\\\\\\\/Somatic/' \\\n  | sed 's/REJECT,Description=\".*\">/REJECT,Description=\"Not Somatic via VarDict\">/' \\\n  | awk -F\\$'\\\\t' -v OFS='\\\\t' '{if (\\$0 !~ /^#/) gsub(/[KMRYSWBVHDXkmryswbvhdx]/, \"N\", \\$4) } {print}' \\\n  | awk -F\\$'\\\\t' -v OFS='\\\\t' '{if (\\$0 !~ /^#/) gsub(/[KMRYSWBVHDXkmryswbvhdx]/, \"N\", \\$5) } {print}' \\\n  | awk -F\\$'\\\\t' -v OFS='\\\\t' '\\$1!~/^#/ && \\$4 == \\$5 {next} {print}' \\\n  | vcfstreamsort \\\n  | bgzip -c > ${outFile} && tabix -p vcf ${outFile} \\\n  \"\"\"\n}"], "list_proc": ["vladsaveliev/cawdor/RunVarDict"], "list_wf_names": ["vladsaveliev/cawdor"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["vladsaveliev"], "nb_wf": 1, "list_wf": ["cawdor"], "list_contrib": ["vladsaveliev"], "nb_contrib": 1, "codes": ["\nprocess ConcatVCFbyInterval {\n  tag {idSample + \"-\" + variantCaller}\n\n  publishDir \"${params.outDir}/VariantCalling/${idSample}/${\"$variantCaller\"}\", mode: params.publishDirMode\n\n  input:\n  set variantCaller, idSample, file(vcfFiles), file(tbiFiles) from vcfsToConcat\n\n  output:\n  set variantCaller, idSample,\n      file(\"*-${variantCaller}.vcf.gz\"), file(\"*-${variantCaller}.vcf.gz.tbi\") into vcfConcatenated\n\n  when: !params.onlyQC\n\n  script:\n  outFile = \"${idSample}-germline-${variantCaller}.vcf.gz\"                                           \n  \"\"\"\n  bcftools concat ${idSample}-interval_*.vcf.gz -a | bcftools sort -Oz -o ${outFile}\n  tabix -p vcf ${outFile}\n  \"\"\"\n}"], "list_proc": ["vladsaveliev/cawdor/ConcatVCFbyInterval"], "list_wf_names": ["vladsaveliev/cawdor"]}, {"nb_reuse": 1, "tools": ["AMBER"], "nb_own": 1, "list_own": ["vladsaveliev"], "nb_wf": 1, "list_wf": ["cawdor"], "list_contrib": ["vladsaveliev"], "nb_contrib": 1, "codes": ["\nprocess RunAmber {\n  label \"purple\"\n\n  tag {idSampleTumour + \"_vs_\" + idSampleNormal}\n\n  publishDir \"${params.outDir}/VariantCalling/${idPatient}/Purple/amber\", mode: params.publishDirMode\n\n  input:\n  set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumour, file(bamTumour), file(baiTumour) from bamsForAmber\n  set file(genomeFasta), file(genomeIndex), file(genomeDict), file(purpleHet) from Channel.value([\n    genomeFasta,\n    genomeIndex,\n    genomeDict,\n    purpleHet\n  ])\n\n  output:\n  set idPatient, idSampleNormal, idSampleTumour, file('amber/*') into amberOutput\n\n  when: !params.onlyQC\n\n  script:\n  jvm_opts = \"-Xms750m -Xmx${task.memory.toGiga()}g\"\n  \"\"\" \\\n  AMBER ${jvm_opts} \\\n  -tumor ${idPatient} \\\n  -tumor_bam ${bamTumour} \\\n  -reference ${idSampleNormal} \\\n  -reference_bam ${bamNormal} \\\n  -ref_genome ${genomeFasta} \\\n  -bed ${purpleHet} \\\n  -threads ${task.cpus} \\\n  -output_dir amber \\\n  \"\"\"\n}"], "list_proc": ["vladsaveliev/cawdor/RunAmber"], "list_wf_names": ["vladsaveliev/cawdor"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["vladsaveliev"], "nb_wf": 1, "list_wf": ["cawdor"], "list_contrib": ["vladsaveliev"], "nb_contrib": 1, "codes": ["\nprocess RunBcftoolsStats {\n  tag {vcf}\n\n  publishDir \"${params.outDir}/Reports/BCFToolsStats\", mode: params.publishDirMode\n\n  input:\n    set variantCaller, file(vcf) from vcfForBCFtools\n\n  output:\n  file (\"*_stats.txt\") into bcfReport\n\n  script:\n  \"\"\"\n  bcftools stats ${vcf} > ${vcf.simpleName}.bcftools_stats.txt\n  \"\"\"\n}"], "list_proc": ["vladsaveliev/cawdor/RunBcftoolsStats"], "list_wf_names": ["vladsaveliev/cawdor"]}, {"nb_reuse": 2, "tools": ["TiPs", "MultiQC"], "nb_own": 2, "list_own": ["yqshao", "vladsaveliev"], "nb_wf": 2, "list_wf": ["tips", "umcaw"], "list_contrib": ["yqshao", "vladsaveliev"], "nb_contrib": 2, "codes": ["\nprocess gromacsLabel {\n    label 'gromacs'\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz')\n\n    script:\n    setup = getParams(labelDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    ln -s ${file(setup.gmxTop)} input.top\n    ln -s ${file(setup.inp)} input.mdp\n    tips convert ${file(setup.ds)} -o input -of pdb\n    gmx grompp -f input.mdp -c input.pdb -p input.top -o input.tpr --maxwarn 5\n    gmx mdrun -s input.tpr -rerun input.pdb --deffnm output\n    echo 4\\\\n0 | gmx energy -f output.edr -o energy.xvg\n    tips convert output.trr --top input.pdb --log energy.xvg -o output -of xyz\n    \"\"\"\n\n    stub:\n    setup = getParams(labelDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}"], "list_proc": ["yqshao/tips/gromacsLabel", "vladsaveliev/umcaw/multiqc"], "list_wf_names": ["yqshao/tips", "vladsaveliev/umcaw"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["vpeddu"], "nb_wf": 1, "list_wf": ["Slowqc"], "list_contrib": ["naliebe", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess kallisto_qc_PE { \n\tcontainer \"quay.io/biocontainers/kallisto:0.46.2--h4f7b962_1\"\n\n    input:\n      tuple file(r1), file(r2)\n\n    output:\n      file \"*.QC\"\n script:\n\n      \"\"\"\n#!/bin/bash\n\tfolderName=`basename ${r1} \"_R1_001.fastq.gz\"`\n\techo \\$folderName\n\tkallisto quant -i ${baseDir}/bin/qc_idx -t ${task.cpus} -o \\$folderName ${r1} ${r2}\n\n\t# find and rename the kallisto files\n\tfor i in `find . -name *.tsv`; do  temp=`echo \\$i | cut -d / -f2`; newfilename=\"\\$temp\"\".tsv\";echo \\$newfilename; echo \\$temp; cp \\$temp/abundance.tsv \\$newfilename.QC; done\n\n\n\"\"\"\n}"], "list_proc": ["vpeddu/Slowqc/kallisto_qc_PE"], "list_wf_names": ["vpeddu/Slowqc"]}, {"nb_reuse": 1, "tools": ["kallisto"], "nb_own": 1, "list_own": ["vpeddu"], "nb_wf": 1, "list_wf": ["Slowqc"], "list_contrib": ["naliebe", "vpeddu"], "nb_contrib": 2, "codes": ["\nprocess kallisto_human_PE { \n\tcontainer \"quay.io/biocontainers/kallisto:0.46.2--h4f7b962_1\"\n\n    input:\n      tuple file(r1), file(r2)\n\t  file kallistoIndex\n\t  val kallistoArgs\n\n    output:\n      tuple file(\"${r1}\"), file(\"${r2}\")\n\t  file \"*.tsv\"\n script:\n\n      \"\"\"\n\t#!/bin/bash\n\tfolderName=`basename ${r1} \"_R1_001.fastq.gz\"`\n\techo \\$folderName\n\tkallisto quant -i ${kallistoIndex} ${kallistoArgs} -t ${task.cpus} -o \\$folderName ${r1} ${r2}\n\n\t# find and rename the kallisto files\n\tfor i in `find . -name *.tsv`; do  temp=`echo \\$i | cut -d / -f2`; newfilename=\"\\$temp\"\".tsv\";echo \\$newfilename; echo \\$temp; cp \\$temp/abundance.tsv \\$newfilename; done\n\"\"\"\n}"], "list_proc": ["vpeddu/Slowqc/kallisto_human_PE"], "list_wf_names": ["vpeddu/Slowqc"]}, {"nb_reuse": 2, "tools": ["kraken2", "BWA"], "nb_own": 2, "list_own": ["wslh-bio", "vpeddu"], "nb_wf": 2, "list_wf": ["ev-meta", "dryad"], "list_contrib": ["AbigailShockey", "vpeddu", "k-florek"], "nb_contrib": 3, "codes": [" process bwa {\n      tag \"$name\"\n      errorStrategy 'ignore'\n      publishDir \"${params.outdir}/mapping/sams\", mode: 'copy',pattern:\"*.sam\"\n\n      input:\n      file(reference) from mapping_reference.first()\n      set val(name), file(reads) from cleaned_reads_mapping\n\n      output:\n      tuple name, file(\"${name}.reference.sam\") into reference_sams\n\n      script:\n      \"\"\"\n      bwa index ${reference}\n      bwa mem ${reference} ${reads[0]} ${reads[1]} > ${name}.reference.sam\n      \"\"\"\n    }", "\nprocess Kraken_prefilter { \npublishDir \"${params.OUTPUT}/Interleave_FASTQ/${base}\", mode: 'symlink', overwrite: true\ncontainer \"staphb/kraken2\"\nbeforeScript 'chmod o+rw .'\ncpus 8\ninput: \n    tuple val(base), file(r1), file(r2)\n    file kraken2_db\noutput: \n    tuple val(\"${base}\"), file(\"${base}.kraken2.report\")\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \n\nkraken2 --db ${kraken2_db} \\\n    --threads ${task.cpus} \\\n    --classified-out ${base}.kraken2.classified \\\n    --output ${base}.kraken2.output \\\n    --report ${base}.kraken2.report \\\n    --gzip-compressed \\\n    --unclassified-out ${base}.kraken2.unclassified \\\n    ${r1} ${r2}\n\n\"\"\"\n}"], "list_proc": ["wslh-bio/dryad/bwa", "vpeddu/ev-meta/Kraken_prefilter"], "list_wf_names": ["vpeddu/ev-meta", "wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["vpeddu"], "nb_wf": 1, "list_wf": ["ev-meta"], "list_contrib": ["vpeddu"], "nb_contrib": 1, "codes": ["\nprocess Minimap2_illumina { \n                                \npublishDir \"${params.OUTPUT}/Minimap2/${base}\", mode: 'symlink'\ncontainer \"quay.io/vpeddu/evmeta:latest\"\nbeforeScript 'chmod o+rw .'\ncpus 8\ninput: \n    tuple val(base), file(r1), file(r2), file(species_fasta)\noutput: \n    tuple val(\"${base}\"), file(\"${base}.sorted.filtered.bam\"), file(\"${base}.sorted.filtered.bam.bai\")\n    tuple val(\"${base}\"), file(\"${base}.unclassified.bam\"), file (\"${base}.unclassified.fastq.gz\")\n\nscript:\n\"\"\"\n#!/bin/bash\n\n#logging\necho \"ls of directory\" \nls -lah \n\necho \"running Minimap2 on ${base}\"\nminimap2 \\\n    -ax sr \\\n    -t ${task.cpus} \\\n    -K 16G \\\n    --split-prefix \\\n    -2 \\\n    ${species_fasta} \\\n    ${r1} ${r2} | samtools view -Sb -@ 4 - > ${base}.bam\n\nsamtools view -Sb -F 4 ${base}.bam > ${base}.filtered.bam\nsamtools sort ${base}.filtered.bam -o ${base}.sorted.filtered.bam \nsamtools index ${base}.sorted.filtered.bam\n# output unclassified reads\nsamtools view -Sb -@  ${task.cpus} -f 4 ${base}.bam > ${base}.unclassified.bam\n\n# cleanup intermediate file\nrm ${base}.bam\n\nsamtools fastq -@ ${task.cpus} ${base}.unclassified.bam | gzip > ${base}.unclassified.fastq.gz\n\n\"\"\"\n}"], "list_proc": ["vpeddu/ev-meta/Minimap2_illumina"], "list_wf_names": ["vpeddu/ev-meta"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["vpeddu"], "nb_wf": 1, "list_wf": ["ev-meta"], "list_contrib": ["vpeddu"], "nb_contrib": 1, "codes": ["\nprocess Host_depletion_nanopore { \npublishDir \"${params.OUTPUT}/Host_filtered/${base}\", mode: 'symlink', overwrite: true\ncontainer \"vpeddu/nanopore_metagenomics:latest\"\nbeforeScript 'chmod o+rw .'\ncpus 8\ninput: \n    tuple val(base), file(r1)\n    file minimap2_host_index\n    file ribosome_trna\n    file minimap2_plasmid_db\noutput: \n    tuple val(\"${base}\"), file(\"${base}.host_filtered.plasmid_removed.fastq.gz\")\n    file \"${base}.host_mapped.bam\"\n    file \"${base}.trna.mapped.bam\"\n    tuple val(\"${base}\"), file(\"${base}.plasmid.fastq.gz\"), env(plasmid_count)\n\nscript:\n                              \nif ( \"${params.CLEAN_RIBOSOME_TRNA}\" == true) {\n    \"\"\"\n    #!/bin/bash\n    #logging\n    echo \"ls of directory\" \n    ls -lah \n\n    #cat ${minimap2_host_index} ${ribosome_trna} > host.fa\n\n    minimap2 \\\n        -ax map-ont \\\n        -t \"\\$((${task.cpus}-2))\" \\\n        -2 \\\n        ${ribosome_trna} \\\n        ${r1} | samtools view -Sb -@ 2 - > ${base}.trna.bam\n\n        samtools fastq -@ 4 -n -f 4 ${base}.trna.bam | pigz > ${base}.trna_filtered.fastq.gz\n        samtools fastq -@ 4 -n -F 4 ${base}.trna.bam > ${base}.trna.mapped.bam\n\n    minimap2 \\\n        -ax map-ont \\\n        -t \"\\$((${task.cpus}-2))\" \\\n        -2 \\\n        ${minimap2_host_index} \\\n        ${base}.trna_filtered.fastq.gz | samtools view -Sb -@ 2 - > ${base}.host_mapped.bam\n        samtools fastq -@ 4 -n -f 4 ${base}.host_mapped.bam | pigz > ${base}.host_filtered.fastq.gz\n    \n    minimap2 \\\n        -ax map-ont \\\n        -t ${task.cpus} \\\n        --sam-hit-only \\\n        ${minimap2_plasmid_db} \\\n        ${base}.host_filtered.fastq.gz | samtools view -F 4 -Sb - > ${base}.plasmid_extraction.bam\n\n\n    samtools view ${base}.plasmid_extraction.bam | cut -f1 | sort | uniq > ${base}.plasmid_read_ids.txt\n\n    plasmid_count=`cat ${base}.plasmid_read_ids.txt | wc -l`\n    echo \"\\$plasmid_count sequences mapped to plasmid\" \n\n    /usr/local/miniconda/bin/seqkit grep -f ${base}.plasmid_read_ids.txt ${base}.host_filtered.fastq.gz | pigz > ${base}.plasmid.fastq.gz \n    /usr/local/miniconda/bin/seqkit grep -v -f ${base}.plasmid_read_ids.txt ${base}.host_filtered.fastq.gz | pigz > ${base}.host_filtered.plasmid_removed.fastq.gz \n\n\n\"\"\"\n    }\n                          \nelse {\n    \"\"\"\n    #!/bin/bash\n    #logging\n    echo \"ls of directory\" \n    ls -lah \n\n    #cat ${minimap2_host_index} ${ribosome_trna} > host.fa\n\n\n    minimap2 \\\n        -ax map-ont \\\n        -t \"\\$((${task.cpus}-2))\" \\\n        -2 \\\n        ${minimap2_host_index} \\\n        ${r1} | samtools view -Sb -@ 2 - > ${base}.host_mapped.bam\n        samtools fastq -@ 4 -n -f 4 ${base}.host_mapped.bam | pigz > ${base}.host_filtered.fastq.gz\n    \n    minimap2 \\\n        -ax map-ont \\\n        -t ${task.cpus} \\\n        --sam-hit-only \\\n        ${minimap2_plasmid_db} \\\n        ${base}.host_filtered.fastq.gz | samtools view -F 4 -Sb - > ${base}.plasmid_extraction.bam\n\n\n    samtools view ${base}.plasmid_extraction.bam | cut -f1 | sort | uniq > ${base}.plasmid_read_ids.txt\n\n    plasmid_count=`cat ${base}.plasmid_read_ids.txt | wc -l`\n    echo \"\\$plasmid_count sequences mapped to plasmid\" \n\n    /usr/local/miniconda/bin/seqkit grep -f ${base}.plasmid_read_ids.txt ${base}.host_filtered.fastq.gz | pigz > ${base}.plasmid.fastq.gz \n    /usr/local/miniconda/bin/seqkit grep -v -f ${base}.plasmid_read_ids.txt ${base}.host_filtered.fastq.gz | pigz > ${base}.host_filtered.plasmid_removed.fastq.gz \n\n    \"\"\"\n    }  \n\n}"], "list_proc": ["vpeddu/ev-meta/Host_depletion_nanopore"], "list_wf_names": ["vpeddu/ev-meta"]}, {"nb_reuse": 1, "tools": ["Flye"], "nb_own": 1, "list_own": ["vpeddu"], "nb_wf": 1, "list_wf": ["ev-meta"], "list_contrib": ["vpeddu"], "nb_contrib": 1, "codes": ["\nprocess Identify_resistant_plasmids { \npublishDir \"${params.OUTPUT}/plasmid_identification/${base}\", mode: 'symlink', overwrite: true\ncontainer \"vpeddu/nanopore_metagenomics:latest\"\nbeforeScript 'chmod o+rw .'\ncpus 8\n\n                                                   \n                                                                       \nerrorStrategy 'ignore'\ninput: \n    tuple val(base), file(plasmid_fastq), val(plasmidreadcount)\n    file amrdb\noutput: \n    tuple val(\"${base}\"), file(\"${base}.amrfinder.out.txt\"), file(\"${base}.plasmid.flye/assembly.fasta\")\nscript:\n\"\"\"\n#!/bin/bash\n#logging\necho \"ls of directory\" \nls -lah \n\n# assemble plasmids with flye\n# meta and plasmid flags are used here to find plasmids from a metagenomics sample \n# need error handling for if nothing is assembled\n/Flye/bin/flye --plasmids \\\n    --meta \\\n    -t ${task.cpus} \\\n    -o ${base}.plasmid.flye \\\n    --nano-hq ${plasmid_fastq}\n\n# run amrfinder on flye assembly\n/amrfinder/amrfinder \\\n    -n ${base}.plasmid.flye/assembly.fasta \\\n    --threads ${task.cpus} \\\n    -d ${amrdb}/2021-12-21.1/ \\\n    -o ${base}.amrfinder.out.txt\n\n\n\"\"\"\n}"], "list_proc": ["vpeddu/ev-meta/Identify_resistant_plasmids"], "list_wf_names": ["vpeddu/ev-meta"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Minimap2"], "nb_own": 1, "list_own": ["vpeddu"], "nb_wf": 1, "list_wf": ["ev-meta"], "list_contrib": ["vpeddu"], "nb_contrib": 1, "codes": ["\nprocess Minimap2_nanopore { \n                                \npublishDir \"${params.OUTPUT}/Minimap2/${base}\", mode: 'symlink'\ncontainer \"vpeddu/nanopore_metagenomics\"\nbeforeScript 'chmod o+rw .'\ncpus 28\nerrorStrategy 'retry'\nmaxRetries params.MINIMAP2_RETRIES\ninput: \n    tuple val(base), file(species_fasta), file(r1)\noutput: \n    tuple val(\"${base}\"), file(\"${base}.sorted.filtered.*.bam\"), file(\"${base}.sorted.filtered.*.bam.bai\")\n    tuple val(\"${base}\"), file (\"${base}.*.unclassified_reads.txt\")\n\nscript:\n                       \n    if ( params.MINIMAPSPLICE ) {\n    \"\"\"\n    #!/bin/bash\n\n    #logging\n    echo \"ls of directory\" \n    ls -lah \n\n    echo \"running Minimap2 DNA on ${base}\"\n    #TODO: FILL IN MINIMAP2 COMMAND \n    minimap2 \\\n        -ax splice \\\n        -t \"\\$((${task.cpus}-2))\" \\\n        -2 \\\n        --split-prefix ${base}.split \\\n        ${species_fasta} \\\n        ${r1} | samtools view -Sb -@ 2 - > ${base}.bam\n\n    samtools view -Sb -F 4 -q 40 ${base}.bam > ${base}.filtered.bam\n    samtools sort -@ ${task.cpus} ${base}.filtered.bam -o ${base}.sorted.filtered.bam \n    #samtools index ${base}.sorted.filtered.bam\n    # output unclassified reads\n    samtools view -Sb -@  ${task.cpus} -f 4 ${base}.bam > ${base}.unclassified.bam\n\n    # cleanup intermediate file\n   # rm ${base}.bam\n\n    species_basename=`basename ${species_fasta} | cut -f1 -d .`\n\n    ##samtools fastq -@ 4 ${base}.unclassified.bam | gzip > ${base}.unclassified.fastq.gz\n    samtools view -Sb ${base}.unclassified.bam | cut -f1 > ${base}.\\$species_basename.unclassified_reads.txt\n    #echo \"reads in filtered bam\"\n    #samtools view -c ${base}.filtered.bam\n\n    #echo \"reads in unclassified bam\"\n    #samtools view -c  ${base}.unclassified.bam\n    \"\"\"\n        }\n\n    else {\n    \"\"\"\n    #!/bin/bash\n\n    #logging\n    echo \"ls of directory\" \n    ls -lah \n\n    species_basename=`basename ${species_fasta} | cut -f1 -d .`\n\n    # if this is the first attempt at running an alignment against this reference for this sample proceed\n\n    if [ \"${task.attempt}\" -eq \"1\" ]\n    then\n        echo \"running Minimap2 RNA on ${base}\"\n        # run minimap2 and pipe to bam output \n        minimap2 \\\n            -ax map-ont \\\n            -t \"\\$((${task.cpus}-4))\" \\\n            -2 \\\n            -K 25M \\\n            --split-prefix ${base}.split \\\n            ${species_fasta} \\\n            ${r1} | samtools view -Sb -@ 4 - > ${base}.bam\n\n        # extract mapped reads and sort \n        samtools view -Sb -F 4 ${base}.bam > ${base}.filtered.bam\n        samtools sort -@ ${task.cpus} ${base}.filtered.bam -o ${base}.sorted.filtered.bam \n\n        # output unclassified reads\n        samtools view -Sb -@  ${task.cpus} -f 4 ${base}.bam > ${base}.unclassified.bam\n\n        # cleanup intermediate file\n        # TODO uncomment later\n        rm ${base}.bam\n\n        # gather the read IDs of unassigned reads to extract from host filtered fastq downstream\n        samtools view ${base}.unclassified.bam | cut -f1 > ${base}.\\$species_basename.\\$RANDOM.unclassified_reads.txt\n        \n        # adding random identifier to species bams to avoid filename collisions while merging later\n        mv ${base}.sorted.filtered.bam ${base}.sorted.filtered.\\$species_basename.\\$RANDOM.bam\n\n        #index merged bam \n        samtools index ${base}.sorted.filtered.*.bam\n\n        # stats for reads mapped and unmapped\n        readsmapped=`samtools view -c ${base}.filtered.bam`\n        readsunmapped=`samtools view -c  ${base}.unclassified.bam`\n        echo \"reads in filtered bam\"\n        echo \\$readsmapped\n\n        echo \"reads in unclassified bam\"\n        echo \\$readsunmapped\n\n        # removing unclassified bam to save space\n        rm ${base}.unclassified.bam\n        \n        # for some reason if Minimap2 fails because it ran out of memory it doesn't exit the process\n        # To check for failed Minimap2, mapped and unmapped reads will both be 0, in which case the process crashes and reattempts\n        if [ \"\\$readsmapped\" -eq \"0\" -a \"\\$readsunmapped\" -eq \"0\" ]\n        then\n            echo \"minimap2 ran out of memory but failed to crash for ${base} retrying with fasta split\"\n            exit 1\n        fi\n    \n    # if process reattempts because it ran out of memory \n    else\n\n        # split the fasta into chunks smaller chunks depending on how many times the process has been attempted \n\n        echo \"running Minimap2 RNA on ${base} attempt ${task.attempt}\"\n        #echo \"fasta being split \\$split_num times\"\n        \n        # faSplit has some weird splitting activity but it works\n        /usr/local/miniconda/bin/faSplit sequence ${species_fasta} ${task.attempt} genus_split\n        \n        #NEED TO FIX: check within the loop for blank output. Minimap2 running out of memory might not crash the loop\n        # something like if bam empty, exit 1\n        for f in `ls genus_split*`\n        do\n            minimap2 \\\n                -ax map-ont \\\n                -t \"\\$((${task.cpus}-4))\" \\\n                -2 \\\n                --split-prefix ${base}.split \\\n                \\$f \\\n                ${r1} | samtools view -Sb -@ 4 - > ${base}.\\$f.bam\n            samtools sort -@ ${task.cpus} ${base}.\\$f.bam -o ${base}.sorted.temp.bam\n            bamcount=`samtools view -c ${base}.sorted.temp.bam`\n            # check if an individual split caused an out of memory error and exit the process \n            if [ \"\\$bamcount\" -eq \"0\" ]\n                then\n                echo \"minimap2 ran out of memory but failed to crash for ${base} retrying with fasta split\"\n                exit 1\n            fi\n            mv ${base}.sorted.temp.bam ${base}.sorted.\\$RANDOM.bam\n        done\n\n        # merge the fasta split alignments \n        samtools merge ${base}.merged.bam ${base}.sorted.*.bam\n\n        # extract mapped reads and sort the bam \n        samtools view -Sb -F 4 ${base}.merged.bam > ${base}.filtered.bam\n        samtools sort -@ ${task.cpus} ${base}.filtered.bam -o ${base}.sorted.filtered.bam \n\n        # output unclassified reads\n        samtools view -Sb -@  ${task.cpus} -f 4 ${base}.merged.bam > ${base}.unclassified.bam\n\n        # cleanup intermediate file to save space\n        rm ${base}.merged.bam\n\n        ##samtools fastq -@ 4 ${base}.unclassified.bam | pigz > ${base}.unclassified.fastq.gz\n        samtools view ${base}.unclassified.bam | cut -f1 > ${base}.\\$species_basename.\\$RANDOM.unclassified_reads.txt\n        \n        mv ${base}.sorted.filtered.bam ${base}.sorted.filtered.\\$species_basename.\\$RANDOM.bam\n        samtools index ${base}.sorted.filtered.*.bam\n\n        readsmapped=`samtools view -c ${base}.filtered.bam`\n        readsunmapped=`samtools view -c  ${base}.unclassified.bam`\n        echo \"reads in filtered bam\"\n        echo \\$readsmapped\n\n        echo \"reads in unclassified bam\"\n        echo \\$readsunmapped\n        \n        # removing unclassified bam to save space\n        rm ${base}.unclassified.bam\n        \n        # for some reason if Minimap2 fails because it ran out of memory it doesn't exit the process\n        # To check for failed Minimap2, mapped and unmapped reads will both be 0, in which case the process crashes and reattempts\n        if [ \"\\$readsmapped\" -eq \"0\" -a \"\\$readsunmapped\" -eq \"0\" ]\n        then\n            echo \"minimap2 ran out of memory but failed to crash for ${base} retrying with fasta split\"\n            exit 1\n        fi\n    fi\n    \"\"\"\n        }\n}"], "list_proc": ["vpeddu/ev-meta/Minimap2_nanopore"], "list_wf_names": ["vpeddu/ev-meta"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["wbazant"], "nb_wf": 1, "list_wf": ["CORALE"], "list_contrib": ["wbazant"], "nb_contrib": 1, "codes": ["\nprocess bowtie2Single {\n  label 'align'\n  input:\n  tuple val(sample), path(readsFastq)\n\n  output:\n  tuple val(sample), path(\"numReads.txt\"), path(\"alignmentsSingle.sam\")\n\n  script:\n  \"\"\"\n  grep -c '^@' ${readsFastq} > numReads.txt\n\n  ${params.bowtie2Command} \\\n    -x ${params.refdb} \\\n    -U ${readsFastq} \\\n    -S alignmentsSingle.sam \n  \"\"\"\n\n}"], "list_proc": ["wbazant/CORALE/bowtie2Single"], "list_wf_names": ["wbazant/CORALE"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["wbazant"], "nb_wf": 1, "list_wf": ["CORALE"], "list_contrib": ["wbazant"], "nb_contrib": 1, "codes": ["\nprocess bowtie2Paired {\n  label 'align'\n  input:\n  tuple val(sample), path(readsFastqR1), path(readsFastqR2)\n\n  output:\n  tuple val(sample), path(\"numReads.txt\"), path(\"alignmentsPaired.sam\")\n\n  script:\n  \"\"\"\n  grep -c '^@' ${readsFastqR1} > numReads.txt\n\n  ${params.bowtie2Command} \\\n    -x ${params.refdb} \\\n    -1 ${readsFastqR1} \\\n    -2 ${readsFastqR2} \\\n    -S alignmentsPaired.sam\n  \"\"\"\n}"], "list_proc": ["wbazant/CORALE/bowtie2Paired"], "list_wf_names": ["wbazant/CORALE"]}, {"nb_reuse": 2, "tools": ["Vireo", "totalVI"], "nb_own": 2, "list_own": ["wslh-bio", "wtsi-hgi"], "nb_wf": 2, "list_wf": ["nf_scrna_deconvolution", "dryad"], "list_contrib": ["AbigailShockey", "k-florek", "wtsi-mercury", "gn5"], "nb_contrib": 4, "codes": ["\nprocess clean_reads {\n  tag \"$name\"\n  publishDir \"${params.outdir}/trimming\", mode: 'copy',pattern:\"*.trim.txt\"\n\n  input:\n  set val(name), file(reads) from read_files_trimming\n\n  output:\n  tuple name, file(\"${name}_clean{_1,_2}.fastq.gz\") into cleaned_reads_shovill, cleaned_reads_fastqc, cleaned_reads_mapping, cleaned_reads_kraken\n  file(\"${name}_clean{_1,_2}.fastq.gz\") into cleaned_reads_snp\n  file(\"${name}.phix.stats.txt\") into phix_cleanning_stats\n  file(\"${name}.adapters.stats.txt\") into adapter_cleanning_stats\n  tuple file(\"${name}.phix.stats.txt\"),file(\"${name}.adapters.stats.txt\"),file(\"${name}.trim.txt\") into multiqc_clean_reads\n  file(\"${name}.trim.txt\") into bbduk_files\n\n  script:\n  \"\"\"\n  bbduk.sh in1=${reads[0]} in2=${reads[1]} out1=${name}.trimmed_1.fastq.gz out2=${name}.trimmed_2.fastq.gz qtrim=${params.trimdirection} qtrim=${params.qualitytrimscore} minlength=${params.minlength} tbo tbe &> ${name}.out\n  repair.sh in1=${name}.trimmed_1.fastq.gz in2=${name}.trimmed_2.fastq.gz out1=${name}.paired_1.fastq.gz out2=${name}.paired_2.fastq.gz\n  bbduk.sh in1=${name}.paired_1.fastq.gz in2=${name}.paired_2.fastq.gz out1=${name}.rmadpt_1.fastq.gz out2=${name}.rmadpt_2.fastq.gz ref=/bbmap/resources/adapters.fa stats=${name}.adapters.stats.txt ktrim=r k=23 mink=11 hdist=1 tpe tbo\n  bbduk.sh in1=${name}.rmadpt_1.fastq.gz in2=${name}.rmadpt_2.fastq.gz out1=${name}_clean_1.fastq.gz out2=${name}_clean_2.fastq.gz outm=${name}.matched_phix.fq ref=/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 stats=${name}.phix.stats.txt\n  grep -E 'Input:|QTrimmed:|Trimmed by overlap:|Total Removed:|Result:' ${name}.out > ${name}.trim.txt\n  \"\"\"\n}", "process vireo_with_genotype {\n    tag \"${samplename}.${donors_gt_vcf}\"\n\n    publishDir \"${params.outdir}/vireo_gt/${samplename}/\", mode: \"${params.vireo.copy_mode}\", overwrite: true,\n\t  saveAs: {filename -> filename.replaceFirst(\"vireo_${samplename}/\",\"\") }\n    \n    when: \n      params.run_with_genotype_input\n    \n    input:\n      tuple val(samplename), path(cell_data), path(donors_gt_vcf)\n    \n    output:\n      tuple val(samplename), path(\"vireo_${samplename}/*\"), emit: output_dir\n      tuple val(samplename), path(\"vireo_${samplename}/donor_ids.tsv\"), emit: sample_donor_ids \n      path(\"vireo_${samplename}/${samplename}.sample_summary.txt\"), emit: sample_summary_tsv\n      path(\"vireo_${samplename}/${samplename}__exp.sample_summary.txt\"), emit: sample__exp_summary_tsv\n\n    script:\n    \"\"\"\n\n      umask 2 # make files group_writable\n\n      vireo -c $cell_data -o vireo_${samplename} -d ${donors_gt_vcf} -t GT\n\n      # add samplename to summary.tsv,\n      # to then have Nextflow concat summary.tsv of all samples into a single file:\n\n      cat vireo_${samplename}/summary.tsv | \\\\\n        tail -n +2 | \\\\\n        sed s\\\"/^/${samplename}\\\\t/\\\"g > vireo_${samplename}/${samplename}.sample_summary.txt\n\n      cat vireo_${samplename}/summary.tsv | \\\\\n        tail -n +2 | \\\\\n        sed s\\\"/^/${samplename}__/\\\"g > vireo_${samplename}/${samplename}__exp.sample_summary.txt\n\n    \"\"\"\n}"], "list_proc": ["wslh-bio/dryad/clean_reads", "wtsi-hgi/nf_scrna_deconvolution/vireo_with_genotype"], "list_wf_names": ["wtsi-hgi/nf_scrna_deconvolution", "wslh-bio/dryad"]}, {"nb_reuse": 2, "tools": ["FastQC", "BCFtools"], "nb_own": 2, "list_own": ["wslh-bio", "wtsi-hgi"], "nb_wf": 2, "list_wf": ["nf_scrna_deconvolution", "dryad"], "list_contrib": ["AbigailShockey", "k-florek", "wtsi-mercury", "gn5"], "nb_contrib": 4, "codes": ["process guzip_vcf {\n    tag \"${samplename}\"\n\n             \n                                \n\n    input: \n        tuple val(samplename), path(genotypes)\n\n    output:\n        tuple val(samplename), path(\"${samplename}.vcf\"), emit: souporcell_vcf\n\n    script:\n    \"\"\"\n      bcftools view ${genotypes} -O v -o ${samplename}.vcf\n    \"\"\"\n}", "\nprocess fastqc {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n  input:\n  set val(name), file(reads) from combined_reads\n\n  output:\n  file(\"*_fastqc.{zip,html}\") into fastqc_results, fastqc_multiqc\n\n  script:\n  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"\n}"], "list_proc": ["wtsi-hgi/nf_scrna_deconvolution/guzip_vcf", "wslh-bio/dryad/fastqc"], "list_wf_names": ["wslh-bio/dryad", "wtsi-hgi/nf_scrna_deconvolution"]}, {"nb_reuse": 2, "tools": ["SAMtools", "kraken2"], "nb_own": 2, "list_own": ["wslh-bio", "xiaoli-dong"], "nb_wf": 2, "list_wf": ["magph", "dryad"], "list_contrib": ["AbigailShockey", "xiaoli-dong", "k-florek"], "nb_contrib": 3, "codes": ["process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools stats --threads ${task.cpus-1} ${reference} ${input} > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess kraken {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/kraken\", mode: 'copy', pattern: \"*.kraken2.txt*\"\n\n  input:\n  set val(name), file(reads) from cleaned_reads_kraken\n\n  output:\n  tuple name, file(\"${name}.kraken2.txt\") into kraken_files, kraken_multiqc\n  file(\"Kraken2_DB.txt\") into kraken_version\n\n  script:\n  \"\"\"\n  kraken2 --db /kraken2-db/minikraken2_v1_8GB --threads ${task.cpus} --report ${name}.kraken2.txt --paired ${reads[0]} ${reads[1]}\n\n  ls /kraken2-db/ > Kraken2_DB.txt\n  \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/SAMTOOLS_STATS", "wslh-bio/dryad/kraken"], "list_wf_names": ["xiaoli-dong/magph", "wslh-bio/dryad"]}, {"nb_reuse": 2, "tools": ["SAMtools", "shovill", "BWA"], "nb_own": 2, "list_own": ["wslh-bio", "xiaoli-dong"], "nb_wf": 2, "list_wf": ["magph", "dryad"], "list_contrib": ["AbigailShockey", "xiaoli-dong", "k-florek"], "nb_contrib": 3, "codes": ["\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools view $options.args $bam > ${prefix}.bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess shovill {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/assembled\", mode: 'copy',pattern:\"*.fa\"\n  publishDir \"${params.outdir}/mapping/sams\", mode: 'copy',pattern:\"*.sam\"\n\n  input:\n  set val(name), file(reads) from cleaned_reads_shovill\n\n  output:\n  tuple name, file(\"${name}.contigs.fa\") into assembled_genomes_quality, assembled_genomes_prokka\n  tuple name, file(\"${name}.assembly.sam\") into assembly_sams\n\n  script:\n  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory} --outdir ./output --R1 ${reads[0]} --R2 ${reads[1]} --force\n  mv ./output/contigs.fa ${name}.contigs.fa\n  bwa index ${name}.contigs.fa\n  bwa mem ${name}.contigs.fa ${reads[0]} ${reads[1]} > ${name}.assembly.sam\n  \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/SAMTOOLS_VIEW", "wslh-bio/dryad/shovill"], "list_wf_names": ["xiaoli-dong/magph", "wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["Taxa", "SIDR"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["dryad"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess prokka_setup {\n  tag \"$name\"\n\n  input:\n  file(kraken) from kraken_prokka\n  set val(name), file(input) from assembled_genomes_prokka\n\n  output:\n  tuple name, file(\"${name}.*.fa\") into prokka_input\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import pandas as pd\n  import shutil\n\n  genomeFile = '${input}'\n  sid = genomeFile.split('.')[0]\n  df = pd.read_csv('kraken_results.tsv', header=0, delimiter='\\\\t')\n  df = df[df['Sample'] == sid]\n  taxa = df.iloc[0]['Primary Species (%)']\n  taxa = taxa.split(' ')\n  taxa = taxa[0] + '_' + taxa[1]\n  shutil.copyfile(genomeFile, f'{sid}.{taxa}.fa')\n  \"\"\"\n}"], "list_proc": ["wslh-bio/dryad/prokka_setup"], "list_wf_names": ["wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["dryad"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess prokka {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/annotated\",mode:'copy'\n\n  input:\n  set val(name), file(assembly) from prokka_input\n\n  output:\n  file(\"${name}.gff\") into annotated_genomes\n  file(\"${name}.prokka.stats.txt\") into prokka_multiqc\n\n  script:\n  \"\"\"\n  filename=${assembly}\n  handle=\\${filename%.*}\n  taxa=\\${handle##*.}\n  genus=\\${taxa%_*}\n  species=\\${taxa##*_}\n\n  prokka --cpu ${task.cpus} --force --compliant --prefix ${name} --genus \\$genus --species \\$species --strain ${name} --mincontiglen 500 --outdir . ${assembly} > ${name}.log\n  mv ${name}.txt ${name}.prokka.stats.txt\n  \"\"\"\n}"], "list_proc": ["wslh-bio/dryad/prokka"], "list_wf_names": ["wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["dryad"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess samtools {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/mapping/bams\", mode: 'copy',pattern:\"*.sorted.bam*\"\n  publishDir \"${params.outdir}/mapping/depth\", mode: 'copy',pattern:\"*.depth.tsv\"\n  publishDir \"${params.outdir}/mapping/stats\", mode: 'copy',pattern:\"*.stats.txt\"\n\n  input:\n  set val(name), file(sam) from sam_files\n\n  output:\n  tuple name, file(\"*.depth.tsv\") into reference_depth_results,assembly_depth_results\n  tuple name, file(\"*.mapped.tsv\") into reference_mapped_results,assembly_mapped_results\n  file(\"*.stats.txt\")\n  file(\"*.sorted.bam*\")\n\n  shell:\n  \"\"\"\n  filename=${sam}\n  handle=\\${filename%.*}\n  type=\\${handle##*.}\n\n  samtools view -S -b ${name}.\\$type.sam > ${name}.\\$type.bam\n  samtools sort ${name}.\\$type.bam > ${name}.\\$type.sorted.bam\n  samtools index ${name}.\\$type.sorted.bam\n  samtools depth -a ${name}.\\$type.sorted.bam > ${name}.\\$type.depth.tsv\n  samtools stats ${name}.\\$type.sorted.bam > ${name}.\\$type.stats.txt\n  samtools view -c -F 260 ${name}.\\$type.sorted.bam > ${name}.\\$type.mapped.tsv\n  samtools view -c ${name}.\\$type.sorted.bam >> ${name}.\\$type.mapped.tsv\n  \"\"\"\n}"], "list_proc": ["wslh-bio/dryad/samtools"], "list_wf_names": ["wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["RDFScape"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["dryad"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": [" process reference_mapping_stats {\n      publishDir \"${params.outdir}/mapping\", mode: 'copy'\n\n      input:\n      file(depth) from reference_depth_results.collect()\n      file(mapped) from reference_mapped_results.collect()\n\n      output:\n      file('mapping_results.tsv') into reference_mapping_tsv\n\n      script:\n      \"\"\"\n      #!/usr/bin/env python3\n      import pandas as pd\n      import os\n      import glob\n      from functools import reduce\n\n      depth_files = glob.glob(\"*.reference.depth.tsv\")\n      depth_dfs = []\n      cols = [\"Sample\",\"Base Pairs Mapped to Reference >1X (%)\",\"Base Pairs Mapped to Reference >40X (%)\"]\n      depth_dfs.append(cols)\n\n      for file in depth_files:\n          sampleID = os.path.basename(file).split(\".\")[0]\n          depth_df = pd.read_csv(file, sep=\"\\\\t\", header=None)\n          overForty = int((len(depth_df[(depth_df[2]>40)])/len(depth_df)) * 100)\n          overOne = int((len(depth_df[(depth_df[2]>1)])/len(depth_df)) * 100)\n          stats = [sampleID, overOne, overForty]\n          depth_dfs.append(stats)\n      depth_df = pd.DataFrame(depth_dfs[1:], columns=depth_dfs[0])\n\n      read_files = glob.glob(\"*.reference.mapped.tsv\")\n      read_dfs = []\n      cols = [\"Sample\",\"Reads Mapped to Reference (%)\"]\n      read_dfs.append(cols)\n      for file in read_files:\n          sampleID = os.path.basename(file).split(\".\")[0]\n          read_df = pd.read_csv(file, sep=\"\\\\t\", header=None)\n          mapped_reads = read_df.iloc[0][0]\n          all_reads = read_df.iloc[1][0]\n          percent_mapped = int((mapped_reads/all_reads) * 100)\n          stats = [sampleID,percent_mapped]\n          read_dfs.append(stats)\n      read_dfs = pd.DataFrame(read_dfs[1:], columns=read_dfs[0])\n\n      dfs = [depth_df, read_dfs]\n      merged = reduce(lambda  left,right: pd.merge(left,right,on=[\"Sample\"], how=\"left\"), dfs)\n      merged[['Reads Mapped to Reference (%)','Base Pairs Mapped to Reference >1X (%)','Base Pairs Mapped to Reference >40X (%)']] = merged[['Reads Mapped to Reference (%)','Base Pairs Mapped to Reference >1X (%)','Base Pairs Mapped to Reference >40X (%)']].astype(str) + '%'\n      merged.to_csv(\"mapping_results.tsv\",sep=\"\\\\t\", index=False, header=True, na_rep=\"NaN\")\n      \"\"\"\n    }"], "list_proc": ["wslh-bio/dryad/reference_mapping_stats"], "list_wf_names": ["wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["RDFScape"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["dryad"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess merge_results {\n  publishDir \"${params.outdir}/\", mode: 'copy'\n\n  input:\n  file(bbduk) from bbduk_tsv\n  file(quast) from quast_tsv\n  file(assembly) from assembly_mapping_tsv\n  file(kraken) from kraken_tsv\n  file(vkraken) from kraken_version.first()\n  file(reference) from reference_mapping_tsv.ifEmpty{ 'empty' }\n\n  output:\n  file('dryad_report.csv')\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import glob\n  import pandas as pd\n  from functools import reduce\n\n  with open('Kraken2_DB.txt', 'r') as krakenFile:\n      krakenDB_version = krakenFile.readline().strip()\n\n\n  files = glob.glob('*.tsv')\n\n  dfs = []\n\n  for file in files:\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n      dfs.append(df)\n\n  merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'],\n                                              how='left'), dfs)\n  merged = merged.assign(krakenDB=krakenDB_version)\n\n  merged = merged.rename(columns={'Contigs':'Contigs (#)','krakenDB':'Kraken Database Verion'})\n\n  merged.to_csv('dryad_report.csv', index=False, sep=',', encoding='utf-8')\n  \"\"\"\n}"], "list_proc": ["wslh-bio/dryad/merge_results"], "list_wf_names": ["wslh-bio/dryad"]}, {"nb_reuse": 1, "tools": ["shovill", "BWA"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess shovill {\n  errorStrategy 'ignore'\n  tag \"$name\"\n\n  publishDir \"${params.outdir}/assembled\", mode: 'copy',pattern:\"*.fa\"\n  publishDir \"${params.outdir}/alignments\", mode: 'copy',pattern:\"*.sam\"\n\n  input:\n  set val(name), file(reads) from cleaned_reads_shovill\n\n  output:\n  tuple name, file(\"${name}.contigs.fa\") into assembled_genomes_quality, assembled_genomes_ar, assembled_genomes_mlst\n  tuple name, file(\"${name}.sam\") into sam_files\n\n  script:\n  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory} --outdir ./output --R1 ${reads[0]} --R2 ${reads[1]} --force\n  mv ./output/contigs.fa ${name}.contigs.fa\n  bwa index ${name}.contigs.fa\n  bwa mem ${name}.contigs.fa ${reads[0]} ${reads[1]} > ${name}.sam\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/shovill"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["MED", "SIDR"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess coverage_stats {\n  publishDir \"${params.outdir}/coverage\", mode: 'copy'\n\n  input:\n  file(cov) from cov_files.collect()\n\n  output:\n  file('coverage_stats.tsv') into coverage_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import glob\n  import os\n  from numpy import median\n  from numpy import average\n\n  # function for summarizing samtools depth files\n  def summarize_depth(file):\n      # get sample id from file name and set up data list\n      sid = os.path.basename(file).split('.')[0]\n      data = []\n      # open samtools depth file and get depth\n      with open(file,'r') as inFile:\n          for line in inFile:\n              data.append(int(line.strip().split()[2]))\n      # get median and average depth\n      med = int(median(data))\n      avg = int(average(data))\n      # return sample id, median and average depth\n      result = f\"{sid}\\\\t{med}\\\\t{avg}\\\\n\"\n      return result\n\n  # get all samtools depth files\n  files = glob.glob(\"*.depth.tsv\")\n\n  # summarize samtools depth files\n  results = map(summarize_depth,files)\n\n  # write results to file\n  with open('coverage_stats.tsv', 'w') as outFile:\n      outFile.write(\"Sample\\\\tMedian Coverage\\\\tAverage Coverage\\\\n\")\n      for result in results:\n          outFile.write(result)\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/coverage_stats"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["RDFScape", "SIDR", "IDSM"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess mlst {\n  tag \"$name\"\n\n  publishDir \"${params.outdir}/mlst\", mode: 'copy', pattern: \"*.mlst.tsv*\"\n\n  input:\n  set val(name), file(input) from assembled_genomes_mlst\n\n  output:\n  file(\"*.tsv\") into mlst_results\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import subprocess as sub\n  import shlex\n  import pandas as pd\n  from functools import reduce\n  import shutil\n  import glob\n\n  # Function for running mlst on samples with multiple schemes\n  def run_schemes(first_scheme,all_schemes,sample):\n      # Subtract selected scheme from list of schemes to get remaining schemes\n      remaining_schemes = list(set(all_schemes) - set([first_scheme]))\n      # Run mlst on remaining schemes\n      for i in remaining_schemes:\n          outFile = open(f'{sample}.{i}.tsv','w')\n          cmd = shlex.split(f'mlst --nopath --exclude sthermophilus --scheme {i} {sample}.contigs.fa')\n          sub.Popen(cmd, stdout=outFile).wait()\n\n  # Read in fasta file\n  inFile = '${input}'\n  sid = inFile.split('.')[0]\n\n  # Open outfile and run mlst\n  outFile = open(f'{sid}.tsv','w')\n  cmd = shlex.split(f'mlst --nopath --exclude sthermophilus {sid}.contigs.fa')\n  sub.Popen(cmd, stdout=outFile).wait()\n\n  # Lists organisms with multiple schemes\n  abaumannii_schemes = ['abaumannii','abaumannii_2']\n  ecoli_schemes = ['ecoli','ecoli_2']\n  leptospira_schemes = ['leptospira','leptospira_2','leptospira_3']\n  vcholerae_schemes = ['vcholerae','vcholerae2']\n\n  # Dictionary of scheme names\n  ids = {'mlstID':['abaumannii','abaumannii_2','ecoli','ecoli_2','leptospira','leptospira_2','leptospira_3','vcholerae2','vcholerae'],\n  'PubMLSTID':['Oxford','Pasteur','Achtman','Pasteur ','Scheme 1','Scheme 2', 'Scheme 3','O1 and O139','']}\n  ids = dict(zip(ids['mlstID'], ids['PubMLSTID']))\n\n  # read in mlst output and get scheme\n  df = pd.read_csv(f'{sid}.tsv', header=None, delimiter='\\\\t')\n  scheme = df.iloc[0][1]\n\n  # Add scheme to mlst file name\n  if scheme == '-':\n      shutil.move(f'{sid}.tsv',f'{sid}.NA.tsv')\n  else:\n      shutil.move(f'{sid}.tsv', f'{sid}.{scheme}.tsv')\n\n  dfs = []\n\n  # Check and run multiple schemes\n  if any(x in scheme for x in abaumannii_schemes):\n      run_schemes(scheme,abaumannii_schemes,sid)\n  if any(x in scheme for x in ecoli_schemes):\n      run_schemes(scheme,ecoli_schemes,sid)\n  if any(x in scheme for x in leptospira_schemes):\n      run_schemes(scheme,leptospira_schemes,sid)\n  if any(x in scheme for x in vcholerae_schemes):\n      run_schemes(scheme,vcholerae_schemes,sid)\n\n  # Get list of mlst files and set up empty list\n  mlst_files = glob.glob('*.tsv')\n\n  # Reformat MLST results and append to empty list\n  for file in mlst_files:\n      df = pd.read_csv(file, header=None, delimiter='\\\\t')\n      df[0] = df[0].str.replace('.contigs.fa', '')\n      df[2] = 'ST' + df[2].astype(str)\n      df[2] = df[2].str.replace('ST-', 'NA')\n\n      if len(mlst_files) > 1:\n          # Replace mlst scheme names with PubMLST scheme names\n          for old, new in ids.items():\n              df[1] = df[1].replace(to_replace=old, value=new)\n      else:\n          # Remove scheme name\n          df.iloc[0,1] = ''\n          df[2] = df[2].str.replace('NA', 'No scheme available')\n      # Join ST to PubMLST scheme names\n      df['MLST Scheme'] = df[[1,2]].agg(' '.join, axis=1)\n      df = df[[0,'MLST Scheme']]\n      df.columns =['Sample','MLST Scheme']\n      df['MLST Scheme'] = df['MLST Scheme'].replace('\\\\s+', ' ', regex=True)\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace('NA -', 'NA', regex=True)\n      dfs.append(df)\n\n  # Merge multiple dataframes (separated by ;) and write to file\n  if len(dfs) > 1:\n      merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'], how='left'), dfs)\n      merged = merged.reindex(sorted(merged.columns,reverse=True), axis=1)\n      merged['MLST Scheme'] = merged.iloc[: , 1:].agg(';'.join, axis=1)\n      merged = merged[['Sample','MLST Scheme']]\n      merged.to_csv(f'{sid}.mlst.tsv', index=False, sep='\\\\t', encoding='utf-8')\n\n  else:\n      df = dfs[0]\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace(' S', 'S')\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace(' N', 'N')\n      df.to_csv(f'{sid}.mlst.tsv', index=False, sep='\\\\t', encoding='utf-8')\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/mlst"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["RDFScape"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess mlst_formatting {\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/mlst\",mode:'copy'\n\n  input:\n  file(mlst) from mlst_results.collect()\n\n  output:\n  file(\"mlst_results.tsv\") into mlst_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  files = glob.glob('*.mlst.tsv')\n  dfs = []\n  for file in files:\n      df = pd.read_csv(file, sep='\\\\t')\n      dfs.append(df)\n  dfs_concat = pd.concat(dfs)\n  dfs_concat['MLST Scheme'] = dfs_concat['MLST Scheme'].str.replace('-:NA', 'No Scheme Available')\n  dfs_concat.to_csv(f'mlst_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/mlst_formatting"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["Taxa", "SIDR", "JSpecies"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess amrfinder_setup {\n  tag \"$name\"\n\n  input:\n  file(kraken) from kraken_amr\n  set val(name), file(input) from assembled_genomes_ar\n\n  output:\n  tuple name, file(\"${name}.*.fa\") into ar_input\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import pandas as pd\n  import glob\n  import shutil\n  \n  # species and genus lists\n  species = ['Acinetobacter_baumannii','Enterococcus_faecalis','Enterococcus_faecium','Staphylococcus_aureus','Staphylococcus_pseudintermedius','Streptococcus_agalactiae','Streptococcus_pneumoniae','Streptococcus_pyogenes']\n  genus = ['Campylobacter','Escherichia','Klebsiella','Salmonella']\n  \n  # get sample name from fasta file\n  genomeFile = '${input}'\n  sid = genomeFile.split('.')[0]\n  \n  # read in kraken results as data frame\n  df = pd.read_csv('kraken_results.tsv', header=0, delimiter='\\\\t')\n  \n  # subset data frame by sample id\n  df = df[df['Sample'] == sid]\n  \n  # get primary species and genus identified\n  if df.empty:\n      taxa_species = 'NA'\n      taxa_genus = 'NA'\n  else:\n      taxa = df.iloc[0]['Primary Species (%)']\n      taxa = taxa.split(' ')\n      taxa_species = taxa[0] + '_' + taxa[1]\n      taxa_genus = taxa[0]\n  \n  # add taxa or genus name to file name if present in lists\n  if any(x in taxa_species for x in species):\n      shutil.copyfile(genomeFile, f'{sid}.{taxa_species}.fa')\n  elif any(x in taxa_genus for x in genus):\n      shutil.copyfile(genomeFile, f'{sid}.{taxa_genus}.fa')\n  elif taxa_genus == 'Shigella':\n      shutil.copyfile(genomeFile, f'{sid}.Escherichia.fa')\n  else:\n      shutil.copyfile(genomeFile, f'{sid}.NA.fa')\n\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/amrfinder_setup"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["readfastafile", "ORGANISMS", "SIDR"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess amrfinder {\n  tag \"$name\"\n  publishDir \"${params.outdir}/amrfinder\",mode: 'copy',pattern:\"*.amr.tsv\"\n\n  input:\n  set val(name), file(input) from ar_input\n\n  output:\n  tuple name, file(\"${name}.amr.tsv\") into ar_predictions\n  file(\"AMRFinderPlus_DB.txt\") into amrfinder_version\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import subprocess as sub\n  import shlex\n  import glob\n  import shutil\n\n  # organism list\n  organisms = ['Acinetobacter_baumannii','Enterococcus_faecalis','Enterococcus_faecium','Staphylococcus_aureus','Staphylococcus_pseudintermedius','Streptococcus_agalactiae','Streptococcus_pneumoniae','Streptococcus_pyogenes','Campylobacter','Escherichia','Klebsiella','Salmonella','Escherichia']\n\n  # get sample id and organism name from fasta file\n  fastaFile = '${input}'\n  sid = fastaFile.split('.')[0]\n  organism = fastaFile.split('.')[1]\n\n  # run amrfinder using --organism if present in organism list\n  if any(x in organism for x in organisms):\n      outFile = open(f'{sid}.amr.tsv','w')\n      cmd = shlex.split(f'amrfinder -n {sid}.{organism}.fa --organism {organism}')\n      sub.Popen(cmd, stdout=outFile).wait()\n  # otherwise run amrfinder without --organism\n  else:\n      outFile = open(f'{sid}.amr.tsv','w')\n      cmd = shlex.split(f'amrfinder -n {sid}.{organism}.fa')\n      sub.Popen(cmd, stdout=outFile).wait()\n\n  # get version information from version file\n  versionFile = \"/amrfinder/data/latest/version.txt\"\n  shutil.copy(versionFile,\"AMRFinderPlus_DB.txt\")\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/amrfinder"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["maskBAD", "RDFScape", "Gene"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess amrfinder_summary {\n  publishDir \"${params.outdir}/amrfinder\",mode:'copy'\n\n  input:\n  file(predictions) from ar_predictions.collect()\n\n  output:\n  file(\"ar_predictions.tsv\")\n  file(\"ar_summary.tsv\") into ar_tsv\n  file(\"selected_ar_genes.tsv\") into selected_ar_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n\n  # get amrfinder output files and set up lists\n  files = glob.glob('*.amr.tsv')\n  dfs = []\n  all_ar_dfs = []\n  selected_ar_dfs = []\n\n  # function for cleanining up amrfinder output\n  def pretty_df(data,sample):\n      data.columns = data.columns.str.replace(' ', '_')\n      data = data.assign(Sample=sample)\n      data = data[['Sample','Gene_symbol','%_Coverage_of_reference_sequence','%_Identity_to_reference_sequence']]\n      pretty_data = data.set_axis(['Sample', 'Gene', 'Coverage', 'Identity'], axis=1, inplace=False)\n      return pretty_data\n\n  # function for joining amrfinder results by a delimiter\n  def join_df(data,sample,delim):\n      gene = data['Gene'].tolist()\n      gene = delim.join(gene)\n      coverage = data['Coverage'].tolist()\n      coverage = delim.join(map(str, coverage))\n      identity = data['Identity'].tolist()\n      identity = delim.join(map(str, identity))\n      joined_data = [[sample,gene,coverage,identity]]\n      joined_data = pd.DataFrame(joined_data, columns = ['Sample', 'Gene', 'Coverage', 'Identity'])\n      return joined_data\n\n  for file in files:\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0]\n      # read in amrfinder results as data frame\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n\n      # clean up data frame\n      df = pretty_df(df,sample_id)\n      dfs.append(df)\n\n      # summarize all results\n      all_ar_df = join_df(df,sample_id,';')\n      all_ar_dfs.append(all_ar_df)\n\n      # subset data frame by selected genes\n      mask = df['Gene'].str.contains(${params.selected_genes}, case=False, na=False)\n      masked_df = df[mask]\n      # check if any select genes were found\n      if masked_df.empty:\n          masked_df = masked_df.append({'Sample' : sample_id, 'Gene' : 'None', 'Coverage' : 'None','Identity' : 'None'}, ignore_index = True)\n      selected_ar_df = join_df(masked_df,sample_id,';')\n      selected_ar_df = selected_ar_df.set_axis(['Sample', 'Selected AMR Genes', 'Selected AMR Genes Coverage', 'Selected AMR Genes Identity'], axis=1, inplace=False)\n      selected_ar_dfs.append(selected_ar_df)\n\n  # concatenate results and write to tsv\n  concat_dfs = pd.concat(dfs)\n  concat_dfs.to_csv('ar_predictions.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n\n  # concatenate joined restults and write to tsv\n  concat_all_ar_dfs = pd.concat(all_ar_dfs)\n  concat_selected_ar_dfs = pd.concat(selected_ar_dfs)\n\n  # concatenate selected genes and write to tsv\n  concat_all_ar_dfs.to_csv('ar_summary.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  concat_selected_ar_dfs.to_csv('selected_ar_genes.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/amrfinder_summary"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["RDFScape"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess merge_results {\n  publishDir \"${params.outdir}/\", mode: 'copy'\n\n  input:\n  file(bbduk) from bbduk_tsv\n  file(quast) from quast_tsv\n  file(coverage) from coverage_tsv\n  file(mlst) from mlst_tsv\n  file(kraken) from kraken_tsv\n  file(amr) from ar_tsv\n  file(selected_ar) from selected_ar_tsv\n  file(vkraken) from kraken_version.first()\n  file(vamrfinder) from amrfinder_version.first()\n\n  output:\n  file('spriggan_report.csv')\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import glob\n  import pandas as pd\n  from functools import reduce\n\n  with open('AMRFinderPlus_DB.txt', 'r') as amrFile:\n      amrfinderDB_version = amrFile.readline().strip()\n\n  with open('Kraken2_DB.txt', 'r') as krakenFile:\n      krakenDB_version = krakenFile.readline().strip()\n\n  files = glob.glob('*.tsv')\n\n  dfs = []\n\n  for file in files:\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n      dfs.append(df)\n\n  merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'],how='left'), dfs)\n  merged = merged.assign(krakenDB=krakenDB_version)\n  merged = merged.assign(amrDB=amrfinderDB_version)\n  merged = merged[['Sample','Total Reads','Reads Removed','Median Coverage','Average Coverage','Contigs','Assembly Length (bp)','N50','Primary Species (%)','Secondary Species (%)','Unclassified Reads (%)','krakenDB','MLST Scheme','Gene','Coverage','Identity','Selected AMR Genes','Selected AMR Genes Coverage','Selected AMR Genes Identity','amrDB']]\n  merged = merged.rename(columns={'Contigs':'Contigs (#)','Average Coverage':'Mean Coverage','Gene':'AMR','Coverage':'AMR Coverage','Identity':'AMR Identity','krakenDB':'Kraken Database Verion','amrDB':'AMRFinderPlus Database Version'})\n\n  merged.to_csv('spriggan_report.csv', index=False, sep=',', encoding='utf-8')\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/merge_results"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["wslh-bio"], "nb_wf": 1, "list_wf": ["spriggan"], "list_contrib": ["AbigailShockey", "k-florek"], "nb_contrib": 2, "codes": ["\nprocess multiqc {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  input:\n  file(a) from multiqc_clean_reads.collect()\n  file(b) from fastqc_multiqc.collect()\n  file(c) from stats_multiqc.collect()\n  file(d) from kraken_multiqc.collect()\n  file(e) from quast_multiqc.collect()\n  file(config) from multiqc_config\n\n  output:\n  file(\"*.html\") into multiqc_output\n\n  script:\n  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"\n}"], "list_proc": ["wslh-bio/spriggan/multiqc"], "list_wf_names": ["wslh-bio/spriggan"]}, {"nb_reuse": 2, "tools": ["FeatureCounts", "MultiQC"], "nb_own": 1, "list_own": ["wtsi-hgi"], "nb_wf": 2, "list_wf": ["nextflow-pipelines", "associations"], "list_contrib": ["Sanger-ad7", "wtsi-mercury", "gn5"], "nb_contrib": 3, "codes": ["\nprocess featureCounts {\n    tag \"${samplename}\"\n    container \"nfcore-rnaseq\"\n    memory = '5G'\n    time '300m'\n    cpus 1\n    errorStrategy { task.attempt <= 5 ? 'retry' : 'ignore' }\n    maxRetries 5\n    publishDir \"${params.outdir}/featureCounts/\", mode: 'symlink',\n        saveAs: {filename ->\n            if (filename.indexOf(\".biotype_counts_mqc.txt\") > 0) \"biotype_counts_mqc/$filename\"\n            else if (filename.indexOf(\".biotype.fc.txt\") > 0) \"biotype_counts/$filename\"\n            else if (filename.indexOf(\".biotype.fc.txt.summary\") > 0) \"biotype_counts_summaries/$filename\"\n            else if (filename.indexOf(\".gene.fc.txt.summary\") > 0) \"gene_count_summaries/$filename\"\n            else if (filename.indexOf(\".gene.fc.txt\") > 0) \"gene_counts/$filename\"\n            else \"$filename\"\n        }\n\n    when:\n    params.run\n    \n    input:\n    set val(aligner), val(samplename), file(thebam)                        \n    file gtf                                          \n    file biotypes_header\n\n    output:\n    set val(aligner), file(\"*.gene.fc.txt\")                   \n    set val(aligner), file(\"*.gene.fc.txt.summary\")                     \n    set val(aligner), file(\"*.biotype_counts_mqc.txt\")                            \n\n    script:\n    def extraparams = params.fcextra.toString() - ~/^dummy/\n    def fc_direction = 0\n    def tag = \"${samplename}.${aligner}\"\n\n    def pairedend = params.singleend ? \"\" : \"-p\"\n    if (params.forward_stranded && !params.unstranded) {\n        fc_direction = 1\n    } else if (params.reverse_stranded && !params.unstranded){\n        fc_direction = 2\n    }\n    outfile = \"${tag}.gene.fc.txt\"\n    \"\"\"\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:\\$PATH\n\n    featureCounts -T ${task.cpus} -a $gtf -g gene_id          \\\\\n      -o ${outfile} $pairedend                                \\\\\n      -s $fc_direction ${extraparams} $thebam\n    cut -f 1,7 ${outfile} > reduced.${outfile}   #  This\n    mv reduced.${outfile} ${outfile}             #  reduces the file size from ~ 30M to ~1M\n    featureCounts -T ${task.cpus} -a $gtf -g gene_id  \\\\\n      -o ${tag}.biotype.fc.txt $pairedend                     \\\\\n      -s $fc_direction ${extraparams} $thebam\n    cut -f 1,7 ${tag}.biotype.fc.txt |                        \\\\\n        tail -n +3 | cat $biotypes_header - >> ${tag}.biotype_counts_mqc.txt\n    \"\"\"\n}", "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.10.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}"], "list_proc": ["wtsi-hgi/nextflow-pipelines/featureCounts", "wtsi-hgi/associations/MULTIQC"], "list_wf_names": ["wtsi-hgi/nextflow-pipelines", "wtsi-hgi/associations"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC"], "nb_own": 1, "list_own": ["wtsi-hgi"], "nb_wf": 2, "list_wf": ["nextflow-pipelines", "associations"], "list_contrib": ["Sanger-ad7", "wtsi-mercury", "gn5"], "nb_contrib": 3, "codes": ["\nprocess index_cram {\n    memory '3G'\n    tag \"$cram_file\"\n    cpus 1\n                   \n    time '100m'\n    queue 'normal'\n    container \"graphtyper\"\n    containerOptions = \"--bind /lustre\"\n                                \n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    publishDir \"${params.outdir}/cram_index/\", mode: 'symlink', overwrite: true, pattern: \"${cram_file}.crai\"\n    maxRetries 3\n\n    when:\n    params.run\n     \n    input:\n    file(cram_file)\n\n    output: \n    tuple file(\"${cram_file}.crai\"), emit: indexes\n\n    script:\n\"\"\" \nsamtools index $cram_file\n\"\"\"\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}"], "list_proc": ["wtsi-hgi/nextflow-pipelines/index_cram", "wtsi-hgi/associations/FASTQC"], "list_wf_names": ["wtsi-hgi/nextflow-pipelines", "wtsi-hgi/associations"]}, {"nb_reuse": 1, "tools": ["Maligner"], "nb_own": 1, "list_own": ["wtsi-hgi"], "nb_wf": 1, "list_wf": ["nextflow-pipelines"], "list_contrib": ["wtsi-mercury", "gn5"], "nb_contrib": 2, "codes": ["\nprocess merge_featureCounts {\n    tag \"$aligner\"\n    scratch '/tmp'\n    stageInMode 'copy'\n    stageOutMode 'rsync'\n    container \"nfcore-rnaseq\"\n    publishDir \"${params.outdir}/combined\", mode: 'symlink'\n    containerOptions = \"--bind /lustre\"\n    label 'merge_feature'\n    memory = '100G'\n    cpus 2\n    time '600m'\n    errorStrategy { task.attempt <= 3 ? 'retry' : 'ignore' }\n    maxRetries 3\n\n    when:\n    params.run\n\n    input:\n    file(collected_fc_gene_txt)\n\n    output:\n    file '*-fc-genecounts.txt'\n    file(\"fofn_gene_featurecount.txt\")\n\n    shell:\n    suffix=['star':'.star.gene.fc.txt', 'hisat2':'.hisat2.gene.fc.txt']\n    aligner = \"star\"                           \n    outputname = \"${params.runtag}-${aligner}-fc-genecounts.txt\"\n    thesuffix  = suffix[aligner] ?: '.txt'\n    '''\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:$PATH\n\n    ls . | grep gene.fc.txt\\$ > fofn_gene_featurecount.txt\n\n    python3 !{workflow.projectDir}/../bin/rna_seq/merge_featurecounts.py        \\\\\n      --rm-suffix !{thesuffix}                                       \\\\\n      -c 1 --skip-comments --header                                  \\\\\n      -o !{outputname} -I fofn_gene_featurecount.txt\n    '''\n}"], "list_proc": ["wtsi-hgi/nextflow-pipelines/merge_featureCounts"], "list_wf_names": ["wtsi-hgi/nextflow-pipelines"]}, {"nb_reuse": 5, "tools": ["TiPs", "MLST", "BWA", "MultiQC"], "nb_own": 4, "list_own": ["yassineS", "yqshao", "zamanianlab", "xiaoli-dong"], "nb_wf": 5, "list_wf": ["tips", "pathogen", "nf-demux", "magph", "Core_RNAseq-nf"], "list_contrib": ["yqshao", "xiaoli-dong", "mzamanian", "yassineS", "chenthorn"], "nb_contrib": 5, "codes": ["\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/mlst:2.19.0--hdfd78af_1\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "\nprocess multiqc {\n    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n\n    input:\n    file multiqc_config from ch_multiqc_config\n                                                                                  \n    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n    file ('software_versions/*') from software_versions_yaml.collect()\n    file workflow_summary from create_workflow_summary(summary)\n\n    output:\n    file \"*multiqc_report.html\" into multiqc_report\n    file \"*_data\"\n    file \"multiqc_plots\"\n\n    script:\n    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n                                                                                       \n    \"\"\"\n    multiqc -f $rtitle $rfilename --config $multiqc_config .\n    \"\"\"\n}", "\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/mlst:2.19.0--hdfd78af_1\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mlst $options.args --label ${prefix} --threads $task.cpus $fasta > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "\nprocess tipsFilter {\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n    label 'tips'\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz'), emit: out\n    path('output.idx'), emit: idx\n\n    script:\n    setup = getParams(defaults, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    tips filter ${fileList(setup.ds)} $setup.inp -o output -of xyz\n    \"\"\"\n\n    stub:\n    setup = getParams(defaults, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}", "\nprocess build_bwa_index {\n\n    cpus huge\n\n    input:\n        file(\"reference.fa\") from bwa_index\n\n    output:\n        file \"reference.*\" into bwa_indices\n\n    \"\"\"\n        bwa index reference.fa\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/MLST", "yassineS/nf-demux/multiqc", "xiaoli-dong/pathogen/MLST", "yqshao/tips/tipsFilter", "zamanianlab/Core_RNAseq-nf/build_bwa_index"], "list_wf_names": ["xiaoli-dong/magph", "yqshao/tips", "yassineS/nf-demux", "zamanianlab/Core_RNAseq-nf", "xiaoli-dong/pathogen"]}, {"nb_reuse": 1, "tools": ["Bowtie"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 1, "list_wf": ["magph"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["process BOWTIE2_BUILD {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bowtie2:2.4.4--py39hbb4e92a_0' :\n        'quay.io/biocontainers/bowtie2:2.4.4--py36hd4290be_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path 'bowtie2'      , emit: index\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bowtie2\n    bowtie2-build $args --threads $task.cpus $fasta bowtie2/${fasta.baseName}\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bowtie2: \\$(echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/BOWTIE2_BUILD"], "list_wf_names": ["xiaoli-dong/magph"]}, {"nb_reuse": 2, "tools": ["Prokka", "TiPs"], "nb_own": 2, "list_own": ["yqshao", "xiaoli-dong"], "nb_wf": 2, "list_wf": ["tips", "magph"], "list_contrib": ["yqshao", "xiaoli-dong"], "nb_contrib": 2, "codes": ["\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"*.gff\"), emit: gff\n    tuple val(meta), path(\"*.gbk\"), emit: gbk\n    tuple val(meta), path(\"*.fna\"), emit: fna\n    tuple val(meta), path(\"*.faa\"), emit: faa\n    tuple val(meta), path(\"*.ffn\"), emit: ffn\n    tuple val(meta), path(\"*.sqn\"), emit: sqn\n    tuple val(meta), path(\"*.fsa\"), emit: fsa\n    tuple val(meta), path(\"*.tbl\"), emit: tbl\n    tuple val(meta), path(\"*.err\"), emit: err\n    tuple val(meta), path(\"*.log\"), emit: log\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n    mv ${prefix}/* .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess tipsFilter {\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n    label 'tips'\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz'), emit: out\n    path('output.idx'), emit: idx\n\n    script:\n    setup = getParams(defaults, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    tips filter ${fileList(setup.ds)} $setup.inp -o output -of xyz\n    \"\"\"\n\n    stub:\n    setup = getParams(defaults, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/PROKKA", "yqshao/tips/tipsFilter"], "list_wf_names": ["yqshao/tips", "xiaoli-dong/magph"]}, {"nb_reuse": 4, "tools": ["StringTie", "Ensembler", "SAMtools", "STEPS", "SCALCE", "STAR"], "nb_own": 3, "list_own": ["yqshao", "zamanianlab", "xiaoli-dong"], "nb_wf": 3, "list_wf": ["tips", "magph", "Core_RNAseq-nf"], "list_contrib": ["mzamanian", "yqshao", "xiaoli-dong", "chenthorn"], "nb_contrib": 4, "codes": ["\nprocess pinnSample {\n    label 'pinn'\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz')\n\n    script:\n    setup = getParams(sampleDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env python3\n    import pinn, os\n    import numpy as np\n    import tensorflow as tf\n    from ase import units\n    from ase.io import read, write\n    from ase.io.trajectory import Trajectory\n    from ase.md import MDLogger\n    from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n    from ase.md.nptberendsen import NPTBerendsen\n    from ase.md.nvtberendsen import NVTBerendsen\n\n    os.symlink(\"${file(setup.inp)}\", \"model\")\n    calc = pinn.get_calc(\"model\")\n    ensemble = \"$setup.pinnEnsemble\"\n    for seed in range($setup.pinnCopies):\n        rng = np.random.default_rng(seed)\n        atoms = read(\"${file(setup.init)}\")\n        atoms.set_calculator(calc)\n        MaxwellBoltzmannDistribution(atoms, $setup.pinnTemp*units.kB, rng=rng)\n        dt = 0.5 * units.fs\n        steps = int($setup.pinnTime*1e3*units.fs/dt)\n        if ensemble=='NPT':\n            dyn = NPTBerendsen(atoms, timestep=dt, temperature=$setup.pinnTemp, pressure=$setup.pinnPress,\n                               taut=dt*$setup.pinnTaut, taup=dt*$setup.pinnTaup, compressibility=$setup.pinnCompress)\n        elif ensemble=='NVT':\n            dyn = NVTBerendsen(atoms, timestep=dt, temperature=$setup.pinnTemp, taut=dt * 100)\n        else:\n            raise NotImplementedError(f\"Unkown ensemble {ensemble}\")\n        interval = int($setup.pinnEvery*1e3*units.fs/dt)\n        dyn.attach(MDLogger(dyn, atoms, 'output.log', mode=\"a\"), interval=interval)\n        dyn.attach(Trajectory('output.traj', 'a', atoms).write, interval=interval)\n        try:\n            dyn.run(steps)\n        except:\n            pass\n    traj = read('output.traj', index=':')\n    [atoms.wrap() for atoms in traj]\n    write('output.xyz', traj)\n    \"\"\"\n\n    stub:\n    setup = getParams(sampleDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}", "\nprocess hisat_stringtie {\n\n    publishDir \"${output}/${params.dir}/hisat\", mode: 'copy', pattern: '**/*'\n\n    cpus small\n    tag { id }\n\n    when:\n      params.hisat\n\n    input:\n        file(\"geneset.gtf.gz\") from geneset_stringtie\n        tuple val(id), (\"${id}.bam\"), file(\"${id}.bam.bai\") from bam_files_stringtie\n\n    output:\n        file(\"${id}/*\") into stringtie_exp\n\n    script:\n\n        \"\"\"\n          zcat geneset.gtf.gz > geneset.gtf\n          stringtie ${id}.bam -p ${task.cpus} -G geneset.gtf -A ${id}/${id}_abund.tab -e -B -o ${id}/${id}_expressed.gtf\n          rm *.gtf\n        \"\"\"\n\n}", "\nprocess star_index {\n\n    cpus big\n\n    when:\n      params.star\n\n    input:\n        file(\"geneset.gtf.gz\") from geneset_star\n        file(\"reference.fa.gz\") from reference_star\n\n    output:\n        file(\"STAR_index/*\") into star_indices\n\n    script:\n        overhang = params.rlen - 1\n\n    \"\"\"\n        zcat reference.fa.gz > reference.fa\n        zcat geneset.gtf.gz > geneset.gtf\n        mkdir STAR_index\n\n        STAR --runThreadN ${task.cpus} --runMode genomeGenerate  --genomeDir STAR_index \\\n          --genomeFastaFiles reference.fa \\\n          --sjdbGTFfile geneset.gtf \\\n          --sjdbOverhang ${overhang}\n    \"\"\"\n\n}", "\nprocess SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n    } else {\n        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n    }\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    script:\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["yqshao/tips/pinnSample", "zamanianlab/Core_RNAseq-nf/hisat_stringtie", "zamanianlab/Core_RNAseq-nf/star_index", "xiaoli-dong/magph/SAMTOOLS_SORT"], "list_wf_names": ["yqshao/tips", "zamanianlab/Core_RNAseq-nf", "xiaoli-dong/magph"]}, {"nb_reuse": 2, "tools": ["SAMtools", "BWA", "pilon"], "nb_own": 2, "list_own": ["zamanianlab", "xiaoli-dong"], "nb_wf": 2, "list_wf": ["magph", "Core_RNAseq-nf"], "list_contrib": ["mzamanian", "chenthorn", "xiaoli-dong"], "nb_contrib": 3, "codes": ["\nprocess PILON {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'pilon=1.24' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/pilon%3A1.24--hdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/pilon%3A1.24--hdfd78af_0\"\n    }\n\n    input:\n    tuple val(meta), path(sorted_bam)\n    tuple val(meta), path(sorted_bamb_idex)\n    tuple val(meta), path(assembly)\n\n    output:\n    tuple val(meta), path(\"*_pilon.fasta\") , emit: assembly\n    path  'versions.yml'                     , emit: versions\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def maxmem = \"-Xmx${task.memory.toGiga()}g\"\n    def round = params.pilon_round ? params.pilon_round : \"\"\n\n    \"\"\"\n    pilon $options.args --genome ${assembly} --frags ${sorted_bam} --output round${round}_pilon \n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(pilon --version 2>&1 | sed 's/^Pilon version //; s/ .*\\$//' )\n    END_VERSIONS\n    \"\"\"\n}", "\nprocess bwa_align_bangkok {\n    publishDir \"${output}/${params.dir}/bwa_stats_bangkok/\", mode: 'copy', pattern: '*.flagstat.txt'\n    publishDir \"${output}/${params.dir}/bams_bangkok\", mode: 'copy', pattern: '*.bam'\n    publishDir \"${output}/${params.dir}/bams_bangkok\", mode: 'copy', pattern: '*.bam.bai'\n\n    cpus big\n    tag { id }\n\n    input:\n        tuple val(id), file(reads) from trimmed_reads_bwa_bangkok\n        file bwa_indices from bwa_indices_bangkok.first()\n\n    output:\n        file(\"${id}.flagstat.txt\") into bwa_stats_bangkok\n        file(\"${id}.bam\") into bam_files_bangkok\n        file(\"${id}.bam.bai\") into bam_indexes_bangkok\n\n    script:\n        index_base = bwa_indices[0].toString() - ~/.fa[.a-z]*/\n\n        \"\"\"\n        bwa aln -o 0 -n 0 -t ${task.cpus} ${index_base}.fa ${reads} > ${id}.sai\n        bwa samse ${index_base}.fa ${id}.sai ${reads} > ${id}.sam\n        samtools view -@ ${task.cpus} -bS ${id}.sam > ${id}.unsorted.bam\n        rm *.sam\n        samtools flagstat ${id}.unsorted.bam\n        samtools sort -@ ${task.cpus} -m 8G -o ${id}.bam ${id}.unsorted.bam\n        rm *.unsorted.bam\n        samtools index -@ ${task.cpus} -b ${id}.bam\n        samtools flagstat ${id}.bam > ${id}.flagstat.txt\n        \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/PILON", "zamanianlab/Core_RNAseq-nf/bwa_align_bangkok"], "list_wf_names": ["zamanianlab/Core_RNAseq-nf", "xiaoli-dong/magph"]}, {"nb_reuse": 2, "tools": ["FastQC", "MLST"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 2, "list_wf": ["pathogen", "magph"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::mlst=2.19.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1\"\n    } else {\n        container \"quay.io/biocontainers/mlst:2.19.0--hdfd78af_1\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mlst $options.args --label ${prefix} --threads $task.cpus $fasta > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}", "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n                                                                          \n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        ${getProcessName(task.process)}:\n            ${getSoftwareName(task.process)}: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}"], "list_proc": ["xiaoli-dong/pathogen/MLST", "xiaoli-dong/magph/FASTQC"], "list_wf_names": ["xiaoli-dong/pathogen", "xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 1, "list_wf": ["magph"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["process SAMTOOLS_FASTQ {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq\n    path  \"versions.yml\"               , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.suffix ? \"${meta.id}${task.ext.suffix}\" : \"${meta.id}\"\n    def endedness = meta.single_end ? \"-0 ${prefix}.fastq.gz\" : \"-1 ${prefix}_1.fastq.gz -2 ${prefix}_2.fastq.gz\"\n\n    \"\"\"\n    samtools fastq \\\\\n        $args \\\\\n        --threads ${task.cpus-1} \\\\\n        $endedness \\\\\n        $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/magph/SAMTOOLS_FASTQ"], "list_wf_names": ["xiaoli-dong/magph"]}, {"nb_reuse": 1, "tools": ["Rgin"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 1, "list_wf": ["pathogen"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["\nprocess RGI {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), publish_id:meta.id) }\n\n    conda (params.enable_conda ? \"python=3.6 bioconda::rgi=5.1.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/rgi%3A5.1.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/rgi:5.1.1--py_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path card_db\n\n    output:\n    tuple val(meta), path('*_rgi.json'), emit: json\n    tuple val(meta), path('*_rgi.txt'), emit: txt\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    def prefix  = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n\n    \"\"\"\n    # Place card_rgi source in a read/write location for container\n    #mkdir card_temp && cp -r /opt/conda/lib/python3.6/site-packages/app/ card_temp\n    mkdir card_temp && cp -r /usr/local/lib/python3.6/site-packages/app/ card_temp\n    export PYTHONPATH=\"\\$(pwd)/card_temp/:\\$PATH\"\n\n    rgi load --card_json ${card_db} --local\n    rgi main -i $fasta -o ${prefix}_rgi -n $task.cpus $options.args\n    \n    #clean up work dir, if it exists\n    [[ -d card_temp ]] && rm -r card_temp\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(rgi main --version | sed 's/rgi //g')\n    END_VERSIONS \n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/pathogen/RGI"], "list_wf_names": ["xiaoli-dong/pathogen"]}, {"nb_reuse": 3, "tools": ["Minimap2", "GmT", "fastPHASE"], "nb_own": 3, "list_own": ["yonghah", "xmzhuo", "xiaoli-dong"], "nb_wf": 3, "list_wf": ["geodata-pipeline", "fastp-nf", "pathogen"], "list_contrib": ["yonghah", "xmzhuo", "xiaoli-dong"], "nb_contrib": 3, "codes": ["\nprocess rangeplot {\n    publishDir 'output', mode: 'copy'\n    input:\n        path \"lnglat.csv\"\n    output:\n        path \"range.pdf\"\n    script:\n        diameter = [\n            0,          \n            *range.collect{it*2}]\n        \"\"\"\n        gmt begin\n            gmt figure range pdf\n            gmt pscoast -Rd -JE125.75/39.02/${maprange}/20c -Gburlywood -Slightblue -A1000 \n            gmt plot lnglat.csv -Sa.2c -Wthicker,blue\n            for r in ${diameter}\n            do\n                gmt plot lnglat.csv -SE-\\$r -Wthin,firebrick\n            done\n        gmt end\n        \"\"\"\n}", "\nprocess fastp {\n    \n    publishDir params.outputDir, mode: 'copy' \n\ttag \"fastp processing: $lane\"\n\n    input:\n    tuple val(lane), file(reads) \n    val advarg \n\n    output:\n    path \"${lane}_*{html,json,fq}*\"\n\n    shell:\n\n    def single = reads instanceof Path\n\n    if (!single)\n\n      '''\n      fastp -i !{reads[0]} -I !{reads[1]} \\\n        -o !{lane}_out.R1.fq.gz -O !{lane}_out.R2.fq.gz \\\n        --unpaired1 !{lane}_out.R1.unpaired.fq.gz --unpaired2 !{lane}_out.R2.unpaired.fq.gz \\\n        -j !{lane}_fastp.json -h !{lane}_fastp.html -R !{lane}_fastp \\\n        !{advarg}\n\t  '''\n    else\n      '''\n      fastp -i !{reads} -o !{lane}_out.fq.gz \\\n        -j !{lane}_fastp.json -h !{lane}_fastp.html -R !{lane}_fastp \\\n        !{advarg}\n\t  '''\n\n}", "\nprocess MINIMAP2_ALIGN_SHORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::minimap2=2.22' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/minimap2:2.21--h5bf99c6_0\"\n    } else {\n        container \"quay.io/biocontainers/minimap2:2.21--h5bf99c6_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(meta), path(reference)\n    output:\n    tuple val(meta), path(\"*.sam\"), emit: sam\n    path \"versions.yml\" , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"$reads\" : \"${reads[0]} ${reads[1]}\"\n    \"\"\"\n    minimap2 \\\\\n        $options.args \\\\\n        -t $task.cpus \\\\\n        $reference \\\\\n        $input_reads \\\\\n        > ${prefix}.sam\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(minimap2 --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["yonghah/geodata-pipeline/rangeplot", "xmzhuo/fastp-nf/fastp", "xiaoli-dong/pathogen/MINIMAP2_ALIGN_SHORT"], "list_wf_names": ["xiaoli-dong/pathogen", "xmzhuo/fastp-nf", "yonghah/geodata-pipeline"]}, {"nb_reuse": 1, "tools": ["ABRicate"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 1, "list_wf": ["pathogen"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["\nprocess ABRICATE_SUMMARIZE {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n    \n    conda (params.enable_conda ? \"bioconda::abricate=0.8.13\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1\"\n    } else {\n        container \"quay.io/biocontainers/abricate:1.0.1--ha8f3691_1\"\n    }\n\n    input:\n    path(reports)\n\n    output:\n    path('*.tsv'), emit: summary\n    path (\"versions.yml\"), emit: versions\n    \n    script:\n    def input = reports.join(' ')\n    \"\"\"\n    #abricate --summary *.tsv > all_abricate_summary.tsv\n    abricate $options.args --summary ${input} > vf.tsv\n    \n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(abricate --version 2>& 1 | sed 's/^abricate //;')\n    END_VERSIONS\n\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/pathogen/ABRICATE_SUMMARIZE"], "list_wf_names": ["xiaoli-dong/pathogen"]}, {"nb_reuse": 1, "tools": ["Prokka"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 1, "list_wf": ["pathogen"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"*.gff\"), emit: gff\n    tuple val(meta), path(\"*.gbk\"), emit: gbk\n    tuple val(meta), path(\"*.fna\"), emit: fna\n    tuple val(meta), path(\"*.faa\"), emit: faa\n    tuple val(meta), path(\"*.ffn\"), emit: ffn\n    tuple val(meta), path(\"*.sqn\"), emit: sqn\n    tuple val(meta), path(\"*.fsa\"), emit: fsa\n    tuple val(meta), path(\"*.tbl\"), emit: tbl\n    tuple val(meta), path(\"*.err\"), emit: err\n    tuple val(meta), path(\"*.log\"), emit: log\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n    mv ${prefix}/* .\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/pathogen/PROKKA"], "list_wf_names": ["xiaoli-dong/pathogen"]}, {"nb_reuse": 1, "tools": ["BiocStyle"], "nb_own": 1, "list_own": ["xiaoli-dong"], "nb_wf": 1, "list_wf": ["pathogen"], "list_contrib": ["xiaoli-dong"], "nb_contrib": 1, "codes": ["\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    \n    script:\n    println(versions)\n\n    \"\"\"\n    #!/usr/bin/env python\n\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"${getProcessName(task.process)}\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        print(\"*********************\")\n        print(f.name)\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\"\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}"], "list_proc": ["xiaoli-dong/pathogen/CUSTOM_DUMPSOFTWAREVERSIONS"], "list_wf_names": ["xiaoli-dong/pathogen"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["xmuyulab"], "nb_wf": 1, "list_wf": ["scRNAseq_pipelines"], "list_contrib": ["lmyiing", "huhehaotecrystal"], "nb_contrib": 2, "codes": ["\nprocess StarIndex {\n\n  cache \"deep\";tag \"step1\"\n  publishDir path: \"${params.outdir}\", mode: 'copy'\n\n  output:\n  file  \"*\"\n\n  \"\"\"\n  mkdir STAR_index\n  STAR \\\\\n  --runThreadN ${params.threads} \\\\\n  --runMode genomeGenerate \\\\\n  --genomeDir STAR_index \\\\\n  --genomeFastaFiles ${params.fasta} \\\\\n  --sjdbGTFfile ${params.gtf}\n  \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/StarIndex"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["xmuyulab"], "nb_wf": 1, "list_wf": ["scRNAseq_pipelines"], "list_contrib": ["lmyiing", "huhehaotecrystal"], "nb_contrib": 2, "codes": ["\nprocess FeatureCount {\n\n  cache \"deep\";tag \"${name}.step4\"\n  publishDir path: \"${params.outdir}/${name}\", mode: 'copy'\n\n  input:\n  set name, x from featureCount\n\n  output:\n  file \"${name}.gene_assigned*\"\n  set val(\"${name}\"), file(\"${name}.Aligned.sortedByCoord.out.bam.featureCounts.bam\") into samtoolsort\n\n  \"\"\"\n  featureCounts -a $params.gtf -o ${name}.gene_assigned -R BAM $x -T $params.threads\n  \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/FeatureCount"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["xmuyulab"], "nb_wf": 1, "list_wf": ["scRNAseq_pipelines"], "list_contrib": ["lmyiing", "huhehaotecrystal"], "nb_contrib": 2, "codes": ["\nprocess Count {\n\n  cache \"deep\";tag \"${name}.step6\"\n  publishDir path: \"${params.outdir}/${name}\", mode: 'copy'\n\n  input:\n  set name, x from count\n\n  output:\n  file \"${name}.counts.tsv.gz\"\n\n  \"\"\"\n  samtools index $x\n  umi_tools count --per-gene --gene-tag=XT --assigned-status-tag=XS --per-cell --wide-format-cell-counts -I $x -S ${name}.counts.tsv.gz\n  \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/Count"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["xmuyulab"], "nb_wf": 1, "list_wf": ["scRNAseq_pipelines"], "list_contrib": ["lmyiing", "huhehaotecrystal"], "nb_contrib": 2, "codes": ["\nprocess CreateSeqDict {\n\n  cache \"deep\";tag \"step1\"\n  publishDir path: \"${params.outdir}\", mode: 'copy'\n\n  output:\n  file  \"genome.dict\" into refFlat\n\n  \"\"\"\n  gatk CreateSequenceDictionary -R $params.fasta -O genome.dict\n  \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/CreateSeqDict"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["xmuyulab"], "nb_wf": 1, "list_wf": ["scRNAseq_pipelines"], "list_contrib": ["lmyiing", "huhehaotecrystal"], "nb_contrib": 2, "codes": ["\nprocess FastqToSam {\n\n  cache \"deep\";tag \"${name}.step1\"\n  publishDir path: \"${params.O}/${name}\", mode: 'copy'\n\n  input:\n  set name, read1, read2 from read_pairs\n\n  output:\n  set val(\"${name}\"),file(\"${name}.unmapped-queryname-sorted.bam\") into to_first\n\n  \"\"\"\n  gatk FastqToSam \\\\\n  --java-options \"-Djava.io.tmpdir=$params.javatmp\" \\\\\n  -F1 $read1 \\\\\n  -F2 $read2 \\\\\n  -O ${name}.unmapped-queryname-sorted.bam \\\\\n  -SM 0_unmapped-queryname-sorted\n  \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/FastqToSam"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines"]}, {"nb_reuse": 1, "tools": ["umis"], "nb_own": 1, "list_own": ["xmuyulab"], "nb_wf": 1, "list_wf": ["scRNAseq_pipelines"], "list_contrib": ["lmyiing", "huhehaotecrystal"], "nb_contrib": 2, "codes": ["\nprocess Fastqtransform {\n\n  cache \"deep\";tag \"${name}.step1\"\n  publishDir path: \"${params.outdir}/${name}\", mode: 'copy'\n\n  input:                                                                         \n\t                                                          \n\t                                                    \n  set name, read1, read2 , x from read_pairs.combine(json)\n\n  output:\n  set val(\"${name}\"),file (\"${name}.formatted.fq\") into to_histogram\n  set val(\"${name}\"),file (\"${name}.formatted.fq\") into to_quasimap\n\n  \"\"\"\n  umis fastqtransform $x $read1 $read2 > ${name}.formatted.fq\n  \"\"\"\n}"], "list_proc": ["xmuyulab/scRNAseq_pipelines/Fastqtransform"], "list_wf_names": ["xmuyulab/scRNAseq_pipelines"]}, {"nb_reuse": 1, "tools": ["TiPs"], "nb_own": 1, "list_own": ["yqshao"], "nb_wf": 1, "list_wf": ["tips"], "list_contrib": ["yqshao"], "nb_contrib": 1, "codes": ["\nprocess lammpsSample {\n    label 'lammps'\n    publishDir {\"$params.publishDir/$setup.subDir\"}, mode: params.publishMode\n\n    input:\n    tuple val(meta), val(inputs)\n\n    output:\n    tuple val(meta), path('output.xyz')\n\n    script:\n    setup = getParams(sampleDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    ln -s ${file(setup.lmpInit)} input.init\n    ln -s ${file(setup.lmpData)} input.data\n    ln -s ${file(setup.lmpSetting)} input.setting\n    $setup.lmpCmd -in ${file(setup.inp)} || echo LAMMPS aborted\n    sed -i '/WARNING/d' output.log\n    tips convert output.dump --log output.log \\\n      --units $setup.lmpUnits --emap '$setup.lmpEmap' -o output -of xyz\n    \"\"\"\n\n    stub:\n    setup = getParams(sampleDflts, inputs)\n    \"\"\"\n    #!/usr/bin/env bash\n    touch output.xyz\n    \"\"\"\n}"], "list_proc": ["yqshao/tips/lammpsSample"], "list_wf_names": ["yqshao/tips"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["Core_RNAseq-nf"], "list_contrib": ["mzamanian", "chenthorn"], "nb_contrib": 2, "codes": ["\nprocess bwa_align {\n    publishDir \"${output}/bwa_stats/\", mode: 'copy', pattern: '*align_.txt'\n    publishDir \"${output}/bams\", mode: 'copy', pattern: '*.bam'\n    publishDir \"${output}/bams\", mode: 'copy', pattern: '*.bam.bai'\n\n    cpus large_core\n    tag { id }\n\n    input:\n        tuple val(id), file(reads) from trimmed_reads_bwa\n        file bwa_indices from bwa_indices.first()\n\n    output:\n        file(\"${id}_align.txt\") into bwa_stats\n        file(\"${id}.bam\") into bam_files\n        file(\"${id}.bam.bai\") into bam_indexes\n\n    script:\n        fa_prefix = reads[0].toString() - ~/(_trim)(\\.fq\\.gz)$/\n        index_base = bwa_indices[0].toString() - ~/.fa[.a-z]*/\n\n        \"\"\"\n        bwa aln -o 0 -n 0 -t ${large_core} ${index_base}.fa ${reads} > ${id}.sai\n        bwa samse ${index_base}.fa ${id}.sai ${reads} > ${id}.sam\n        samtools view -bS ${id}.sam > ${id}.unsorted.bam\n        rm *.sam\n        samtools flagstat ${id}.unsorted.bam\n        samtools sort -@ ${large_core} -o ${id}.bam ${id}.unsorted.bam\n        rm *.unsorted.bam\n        samtools index -b ${id}.bam\n        samtools flagstat ${id}.bam > ${id}_align.txt\n        \"\"\"\n}"], "list_proc": ["zamanianlab/Core_RNAseq-nf/bwa_align"], "list_wf_names": ["zamanianlab/Core_RNAseq-nf"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["Core_RNAseq-nf"], "list_contrib": ["mzamanian", "chenthorn"], "nb_contrib": 2, "codes": ["\nprocess fastqc {\n\n    publishDir \"${output}/${params.dir}/fastqc\", mode: 'copy', pattern: '*_fastqc.{zip,html}'\n\n    cpus small\n    tag { id }\n\n    when:\n      params.qc\n\n    input:\n    tuple val(id), file(forward), file(reverse) from trimmed_reads_qc\n\n    output:\n    file \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n\n    \"\"\"\n      fastqc -q $forward $reverse -t ${task.cpus}\n    \"\"\"\n}"], "list_proc": ["zamanianlab/Core_RNAseq-nf/fastqc"], "list_wf_names": ["zamanianlab/Core_RNAseq-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["Core_RNAseq-nf"], "list_contrib": ["mzamanian", "chenthorn"], "nb_contrib": 2, "codes": ["\nprocess bam_qc {\n\n    publishDir \"${output}/${params.dir}/align_qc\", mode: 'copy', pattern: '*_QC.txt'\n\n    cpus small\n\n    when:\n      params.qc\n\n    input:\n        file(\"geneset.gtf.gz\") from geneset_qc\n        tuple val(id), file(bam), file(bai) from bam_files_qc\n\n    output:\n        file(\"*_QC.txt\") into align_qc\n\n    script:\n\n    \"\"\"\n      zcat geneset.gtf.gz > geneset.gtf\n      awk '{ if (\\$0 ~ \"transcript_id\") print \\$0; else print \\$0\" transcript_id \"\";\"; }' geneset.gtf | gtf2bed - > geneset.gtf.bed\n      cat geneset.gtf.bed | sed '/\\tgene\\t/!d' | sed '/protein_coding/!d' | awk -v OFS='\\t' '{print \\$1, \\$2, \\$3, \\$4, \\$6, \\$8}' > geneset.gene.bed\n      cat geneset.gtf.bed | sed '/\\texon\\t/!d' | sed '/protein_coding/!d' | awk -v OFS='\\t' '{print \\$1, \\$2, \\$3, \\$4, \\$6, \\$8}' > geneset.exon.bed\n      cat geneset.gtf.bed | sed '/\\tfive_prime_utr\\t/!d' | sed '/protein_coding/!d' | awk -v OFS='\\t' '{print \\$1, \\$2, \\$3, \\$4, \\$6, \\$8}' > geneset.5utr.bed\n      cat geneset.gtf.bed | sed '/\\tthree_prime_utr\\t/!d' | sed '/protein_coding/!d' | awk -v OFS='\\t' '{print \\$1, \\$2, \\$3, \\$4, \\$6, \\$8}' > geneset.3utr.bed\n\n      echo -n \"total,\" >> ${bam}_QC.txt\n      samtools view -c ${bam} >> ${bam}_QC.txt\n      echo -n \"mapped,\" >> ${bam}_QC.txt\n      samtools view -F 0x4 -c ${bam} >> ${bam}_QC.txt\n      echo -n \"unique,\" >> ${bam}_QC.txt\n      samtools view -F 0x4 -q 60 -c ${bam} >> ${bam}_QC.txt\n\n      samtools view -L geneset.gene.bed -h ${bam} > tmp.sam\n      echo -n \"gene,\" >> ${bam}_QC.txt\n      samtools view -F 0x4 -q 60 -c tmp.sam >> ${bam}_QC.txt\n      rm tmp.sam\n      samtools view -L geneset.exon.bed -h ${bam} > tmp.sam\n      echo -n \"exon,\" >> ${bam}_QC.txt\n      samtools view -F 0x4 -q 60 -c tmp.sam >> ${bam}_QC.txt\n      rm tmp.sam\n      samtools view -L geneset.5utr.bed -h ${bam} > tmp.sam\n      echo -n \"5utr,\" >> ${bam}_QC.txt\n      samtools view -F 0x4 -q 60 -c tmp.sam >> ${bam}_QC.txt\n      rm tmp.sam\n      samtools view -L geneset.3utr.bed -h ${bam} > tmp.sam\n      echo -n \"3utr,\" >> ${bam}_QC.txt\n      samtools view -F 0x4 -q 60 -c tmp.sam >> ${bam}_QC.txt\n      rm tmp.sam\n    \"\"\"\n}"], "list_proc": ["zamanianlab/Core_RNAseq-nf/bam_qc"], "list_wf_names": ["zamanianlab/Core_RNAseq-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "HISAT2", "StringTie"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["Core_RNAseq-nf"], "list_contrib": ["mzamanian", "chenthorn"], "nb_contrib": 2, "codes": ["\nprocess hisat2_stringtie {\n\n    publishDir \"${output}/expression\", mode: 'copy', pattern: '**/*'\n    publishDir \"${output}/expression\", mode: 'copy', pattern: '*.hisat2_log.txt'\n    publishDir \"${output}/bams\", mode: 'copy', pattern: '*.bam'\n    publishDir \"${output}/bams\", mode: 'copy', pattern: '*.bam.bai'\n\n    cpus large_core\n    tag { id }\n\n    input:\n        tuple  val(id), file(forward), file(reverse) from trimmed_reads_hisat\n        file(\"geneset.gtf.gz\") from geneset_stringtie\n        file hs2_indices from hs2_indices.first()\n\n    output:\n        file \"${id}.hisat2_log.txt\" into alignment_logs\n        file(\"${id}/*\") into stringtie_exp\n        file(\"${id}.bam\") into bam_files\n        file(\"${id}.bam.bai\") into bam_indexes\n\n    script:\n        index_base = hs2_indices[0].toString() - ~/.\\d.ht2/\n\n    \"\"\"\n        hisat2 -p ${large_core} -x $index_base -1 ${forward} -2 ${reverse} -S ${id}.sam --rg-id \"${id}\" --rg \"SM:${id}\" --rg \"PL:ILLUMINA\" 2> ${id}.hisat2_log.txt\n        samtools view -bS ${id}.sam > ${id}.unsorted.bam\n        rm *.sam\n        samtools flagstat ${id}.unsorted.bam\n        samtools sort -@ ${large_core} -o ${id}.bam ${id}.unsorted.bam\n        rm *.unsorted.bam\n        samtools index -b ${id}.bam\n        zcat geneset.gtf.gz > geneset.gtf\n        stringtie ${id}.bam -p ${large_core} -G geneset.gtf -A ${id}/${id}_abund.tab -e -B -o ${id}/${id}_expressed.gtf\n        rm *.gtf\n    \"\"\"\n}"], "list_proc": ["zamanianlab/Core_RNAseq-nf/hisat2_stringtie"], "list_wf_names": ["zamanianlab/Core_RNAseq-nf"]}, {"nb_reuse": 3, "tools": ["SAMtools", "FastQC", "BCFtools"], "nb_own": 3, "list_own": ["zamanianlab", "zellerlab", "zenomeplatform"], "nb_wf": 3, "list_wf": ["nf-germline-mapping", "vortex_light", "RNAseq-VC-nf"], "list_contrib": ["alex-aug", "dembra96", "cschu", "wheelern"], "nb_contrib": 4, "codes": ["\nprocess fastqc_raw {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/fastqc/raw/\", mode: 'copy'\n\n    input:\n    set val(sample_name),\n        file(fastq_1),\n        file(fastq_2),\n        val(bio_type),\n        val(seq_type),\n        val(seq_machine),\n        val(flowcell_id),\n        val(lane),\n        val(barcode),\n        val(read_group_LB) from ch_input_fastq_for_qc\n\n    output:\n    set val(sample_name), file(\"fastqc_${sample_name}_raw_logs\") into ch_fastq_qc_raw\n\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_name}_raw_logs\n\n    fastqc --outdir fastqc_${sample_name}_raw_logs \\\n        --format fastq \\\n        --quiet \\\n        --threads ${task.cpus} \\\n        ${fastq_1} ${fastq_2}\n    \"\"\"\n  }", "process dehumanise {\n    publishDir \"$output_dir\", mode: params.publish_mode, pattern: \"no_human/*/*.fastq.gz\"\n\n    input:\n    tuple val(sample), path(fq)\n\n    output:\n    tuple val(sample), path(\"no_human/${sample}/${sample}.bam\"), emit: bam\n    tuple val(sample), path(\"no_human/${sample}/${sample}*.fastq.gz\"), emit: fq\n    tuple val(\"${sample}.full\"), path(\"${sample}.full.bam\"), emit: full_bam\n\n    script:\n    def in_fq = (fq.size() == 2) ? \"in=${fq[0]} in2=${fq[1]}\" : \"in=${fq[0]}\";\n    def maxmem = task.memory.toGiga()\n    \"\"\"\n    set -o pipefail\n    mkdir -p no_human/${sample}\n    ln -s ${params.decon_ref}\n    bbmap.sh -Xmx${maxmem}g t=$task.cpus ${in_fq} outu=unmapped.sam outm=mapped.sam idfilter=${params.decon_minid}\n    cp unmapped.sam full.sam\n    samtools view mapped.sam >> full.sam\n    samtools view -f 4 mapped.sam >> unmapped.sam\n    samtools view -f 8 mapped.sam >> unmapped.sam\n\n    samtools collate -@ $task.cpus -O unmapped.sam | samtools view -buh > unmapped.bam\n\n    if [[ \"\\$?\" -eq 0 ]];\n    then\n        samtools fastq -@ task.cpus -0 ${sample}_other.fastq.gz -1 ${sample}_R1.fastq.gz -2 ${sample}_R2.fastq.gz unmapped.bam\n        if  [[ \"\\$?\" -eq 0 ]];\n        then\n\n            if [[ -z \"\\$(gzip -dc ${sample}_R1.fastq.gz | head -n 1)\" ]];\n            then\n                if [[ ! -z \"\\$(gzip -dc ${sample}_other.fastq.gz | head -n 1)\" ]];\n                then\n                    mv -v ${sample}_other.fastq.gz no_human/${sample}/${sample}_R1.fastq.gz;\n                fi;\n            else\n                    mv -v ${sample}_R1.fastq.gz no_human/${sample}/;\n                    if [[ ! -z \"\\$(gzip -dc ${sample}_R2.fastq.gz | head -n 1)\" ]];\n                    then\n                        mv -v ${sample}_R2.fastq.gz no_human/${sample}/;\n                    fi;\n            fi;\n\n            mv -v unmapped.bam no_human/${sample}/${sample}.bam\n            ls -l *.fastq.gz\n            ls -l no_human/${sample}/\n            rm -rf *.fastq.gz\n\n            samtools view -buh full.sam > ${sample}.full.bam\n        fi;\n    fi;\n\n    \"\"\"\n}", "\nprocess remove_indels {\n\n      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*_filter.bcf'\n\n      input:\n          tuple val(id), file(vcf) from vcf1\n\n      output:\n          tuple val(id), file(\"${id}_1_filter.bcf\") into snp_filter\n\n      when:\n          params.vcf\n\n      \"\"\"\n          bcftools view --threads 8 --exclude-types indels -Ob -o ${id}_1_filter.bcf ${vcf}\n          bcftools index ${id}_1_filter.bcf\n      \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/fastqc_raw", "zellerlab/vortex_light/dehumanise", "zamanianlab/RNAseq-VC-nf/remove_indels"], "list_wf_names": ["zellerlab/vortex_light", "zamanianlab/RNAseq-VC-nf", "zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 2, "tools": ["BCFtools", "Flexbar"], "nb_own": 2, "list_own": ["zamanianlab", "zenomeplatform"], "nb_wf": 2, "list_wf": ["nf-germline-mapping", "RNAseq-VC-nf"], "list_contrib": ["dembra96", "alex-aug", "wheelern"], "nb_contrib": 3, "codes": ["\nprocess quality_filter {\n\n      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*_filter.bcf'\n\n      input:\n          tuple val(id), file(vcf) from snp_filter\n\n      output:\n          tuple val(id), file(\"${id}_2_filter.bcf\") into quality_filter\n\n      when:\n          params.vcf\n\n      \"\"\"\n          bcftools filter --threads 8 -e 'QUAL < 30' -Ob -o ${id}_2_filter.bcf ${vcf}\n          bcftools index ${id}_2_filter.bcf\n      \"\"\"\n}", "\nprocess trim_fastqc {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/trimmed_reads/\", mode: 'copy'\n\n    input:\n    set val(sample_name),\n        file(fastq_1),\n        file(fastq_2),\n        val(bio_type),\n        val(seq_type),\n        val(seq_machine),\n        val(flowcell_id),\n        val(lane),\n        val(barcode),\n        val(read_group_LB) from ch_input_fastq_to_trim\n    each file(adapters) from ch_adapters\n\n    output:\n    set val(sample_name),\n        file(\"${fastq_1.simpleName}_trim.fastq.gz\"),\n        file(\"${fastq_2.simpleName}_trim.fastq.gz\"),\n        val(bio_type),\n        val(seq_type),\n        val(seq_machine),\n        val(flowcell_id),\n        val(lane),\n        val(barcode),\n        val(read_group_LB) into (ch_fastq_trimmed_to_map, ch_fastq_trimmed_for_qc)\n    set val(sample_name), file(\"flexbar_${sample_name}.log\") into ch_trimming_report\n\n    script:\n                                                        \n    adapters_param = params.adapters ? \"--adapters ${adapters}\" : \"\"\n    \"\"\"\n    flexbar --adapter-min-overlap ${params.adapter_min_overlap} \\\n        --adapter-trim-end ${params.adapter_trim_end} \\\n        --pre-trim-left ${params.pre_trim_left} \\\n        --max-uncalled ${params.max_uncalled} \\\n        --min-read-length ${params.min_read_length} \\\n        --threads ${task.cpus} \\\n        --zip-output GZ \\\n        --reads ${fastq_1} \\\n        --reads2 ${fastq_2} \\\n        --output-reads ${fastq_1.simpleName}_trim.fastq \\\n        --output-reads2 ${fastq_2.simpleName}_trim.fastq \\\n        --output-log flexbar_${sample_name}.log \\\n        $adapters_param\n\n        # no .gz in the end of output files is important, its added automatically by flexbar because of --zip-output GZ option.\n    \"\"\"\n  }"], "list_proc": ["zamanianlab/RNAseq-VC-nf/quality_filter", "zenomeplatform/nf-germline-mapping/trim_fastqc"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf", "zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess allelic_balance {\n\n      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*_filter.bcf'\n\n      input:\n          tuple val(id), file(vcf) from qual_depth_filter\n\n      output:\n          tuple val(id), file(\"${id}_5_filter.bcf\") into allele_filter\n\n      when:\n          params.vcf\n\n      \"\"\"\n          bcftools filter --threads 8 -e 'MAF < 0.2' -Ou | bcftools filter --threads 8 -e 'FORMAT/AO < 3' -Ob -o ${id}_5_filter.bcf ${vcf}\n          bcftools index ${id}_5_filter.bcf\n      \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/allelic_balance"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["STAR"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess star_align_first {\n\n    publishDir \"${output}/star_log\", mode: 'copy', pattern: '*Log*'\n    publishDir \"${output}/sj\", mode: 'copy', pattern: '*.tab'\n\n    cpus large_core\n    tag \"$id\"\n\n    input:\n        tuple val(id), file(forward), file(reverse) from trimmed_reads_star1\n        file index from star_index\n\n    output:\n        file(\"*out.tab\") into mapped_splice_junctions\n\n    when:\n        params.fq && !params.bam\n\n    \"\"\"\n        STAR \\\\\n          --readFilesIn ${forward} ${reverse} \\\\\n          --readFilesCommand zcat \\\\\n          --genomeDir ${index} \\\\\n          --genomeLoad LoadAndRemove \\\\\n          --runThreadN ${task.cpus} \\\\\n          --outFilterType BySJout \\\\\n          --alignIntronMax 1000000 \\\\\n          --alignMatesGapMax 1000000 \\\\\n          --outFilterMismatchNmax 999 \\\\\n          --outFilterMismatchNoverReadLmax 0.04 \\\\\n          --outFilterMultimapNmax 20 \\\\\n          --outFileNamePrefix ${id}.\n    \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/star_align_first"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess star_align_second {\n\n    publishDir \"${output}/star_log\", mode: 'copy', pattern: '*Log*'\n    publishDir \"${output}/alignment_stats\", mode: 'copy', pattern: '*alignment_stats.log'\n    publishDir \"${output}/bams\", mode: 'copy', pattern: '*.bam'\n    publishDir \"${output}/bams\", mode: 'copy', pattern: '*.bam.bai'\n\n    cpus large_core\n    tag \"$id\"\n\n    input:\n        tuple val(id), file(forward), file(reverse) from trimmed_reads_star2\n        file index from star_index\n        file junctions from concatenated_junctions\n\n    output:\n        file \"${id}.Log.out\" into alignment_logs\n        tuple val(id), file(\"${id}.bam\") into bam_files\n\n    when:\n        params.fq && !params.bam\n\n    \"\"\"\n        SM=`echo ${id} | cut -c1-3 | tr -d '\\n'`\n        ID=${id}\n        LB=${id}\n\n        STAR \\\\\n        --readFilesIn ${forward} ${reverse} \\\\\n        --readFilesCommand zcat \\\\\n        --genomeDir ${index} \\\\\n        --sjdbFileChrStartEnd ${junctions} \\\\\n        --runThreadN ${task.cpus} \\\\\n        --outSAMtype BAM Unsorted \\\\\n        --outSAMattributes NH HI AS NM MD \\\\\n        --outSAMattrRGline ID:\"\\$ID\" PL:illumina SM:\"\\$SM\" LB:\"\\$LB\" \\\\\n        --outFilterType BySJout \\\\\n        --alignIntronMax 1000000 \\\\\n        --alignMatesGapMax 1000000 \\\\\n        --outFilterMismatchNmax 999 \\\\\n        --outFilterMismatchNoverReadLmax 0.04 \\\\\n        --outFilterMultimapNmax 20 \\\\\n        --outFileNamePrefix ${id}.\n\n        mv ${id}.Aligned.out.bam  ${id}.bam\n        samtools flagstat -@ ${task.cpus} ${id}.bam > ${id}_alignment_stats.log\n    \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/star_align_second"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess fetch_variants {\n\n    output:\n        tuple file(\"known_variants.vcf.gz\"), file(\"known_variants.vcf.gz.csi\"), file(\"known_variants.vcf.gz.tbi\") into known_variants\n\n    when:\n        params.bam\n\n    \"\"\"\n        wget ftp://ftp.ensemblgenomes.org/pub/metazoa/release-46/variation/vcf/aedes_aegypti_lvpagwg/aedes_aegypti_lvpagwg.vcf.gz \\\n          -O known_variants.vcf.gz\n\n        wget ftp://ftp.ensemblgenomes.org/pub/metazoa/release-46/variation/vcf/aedes_aegypti_lvpagwg/aedes_aegypti_lvpagwg.vcf.gz.csi \\\n          -O known_variants.vcf.gz.csi\n\n        gatk IndexFeatureFile \\\n          -I known_variants.vcf.gz\n\n    \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/fetch_variants"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess recalibrate_bases {\n\n    input:\n        tuple val(id), file(bam) from split_bams1\n        tuple file(vcf), file(index_csi), file(index_tbi) from known_variants\n        file(intervals) from intervals1.first()\n        file(reference_dict) from dict1.first()\n\n    output:\n        tuple id, file(\"${id}_recal_data.table\") into brdt1, brdt2\n\n    when:\n        params.bam\n\n    \"\"\"\n        gatk BaseRecalibrator \\\n          -R \"${aedesgenome}/genome.fa\" \\\n          --sequence-dictionary ${reference_dict} \\\n          -I ${bam} \\\n          -L ${intervals} \\\n          --known-sites ${vcf} \\\n          -O \"${id}_recal_data.table\"\n    \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/recalibrate_bases"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess apply_recalibration {\n\n    input:\n        tuple val(id), file(bam), file(recal_table) from bam_recal_table\n        file(intervals) from intervals2.first()\n        file(reference_dict) from dict2.first()\n\n    output:\n        tuple stdout, file(\"${id}_recal.bam\") into recal_bams\n\n    when:\n        params.bam\n\n    \"\"\"\n        gatk ApplyBQSR \\\n          -R \"${aedesgenome}/genome.fa\" \\\n          --sequence-dictionary ${reference_dict} \\\n          -I ${bam} \\\n          -L ${intervals} \\\n          --bqsr-recal-file ${recal_table} \\\n          -O \"${id}_recal.bam\"\n\n        echo ${id} | cut -c1-3 | tr -d '\\n'\n    \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/apply_recalibration"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["SAMtools", "Picard"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess merge_bams_by_strain {\n\n    input:\n        tuple val(id), file(bams) from recal_bams.groupTuple()\n\n    output:\n        tuple id, file(\"${id}_rg_merged.bam\") into merged_by_strain_bams_hc, merged_by_strain_bams_fb\n\n    when:\n        params.bam\n\n    \"\"\"\n        samtools merge -cp ${id}_merged.bam ${bams.join(\" \")}\n\n        picard AddOrReplaceReadGroups \\\n          I=${id}_merged.bam \\\n          O=${id}_rg_merged.bam \\\n          RGID=${id} \\\n          RGLB=${id} \\\n          RGPL=ILLUMINA \\\n          RGPU=NA \\\n          RGSM=${id}\n\n        samtools index ${id}_rg_merged.bam\n    \"\"\"\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/merge_bams_by_strain"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["FreeBayes"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess freebayes {\n\n      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*.vcf'\n\n      input:\n          tuple val(id), file(bam) from merged_by_strain_bams_fb\n          file(intervals) from intervals_bed.first()\n\n      output:\n          file(\"${id}.vcf\") into vcfs_fb\n\n      when:\n          params.bam\n\n      \"\"\"\n          ## for attempting freebayes-parallel\n          # bedtools getfasta -fi \"${aedesgenome}/genome.fa\" -bed ${intervals} -fo transcript_intervals.fa\n          # samtools faidx transcript_intervals.fa\n          # cat transcript_intervals.bed | awk '{print \\$1 \":\" \\$2 \"-\" \\$3}' > transcript_intervals.txt\n\n          freebayes \\\n            -f \"${aedesgenome}/genome.fa\" \\\n            -b ${bam} > \"${id}.vcf\"\n\n          bgzip -@ 8 \"${id}.vcf > ${id}.vcf.gz\n      \"\"\"\n\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/freebayes"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 1, "tools": ["BCFtools"], "nb_own": 1, "list_own": ["zamanianlab"], "nb_wf": 1, "list_wf": ["RNAseq-VC-nf"], "list_contrib": ["wheelern"], "nb_contrib": 1, "codes": ["\nprocess filter_variants {\n\n      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*_1_filter.vcf.gz'\n\n      input:\n          tuple val(id), file(vcf) from input_vcf\n\n      output:\n          file(\"${id}_1_filter.vcf.gz\")\n\n      when:\n          params.vcf\n\n      \"\"\"\n          bcftools filter --threads 8 -e 'QUAL < 30' -Ou ${vcf} | \\\n            bcftools filter --threads 8 -e 'INFO/DP < 10' -Ou | \\\n            bcftools filter --threads 8 -e 'MAF < 0.2' -Ou | \\\n            bcftools filter --threads 8 -e 'INFO/DP > 500' -Oz -o ${id}_1_filter.vcf.gz\n\n          bcftools index ${id}_1_filter.vcf.gz\n\n          bcftools view -r 1:1-20000000 -Oz -o ${id}_2_filter.vcf.gz ${id}_1_filter.vcf.gz\n      \"\"\"\n\n}"], "list_proc": ["zamanianlab/RNAseq-VC-nf/filter_variants"], "list_wf_names": ["zamanianlab/RNAseq-VC-nf"]}, {"nb_reuse": 2, "tools": ["SAMtools", "FastQC"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 2, "list_wf": ["gaga2", "vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process fastqc {\n    publishDir params.output_dir, mode: params.publish_mode, pattern: \"raw_counts/*.txt\"\n\n    input:\n    tuple val(sample), path(reads)\n\n    output:\n    tuple val(sample), path(\"fastqc/*/*fastqc_data.txt\"), emit: reports\n    tuple val(sample), path(\"raw_counts/${sample.id}.txt\"), emit: counts\n\n    script:\n    def process_r2 = (sample.is_paired) ? \"fastqc -t $task.cpus --extract --outdir=fastqc ${sample.id}_R2.fastq.gz && mv fastqc/${sample.id}_R2_fastqc/fastqc_data.txt fastqc/${sample.id}_R2_fastqc/${sample.id}_R2_fastqc_data.txt\" : \"\";\n\n    \"\"\"\n    mkdir -p fastqc\n    mkdir -p raw_counts\n    fastqc -t $task.cpus --extract --outdir=fastqc ${sample.id}_R1.fastq.gz && mv fastqc/${sample.id}_R1_fastqc/fastqc_data.txt fastqc/${sample.id}_R1_fastqc/${sample.id}_R1_fastqc_data.txt\n    ${process_r2}\n    grep \"Total Sequences\" fastqc/*/*data.txt > seqcount.txt\n    echo \\$(wc -l seqcount.txt)\\$'\\t'\\$(head -n1 seqcount.txt | cut -f 2) > raw_counts/${sample.id}.txt\n    \"\"\"\n}", "process dehumanise {\n    publishDir \"$output_dir\", mode: params.publish_mode, pattern: \"no_human/*/*.fastq.gz\"\n\n    input:\n    tuple val(sample), path(fq)\n\n    output:\n    tuple val(sample), path(\"no_human/${sample}/${sample}.bam\"), emit: bam\n    tuple val(sample), path(\"no_human/${sample}/${sample}*.fastq.gz\"), emit: fq\n    tuple val(\"${sample}.full\"), path(\"${sample}.full.bam\"), emit: full_bam\n\n    script:\n    def in_fq = (fq.size() == 2) ? \"in=${fq[0]} in2=${fq[1]}\" : \"in=${fq[0]}\";\n    def maxmem = task.memory.toGiga()\n    \"\"\"\n    set -o pipefail\n    mkdir -p no_human/${sample}\n    ln -s ${params.decon_ref}\n    bbmap.sh -Xmx${maxmem}g t=$task.cpus ${in_fq} outu=unmapped.sam outm=mapped.sam idfilter=${params.decon_minid}\n    cp unmapped.sam full.sam\n    samtools view mapped.sam >> full.sam\n    samtools view -f 4 mapped.sam >> unmapped.sam\n    samtools view -f 8 mapped.sam >> unmapped.sam\n\n    samtools collate -@ $task.cpus -O unmapped.sam | samtools view -buh > unmapped.bam\n\n    if [[ \"\\$?\" -eq 0 ]];\n    then\n        samtools fastq -@ task.cpus -0 ${sample}_other.fastq.gz -1 ${sample}_R1.fastq.gz -2 ${sample}_R2.fastq.gz unmapped.bam\n        if  [[ \"\\$?\" -eq 0 ]];\n        then\n\n            if [[ -z \"\\$(gzip -dc ${sample}_R1.fastq.gz | head -n 1)\" ]];\n            then\n                if [[ ! -z \"\\$(gzip -dc ${sample}_other.fastq.gz | head -n 1)\" ]];\n                then\n                    mv -v ${sample}_other.fastq.gz no_human/${sample}/${sample}_R1.fastq.gz;\n                fi;\n            else\n                    mv -v ${sample}_R1.fastq.gz no_human/${sample}/;\n                    if [[ ! -z \"\\$(gzip -dc ${sample}_R2.fastq.gz | head -n 1)\" ]];\n                    then\n                        mv -v ${sample}_R2.fastq.gz no_human/${sample}/;\n                    fi;\n            fi;\n\n            mv -v unmapped.bam no_human/${sample}/${sample}.bam\n            ls -l *.fastq.gz\n            ls -l no_human/${sample}/\n            rm -rf *.fastq.gz\n\n            samtools view -buh full.sam > ${sample}.full.bam\n        fi;\n    fi;\n\n    \"\"\"\n}"], "list_proc": ["zellerlab/gaga2/fastqc", "zellerlab/vortex_light/dehumanise"], "list_wf_names": ["zellerlab/vortex_light", "zellerlab/gaga2"]}, {"nb_reuse": 1, "tools": ["Figaro"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["gaga2"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["\nprocess figaro {\n\tpublishDir \"${params.output_dir}\", mode: params.publish_mode\n\n\tinput:\n\tpath input_reads\n\tval is_paired_end\n\n\toutput:\n\tpath(\"figaro/trimParameters.json\"), emit: trim_params\n\tpath(\"figaro/*.png\")\n\n\tscript:\n\tdef paired_params = (is_paired_end == true) ? \"-r ${params.right_primer} -m ${params.min_overlap}\" : \"\"\n\n\t\"\"\"\n\tfigaro -i . -o figaro/ -a ${params.amplicon_length} -f ${params.left_primer} ${paired_params}\n\t\"\"\"\n}"], "list_proc": ["zellerlab/gaga2/figaro"], "list_wf_names": ["zellerlab/gaga2"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process pathseq {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    tuple val(sample), path(bam)\n\tpath(pathseq_db)\n\n    output:\n    tuple val(sample), path(\"${sample.id}/${sample.id}.pathseq.score*\"), emit: scores\n    tuple val(sample), path(\"${sample.id}/${sample.id}.pathseq.bam*\"), emit: bam\n\n    script:\n    def maxmem = task.memory.toGiga()\n    \"\"\"\n    mkdir -p ${sample.id}\n    gatk --java-options \\\"-Xmx${maxmem}g\\\" PathSeqPipelineSpark \\\\\n        --input $bam \\\\\n        --filter-bwa-image ${pathseq_db}/reference.fasta.img \\\\\n        --kmer-file ${pathseq_db}/host.hss \\\\\n        --min-clipped-read-length ${params.pathseq_min_clipped_read_length} \\\\\n        --microbe-fasta ${pathseq_db}/microbe.fasta \\\\\n        --microbe-bwa-image ${pathseq_db}/microbe.fasta.img \\\\\n        --taxonomy-file ${pathseq_db}/microbe.db \\\\\n        --output ${sample.id}/${sample.id}.pathseq.bam \\\\\n        --scores-output ${sample.id}/${sample.id}.pathseq.scores \\\\\n        --score-metrics ${sample.id}/${sample.id}.pathseq.score_metrics\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/pathseq"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process flagstats {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    tuple val(sample), path(bam)\n\n    output:\n    tuple val(sample), path(\"${sample.id}/${sample.id}.flagstats.txt\"), emit: flagstats\n\n    script:\n    \"\"\"\n    mkdir -p ${sample.id}\n    samtools flagstat $bam > \"${sample.id}/${sample.id}.flagstats.txt\"\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/flagstats"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process remove_host_kraken2 {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    tuple val(sample), path(fq)\n\tpath(kraken_db)\n\n    output:\n    tuple val(sample), path(\"no_host/${sample.id}/${sample.id}_R*.fastq.gz\"), emit: reads\n\n    script:\n    def out_options = (sample.is_paired) ? \"--paired --unclassified-out ${sample.id}#.fastq\" : \"--unclassified-out ${sample.id}_1.fastq\"\n    def move_r2 = (sample.is_paired) ? \"gzip -c ${sample.id}_2.fastq > no_host/${sample.id}/${sample.id}_R2.fastq.gz\" : \"\"\n\n    \"\"\"\n    mkdir -p no_host/${sample.id}\n    kraken2 --threads $task.cpus --db ${kraken_db} ${out_options} --output kraken_read_report.txt --report kraken_report.txt --report-minimizer-data --gzip-compressed --minimum-hit-groups ${params.kraken2_min_hit_groups} $fq\n\n    gzip -c ${sample.id}_1.fastq > no_host/${sample.id}/${sample.id}_R1.fastq.gz\n    ${move_r2}\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/remove_host_kraken2"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process bam2fq {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    tuple val(sample), path(bam)\n\n    output:\n    stdout\n    tuple val(sample), path(\"fastq/${sample.id}/${sample.id}*.fastq.gz\"), emit: reads\n\n    script:\n    \"\"\"\n    set -o pipefail\n    mkdir -p fastq/${sample.id}\n    samtools collate -@ $task.cpus -u -O $bam | samtools fastq -F 0x900 -0 ${sample.id}_other.fastq.gz -1 ${sample.id}_R1.fastq.gz -2 ${sample.id}_R2.fastq.gz\n\n    if [[ \"\\$?\" -eq 0 ]];\n    then\n\n        if [[ -z \"\\$(gzip -dc ${sample.id}_R1.fastq.gz | head -n 1)\" ]];\n        then\n            if [[ ! -z \"\\$(gzip -dc ${sample.id}_other.fastq.gz | head -n 1)\" ]];\n            then\n                mv -v ${sample.id}_other.fastq.gz fastq/${sample.id}/${sample.id}_R1.fastq.gz;\n            fi;\n        else\n                mv -v ${sample.id}_R1.fastq.gz fastq/${sample.id}/;\n                if [[ ! -z \"\\$(gzip -dc ${sample.id}_R2.fastq.gz | head -n 1)\" ]];\n                then\n                    mv -v ${sample.id}_R2.fastq.gz fastq/${sample.id}/;\n                fi;\n        fi;\n\n        ls -l *.fastq.gz\n        ls -l fastq/${sample.id}/*.fastq.gz\n        rm -rf *.fastq.gz\n    fi;\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/bam2fq"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["\nprocess fq2bam {\n    input:\n    tuple val(sample), path(fq)\n\n    output:\n    tuple val(sample), path(\"out/${sample.id}.bam\"), emit: reads\n\n    script:\n\tdef maxmem = task.memory.toGiga()\n\tdef r2 = (sample.is_paired) ? \"in2=${sample.id}_R2.fastq.gz\" : \"\"\n\n\t\"\"\"\n\tmkdir -p out/\n\treformat.sh -Xmx${maxmem}g in=${sample.id}_R1.fastq.gz ${r2} trimreaddescription=t out=stdout.bam | samtools addreplacerg -r \"ID:A\" -r \"SM:${sample.id}\" -o out/${sample.id}.bam -\n\t\"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/fq2bam"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process multiqc {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    path(reports)\n\tpath(multiqc_config)\n\n    output:\n    path(\"multiqc_report.html\")\n\n    script:\n    def send_report = (params.email) ? \"echo . | mailx -s 'multiqc_report' -a multiqc_report.html ${params.email}\" : \"\"\n    \"\"\"\n    multiqc -c ${multiqc_config} .\n    ${send_report}\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/multiqc"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["kraken2"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process kraken2 {\n    publishDir params.output_dir, mode: params.publish_mode\n\n    input:\n    tuple val(sample), path(reads)\n\tpath(kraken_db)\n\n    output:\n    tuple val(sample), path(\"${sample.id}/${sample.id}.kraken2_report.txt\"), emit: kraken2_out\n\n    script:\n    def is_paired = (sample.is_paired) ? \"--paired\" : \"\";\n    \"\"\"\n    mkdir -p ${sample.id}\n    kraken2 --db ${kraken_db} --threads $task.cpus --minimum-hit-groups ${params.kraken2_min_hit_groups} --gzip-compressed --report ${sample.id}/${sample.id}.kraken2_report.txt ${is_paired} \\$(ls $reads)\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/kraken2"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["zellerlab"], "nb_wf": 1, "list_wf": ["vortex_light"], "list_contrib": ["cschu"], "nb_contrib": 1, "codes": ["process fastqc {\n    publishDir params.output_dir, mode: params.publish_mode, pattern: \"raw_counts/*.txt\"\n\n    input:\n    tuple val(sample), path(reads)\n\n    output:\n    tuple val(sample), path(\"fastqc/*/*fastqc_data.txt\"), emit: reports\n    tuple val(sample), path(\"raw_counts/${sample.id}.txt\"), emit: counts\n\n    script:\n    def process_r2 = (sample.is_paired) ? \"fastqc -t $task.cpus --extract --outdir=fastqc ${sample.id}_R2.fastq.gz && mv fastqc/${sample.id}_R2_fastqc/fastqc_data.txt fastqc/${sample.id}_R2_fastqc/${sample.id}_R2_fastqc_data.txt\" : \"\";\n\n    \"\"\"\n    mkdir -p fastqc\n    mkdir -p raw_counts\n    fastqc -t $task.cpus --extract --outdir=fastqc ${sample.id}_R1.fastq.gz && mv fastqc/${sample.id}_R1_fastqc/fastqc_data.txt fastqc/${sample.id}_R1_fastqc/${sample.id}_R1_fastqc_data.txt\n    ${process_r2}\n\n    grep \"Total Sequences\" fastqc/*/*data.txt > seqcount.txt\n    echo \\$(wc -l seqcount.txt)\\$'\\t'\\$(head -n1 seqcount.txt | cut -f 2) > raw_counts/${sample.id}.txt\n    \"\"\"\n}"], "list_proc": ["zellerlab/vortex_light/fastqc"], "list_wf_names": ["zellerlab/vortex_light"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess fastqc_trimmed {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/fastqc/trimmed/\", mode: 'copy'\n\n    input:\n    set val(sample_name),\n        file(fastq_1),\n        file(fastq_2),\n        val(bio_type),\n        val(seq_type),\n        val(seq_machine),\n        val(flowcell_id),\n        val(lane),\n        val(barcode),\n        val(read_group_LB) from ch_fastq_trimmed_for_qc\n\n    output:\n    set val(sample_name), file(\"fastqc_${sample_name}_trimmed_logs\") into ch_fastq_qc_trimmed\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_name}_trimmed_logs\n\n    fastqc --outdir fastqc_${sample_name}_trimmed_logs \\\n        --format fastq \\\n        --quiet \\\n        --threads ${task.cpus} \\\n        ${fastq_1} ${fastq_2}\n    \"\"\"\n  }"], "list_proc": ["zenomeplatform/nf-germline-mapping/fastqc_trimmed"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": [" process multiqc_prealignment_report_by_sample {\n      tag \"$sample_name\"\n      label 'low_memory'\n      publishDir \"${params.outdir}/multiqc_prealignment_report/${sample_name}/\", mode: 'copy'\n\n      input:\n      set val(sample_name), file(fastqc_raw_dir), file(trimming_log), file(fastqc_trimmed_dir) from ch_prealignment_multiqc_files_by_sample\n\n      output:\n      file(\"multiqc_report_${sample_name}.html\")\n\n      script:\n      \"\"\"\n      multiqc . --filename \"multiqc_report_${sample_name}.html\"\n      \"\"\"\n    }"], "list_proc": ["zenomeplatform/nf-germline-mapping/multiqc_prealignment_report_by_sample"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 2, "tools": ["Bowtie", "MultiQC"], "nb_own": 2, "list_own": ["zsteve", "zenomeplatform"], "nb_wf": 2, "list_wf": ["nf-germline-mapping", "nf-ATAC"], "list_contrib": ["zsteve", "dembra96", "alex-aug"], "nb_contrib": 3, "codes": [" process multiqc_prealignment_report_all {\n      label 'low_memory'\n      publishDir \"${params.outdir}/multiqc_prealignment_report/\", mode: 'copy'\n\n      input:\n      file(\"*\") from ch_prealignment_multiqc_files_all.collect()\n\n      output:\n      file(\"multiqc_report.html\")\n\n      script:\n      \"\"\"\n      multiqc .\n      \"\"\"\n    }", "\nprocess sampleMapToReference {\n                               \n             \n  publishDir {sampleInfo[\"baseDirOut\"]}\n  input:\n    set val(sampleInfo), file(reads) from trimmedPairOut\n  output:\n    file 'bowtie2_output/*'\n    set val(sampleInfo), file('bowtie2_output/*.sam') into mappedSamOut\n  script:\n    output_sam_name = sampleInfo[\"ID\"]+'.sam'\n    \"\"\"\n    mkdir -p bowtie2_output;\n    bowtie2 ${getFlagString(def_cmd_params, \"bowtie2\", \"-X2000\")}\\\n    ${getFlagString(def_cmd_params, \"bowtie2\", \"--no-mixed\")}\\\n    ${getFlagString(def_cmd_params, \"bowtie2\", \"--no-discordant\")}\\\n    -p ${def_cmd_params[\"bowtie2\"][\"-p\"]}\\\n    -x ${def_cmd_params[\"bowtie2\"][\"-x\"]}\\\n    -1 ${reads[0]}\\\n    -2 ${reads[1]}\\\n    -S bowtie2_output/$output_sam_name\\\n    > bowtie2_output/bowtie2.stdout 2> bowtie2_output/${sampleInfo[\"ID\"]}_bowtie2.out\n    \"\"\"\n                                                                                         \n                                                                        \n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/multiqc_prealignment_report_all", "zsteve/nf-ATAC/sampleMapToReference"], "list_wf_names": ["zsteve/nf-ATAC", "zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["SAMtools", "BWA"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess map_reads {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", mode: 'copy'\n\n    input:\n    set val(sample_name),\n        file(fastq_1),\n        file(fastq_2),\n        val(bio_type),\n        val(seq_type),\n        val(seq_machine),\n        val(flowcell_id),\n        val(lane),\n        val(barcode),\n        val(read_group_LB) from ch_fastq_trimmed_to_map\n    each file(bwa_indexes) from ch_bwa\n\n    output:\n    set val(sample_name), file(\"${fastq_1.simpleName}_L${read_group_LB}.bam\") into ch_mapped_reads\n\n    script:\n    seq_type_modified = sample_name[2]\n\n    if (params.seq_machine_catalog.keySet().contains(seq_machine)) {\n      seq_machine_modified = params.seq_machine_catalog[seq_machine].full_name\n    } else {\n      log.warn \"Sample ${fastq_1.name} has unknown sequening machine type \\\"${seq_machine}\\\". \\nKnow machines in catalog: ${params.seq_machine_catalog.keySet().join(\", \")}. \\nSample will receive \\\"Unknown_seq_machine\\\" value.\"\n      seq_machine_modified = \"Unknown_seq_machine\"\n    }\n\n    \"\"\"\n    bwa mem -t ${task.cpus} \\\n        -Y \\\n        -R \"@RG\\\\tID:${seq_type_modified}\\\\tPL:${seq_machine_modified}\\\\tPU:v${flowcell_id}.${lane}.${barcode}\\\\tLB:exome_lib${read_group_LB}\\\\tSM:${sample_name}\" \\\n        ${bwa_indexes[1].baseName} \\\n        ${fastq_1} \\\n        ${fastq_2} \\\n      | samtools view -bS -@${task.cpus} - > ${fastq_1.simpleName}_L${read_group_LB}.bam;\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/map_reads"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess merge_bams_by_sample {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam_files) from ch_mapped_reads_grouped_by_sample\n\n    output:\n    set val(sample_name), file(\"${sample_name}.merged.bam\") into ch_mapped_reads_merged_by_sample\n\n    script:\n    \"\"\"\n    samtools merge -@ ${task.cpus} ${sample_name}.merged.bam ${bam_files}\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/merge_bams_by_sample"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess sort_bams_by_name {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(merged_bam) from ch_mapped_reads_merged_by_sample\n\n    output:\n    set val(sample_name), file(\"${sample_name}.namesorted.bam\") into ch_mapped_reads_sorted_by_name\n\n    script:\n    \"\"\"\n    picard SortSam \\\n        I=${merged_bam} \\\n        O=${sample_name}.namesorted.bam \\\n        SO=queryname\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/sort_bams_by_name"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess mark_duplicates  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(sorted_bam) from ch_mapped_reads_sorted_by_name\n\n    output:\n    set val(sample_name), file(\"${sample_name}.namesorted_mrkdup.bam\") into ch_mapped_reads_mrkdup\n    file(\"${sample_name}_mrkdup_metrics.txt\") into ch_ch_mapped_reads_mrkdup_metrics\n\n    script:\n    \"\"\"\n    picard MarkDuplicates \\\n        I=${sorted_bam} \\\n        O=${sample_name}.namesorted_mrkdup.bam \\\n        ASSUME_SORT_ORDER=queryname \\\n        METRICS_FILE=${sample_name}_mrkdup_metrics.txt \\\n        QUIET=true \\\n        COMPRESSION_LEVEL=0 \\\n        VALIDATION_STRINGENCY=LENIENT\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/mark_duplicates"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess sort_bams_by_coord  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam_mrkdupl) from ch_mapped_reads_mrkdup\n\n    output:\n    set val(sample_name), file(\"${sample_name}.sorted_mrkdup.bam\"), file(\"${sample_name}.sorted_mrkdup.bai\") into ch_mapped_reads_mrkdup_sorted\n\n    script:\n    \"\"\"\n    picard SortSam \\\n        I=${bam_mrkdupl} \\\n        O=${sample_name}.sorted_mrkdup.bam \\\n        SO=coordinate\n\n    picard BuildBamIndex \\\n        I=${sample_name}.sorted_mrkdup.bam\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/sort_bams_by_coord"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess calculate_BQSR  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", pattern: \"*.table\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam), file(bam_index) from ch_mapped_reads_mrkdup_sorted\n    file(fasta) from ch_refgenome\n    file(fasta_fai) from ch_refgenome_index\n    file(fasta_dict) from ch_refgenome_dict\n    file(known_sites_1) from ch_known_sites_1\n    file(known_sites_1_index) from ch_known_sites_1_index\n    each file(known_sites_2) from ch_known_sites_2\n    each file(known_sites_2_index) from ch_known_sites_2_index\n    each file(known_sites_3) from ch_known_sites_3\n    each file(known_sites_3_index) from ch_known_sites_3_index\n    each file(target_regions) from ch_target_regions\n\n    output:\n    set val(sample_name), file(bam), file(bam_index), file(\"${sample_name}.sorted_mrkdup_bqsr.table\") into ch_mapped_reads_with_BQSR\n\n    script:\n    optional_known_sites_2_arg = params.known_sites_2 ? \"--known-sites ${known_sites_2}\" : \"\"\n    optional_known_sites_3_arg = params.known_sites_3 ? \"--known-sites ${known_sites_3}\" : \"\"\n    optional_target_regions_file      = params.target_regions ? \"-L ${target_regions}\" : \"\"\n    \"\"\"\n    gatk BaseRecalibrator \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -O ${sample_name}.sorted_mrkdup_bqsr.table \\\n        --known-sites ${known_sites_1} \\\n        ${optional_known_sites_2_arg} \\\n        ${optional_known_sites_3_arg} \\\n        --preserve-qscores-less-than ${params.bqsr_preserve_qscores_less_than} \\\n        --disable-bam-index-caching \\\n        ${optional_target_regions_file}\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/calculate_BQSR"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard", "GATK"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess apply_BQSR  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/align/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam), file(bam_index), file(bqsr_table) from ch_mapped_reads_with_BQSR\n    file(fasta) from ch_refgenome\n    file(fasta_fai) from ch_refgenome_index\n    file(fasta_dict) from ch_refgenome_dict\n\n    output:\n    set val(sample_name), file(\"${sample_name}.sorted_mrkdup_bqsr.bam\"), file(\"${sample_name}.sorted_mrkdup_bqsr.bai\") into ch_recalibrated_mapped_reads\n\n    script:\n    arg_static_quantized_quals = \"\"\n    if (params.bqsr_static_quantized_quals) {\n      static_quantized_quals_array = params.bqsr_static_quantized_quals.toString().split(',')\n      for(qual in static_quantized_quals_array) {\n         arg_static_quantized_quals += \"--static-quantized-quals ${qual} \"\n      }\n    }\n    \"\"\"\n    gatk ApplyBQSR \\\n        -R ${fasta} \\\n        -I ${bam} \\\n        -O ${sample_name}.sorted_mrkdup_bqsr.bam \\\n        --bqsr-recal-file ${bqsr_table} \\\n        --preserve-qscores-less-than ${params.bqsr_preserve_qscores_less_than} \\\n        ${arg_static_quantized_quals}\n\n    picard BuildBamIndex \\\n        I=${sample_name}.sorted_mrkdup_bqsr.bam\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/apply_BQSR"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["SAMtools"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess qc_samtools_flagstat  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/post_align_qc/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam), file(bai) from ch_recalibrated_mapped_reads_for_samtools_flagstat\n\n    output:\n    set val(sample_name), file(\"${sample_name}.sorted_mrkdup_bqsr_flagstat.txt\") into ch_samtools_flagstat\n\n    script:\n    \"\"\"\n    samtools flagstat ${bam} > \"${sample_name}.sorted_mrkdup_bqsr_flagstat.txt\"\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/qc_samtools_flagstat"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess qc_insert_size  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/post_align_qc/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam), file(bai) from ch_recalibrated_mapped_reads_for_insert_size_qc\n\n    output:\n    set val(sample_name), file(\"${sample_name}.sorted_mrkdup_bqsr_insert_size_metrics.txt\") into ch_insert_size_qc\n    file(\"${sample_name}.sorted_mrkdup_bqsr_insert_size_metrics.pdf\") into ch_insert_size_qc_pdf\n    script:\n    \"\"\"\n    picard CollectInsertSizeMetrics \\\n        I=${bam} \\\n        O=${sample_name}.sorted_mrkdup_bqsr_insert_size_metrics.txt \\\n        H=${sample_name}.sorted_mrkdup_bqsr_insert_size_metrics.pdf\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/qc_insert_size"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess qc_alignment_summary  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/post_align_qc/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam), file(bai) from ch_recalibrated_mapped_reads_for_alignment_summary\n    file(fasta) from ch_refgenome\n    file(fasta_fai) from ch_refgenome_index\n    file(fasta_dict) from ch_refgenome_dict\n\n    output:\n    set val(sample_name), file(\"${sample_name}.sorted_mrkdup_bqsr_alignment_metrics.txt\") into ch_alignment_summary_qc\n\n    script:\n    \"\"\"\n    picard CollectAlignmentSummaryMetrics \\\n        I=${bam} \\\n        O=${sample_name}.sorted_mrkdup_bqsr_alignment_metrics.txt \\\n        R=${fasta}\n    \"\"\"\n}"], "list_proc": ["zenomeplatform/nf-germline-mapping/qc_alignment_summary"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 2, "tools": ["Picard", "MultiQC"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess qc_sequencing_artifact  {\n    tag \"$sample_name\"\n    label 'low_memory'\n    publishDir \"${params.outdir}/${sample_name}/post_align_qc/\", mode: 'copy'\n\n    input:\n    set val(sample_name), file(bam), file(bai) from ch_recalibrated_mapped_reads_for_sequencing_artifact\n    file(fasta) from ch_refgenome\n    file(fasta_fai) from ch_refgenome_index\n    file(fasta_dict) from ch_refgenome_dict\n\n    output:\n    set val(sample_name), file(\"${sample_name}.sorted_mrkdup_bqsr_artifact_metrics.txt.*\") into ch_sequencing_artifact_qc\n\n    script:\n    \"\"\"\n    picard CollectSequencingArtifactMetrics \\\n        I=${bam} \\\n        O=${sample_name}.sorted_mrkdup_bqsr_artifact_metrics.txt \\\n        R=${fasta}\n    \"\"\"\n}", " process multiqc_postalignment_report_by_sample {\n     tag \"$sample_name\"\n     label 'low_memory'\n     publishDir \"${params.outdir}/multiqc_postalignment_report/${sample_name}/\", mode: 'copy'\n\n     input:\n     set val(sample_name), file(\"*\") from ch_postalignment_multiqc_files_by_sample\n\n     output:\n     file(\"multiqc_report_${sample_name}.html\")\n\n     script:\n     \"\"\"\n     multiqc . --filename \"multiqc_report_${sample_name}.html\"\n     \"\"\"\n   }"], "list_proc": ["zenomeplatform/nf-germline-mapping/qc_sequencing_artifact", "zenomeplatform/nf-germline-mapping/multiqc_postalignment_report_by_sample"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": [" process qc_collect_hs_metrics {\n      tag \"$sample_name\"\n      label 'low_memory'\n      publishDir \"${params.outdir}/${sample_name}/post_align_qc/\", mode: 'copy'\n\n      input:\n      set val(sample_name), file(bam), file(bai) from ch_recalibrated_mapped_reads_for_collect_hs_metrics\n      file(fasta) from ch_refgenome\n      file(fasta_fai) from ch_refgenome_index\n      file(fasta_dict) from ch_refgenome_dict\n      each file(target_regions) from ch_target_regions\n      each file(\"bait_regions_file\") from ch_bait_regions\n\n      output:\n      set val(sample_name), file(\"${sample_name}.sorted_mrkdup_bqsr_hs_metrics.txt\") into ch_collect_hs_metrics_qc\n\n      script:\n      \"\"\"\n      picard CollectHsMetrics \\\n          I=${bam} \\\n          O=${sample_name}.sorted_mrkdup_bqsr_hs_metrics.txt \\\n          R=${fasta} \\\n          BI=\"bait_regions_file\" \\\n          TI=${target_regions}\n      \"\"\"\n  }"], "list_proc": ["zenomeplatform/nf-germline-mapping/qc_collect_hs_metrics"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": ["\nprocess fastqc_mapped {\n      tag \"$sample_name\"\n      label 'low_memory'\n      publishDir \"${params.outdir}/${sample_name}/post_align_qc/\", mode: 'copy'\n\n      input:\n      set val(sample_name), file(bam), file(bai) from ch_recalibrated_mapped_reads_for_fastqc_mapped\n\n      output:\n      set val(sample_name), file(\"${bam.baseName}_fastqc.*\") into ch_fastqc_mapped\n\n      script:\n      \"\"\"\n      fastqc -t ${task.threads} ${bam}\n      \"\"\"\n  }"], "list_proc": ["zenomeplatform/nf-germline-mapping/fastqc_mapped"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["MultiQC"], "nb_own": 1, "list_own": ["zenomeplatform"], "nb_wf": 1, "list_wf": ["nf-germline-mapping"], "list_contrib": ["dembra96", "alex-aug"], "nb_contrib": 2, "codes": [" process multiqc_postalignment_report_all {\n      label 'low_memory'\n      publishDir \"${params.outdir}/multiqc_postalignment_report/\", mode: 'copy'\n\n      input:\n      file(\"*\") from ch_postalignment_multiqc_files_all.flatten().collect()\n\n      output:\n      file(\"multiqc_report.html\")\n\n      script:\n      \"\"\"\n      multiqc .\n      \"\"\"\n    }"], "list_proc": ["zenomeplatform/nf-germline-mapping/multiqc_postalignment_report_all"], "list_wf_names": ["zenomeplatform/nf-germline-mapping"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["zhangtong516"], "nb_wf": 1, "list_wf": ["icpipe-nextflow"], "list_contrib": ["zhangtong516"], "nb_contrib": 1, "codes": ["\nprocess trim_adaptor {\n    tag \"${name}\"\n    publishDir \"${params.outDir}\" + \"/02_trimmedFastq/\", mode: 'copy'\n    input:\n    set val(name), file(collapsed_reads_fastq_gz) from collapsed_reads\n\n    output:\n    set val(name), file(\"*.collapsed.trimmed.fastq.gz\") into trimmed_reads\n    set val(name), file(\"*.trimmed.read_counts.txt\") into trimmed_read_count\n    file(\"*.read.trim.log.sum\")\n    file(\"*.trimAdaptor.log\")\n\n    script:\n    \"\"\"\n    cutadapt -j ${task.cpus} -n 2 -a ${params.adp1} -g ${params.adp2} -m ${params.minLength} \\\n        ${collapsed_reads_fastq_gz}  -o ${name}.collapsed.trimmed.fastq.gz  > ${name}.trimAdaptor.log\n\n    perl ${baseDir}/scripts/parse_trim_log.pl ${name}.trimAdaptor.log  > ${name}.read.trim.log.sum\n\n    zcat ${name}.collapsed.trimmed.fastq.gz | wc -l | awk '{print \"trimmed_reads\\t\"\\$1/4}' > ${name}.trimmed.read_counts.txt\n    \"\"\"\n}"], "list_proc": ["zhangtong516/icpipe-nextflow/trim_adaptor"], "list_wf_names": ["zhangtong516/icpipe-nextflow"]}, {"nb_reuse": 1, "tools": ["SAMtools", "STAR"], "nb_own": 1, "list_own": ["zhangtong516"], "nb_wf": 1, "list_wf": ["icpipe-nextflow"], "list_contrib": ["zhangtong516"], "nb_contrib": 1, "codes": ["\nprocess star_mapping_genome {\n    tag \"${name}\"\n    publishDir \"${params.outDir}\" + \"/05_mappedResult/\", mode: 'copy'\n\n    input:\n    set val(name), file(rm_smallRNA_fastq) from removed_smallRNA_fq\n\n    output:\n    set val(name), file(\"*.map_genome.sorted.bam\") into mapped_bam_for_fpkm\n    set val(name), file(\"*.map_genome.sorted.bam\") into mapped_bam_for_sam2tab\n    set val(name), file(\"*.map_genome.sorted.bam\") into mapped_bam_for_coverage\n    set val(name), file(\"*.map_genome.unsorted.bam\") into unsorted_bam\n    set val(name), file(\"*.map_genome.Log.progress.out\") into log_progress\n    set val(name), file(\"*.map_genome.Log.final.out\") into log_final\n    set val(name), file(\"*.map_genome.Log.out\") into log_out\n    set val(name), file(\"*.map_genome.Log.std.out\") into log_std\n\n\n    script:\n    \"\"\"\n    STAR --readFilesIn ${rm_smallRNA_fastq} \\\n        --outFileNamePrefix ${name}.map_genome. \\\n        --genomeDir ${params.genome_dir}/star \\\n        --runThreadN ${task.cpus} \\\n        --genomeLoad NoSharedMemory \\\n        --runMode alignReads \\\n        --outSAMtype BAM Unsorted \\\n        --outSAMmultNmax 1 \\\n        --outFilterMultimapNmax 1 \\\n        --outFilterMismatchNmax 2 \\\n        --outFilterIntronMotifs RemoveNoncanonicalUnannotated \\\n        --outSAMstrandField intronMotif \\\n        --outSJfilterOverhangMin 30 12 12 12 \\\n        --alignEndsType EndToEnd \\\n        --outSAMattributes All \\\n        --outSAMunmapped Within \\\n        --alignIntronMin 20 \\\n        --alignIntronMax 1000000 \\\n        --alignMatesGapMax 1000000 \\\n        --alignSJDBoverhangMin 1 \\\n        --outStd BAM_Unsorted > ${name}.map_genome.unsorted.bam\n\n    samtools view -h ${name}.map_genome.unsorted.bam |\\\n        awk '\\$0~/^@/{print \\$0}\\$0!~/^@/{for(i=12;i<NF;i++){if(substr(\\$i,1,4)==\"MD:Z\"){if(and(16,\\$2)==0){ if(\\$i!~/^MD:Z:0/ ) print \\$0; }else{if(\\$i!~/^MD:Z:.*0\\$/) print \\$0; }}}}' |\\\n        samtools view --threads ${task.cpus} -bh - |\\\n        samtools sort - -m 2G --threads ${task.cpus} \\\n            -o ${name}.map_genome.sorted.bam\n    \"\"\"\n}"], "list_proc": ["zhangtong516/icpipe-nextflow/star_mapping_genome"], "list_wf_names": ["zhangtong516/icpipe-nextflow"]}, {"nb_reuse": 1, "tools": ["FeatureCounts"], "nb_own": 1, "list_own": ["zhangtong516"], "nb_wf": 1, "list_wf": ["icpipe-nextflow"], "list_contrib": ["zhangtong516"], "nb_contrib": 1, "codes": ["\nprocess estimate_rpkm{\n    tag \"${name}\"\n    publishDir \"${params.outDir}\" + \"/06_fpkm/\", mode: 'copy'\n\n    input:\n    set val(name), file(mapped_bam) from mapped_bam_for_fpkm\n\n    output:\n    set val(name), file(\"*.gene.txt\") into gene_rpkm\n    set val(name), file(\"*.txn.txt\") into txn_rpkm\n    set val(name), file(\"*.gene.txt.summary\") into gene_rpkm_summary\n    set val(name), file(\"*.txn.txt.summary\") into txn_rpkm_summary\n    set val(name), file(\"*.isoforms.fpkm_tracking\") into isoform_fpkm\n    set val(name), file(\"*.isoforms.fpkm_tracking\") into isoform_fpkm2\n    set val(name), file(\"*.genes.fpkm_tracking\") into gene_fpkm\n    set val(name), file(\"*.skipped.gtf\") into skipped_gtf\n    set val(name), file(\"*.transcripts.gtf\") into transcripts_gtf\n\n    script:\n    \"\"\"\n    icSHAPE-pipe calcFPKM -i ${mapped_bam} -o ${name} \\\n        -G ${params.genome_dir}/Homo_sapiens.GRCh38.100.gtf -p ${task.cpus}\n\n    mv ./${name}/isoforms.fpkm_tracking ${name}.isoforms.fpkm_tracking\n    mv ./${name}/genes.fpkm_tracking ${name}.genes.fpkm_tracking\n    mv ./${name}/skipped.gtf ${name}.skipped.gtf\n    mv ./${name}/transcripts.gtf ${name}.transcripts.gtf\n\n    featureCounts -T ${task.cpus} \\\n        -a ${params.genome_dir}/Homo_sapiens.GRCh38.100.gtf \\\n        -g transcript_id \\\n        -o ${name}.txn.txt \\\n        ${mapped_bam}\n\n    featureCounts -T ${task.cpus} \\\n        -a ${params.genome_dir}/Homo_sapiens.GRCh38.100.gtf \\\n        -g gene_id \\\n        -o ${name}.gene.txt \\\n        ${mapped_bam}\n    \"\"\"\n}"], "list_proc": ["zhangtong516/icpipe-nextflow/estimate_rpkm"], "list_wf_names": ["zhangtong516/icpipe-nextflow"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess genotype {\n\n\tlabel 'himem'\n\tconda CONDA\n\terrorStrategy 'terminate'\n\tpublishDir OUT, mode: 'move'\t  \n        executor = 'slurm'\n\n        input:\n        val(GVCF) from SampleData\n\n        output:\n        file({ \"${CALLD}\" }) into result\n\n        script:\n\tCHR = \"${GVCF.CHR}\"\n\tCALLD = \"${GVCF.CHR}.vcf\"\n\tDB = \"../../../../${GVCF.CHR}_db\"\n\n\t\"\"\"\n\tgatk GenotypeGVCFs -V gendb://$DB -R ${REF} -L ${CHR} -O ${CALLD}\n        \"\"\"\n}"], "list_proc": ["zlye/RVE/genotype"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["BWA"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess bwa {\n\tlabel 'bwa'\n\tconda CONDA\n\terrorStrategy 'terminate'\n\n        input:\n\tval(RAW) from SampleData\n\n        output:\n        set val(ID), val(SID), file({ \"${MAP}\" }) into aligned\n\n  script:\n\t      SID = \"${RAW.SID}\"\n\t      ID = \"${RAW.RID}\"\n        RG = \"\\'@RG\\\\tID:${RAW.P1}\\\\tSM:${RAW.SID}\\\\tPL:ILLUMINA\\'\"\n        MAP = \"${RAW.RID}.sam\"\n        P1 = file(\"${RAW.P1}\")\n        P2 = file(\"${RAW.P2}\")\n\n        \"\"\"\n        bwa mem -t 6 -M -R ${RG} ${REF} ${P1} ${P2} > ${MAP}\n\t\"\"\"\n}"], "list_proc": ["zlye/RVE/bwa"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess sort {\n\tlabel 'short_run'\n\tconda CONDA\n\n        input:\n        set val(ID), val(SID), file(SAM) from aligned\n\n        output:\n        set val(ID), val(SID), file({ \"${SBAM}\" }) into sorted\n\n        script:\n\n        SBAM = \"${ID}.sorted.bam\"\n        del_sam = SAM.getName()\n         \n\t\"\"\"\n  picard SortSam INPUT=${SAM} OUTPUT=${SBAM} SORT_ORDER=coordinate\n\t\"\"\"\n}"], "list_proc": ["zlye/RVE/sort"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess rmdup {\n\tlabel 'long_run'\n\tconda '/home/znl207/mysoftware/miniconda3'\n\n        input:\n         set val(SID), file(SBAM) from merg_sid\n\n        output:\n         set val(SID), file({ \"${RMDUP}\" }) into rmdup\n\n        script:\n\n\t ALL_IN = SBAM.collect { \"INPUT=$it\" }.join(' ')\n         RMDUP = \"${SID}.dedup.bam\"\n         RMET = \"${SID}.dedup.met\"\n\n       \t \"\"\"\n         picard MarkDuplicates ${ALL_IN} OUTPUT=${RMDUP} METRICS_FILE=${RMET}\n         \"\"\"\n}"], "list_proc": ["zlye/RVE/rmdup"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess checkbam {\n\tlabel 'short_run'\n\tconda CONDA\n\n\tinput:\n\tset val(SID), file(RMDUP) from rmdup\n\n\toutput:\n\tset val(SID), file(RMDUP) into bchecked\n\n\tscript:\n\t\"\"\"\n\tpicard ValidateSamFile INPUT=${RMDUP} MODE=SUMMARY\n\t\"\"\"\n}"], "list_proc": ["zlye/RVE/checkbam"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess index {\n\tlabel 'short_run'\n\tconda CONDA\n\tpublishDir params.bam_outdir, mode: 'copy'\n\n        input:\n        set val(SID), file(RMDUP) from bchecked\n\n        output:\n        set val(SID), file({ \"${RMDUP}\" }), file({ \"${BAI}\" }) into index\n\n        script:\n\n        BAI = \"${SID}.dedup.bai\"\n        \n      \t\"\"\"\n        picard BuildBamIndex INPUT=${RMDUP} OUTPUT=${BAI}\n        \"\"\"\n}"], "list_proc": ["zlye/RVE/index"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess haplocall {\n\tlabel 'medmem'\n\tconda CONDA\n\terrorStrategy = 'retry'\n\tmaxRetries = 3\n\n \tinput:\n \tset val(SID), file(RMDUP), file(BAI) from index\n \n \toutput:\n \tfile({ \"${CALL}\" }) into (called)\n\tfile({ \"${VCI}\" }) into (vcfindex)\n \n \tscript:\n \t\n \tCALL = \"${SID}.g.vcf\"\n\tVCI = \"${SID}.g.vcf.idx\"\n \t\"\"\"\n \tgatk HaplotypeCaller -ERC GVCF -R ${REF} -I ${RMDUP} -O ${CALL}\t\n \t\"\"\"\n}"], "list_proc": ["zlye/RVE/haplocall"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess checkvcf {\n\tlabel 'long_run'\n\tconda '/home/znl207/mysoftware/miniconda3'\n\n\tinput:\n\tfile(CALL) from called\n\t\n\toutput:\n\tfile(CALL) into vchecked\n\n\tscript:\n\n\t\"\"\"\n\tgatk ValidateVariants --java-options \"-Xmx40G\" \\\n\t--validation-type-to-exclude ALLELES -R ${REF} -V ${CALL}\n\t\"\"\"\n}"], "list_proc": ["zlye/RVE/checkvcf"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["GATK"], "nb_own": 1, "list_own": ["zlye"], "nb_wf": 1, "list_wf": ["RVE"], "list_contrib": ["zlye"], "nb_contrib": 1, "codes": ["\nprocess combine {\n \tlabel 'himem'\n\tconda CONDA\n\n\tinput:           \n\tval(CHROM) from SampleData2\n\tfile(CALL) from listed\n\tfile(VCI) from vcflist\n\n\toutput:\n\tfile( \"${DB}\" ) \n\n\tscript:\n\tCHR = \"${CHROM.CHR}\"\n\tCOMB_IN = CALL.collect { \"--variant $it\" }.join(' ')\n\tDB = \"../../../../${CHROM.CHR}_db\"\n\n\t\"\"\"\n\tgatk GenomicsDBImport -R ${REF} ${COMB_IN} --genomicsdb-workspace-path $DB -L ${CHR}\n\t\"\"\" \n}"], "list_proc": ["zlye/RVE/combine"], "list_wf_names": ["zlye/RVE"]}, {"nb_reuse": 1, "tools": ["MEGAHIT"], "nb_own": 1, "list_own": ["znestor"], "nb_wf": 1, "list_wf": ["chainsaw-test"], "list_contrib": ["znestor"], "nb_contrib": 1, "codes": ["\nprocess megahit {\n  container \"bioboxes/megahit\"\n\n  input:\n    path reads from Channel.fromPath(params.input)\n\n  output:\n    path \"megahit_out/final.contigs.fa\" into contigCh\n\n  \"\"\"\n    megahit --12 $reads\n  \"\"\"\n}"], "list_proc": ["znestor/chainsaw-test/megahit"], "list_wf_names": ["znestor/chainsaw-test"]}, {"nb_reuse": 1, "tools": ["FastQC"], "nb_own": 1, "list_own": ["zsteve"], "nb_wf": 1, "list_wf": ["nf-ATAC"], "list_contrib": ["zsteve"], "nb_contrib": 1, "codes": ["\nprocess sampleFastQC {\n             \n  publishDir {sampleInfo[\"baseDirOut\"]}\n  input:\n    set val(sampleInfo), file(reads) from readPairOut_FQC\n  output:\n    file '*'\n    file 'fastqc_output' into fastQCReport\n  script:\n    \"\"\"mkdir -p fastqc_output;\n    fastqc -t $params.numCpus -o fastqc_output $reads \\\n      > fastqc.stdout 2> fastqc.stderr\n    \"\"\"\n}"], "list_proc": ["zsteve/nf-ATAC/sampleFastQC"], "list_wf_names": ["zsteve/nf-ATAC"]}, {"nb_reuse": 1, "tools": ["Cutadapt"], "nb_own": 1, "list_own": ["zsteve"], "nb_wf": 1, "list_wf": ["nf-ATAC"], "list_contrib": ["zsteve"], "nb_contrib": 1, "codes": ["\nprocess sampleCutadapt {\n              \n  publishDir {sampleInfo[\"baseDirOut\"]}\n  input:\n    set val(sampleInfo), file(reads) from readPairOut\n  output:\n    file 'cutadapt_output/*'\n    set val(sampleInfo), file('cutadapt_output/*_trimmed*') into trimmedPairOut\n                                     \n  script:\n    read1_trimmed_name = file(reads[0]).getName().toString().replaceAll(/_R1/, '_R1_trimmed')\n    read2_trimmed_name = file(reads[1]).getName().toString().replaceAll(/_R2/, '_R2_trimmed')\n    \"\"\"\n    mkdir -p cutadapt_output; \\\n    cutadapt \\\n    -a ${def_cmd_params[\"cutadapt\"][\"-a\"]} -A ${def_cmd_params[\"cutadapt\"][\"-A\"]}\\\n    --info-file=cutadapt_output/cutadapt_info_file\\\n    -q ${def_cmd_params[\"cutadapt\"][\"-q\"]}\\\n    --minimum-length ${def_cmd_params[\"cutadapt\"][\"--minimum-length\"]}\\\n    -o cutadapt_output/${read1_trimmed_name}\\\n    -p cutadapt_output/${read2_trimmed_name}\\\n    $reads > cutadapt_output/cutadapt.stdout 2> cutadapt_output/cutadapt.stderr\n    \"\"\"\n}"], "list_proc": ["zsteve/nf-ATAC/sampleCutadapt"], "list_wf_names": ["zsteve/nf-ATAC"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zsteve"], "nb_wf": 1, "list_wf": ["nf-ATAC"], "list_contrib": ["zsteve"], "nb_contrib": 1, "codes": ["\nprocess sampleSamToBam {\n             \n  publishDir {sampleInfo[\"baseDirOut\"]}\n  echo true\n  input:\n    set val(sampleInfo), file(samFile) from mappedSamOut\n  output:\n    file 'samtobam_output/*'\n\t\tfile 'samtobam_output/*.bam.bai'\n    set val(sampleInfo), file('samtobam_output/*.bam') into mappedBamOut_getStat,mappedBamOut\n    set val(sampleInfo), file('samtobam_output/*.bam'), file('samtobam_output/*.bai') into mappedBamOut_QC\n  script:\n    output_name = sampleInfo[\"ID\"]\n\t\t                                                                                                                      \n    \"\"\"\n      mkdir -p samtobam_output;\n      picard SortSam SO=coordinate INPUT=$samFile OUTPUT=samtobam_output/${output_name}.bam CREATE_INDEX=true\\\n      > samtobam_output/picard_sortsam.stdout 2> samtobam_output/picard_sortsam.stderr\n    \tmv samtobam_output/*.bai samtobam_output/${output_name}.bam.bai\n\t\t\"\"\"\n}"], "list_proc": ["zsteve/nf-ATAC/sampleSamToBam"], "list_wf_names": ["zsteve/nf-ATAC"]}, {"nb_reuse": 1, "tools": ["SAMtools", "QualiMap"], "nb_own": 1, "list_own": ["zsteve"], "nb_wf": 1, "list_wf": ["nf-ATAC"], "list_contrib": ["zsteve"], "nb_contrib": 1, "codes": ["\nprocess sampleGetMapStats {\n             \n                                                                \n  publishDir {sampleInfo[\"baseDirOut\"]}\n  input:\n    set val(sampleInfo), file(bamFile) from mappedBamOut_getStat\n  output:\n    file 'flagstat_output/*'\n\t\tfile 'qualimap_output*/*'\n    file 'flagstat_output/*_flagstat.txt' into flagStatOut\n  script:\n    output_flagstat_name = sampleInfo[\"ID\"] + '_flagstat.txt'\n    \"\"\"\n      mkdir -p flagstat_output;\n      samtools flagstat $bamFile > flagstat_output/$output_flagstat_name\\\n      2> samtools_flagstat.stderr\n\t\t\t\n\t\t\tmkdir -p qualimap_output_${sampleInfo[\"ID\"]};\n\t\t\tqualimap bamqc -bam $bamFile -outdir qualimap_output_${sampleInfo[\"ID\"]}\n\t\t\"\"\"\t\n}"], "list_proc": ["zsteve/nf-ATAC/sampleGetMapStats"], "list_wf_names": ["zsteve/nf-ATAC"]}, {"nb_reuse": 1, "tools": ["Picard"], "nb_own": 1, "list_own": ["zsteve"], "nb_wf": 1, "list_wf": ["nf-ATAC"], "list_contrib": ["zsteve"], "nb_contrib": 1, "codes": ["\nprocess sampleFilterDedup {\n                            \n  publishDir {sampleInfo[\"baseDirOut\"]}\n  input:\n    set val(sampleInfo), file(bamFile) from filteredMMMROut\n  output:\n    file 'filtering_output/*'\n    set val(sampleInfo), file('filtering_output/*_dedup.bam') into filteredDedupOut_makeTags, \\\n                          filteredDedupOut_callPeaks_MACS\n  script:\n    output_bam_name = sampleInfo[\"ID\"] + '_dedup.bam'\n    output_metrics_name = sampleInfo[\"ID\"] + '_dedup_metrics'\n    \"\"\"\n      mkdir -p filtering_output;\n      picard MarkDuplicates\\\n      INPUT=$bamFile\\\n      OUTPUT=filtering_output/$output_bam_name\\\n      METRICS_FILE=filtering_output/$output_metrics_name\\\n      REMOVE_DUPLICATES=${def_cmd_params[\"markduplicates\"][\"REMOVE_DUPLICATES\"]}\\\n      ASSUME_SORTED=${def_cmd_params[\"markduplicates\"][\"ASSUME_SORTED\"]}\\\n      VALIDATION_STRINGENCY=${def_cmd_params[\"markduplicates\"][\"VALIDATION_STRINGENCY\"]}\\\n      CREATE_INDEX=${def_cmd_params[\"markduplicates\"][\"CREATE_INDEX\"]}\\\n      > filtering_output/picard_markduplicates.stdout 2> filtering_output/picard_markduplicates.stderr\n    \"\"\"\n}"], "list_proc": ["zsteve/nf-ATAC/sampleFilterDedup"], "list_wf_names": ["zsteve/nf-ATAC"]}]